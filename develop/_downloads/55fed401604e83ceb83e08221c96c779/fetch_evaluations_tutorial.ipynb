{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Fetching Evaluations\n\nEvaluations contain a concise summary of the results of all runs made. Each evaluation\nprovides information on the dataset used, the flow applied, the setup used, the metric\nevaluated, and the result obtained on the metric, for each such run made. These collection\nof results can be used for efficient benchmarking of an algorithm and also allow transparent\nreuse of results from previous experiments on similar parameters.\n\nIn this example, we shall do the following:\n\n* Retrieve evaluations based on different metrics\n* Fetch evaluations pertaining to a specific task\n* Sort the obtained results in descending order of the metric\n* Plot a cumulative distribution function for the evaluations\n* Compare the top 10 performing flows based on the evaluation performance\n* Retrieve evaluations with hyperparameter settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD 3-Clause\n\nimport openml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Listing evaluations\nEvaluations can be retrieved from the database in the chosen output format.\nRequired filters can be applied to retrieve results from runs as required.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We shall retrieve a small set (only 10 entries) to test the listing function for evaluations\nopenml.evaluations.list_evaluations(\n    function=\"predictive_accuracy\", size=10, output_format=\"dataframe\"\n)\n\n# Using other evaluation metrics, 'precision' in this case\nevals = openml.evaluations.list_evaluations(\n    function=\"precision\", size=10, output_format=\"dataframe\"\n)\n\n# Querying the returned results for precision above 0.98\nprint(evals[evals.value > 0.98])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Viewing a sample task\nOver here we shall briefly take a look at the details of the task.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We will start by displaying a simple *supervised classification* task:\ntask_id = 167140  # https://www.openml.org/t/167140\ntask = openml.tasks.get_task(task_id)\nprint(task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtaining all the evaluations for the task\nWe'll now obtain all the evaluations that were uploaded for the task\nwe displayed previously.\nNote that we now filter the evaluations based on another parameter 'task'.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metric = \"predictive_accuracy\"\nevals = openml.evaluations.list_evaluations(\n    function=metric, tasks=[task_id], output_format=\"dataframe\"\n)\n# Displaying the first 10 rows\nprint(evals.head(n=10))\n# Sorting the evaluations in decreasing order of the metric chosen\nevals = evals.sort_values(by=\"value\", ascending=False)\nprint(\"\\nDisplaying head of sorted dataframe: \")\nprint(evals.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obtaining CDF of metric for chosen task\nWe shall now analyse how the performance of various flows have been on this task,\nby seeing the likelihood of the accuracy obtained across all runs.\nWe shall now plot a cumulative distributive function (CDF) for the accuracies obtained.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n\n\ndef plot_cdf(values, metric=\"predictive_accuracy\"):\n    max_val = max(values)\n    n, bins, patches = plt.hist(values, density=True, histtype=\"step\", cumulative=True, linewidth=3)\n    patches[0].set_xy(patches[0].get_xy()[:-1])\n    plt.xlim(max(0, min(values) - 0.1), 1)\n    plt.title(\"CDF\")\n    plt.xlabel(metric)\n    plt.ylabel(\"Likelihood\")\n    plt.grid(visible=True, which=\"major\", linestyle=\"-\")\n    plt.minorticks_on()\n    plt.grid(visible=True, which=\"minor\", linestyle=\"--\")\n    plt.axvline(max_val, linestyle=\"--\", color=\"gray\")\n    plt.text(max_val, 0, \"%.3f\" % max_val, fontsize=9)\n    plt.show()\n\n\nplot_cdf(evals.value, metric)\n# This CDF plot shows that for the given task, based on the results of the\n# runs uploaded, it is almost certain to achieve an accuracy above 52%, i.e.,\n# with non-zero probability. While the maximum accuracy seen till now is 96.5%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing top 10 performing flows\nLet us now try to see which flows generally performed the best for this task.\nFor this, we shall compare the top performing flows.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\n\n\ndef plot_flow_compare(evaluations, top_n=10, metric=\"predictive_accuracy\"):\n    # Collecting the top 10 performing unique flow_id\n    flow_ids = evaluations.flow_id.unique()[:top_n]\n\n    df = pd.DataFrame()\n    # Creating a data frame containing only the metric values of the selected flows\n    #   assuming evaluations is sorted in decreasing order of metric\n    for i in range(len(flow_ids)):\n        flow_values = evaluations[evaluations.flow_id == flow_ids[i]].value\n        df = pd.concat([df, flow_values], ignore_index=True, axis=1)\n    fig, axs = plt.subplots()\n    df.boxplot()\n    axs.set_title(\"Boxplot comparing \" + metric + \" for different flows\")\n    axs.set_ylabel(metric)\n    axs.set_xlabel(\"Flow ID\")\n    axs.set_xticklabels(flow_ids)\n    axs.grid(which=\"major\", linestyle=\"-\", linewidth=\"0.5\", color=\"gray\", axis=\"y\")\n    axs.minorticks_on()\n    axs.grid(which=\"minor\", linestyle=\"--\", linewidth=\"0.5\", color=\"gray\", axis=\"y\")\n    # Counting the number of entries for each flow in the data frame\n    #   which gives the number of runs for each flow\n    flow_freq = list(df.count(axis=0, numeric_only=True))\n    for i in range(len(flow_ids)):\n        axs.text(i + 1.05, np.nanmin(df.values), str(flow_freq[i]) + \"\\nrun(s)\", fontsize=7)\n    plt.show()\n\n\nplot_flow_compare(evals, metric=metric, top_n=10)\n# The boxplots below show how the flows perform across multiple runs on the chosen\n# task. The green horizontal lines represent the median accuracy of all the runs for\n# that flow (number of runs denoted at the bottom of the boxplots). The higher the\n# green line, the better the flow is for the task at hand. The ordering of the flows\n# are in the descending order of the higest accuracy value seen under that flow.\n\n# Printing the corresponding flow names for the top 10 performing flow IDs\ntop_n = 10\nflow_ids = evals.flow_id.unique()[:top_n]\nflow_names = evals.flow_name.unique()[:top_n]\nfor i in range(top_n):\n    print((flow_ids[i], flow_names[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Obtaining evaluations with hyperparameter settings\nWe'll now obtain the evaluations of a task and a flow with the hyperparameters\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# List evaluations in descending order based on predictive_accuracy with\n# hyperparameters\nevals_setups = openml.evaluations.list_evaluations_setups(\n    function=\"predictive_accuracy\", tasks=[31], size=100, sort_order=\"desc\"\n)\n\n\"\"\nprint(evals_setups.head())\n\n\"\"\n# Return evaluations for flow_id in descending order based on predictive_accuracy\n# with hyperparameters. parameters_in_separate_columns returns parameters in\n# separate columns\nevals_setups = openml.evaluations.list_evaluations_setups(\n    function=\"predictive_accuracy\", flows=[6767], size=100, parameters_in_separate_columns=True\n)\n\n\"\"\nprint(evals_setups.head(10))\n\n\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}