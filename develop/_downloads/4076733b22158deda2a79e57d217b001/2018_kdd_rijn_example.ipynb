{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# van Rijn and Hutter (2018)\n\nA tutorial on how to reproduce the paper *Hyperparameter Importance Across Datasets*.\n\n## Example Deprecation Warning!\n\nThis example is not supported anymore by the OpenML-Python developers. The example is kept for reference purposes but not tested anymore.\n\n## Publication\n\n| Hyperparameter importance across datasets\n| Jan N. van Rijn and Frank Hutter\n| In *Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*, 2018\n| Available at https://dl.acm.org/doi/10.1145/3219819.3220058\n\n## Requirements\n\nThis is a Unix-only tutorial, as the requirements can not be satisfied on a Windows machine (Untested on other\nsystems).\n\nThe following Python packages are required:\n\npip install openml[examples,docs] fanova ConfigSpace<1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD 3-Clause\n\nimport sys\n\nif sys.platform == \"win32\":  # noqa\n    print(\n        \"The pyrfr library (requirement of fanova) can currently not be installed on Windows systems\"\n    )\n    exit()\n\n# DEPRECATED EXAMPLE -- Avoid running this code in our CI/CD pipeline\nprint(\"This example is deprecated, remove the `if False` in this code to use it manually.\")\nif False:\n    import json\n    import fanova\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    import seaborn as sns\n\n    import openml\n\n\n    ##############################################################################\n    # With the advent of automated machine learning, automated hyperparameter\n    # optimization methods are by now routinely used in data mining. However, this\n    # progress is not yet matched by equal progress on automatic analyses that\n    # yield information beyond performance-optimizing hyperparameter settings.\n    # In this example, we aim to answer the following two questions: Given an\n    # algorithm, what are generally its most important hyperparameters?\n    #\n    # This work is carried out on the OpenML-100 benchmark suite, which can be\n    # obtained by ``openml.study.get_suite('OpenML100')``. In this example, we\n    # conduct the experiment on the Support Vector Machine (``flow_id=7707``)\n    # with specific kernel (we will perform a post-process filter operation for\n    # this). We should set some other experimental parameters (number of results\n    # per task, evaluation measure and the number of trees of the internal\n    # functional Anova) before the fun can begin.\n    #\n    # Note that we simplify the example in several ways:\n    #\n    # 1) We only consider numerical hyperparameters\n    # 2) We consider all hyperparameters that are numerical (in reality, some\n    #    hyperparameters might be inactive (e.g., ``degree``) or irrelevant\n    #    (e.g., ``random_state``)\n    # 3) We assume all hyperparameters to be on uniform scale\n    #\n    # Any difference in conclusion between the actual paper and the presented\n    # results is most likely due to one of these simplifications. For example,\n    # the hyperparameter C looks rather insignificant, whereas it is quite\n    # important when it is put on a log-scale. All these simplifications can be\n    # addressed by defining a ConfigSpace. For a more elaborated example that uses\n    # this, please see:\n    # https://github.com/janvanrijn/openml-pimp/blob/d0a14f3eb480f2a90008889f00041bdccc7b9265/examples/plot/plot_fanova_aggregates.py # noqa F401\n\n    suite = openml.study.get_suite(\"OpenML100\")\n    flow_id = 7707\n    parameter_filters = {\"sklearn.svm.classes.SVC(17)_kernel\": \"sigmoid\"}\n    evaluation_measure = \"predictive_accuracy\"\n    limit_per_task = 500\n    limit_nr_tasks = 15\n    n_trees = 16\n\n    fanova_results = []\n    # we will obtain all results from OpenML per task. Practice has shown that this places the bottleneck on the\n    # communication with OpenML, and for iterated experimenting it is better to cache the results in a local file.\n    for idx, task_id in enumerate(suite.tasks):\n        if limit_nr_tasks is not None and idx >= limit_nr_tasks:\n            continue\n        print(\n            \"Starting with task %d (%d/%d)\"\n            % (task_id, idx + 1, len(suite.tasks) if limit_nr_tasks is None else limit_nr_tasks)\n        )\n        # note that we explicitly only include tasks from the benchmark suite that was specified (as per the for-loop)\n        evals = openml.evaluations.list_evaluations_setups(\n            evaluation_measure,\n            flows=[flow_id],\n            tasks=[task_id],\n            size=limit_per_task,\n            output_format=\"dataframe\",\n        )\n\n        performance_column = \"value\"\n        # make a DataFrame consisting of all hyperparameters (which is a dict in setup['parameters']) and the performance\n        # value (in setup['value']). The following line looks a bit complicated, but combines 2 tasks: a) combine\n        # hyperparameters and performance data in a single dict, b) cast hyperparameter values to the appropriate format\n        # Note that the ``json.loads(...)`` requires the content to be in JSON format, which is only the case for\n        # scikit-learn setups (and even there some legacy setups might violate this requirement). It will work for the\n        # setups that belong to the flows embedded in this example though.\n        try:\n            setups_evals = pd.DataFrame(\n                [\n                    dict(\n                        **{name: json.loads(value) for name, value in setup[\"parameters\"].items()},\n                        **{performance_column: setup[performance_column]}\n                    )\n                    for _, setup in evals.iterrows()\n                ]\n            )\n        except json.decoder.JSONDecodeError as e:\n            print(\"Task %d error: %s\" % (task_id, e))\n            continue\n        # apply our filters, to have only the setups that comply to the hyperparameters we want\n        for filter_key, filter_value in parameter_filters.items():\n            setups_evals = setups_evals[setups_evals[filter_key] == filter_value]\n        # in this simplified example, we only display numerical and float hyperparameters. For categorical hyperparameters,\n        # the fanova library needs to be informed by using a configspace object.\n        setups_evals = setups_evals.select_dtypes(include=[\"int64\", \"float64\"])\n        # drop rows with unique values. These are by definition not an interesting hyperparameter, e.g., ``axis``,\n        # ``verbose``.\n        setups_evals = setups_evals[\n            [\n                c\n                for c in list(setups_evals)\n                if len(setups_evals[c].unique()) > 1 or c == performance_column\n            ]\n        ]\n        # We are done with processing ``setups_evals``. Note that we still might have some irrelevant hyperparameters, e.g.,\n        # ``random_state``. We have dropped some relevant hyperparameters, i.e., several categoricals. Let's check it out:\n\n        # determine x values to pass to fanova library\n        parameter_names = [\n            pname for pname in setups_evals.columns.to_numpy() if pname != performance_column\n        ]\n        evaluator = fanova.fanova.fANOVA(\n            X=setups_evals[parameter_names].to_numpy(),\n            Y=setups_evals[performance_column].to_numpy(),\n            n_trees=n_trees,\n        )\n        for idx, pname in enumerate(parameter_names):\n            try:\n                fanova_results.append(\n                    {\n                        \"hyperparameter\": pname.split(\".\")[-1],\n                        \"fanova\": evaluator.quantify_importance([idx])[(idx,)][\"individual importance\"],\n                    }\n                )\n            except RuntimeError as e:\n                # functional ANOVA sometimes crashes with a RuntimeError, e.g., on tasks where the performance is constant\n                # for all configurations (there is no variance). We will skip these tasks (like the authors did in the\n                # paper).\n                print(\"Task %d error: %s\" % (task_id, e))\n                continue\n\n    # transform ``fanova_results`` from a list of dicts into a DataFrame\n    fanova_results = pd.DataFrame(fanova_results)\n\n    ##############################################################################\n    # make the boxplot of the variance contribution. Obviously, we can also use\n    # this data to make the Nemenyi plot, but this relies on the rather complex\n    # ``Orange`` dependency (``pip install Orange3``). For the complete example,\n    # the reader is referred to the more elaborate script (referred to earlier)\n    fig, ax = plt.subplots()\n    sns.boxplot(x=\"hyperparameter\", y=\"fanova\", data=fanova_results, ax=ax)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_ylabel(\"Variance Contribution\")\n    ax.set_xlabel(None)\n    plt.tight_layout()\n    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}