{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Measuring runtimes for Scikit-learn models\n\nThe runtime of machine learning models on specific datasets can be a deciding\nfactor on the choice of algorithms, especially for benchmarking and comparison\npurposes. OpenML's scikit-learn extension provides runtime data from runs of\nmodel fit and prediction on tasks or datasets, for both the CPU-clock as well\nas the actual wallclock-time incurred. The objective of this example is to\nillustrate how to retrieve such timing measures, and also offer some potential\nmeans of usage and interpretation of the same.\n\nIt should be noted that there are multiple levels at which parallelism can occur.\n\n* At the outermost level, OpenML tasks contain fixed data splits, on which the\n  defined model/flow is executed. Thus, a model can be fit on each OpenML dataset fold\n  in parallel using the `n_jobs` parameter to `run_model_on_task` or `run_flow_on_task`\n  (illustrated under Case 2 & 3 below).\n\n* The model/flow specified can also include scikit-learn models that perform their own\n  parallelization. For instance, by specifying `n_jobs` in a Random Forest model definition\n  (covered under Case 2 below).\n\n* The sklearn model can further be an HPO estimator and contain it's own parallelization.\n  If the base estimator used also supports `parallelization`, then there's at least a 2-level nested\n  definition for parallelization possible (covered under Case 3 below).\n\nWe shall cover these 5 representative scenarios for:\n\n* (Case 1) Retrieving runtimes for Random Forest training and prediction on each of the\n  cross-validation folds\n\n* (Case 2) Testing the above setting in a parallel setup and monitor the difference using\n  runtimes retrieved\n\n* (Case 3) Comparing RandomSearchCV and GridSearchCV on the above task based on runtimes\n\n* (Case 4) Running models that don't run in parallel or models which scikit-learn doesn't\n  parallelize\n\n* (Case 5) Running models that do not release the Python Global Interpreter Lock (GIL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD 3-Clause\n\nimport openml\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom joblib.parallel import parallel_backend\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing tasks and scikit-learn models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "task_id = 167119\n\ntask = openml.tasks.get_task(task_id)\nprint(task)\n\n# Viewing associated data\nn_repeats, n_folds, n_samples = task.get_split_dimensions()\nprint(\n    \"Task {}: number of repeats: {}, number of folds: {}, number of samples {}.\".format(\n        task_id,\n        n_repeats,\n        n_folds,\n        n_samples,\n    )\n)\n\n\n# Creating utility function\ndef print_compare_runtimes(measures):\n    for repeat, val1 in measures[\"usercpu_time_millis_training\"].items():\n        for fold, val2 in val1.items():\n            print(\n                \"Repeat #{}-Fold #{}: CPU-{:.3f} vs Wall-{:.3f}\".format(\n                    repeat, fold, val2, measures[\"wall_clock_time_millis_training\"][repeat][fold]\n                )\n            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Case 1: Running a Random Forest model on an OpenML task\nWe'll run a Random Forest model and obtain an OpenML run object. We can\nsee the evaluations recorded per fold for the dataset and the information\navailable for this run.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(n_estimators=10)\n\nrun1 = openml.runs.run_model_on_task(\n    model=clf,\n    task=task,\n    upload_flow=False,\n    avoid_duplicate_runs=False,\n)\nmeasures = run1.fold_evaluations\n\nprint(\"The timing and performance metrics available: \")\nfor key in measures.keys():\n    print(key)\nprint()\n\nprint(\n    \"The performance metric is recorded under `predictive_accuracy` per \"\n    \"fold and can be retrieved as: \"\n)\nfor repeat, val1 in measures[\"predictive_accuracy\"].items():\n    for fold, val2 in val1.items():\n        print(\"Repeat #{}-Fold #{}: {:.4f}\".format(repeat, fold, val2))\n    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The remaining entries recorded in `measures` are the runtime records\nrelated as:\n\nusercpu_time_millis = usercpu_time_millis_training + usercpu_time_millis_testing\n\nwall_clock_time_millis = wall_clock_time_millis_training + wall_clock_time_millis_testing\n\nThe timing measures recorded as `*_millis_training` contain the per\nrepeat-per fold timing incurred for the execution of the `.fit()` procedure\nof the model. For `usercpu_time_*` the time recorded using `time.process_time()`\nis converted to `milliseconds` and stored. Similarly, `time.time()` is used\nto record the time entry for `wall_clock_time_*`. The `*_millis_testing` entry\nfollows the same procedure but for time taken for the `.predict()` procedure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Comparing the CPU and wall-clock training times of the Random Forest model\nprint_compare_runtimes(measures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Case 2: Running Scikit-learn model on an OpenML task in parallel\nRedefining the model to allow parallelism with `n_jobs=2` (2 cores)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(n_estimators=10, n_jobs=2)\n\nrun2 = openml.runs.run_model_on_task(\n    model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False\n)\nmeasures = run2.fold_evaluations\n# The wall-clock time recorded per fold should be lesser than Case 1 above\nprint_compare_runtimes(measures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running a Random Forest model on an OpenML task in parallel (all cores available):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Redefining the model to use all available cores with `n_jobs=-1`\nclf = RandomForestClassifier(n_estimators=10, n_jobs=-1)\n\nrun3 = openml.runs.run_model_on_task(\n    model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False\n)\nmeasures = run3.fold_evaluations\n# The wall-clock time recorded per fold should be lesser than the case above,\n# if more than 2 CPU cores are available. The speed-up is more pronounced for\n# larger datasets.\nprint_compare_runtimes(measures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now observe that the ratio of CPU time to wallclock time is lower\nthan in case 1. This happens because joblib by default spawns subprocesses\nfor the workloads for which CPU time cannot be tracked. Therefore, interpreting\nthe reported CPU and wallclock time requires knowledge of the parallelization\napplied at runtime.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the same task with a different parallel backend. Joblib provides multiple\nbackends: {`loky` (default), `multiprocessing`, `dask`, `threading`, `sequential`}.\nThe backend can be explicitly set using a joblib context manager. The behaviour of\nthe job distribution can change and therefore the scale of runtimes recorded too.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with parallel_backend(backend=\"multiprocessing\", n_jobs=-1):\n    run3_ = openml.runs.run_model_on_task(\n        model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False\n    )\nmeasures = run3_.fold_evaluations\nprint_compare_runtimes(measures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The CPU time interpretation becomes ambiguous when jobs are distributed over an\nunknown number of cores or when subprocesses are spawned for which the CPU time\ncannot be tracked, as in the examples above. It is impossible for OpenML-Python\nto capture the availability of the number of cores/threads, their eventual\nutilisation and whether workloads are executed in subprocesses, for various\ncases that can arise as demonstrated in the rest of the example. Therefore,\nthe final interpretation of the runtimes is left to the `user`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Case 3: Running and benchmarking HPO algorithms with their runtimes\nWe shall now optimize a similar RandomForest model for the same task using\nscikit-learn's HPO support by using GridSearchCV to optimize our earlier\nRandomForest model's hyperparameter `n_estimators`. Scikit-learn also provides a\n`refit_time_` for such HPO models, i.e., the time incurred by training\nand evaluating the model on the best found parameter setting. This is\nincluded in the `wall_clock_time_millis_training` measure recorded.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n\n\nclf = RandomForestClassifier(n_estimators=10, n_jobs=2)\n\n# GridSearchCV model\nn_iter = 5\ngrid_pipe = GridSearchCV(\n    estimator=clf,\n    param_grid={\"n_estimators\": np.linspace(start=1, stop=50, num=n_iter).astype(int).tolist()},\n    cv=2,\n    n_jobs=2,\n)\n\nrun4 = openml.runs.run_model_on_task(\n    model=grid_pipe, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2\n)\nmeasures = run4.fold_evaluations\nprint_compare_runtimes(measures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Like any optimisation problem, scikit-learn's HPO estimators also generate\na sequence of configurations which are evaluated, using which the best found\nconfiguration is tracked throughout the trace.\nThe OpenML run object stores these traces as OpenMLRunTrace objects accessible\nusing keys of the pattern (repeat, fold, iterations). Here `fold` implies the\nouter-cross validation fold as obtained from the task data splits in OpenML.\nGridSearchCV here performs grid search over the inner-cross validation folds as\nparameterized by the `cv` parameter. Since `GridSearchCV` in this example performs a\n`2-fold` cross validation, the runtime recorded per repeat-per fold in the run object\nis for the entire `fit()` procedure of GridSearchCV thus subsuming the runtimes of\nthe 2-fold (inner) CV search performed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We earlier extracted the number of repeats and folds for this task:\nprint(\"# repeats: {}\\n# folds: {}\".format(n_repeats, n_folds))\n\n# To extract the training runtime of the first repeat, first fold:\nprint(run4.fold_evaluations[\"wall_clock_time_millis_training\"][0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To extract the training runtime of the 1-st repeat, 4-th (outer) fold and also\nto fetch the parameters and performance of the evaluations made during\nthe 1-st repeat, 4-th fold evaluation by the Grid Search model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_repeat = 0\n_fold = 3\nprint(\n    \"Total runtime for repeat {}'s fold {}: {:4f} ms\".format(\n        _repeat, _fold, run4.fold_evaluations[\"wall_clock_time_millis_training\"][_repeat][_fold]\n    )\n)\nfor i in range(n_iter):\n    key = (_repeat, _fold, i)\n    r = run4.trace.trace_iterations[key]\n    print(\n        \"n_estimators: {:>2} - score: {:.3f}\".format(\n            r.parameters[\"parameter_n_estimators\"], r.evaluation\n        )\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scikit-learn's HPO estimators also come with an argument `refit=True` as a default.\nIn our previous model definition it was set to True by default, which meant that the best\nfound hyperparameter configuration was used to refit or retrain the model without any inner\ncross validation. This extra refit time measure is provided by the scikit-learn model as the\nattribute `refit_time_`.\nThis time is included in the `wall_clock_time_millis_training` measure.\n\nFor non-HPO estimators, `wall_clock_time_millis = wall_clock_time_millis_training + wall_clock_time_millis_testing`.\n\nFor HPO estimators, `wall_clock_time_millis = wall_clock_time_millis_training + wall_clock_time_millis_testing + refit_time`.\n\nThis refit time can therefore be explicitly extracted in this manner:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def extract_refit_time(run, repeat, fold):\n    refit_time = (\n        run.fold_evaluations[\"wall_clock_time_millis\"][repeat][fold]\n        - run.fold_evaluations[\"wall_clock_time_millis_training\"][repeat][fold]\n        - run.fold_evaluations[\"wall_clock_time_millis_testing\"][repeat][fold]\n    )\n    return refit_time\n\n\nfor repeat in range(n_repeats):\n    for fold in range(n_folds):\n        print(\n            \"Repeat #{}-Fold #{}: {:.4f}\".format(\n                repeat, fold, extract_refit_time(run4, repeat, fold)\n            )\n        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Along with the GridSearchCV already used above, we demonstrate how such\noptimisation traces can be retrieved by showing an application of these\ntraces - comparing the speed of finding the best configuration using\nRandomizedSearchCV and GridSearchCV available with scikit-learn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# RandomizedSearchCV model\nrs_pipe = RandomizedSearchCV(\n    estimator=clf,\n    param_distributions={\n        \"n_estimators\": np.linspace(start=1, stop=50, num=15).astype(int).tolist()\n    },\n    cv=2,\n    n_iter=n_iter,\n    n_jobs=2,\n)\nrun5 = openml.runs.run_model_on_task(\n    model=rs_pipe, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since for the call to ``openml.runs.run_model_on_task`` the parameter\n``n_jobs`` is set to its default ``None``, the evaluations across the OpenML folds\nare not parallelized. Hence, the time recorded is agnostic to the ``n_jobs``\nbeing set at both the HPO estimator ``GridSearchCV`` as well as the base\nestimator ``RandomForestClassifier`` in this case. The OpenML extension only records the\ntime taken for the completion of the complete ``fit()`` call, per-repeat per-fold.\n\nThis notion can be used to extract and plot the best found performance per\nfold by the HPO model and the corresponding time taken for search across\nthat fold. Moreover, since ``n_jobs=None`` for ``openml.runs.run_model_on_task``\nthe runtimes per fold can be cumulatively added to plot the trace against time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def extract_trace_data(run, n_repeats, n_folds, n_iter, key=None):\n    key = \"wall_clock_time_millis_training\" if key is None else key\n    data = {\"score\": [], \"runtime\": []}\n    for i_r in range(n_repeats):\n        for i_f in range(n_folds):\n            data[\"runtime\"].append(run.fold_evaluations[key][i_r][i_f])\n            for i_i in range(n_iter):\n                r = run.trace.trace_iterations[(i_r, i_f, i_i)]\n                if r.selected:\n                    data[\"score\"].append(r.evaluation)\n                    break\n    return data\n\n\ndef get_incumbent_trace(trace):\n    best_score = 1\n    inc_trace = []\n    for i, r in enumerate(trace):\n        if i == 0 or (1 - r) < best_score:\n            best_score = 1 - r\n        inc_trace.append(best_score)\n    return inc_trace\n\n\ngrid_data = extract_trace_data(run4, n_repeats, n_folds, n_iter)\nrs_data = extract_trace_data(run5, n_repeats, n_folds, n_iter)\n\nplt.clf()\nplt.plot(\n    np.cumsum(grid_data[\"runtime\"]), get_incumbent_trace(grid_data[\"score\"]), label=\"Grid Search\"\n)\nplt.plot(\n    np.cumsum(rs_data[\"runtime\"]), get_incumbent_trace(rs_data[\"score\"]), label=\"Random Search\"\n)\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"Wallclock time (in milliseconds)\")\nplt.ylabel(\"1 - Accuracy\")\nplt.title(\"Optimisation Trace Comparison\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Case 4: Running models that scikit-learn doesn't parallelize\nBoth scikit-learn and OpenML depend on parallelism implemented through `joblib`.\nHowever, there can be cases where either models cannot be parallelized or don't\ndepend on joblib for its parallelism. 2 such cases are illustrated below.\n\nRunning a Decision Tree model that doesn't support parallelism implicitly, but\nusing OpenML to parallelize evaluations for the outer-cross validation folds.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeClassifier()\n\nrun6 = openml.runs.run_model_on_task(\n    model=dt, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2\n)\nmeasures = run6.fold_evaluations\nprint_compare_runtimes(measures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although the decision tree does not run in parallel, it can release the\n[Python GIL](https://docs.python.org/dev/glossary.html#term-global-interpreter-lock).\nThis can result in surprising runtime measures as demonstrated below:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with parallel_backend(\"threading\", n_jobs=-1):\n    run7 = openml.runs.run_model_on_task(\n        model=dt, task=task, upload_flow=False, avoid_duplicate_runs=False\n    )\nmeasures = run7.fold_evaluations\nprint_compare_runtimes(measures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running a Neural Network from scikit-learn that uses scikit-learn independent\nparallelism using libraries such as [MKL, OpenBLAS or BLIS](https://scikit-learn.org/stable/computing/parallelism.html#parallel-numpy-and-scipy-routines-from-numerical-libraries).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mlp = MLPClassifier(max_iter=10)\n\nrun8 = openml.runs.run_model_on_task(\n    model=mlp, task=task, upload_flow=False, avoid_duplicate_runs=False\n)\nmeasures = run8.fold_evaluations\nprint_compare_runtimes(measures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Case 5: Running Scikit-learn models that don't release GIL\nCertain Scikit-learn models do not release the [Python GIL](https://docs.python.org/dev/glossary.html#term-global-interpreter-lock) and\nare also not executed in parallel via a BLAS library. In such cases, the\nCPU times and wallclock times are most likely trustworthy. Note however\nthat only very few models such as naive Bayes models are of this kind.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = GaussianNB()\n\nwith parallel_backend(\"multiprocessing\", n_jobs=-1):\n    run9 = openml.runs.run_model_on_task(\n        model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False\n    )\nmeasures = run9.fold_evaluations\nprint_compare_runtimes(measures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summmary\nThe scikit-learn extension for OpenML-Python records model runtimes for the\nCPU-clock and the wall-clock times. The above examples illustrated how these\nrecorded runtimes can be extracted when using a scikit-learn model and under\nparallel setups too. To summarize, the scikit-learn extension measures the:\n\n* `CPU-time` & `wallclock-time` for the whole run\n\n  * A run here corresponds to a call to `run_model_on_task` or `run_flow_on_task`\n  * The recorded time is for the model fit for each of the outer-cross validations folds,\n    i.e., the OpenML data splits\n\n* Python's `time` module is used to compute the runtimes\n\n  * `CPU-time` is recorded using the responses of `time.process_time()`\n  * `wallclock-time` is recorded using the responses of `time.time()`\n\n* The timings recorded by OpenML per outer-cross validation fold is agnostic to\n  model parallelisation\n\n  * The wallclock times reported in Case 2 above highlights the speed-up on using `n_jobs=-1`\n    in comparison to `n_jobs=2`, since the timing recorded by OpenML is for the entire\n    `fit()` procedure, whereas the parallelisation is performed inside `fit()` by scikit-learn\n  * The CPU-time for models that are run in parallel can be difficult to interpret\n\n* `CPU-time` & `wallclock-time` for each search per outer fold in an HPO run\n\n  * Reports the total time for performing search on each of the OpenML data split, subsuming\n    any sort of parallelism that happened as part of the HPO estimator or the underlying\n    base estimator\n  * Also allows extraction of the `refit_time` that scikit-learn measures using `time.time()`\n    for retraining the model per outer fold, for the best found configuration\n\n* `CPU-time` & `wallclock-time` for models that scikit-learn doesn't parallelize\n\n  * Models like Decision Trees or naive Bayes don't parallelize and thus both the wallclock and\n    CPU times are similar in runtime for the OpenML call\n  * However, models implemented in Cython, such as the Decision Trees can release the GIL and\n    still run in parallel if a `threading` backend is used by joblib.\n  * Scikit-learn Neural Networks can undergo parallelization implicitly owing to thread-level\n    parallelism involved in the linear algebraic operations and thus the wallclock-time and\n    CPU-time can differ.\n\nBecause of all the cases mentioned above it is crucial to understand which case is triggered\nwhen reporting runtimes for scikit-learn models measured with OpenML-Python!\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}