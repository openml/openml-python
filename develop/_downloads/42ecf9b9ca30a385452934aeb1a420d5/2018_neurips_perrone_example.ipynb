{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Perrone et al. (2018)\n\nA tutorial on how to build a surrogate model based on OpenML data as done for *Scalable\nHyperparameter Transfer Learning* by Perrone et al..\n\n## Publication\n\n| Scalable Hyperparameter Transfer Learning\n| Valerio Perrone and Rodolphe Jenatton and Matthias Seeger and Cedric Archambeau\n| In *Advances in Neural Information Processing Systems 31*, 2018\n| Available at https://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning.pdf\n\nThis example demonstrates how OpenML runs can be used to construct a surrogate model.\n\nIn the following section, we shall do the following:\n\n* Retrieve tasks and flows as used in the experiments by Perrone et al. (2018).\n* Build a tabular data by fetching the evaluations uploaded to OpenML.\n* Impute missing values and handle categorical data before building a Random Forest model that\n  maps hyperparameter values to the area under curve score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD 3-Clause\n\nimport openml\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\n\nflow_type = \"svm\"  # this example will use the smaller svm flow evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The subsequent functions are defined to fetch tasks, flows, evaluations and preprocess them into\na tabular format that can be used to build models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fetch_evaluations(run_full=False, flow_type=\"svm\", metric=\"area_under_roc_curve\"):\n    \"\"\"\n    Fetch a list of evaluations based on the flows and tasks used in the experiments.\n\n    Parameters\n    ----------\n    run_full : boolean\n        If True, use the full list of tasks used in the paper\n        If False, use 5 tasks with the smallest number of evaluations available\n    flow_type : str, {'svm', 'xgboost'}\n        To select whether svm or xgboost experiments are to be run\n    metric : str\n        The evaluation measure that is passed to openml.evaluations.list_evaluations\n\n    Returns\n    -------\n    eval_df : dataframe\n    task_ids : list\n    flow_id : int\n    \"\"\"\n    # Collecting task IDs as used by the experiments from the paper\n    # fmt: off\n    if flow_type == \"svm\" and run_full:\n        task_ids = [\n            10101, 145878, 146064, 14951, 34537, 3485, 3492, 3493, 3494,\n            37, 3889, 3891, 3899, 3902, 3903, 3913, 3918, 3950, 9889,\n            9914, 9946, 9952, 9967, 9971, 9976, 9978, 9980, 9983,\n        ]\n    elif flow_type == \"svm\" and not run_full:\n        task_ids = [9983, 3485, 3902, 3903, 145878]\n    elif flow_type == \"xgboost\" and run_full:\n        task_ids = [\n            10093, 10101, 125923, 145847, 145857, 145862, 145872, 145878,\n            145953, 145972, 145976, 145979, 146064, 14951, 31, 3485,\n            3492, 3493, 37, 3896, 3903, 3913, 3917, 3918, 3, 49, 9914,\n            9946, 9952, 9967,\n        ]\n    else:  # flow_type == 'xgboost' and not run_full:\n        task_ids = [3903, 37, 3485, 49, 3913]\n    # fmt: on\n\n    # Fetching the relevant flow\n    flow_id = 5891 if flow_type == \"svm\" else 6767\n\n    # Fetching evaluations\n    eval_df = openml.evaluations.list_evaluations_setups(\n        function=metric,\n        tasks=task_ids,\n        flows=[flow_id],\n        uploaders=[2702],\n        output_format=\"dataframe\",\n        parameters_in_separate_columns=True,\n    )\n    return eval_df, task_ids, flow_id\n\n\ndef create_table_from_evaluations(\n    eval_df, flow_type=\"svm\", run_count=np.iinfo(np.int64).max, task_ids=None\n):\n    \"\"\"\n    Create a tabular data with its ground truth from a dataframe of evaluations.\n    Optionally, can filter out records based on task ids.\n\n    Parameters\n    ----------\n    eval_df : dataframe\n        Containing list of runs as obtained from list_evaluations()\n    flow_type : str, {'svm', 'xgboost'}\n        To select whether svm or xgboost experiments are to be run\n    run_count : int\n        Maximum size of the table created, or number of runs included in the table\n    task_ids : list, (optional)\n        List of integers specifying the tasks to be retained from the evaluations dataframe\n\n    Returns\n    -------\n    eval_table : dataframe\n    values : list\n    \"\"\"\n    if task_ids is not None:\n        eval_df = eval_df[eval_df[\"task_id\"].isin(task_ids)]\n    if flow_type == \"svm\":\n        colnames = [\"cost\", \"degree\", \"gamma\", \"kernel\"]\n    else:\n        colnames = [\n            \"alpha\",\n            \"booster\",\n            \"colsample_bylevel\",\n            \"colsample_bytree\",\n            \"eta\",\n            \"lambda\",\n            \"max_depth\",\n            \"min_child_weight\",\n            \"nrounds\",\n            \"subsample\",\n        ]\n    eval_df = eval_df.sample(frac=1)  # shuffling rows\n    eval_df = eval_df.iloc[:run_count, :]\n    eval_df.columns = [column.split(\"_\")[-1] for column in eval_df.columns]\n    eval_table = eval_df.loc[:, colnames]\n    value = eval_df.loc[:, \"value\"]\n    return eval_table, value\n\n\ndef list_categorical_attributes(flow_type=\"svm\"):\n    if flow_type == \"svm\":\n        return [\"kernel\"]\n    return [\"booster\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fetching the data from OpenML\nNow, we read all the tasks and evaluations for them and collate into a table.\nHere, we are reading all the tasks and evaluations for the SVM flow and\npre-processing all retrieved evaluations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eval_df, task_ids, flow_id = fetch_evaluations(run_full=False, flow_type=flow_type)\nX, y = create_table_from_evaluations(eval_df, flow_type=flow_type)\nprint(X.head())\nprint(\"Y : \", y[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating pre-processing and modelling pipelines\nThe two primary tasks are to impute the missing values, that is, account for the hyperparameters\nthat are not available with the runs from OpenML. And secondly, to handle categorical variables\nusing One-hot encoding prior to modelling.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Separating data into categorical and non-categorical (numeric for this example) columns\ncat_cols = list_categorical_attributes(flow_type=flow_type)\nnum_cols = list(set(X.columns) - set(cat_cols))\n\n# Missing value imputers for numeric columns\nnum_imputer = SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=-1)\n\n# Creating the one-hot encoder for numerical representation of categorical columns\nenc = OneHotEncoder(handle_unknown=\"ignore\")\n\n# Combining column transformers\nct = ColumnTransformer([(\"cat\", enc, cat_cols), (\"num\", num_imputer, num_cols)])\n\n# Creating the full pipeline with the surrogate model\nclf = RandomForestRegressor(n_estimators=50)\nmodel = Pipeline(steps=[(\"preprocess\", ct), (\"surrogate\", clf)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building a surrogate model on a task's evaluation\nThe same set of functions can be used for a single task to retrieve a singular table which can\nbe used for the surrogate model construction. We shall use the SVM flow here to keep execution\ntime simple and quick.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Selecting a task for the surrogate\ntask_id = task_ids[-1]\nprint(\"Task ID : \", task_id)\nX, y = create_table_from_evaluations(eval_df, task_ids=[task_id], flow_type=\"svm\")\n\nmodel.fit(X, y)\ny_pred = model.predict(X)\n\nprint(\"Training RMSE : {:.5}\".format(mean_squared_error(y, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the surrogate model\nThe surrogate model built from a task's evaluations fetched from OpenML will be put into\ntrivial action here, where we shall randomly sample configurations and observe the trajectory\nof the area under curve (auc) we can obtain from the surrogate we've built.\n\nNOTE: This section is written exclusively for the SVM flow\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Sampling random configurations\ndef random_sample_configurations(num_samples=100):\n    colnames = [\"cost\", \"degree\", \"gamma\", \"kernel\"]\n    ranges = [\n        (0.000986, 998.492437),\n        (2.0, 5.0),\n        (0.000988, 913.373845),\n        ([\"linear\", \"polynomial\", \"radial\", \"sigmoid\"]),\n    ]\n    X = pd.DataFrame(np.nan, index=range(num_samples), columns=colnames)\n    for i in range(len(colnames)):\n        if len(ranges[i]) == 2:\n            col_val = np.random.uniform(low=ranges[i][0], high=ranges[i][1], size=num_samples)\n        else:\n            col_val = np.random.choice(ranges[i], size=num_samples)\n        X.iloc[:, i] = col_val\n    return X\n\n\nconfigs = random_sample_configurations(num_samples=1000)\nprint(configs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preds = model.predict(configs)\n\n# tracking the maximum AUC obtained over the functions evaluations\npreds = np.maximum.accumulate(preds)\n# computing regret (1 - predicted_auc)\nregret = 1 - preds\n\n# plotting the regret curve\nplt.plot(regret)\nplt.title(\"AUC regret for Random Search on surrogate\")\nplt.xlabel(\"Numbe of function evaluations\")\nplt.ylabel(\"Regret\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}