{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tasks\n\nA tutorial on how to list and download tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD 3-Clause\n\nimport openml\nfrom openml.tasks import TaskType\nimport pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tasks are identified by IDs and can be accessed in two different ways:\n\n1. In a list providing basic information on all tasks available on OpenML.\n   This function will not download the actual tasks, but will instead download\n   meta data that can be used to filter the tasks and retrieve a set of IDs.\n   We can filter this list, for example, we can only list tasks having a\n   special tag or only tasks for a specific target such as\n   *supervised classification*.\n2. A single task by its ID. It contains all meta information, the target\n   metric, the splits and an iterator which can be used to access the\n   splits in a useful manner.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Listing tasks\n\nWe will start by simply listing only *supervised classification* tasks.\n**openml.tasks.list_tasks()** returns a dictionary of dictionaries by default, but we\nrequest a\n[pandas dataframe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\ninstead to have better visualization capabilities and easier access:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tasks = openml.tasks.list_tasks(\n    task_type=TaskType.SUPERVISED_CLASSIFICATION, output_format=\"dataframe\"\n)\nprint(tasks.columns)\nprint(f\"First 5 of {len(tasks)} tasks:\")\nprint(tasks.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can filter the list of tasks to only contain datasets with more than\n500 samples, but less than 1000 samples:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filtered_tasks = tasks.query(\"NumberOfInstances > 500 and NumberOfInstances < 1000\")\nprint(list(filtered_tasks.index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Number of tasks\nprint(len(filtered_tasks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can further restrict the tasks to all have the same resampling strategy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filtered_tasks = filtered_tasks.query('estimation_procedure == \"10-fold Crossvalidation\"')\nprint(list(filtered_tasks.index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Number of tasks\nprint(len(filtered_tasks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resampling strategies can be found on the\n[OpenML Website](https://www.openml.org/search?type=measure&q=estimation%20procedure).\n\nSimilar to listing tasks by task type, we can list tasks by tags:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tasks = openml.tasks.list_tasks(tag=\"OpenML100\", output_format=\"dataframe\")\nprint(f\"First 5 of {len(tasks)} tasks:\")\nprint(tasks.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Furthermore, we can list tasks based on the dataset id:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tasks = openml.tasks.list_tasks(data_id=1471, output_format=\"dataframe\")\nprint(f\"First 5 of {len(tasks)} tasks:\")\nprint(tasks.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition, a size limit and an offset can be applied both separately and simultaneously:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tasks = openml.tasks.list_tasks(size=10, offset=50, output_format=\"dataframe\")\nprint(tasks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**OpenML 100**\nis a curated list of 100 tasks to start using OpenML. They are all\nsupervised classification tasks with more than 500 instances and less than 50000\ninstances per task. To make things easier, the tasks do not contain highly\nunbalanced data and sparse data. However, the tasks include missing values and\ncategorical features. You can find out more about the *OpenML 100* on\n[the OpenML benchmarking page](https://docs.openml.org/benchmark/).\n\nFinally, it is also possible to list all tasks on OpenML with:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tasks = openml.tasks.list_tasks(output_format=\"dataframe\")\nprint(len(tasks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise\n\nSearch for the tasks on the 'eeg-eye-state' dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tasks.query('name==\"eeg-eye-state\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading tasks\n\nWe provide two functions to download tasks, one which downloads only a\nsingle task by its ID, and one which takes a list of IDs and downloads\nall of these tasks:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "task_id = 31\ntask = openml.tasks.get_task(task_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Properties of the task are stored as member variables:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ids = [2, 1891, 31, 9983]\ntasks = openml.tasks.get_tasks(ids)\nprint(tasks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating tasks\n\nYou can also create new tasks. Take the following into account:\n\n* You can only create tasks on *active* datasets\n* For now, only the following tasks are supported: classification, regression,\n  clustering, and learning curve analysis.\n* For now, tasks can only be created on a single dataset.\n* The exact same task must not already exist.\n\nCreating a task requires the following input:\n\n* task_type: The task type ID, required (see below). Required.\n* dataset_id: The dataset ID. Required.\n* target_name: The name of the attribute you aim to predict. Optional.\n* estimation_procedure_id : The ID of the estimation procedure used to create train-test\n  splits. Optional.\n* evaluation_measure: The name of the evaluation measure. Optional.\n* Any additional inputs for specific tasks\n\nIt is best to leave the evaluation measure open if there is no strong prerequisite for a\nspecific measure. OpenML will always compute all appropriate measures and you can filter\nor sort results on your favourite measure afterwards. Only add an evaluation measure if\nnecessary (e.g. when other measure make no sense), since it will create a new task, which\nscatters results across tasks.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the test server for the rest of this tutorial.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>.. include:: ../../test_server_usage_warning.txt</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "openml.config.start_using_configuration_for_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example\n\nLet's create a classification task on a dataset. In this example we will do this on the\nIris dataset (ID=128 (on test server)). We'll use 10-fold cross-validation (ID=1),\nand *predictive accuracy* as the predefined measure (this can also be left open).\nIf a task with these parameters exists, we will get an appropriate exception.\nIf such a task doesn't exist, a task will be created and the corresponding task_id\nwill be returned.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    my_task = openml.tasks.create_task(\n        task_type=TaskType.SUPERVISED_CLASSIFICATION,\n        dataset_id=128,\n        target_name=\"class\",\n        evaluation_measure=\"predictive_accuracy\",\n        estimation_procedure_id=1,\n    )\n    my_task.publish()\nexcept openml.exceptions.OpenMLServerException as e:\n    # Error code for 'task already exists'\n    if e.code == 614:\n        # Lookup task\n        tasks = openml.tasks.list_tasks(data_id=128, output_format=\"dataframe\")\n        tasks = tasks.query(\n            'task_type == \"Supervised Classification\" '\n            'and estimation_procedure == \"10-fold Crossvalidation\" '\n            'and evaluation_measures == \"predictive_accuracy\"'\n        )\n        task_id = tasks.loc[:, \"tid\"].values[0]\n        print(\"Task already exists. Task ID is\", task_id)\n\n# reverting to prod server\nopenml.config.stop_using_configuration_for_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* [Complete list of task types](https://www.openml.org/search?type=task_type).\n* [Complete list of model estimation procedures](https://www.openml.org/search?q=%2520measure_type%3Aestimation_procedure&type=measure).\n* [Complete list of evaluation measures](https://www.openml.org/search?q=measure_type%3Aevaluation_measure&type=measure).\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}