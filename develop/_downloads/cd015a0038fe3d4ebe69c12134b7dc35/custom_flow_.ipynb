{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Creating and Using a Custom Flow\n\nThe most convenient way to create a flow for your machine learning workflow is to generate it\nautomatically as described in the `sphx_glr_examples_30_extended_flow_id_tutorial.py` tutorial.\nHowever, there are scenarios where this is not possible, such\nas when the flow uses a framework without an extension or when the flow is described by a script.\n\nIn those cases you can still create a custom flow by following the steps of this tutorial.\nAs an example we will use the flows generated for the [AutoML Benchmark](https://openml.github.io/automlbenchmark/),\nand also show how to link runs to the custom flow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD 3-Clause\n\nfrom collections import OrderedDict\nimport numpy as np\n\nimport openml\nfrom openml import OpenMLClassificationTask\nfrom openml.runs.functions import format_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>.. include:: ../../test_server_usage_warning.txt</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "openml.config.start_using_configuration_for_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Defining the flow\nThe first step is to define all the hyperparameters of your flow.\nThe API pages feature a descriptions of each variable of the :class:`openml.flows.OpenMLFlow`.\nNote that `external version` and `name` together uniquely identify a flow.\n\nThe AutoML Benchmark runs AutoML systems across a range of tasks.\nOpenML stores Flows for each AutoML system. However, the AutoML benchmark adds\npreprocessing to the flow, so should be described in a new flow.\n\nWe will break down the flow arguments into several groups, for the tutorial.\nFirst we will define the name and version information.\nMake sure to leave enough information so others can determine exactly which\nversion of the package/script is used. Use tags so users can find your flow easily.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "general = dict(\n    name=\"automlbenchmark_autosklearn\",\n    description=(\n        \"Auto-sklearn as set up by the AutoML Benchmark\"\n        \"Source: https://github.com/openml/automlbenchmark/releases/tag/v0.9\"\n    ),\n    external_version=\"amlb==0.9\",\n    language=\"English\",\n    tags=[\"amlb\", \"benchmark\", \"study_218\"],\n    dependencies=\"amlb==0.9\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we define the flow hyperparameters. We define their name and default value in `parameters`,\nand provide meta-data for each hyperparameter through `parameters_meta_info`.\nNote that even though the argument name is `parameters` they describe the hyperparameters.\nThe use of ordered dicts is required.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flow_hyperparameters = dict(\n    parameters=OrderedDict(time=\"240\", memory=\"32\", cores=\"8\"),\n    parameters_meta_info=OrderedDict(\n        cores=OrderedDict(description=\"number of available cores\", data_type=\"int\"),\n        memory=OrderedDict(description=\"memory in gigabytes\", data_type=\"int\"),\n        time=OrderedDict(description=\"time in minutes\", data_type=\"int\"),\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is possible to build a flow which uses other flows.\nFor example, the Random Forest Classifier is a flow, but you could also construct a flow\nwhich uses a Random Forest Classifier in a ML pipeline. When constructing the pipeline flow,\nyou can use the Random Forest Classifier flow as a *subflow*. It allows for\nall hyperparameters of the Random Classifier Flow to also be specified in your pipeline flow.\n\nNote: you can currently only specific one subflow as part of the components.\n\nIn this example, the auto-sklearn flow is a subflow: the auto-sklearn flow is entirely executed as part of this flow.\nThis allows people to specify auto-sklearn hyperparameters used in this flow.\nIn general, using a subflow is not required.\n\nNote: flow 9313 is not actually the right flow on the test server,\nbut that does not matter for this demonstration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "autosklearn_flow = openml.flows.get_flow(9313)  # auto-sklearn 0.5.1\nsubflow = dict(\n    components=OrderedDict(automl_tool=autosklearn_flow),\n    # If you do not want to reference a subflow, you can use the following:\n    # components=OrderedDict(),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With all parameters of the flow defined, we can now initialize the OpenMLFlow and publish.\nBecause we provided all the details already, we do not need to provide a `model` to the flow.\n\nIn our case, we don't even have a model. It is possible to have a model but still require\nto follow these steps when the model (python object) does not have an extensions from which\nto automatically extract the hyperparameters.\nSo whether you have a model with no extension or no model at all, explicitly set\nthe model of the flow to `None`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "autosklearn_amlb_flow = openml.flows.OpenMLFlow(\n    **general,\n    **flow_hyperparameters,\n    **subflow,\n    model=None,\n)\nautosklearn_amlb_flow.publish()\nprint(f\"autosklearn flow created: {autosklearn_amlb_flow.flow_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Using the flow\nThis Section will show how to upload run data for your custom flow.\nTake care to change the values of parameters as well as the task id,\nto reflect the actual run.\nTask and parameter values in the example are fictional.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flow_id = autosklearn_amlb_flow.flow_id\n\nparameters = [\n    OrderedDict([(\"oml:name\", \"cores\"), (\"oml:value\", 4), (\"oml:component\", flow_id)]),\n    OrderedDict([(\"oml:name\", \"memory\"), (\"oml:value\", 16), (\"oml:component\", flow_id)]),\n    OrderedDict([(\"oml:name\", \"time\"), (\"oml:value\", 120), (\"oml:component\", flow_id)]),\n]\n\ntask_id = 1200  # Iris Task\ntask = openml.tasks.get_task(task_id)\ndataset_id = task.get_dataset().dataset_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last bit of information for the run we need are the predicted values.\nThe exact format of the predictions will depend on the task.\n\nThe predictions should always be a list of lists, each list should contain:\n\n- the repeat number: for repeated evaluation strategies. (e.g. repeated cross-validation)\n- the fold number: for cross-validation. (what should this be for holdout?)\n- 0: this field is for backward compatibility.\n- index: the row (of the original dataset) for which the prediction was made.\n- p_1, ..., p_c: for each class the predicted probability of the sample\n  belonging to that class. (no elements for regression tasks)\n  Make sure the order of these elements follows the order of `task.class_labels`.\n- the predicted class/value for the sample\n- the true class/value for the sample\n\nWhen using openml-python extensions (such as through `run_model_on_task`),\nall of this formatting is automatic.\nUnfortunately we can not automate this procedure for custom flows,\nwhich means a little additional effort is required.\n\nHere we generated some random predictions in place.\nYou can ignore this code, or use it to better understand the formatting of the predictions.\n\nFind the repeats/folds for this task:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_repeats, n_folds, _ = task.get_split_dimensions()\nall_test_indices = [\n    (repeat, fold, index)\n    for repeat in range(n_repeats)\n    for fold in range(n_folds)\n    for index in task.get_train_test_split_indices(fold, repeat)[1]\n]\n\n# random class probabilities (Iris has 150 samples and 3 classes):\nr = np.random.rand(150 * n_repeats, 3)\n# scale the random values so that the probabilities of each sample sum to 1:\ny_proba = r / r.sum(axis=1).reshape(-1, 1)\ny_pred = y_proba.argmax(axis=1)\n\nclass_map = dict(zip(range(3), task.class_labels))\n_, y_true = task.get_X_and_y()\ny_true = [class_map[y] for y in y_true]\n\n# We format the predictions with the utility function `format_prediction`.\n# It will organize the relevant data in the expected format/order.\npredictions = []\nfor where, y, yp, proba in zip(all_test_indices, y_true, y_pred, y_proba):\n    repeat, fold, index = where\n\n    prediction = format_prediction(\n        task=task,\n        repeat=repeat,\n        fold=fold,\n        index=index,\n        prediction=class_map[yp],\n        truth=y,\n        proba={c: pb for (c, pb) in zip(task.class_labels, proba)},\n    )\n    predictions.append(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we can create the OpenMLRun object and upload.\nWe use the argument setup_string because the used flow was a script.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmark_command = f\"python3 runbenchmark.py auto-sklearn medium -m aws -t 119\"\nmy_run = openml.runs.OpenMLRun(\n    task_id=task_id,\n    flow_id=flow_id,\n    dataset_id=dataset_id,\n    parameter_settings=parameters,\n    setup_string=benchmark_command,\n    data_content=predictions,\n    tags=[\"study_218\"],\n    description_text=\"Run generated by the Custom Flow tutorial.\",\n)\nmy_run.publish()\nprint(\"run created:\", my_run.run_id)\n\nopenml.config.stop_using_configuration_for_example()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}