{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmark studies\nHow to list, download and upload benchmark studies.\nIn contrast to [benchmark suites](https://docs.openml.org/benchmark/#benchmarking-suites) which\nhold a list of tasks, studies hold a list of runs. As runs contain all information on flows and\ntasks, all required information about a study can be retrieved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD 3-Clause\n\nimport uuid\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport openml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Listing studies\n\n* Use the output_format parameter to select output type\n* Default gives ``dict``, but we'll use ``dataframe`` to obtain an\n  easier-to-work-with data structure\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "studies = openml.study.list_studies(output_format=\"dataframe\", status=\"all\")\nprint(studies.head(n=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Downloading studies\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is done based on the study ID.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "study = openml.study.get_study(123)\nprint(study)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Studies also features a description:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(study.description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Studies are a container for runs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(study.runs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can use the evaluation listing functionality to learn more about\nthe evaluations available for the conducted runs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluations = openml.evaluations.list_evaluations(\n    function=\"predictive_accuracy\",\n    output_format=\"dataframe\",\n    study=study.study_id,\n)\nprint(evaluations.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the test server for the rest of this tutorial.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>.. include:: ../../test_server_usage_warning.txt</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "openml.config.start_using_configuration_for_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Uploading studies\n\nCreating a study is as simple as creating any kind of other OpenML entity.\nIn this examples we'll create a few runs for the OpenML-100 benchmark\nsuite which is available on the OpenML test server.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Model to be used\nclf = RandomForestClassifier()\n\n# We'll create a study with one run on 3 datasets present in the suite\ntasks = [115, 259, 307]\n\n# To verify\n# https://test.openml.org/api/v1/study/1\nsuite = openml.study.get_suite(\"OpenML100\")\nprint(all([t_id in suite.tasks for t_id in tasks]))\n\nrun_ids = []\nfor task_id in tasks:\n    task = openml.tasks.get_task(task_id)\n    run = openml.runs.run_model_on_task(clf, task)\n    run.publish()\n    run_ids.append(run.run_id)\n\n# The study needs a machine-readable and unique alias. To obtain this,\n# we simply generate a random uuid.\nalias = uuid.uuid4().hex\n\nnew_study = openml.study.create_study(\n    name=\"Test-Study\",\n    description=\"Test study for the Python tutorial on studies\",\n    run_ids=run_ids,\n    alias=alias,\n    benchmark_suite=suite.study_id,\n)\nnew_study.publish()\nprint(new_study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "openml.config.stop_using_configuration_for_example()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}