{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmark suites\n\nThis is a brief showcase of OpenML benchmark suites, which were introduced by\n[Bischl et al. (2019)](https://arxiv.org/abs/1708.03731v2). Benchmark suites standardize the\ndatasets and splits to be used in an experiment or paper. They are fully integrated into OpenML\nand simplify both the sharing of the setup and the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD 3-Clause\n\nimport openml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenML-CC18\n\nAs an example we have a look at the OpenML-CC18, which is a suite of 72 classification datasets\nfrom OpenML which were carefully selected to be usable by many algorithms and also represent\ndatasets commonly used in machine learning research. These are all datasets from mid-2018 that\nsatisfy a large set of clear requirements for thorough yet practical benchmarking:\n\n1. the number of observations are between 500 and 100,000 to focus on medium-sized datasets,\n2. the number of features does not exceed 5,000 features to keep the runtime of the algorithms\n   low\n3. the target attribute has at least two classes with no class having less than 20 observations\n4. the ratio of the minority class and the majority class is above 0.05 (to eliminate highly\n   imbalanced datasets which require special treatment for both algorithms and evaluation\n   measures).\n\nA full description can be found in the [OpenML benchmarking docs](https://docs.openml.org/benchmark/#openml-cc18).\n\nIn this example we'll focus on how to use benchmark suites in practice.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading benchmark suites\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# OpenML Benchmarking Suites and the OpenML-CC18\n# https://www.openml.org/s/99\nsuite = openml.study.get_suite(\"OpenML-CC18\")\nprint(suite)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The benchmark suite does not download the included tasks and datasets itself, but only contains\na list of which tasks constitute the study.\n\nTasks can then be accessed via\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tasks = suite.tasks\nprint(tasks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and iterated over for benchmarking. For speed reasons we only iterate over the first three tasks:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for task_id in tasks[:3]:\n    task = openml.tasks.get_task(task_id)\n    print(task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further examples\n\n* `sphx_glr_examples_30_extended_suites_tutorial.py`\n* `sphx_glr_examples_30_extended_study_tutorial.py`\n* `sphx_glr_examples_40_paper_2018_ida_strang_example.py`\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}