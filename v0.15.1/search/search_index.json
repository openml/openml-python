{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenML","text":"<p>Collaborative Machine Learning in Python</p> <p>Welcome to the documentation of the OpenML Python API, a connector to the collaborative machine learning platform OpenML.org. The OpenML Python package allows to use datasets and tasks from OpenML together with scikit-learn and share the results online.</p>"},{"location":"#example","title":"Example","text":"<pre><code>import openml\nfrom sklearn import impute, tree, pipeline\n\n# Define a scikit-learn classifier or pipeline\nclf = pipeline.Pipeline(\n    steps=[\n        ('imputer', impute.SimpleImputer()),\n        ('estimator', tree.DecisionTreeClassifier())\n    ]\n)\n# Download the OpenML task for the pendigits dataset with 10-fold\n# cross-validation.\ntask = openml.tasks.get_task(32)\n# Run the scikit-learn model on the task.\nrun = openml.runs.run_model_on_task(clf, task)\n# Publish the experiment on OpenML (optional, requires an API key.\n# You can get your own API key by signing up to OpenML.org)\nrun.publish()\nprint(f'View the run online: {run.openml_url}')\n</code></pre> <p>Find more examples in the sidebar on the left.</p>"},{"location":"#how-to-get-openml-for-python","title":"How to get OpenML for python","text":"<p>You can install the OpenML package via <code>pip</code> (we recommend using a virtual environment):</p> <pre><code>python -m pip install openml\n</code></pre> <p>For more advanced installation information, please see the \"Introduction\" example.</p>"},{"location":"#further-information","title":"Further information","text":"<ul> <li>OpenML documentation</li> <li>OpenML client APIs</li> <li>OpenML developer guide</li> <li>Contact information</li> <li>Citation request</li> <li>OpenML blog</li> <li>OpenML twitter account</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contribution to the OpenML package is highly appreciated. Please see the \"Contributing\" page for more information.</p>"},{"location":"#citing-openml-python","title":"Citing OpenML-Python","text":"<p>If you use OpenML-Python in a scientific publication, we would appreciate a reference to our JMLR-MLOSS paper  \"OpenML-Python: an extensible Python API for OpenML\":</p> BibtexMLA <pre><code>@article{JMLR:v22:19-920,\n    author  = {Matthias Feurer and Jan N. van Rijn and Arlind Kadra and Pieter Gijsbers and Neeratyoy Mallik and Sahithya Ravi and Andreas M\u00c3\u00bcller and Joaquin Vanschoren and Frank Hutter},\n    title   = {OpenML-Python: an extensible Python API for OpenML},\n    journal = {Journal of Machine Learning Research},\n    year    = {2021},\n    volume  = {22},\n    number  = {100},\n    pages   = {1--5},\n    url     = {http://jmlr.org/papers/v22/19-920.html}\n}\n</code></pre> <p>Feurer, Matthias, et al.  \"OpenML-Python: an extensible Python API for OpenML.\" Journal of Machine Learning Research 22.100 (2021):1\u22125.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contribution to the OpenML package is highly appreciated in all forms. In particular, a few ways to contribute to openml-python are:</p> <ul> <li>A direct contribution to the package, by means of improving the     code, documentation or examples. To get started, see this     file     with details on how to set up your environment to develop for     openml-python.</li> <li>A contribution to an openml-python extension. An extension package     allows OpenML to interface with a machine learning package (such     as scikit-learn or keras). These extensions are hosted in separate     repositories and may have their own guidelines. For more     information, see also extensions.</li> <li>Bug reports. If something doesn't work for you or is cumbersome,     please open a new issue to let us know about the problem. See     this     section.</li> <li>Cite OpenML if you use it in a     scientific publication.</li> <li>Visit one of our hackathons.</li> <li>Contribute to another OpenML project, such as the main OpenML     project.</li> </ul>"},{"location":"extensions/","title":"Extensions","text":"<p>OpenML-Python provides an extension interface to connect other machine learning libraries than scikit-learn to OpenML. Please check the <code>api_extensions</code> and use the scikit-learn extension in <code>openml.extensions.sklearn.SklearnExtension</code>{.interpreted-text role=\"class\"} as a starting point.</p>"},{"location":"extensions/#list-of-extensions","title":"List of extensions","text":"<p>Here is a list of currently maintained OpenML extensions:</p> <ul> <li><code>openml.extensions.sklearn.SklearnExtension</code>{.interpreted-text     role=\"class\"}</li> <li>openml-keras</li> <li>openml-pytorch</li> <li>openml-tensorflow (for tensorflow     2+)</li> </ul>"},{"location":"extensions/#connecting-new-machine-learning-libraries","title":"Connecting new machine learning libraries","text":""},{"location":"extensions/#content-of-the-library","title":"Content of the Library","text":"<p>To leverage support from the community and to tap in the potential of OpenML, interfacing with popular machine learning libraries is essential. The OpenML-Python package is capable of downloading meta-data and results (data, flows, runs), regardless of the library that was used to upload it. However, in order to simplify the process of uploading flows and runs from a specific library, an additional interface can be built. The OpenML-Python team does not have the capacity to develop and maintain such interfaces on its own. For this reason, we have built an extension interface to allows others to contribute back. Building a suitable extension for therefore requires an understanding of the current OpenML-Python support.</p> <p>The <code>sphx_glr_examples_20_basic_simple_flows_and_runs_tutorial.py</code>{.interpreted-text role=\"ref\"} tutorial shows how scikit-learn currently works with OpenML-Python as an extension. The sklearn extension packaged with the openml-python repository can be used as a template/benchmark to build the new extension.</p>"},{"location":"extensions/#api","title":"API","text":"<ul> <li>The extension scripts must import the [openml]{.title-ref} package     and be able to interface with any function from the OpenML-Python     <code>api</code>.</li> <li>The extension has to be defined as a Python class and must inherit     from <code>openml.extensions.Extension</code>.</li> <li>This class needs to have all the functions from [class     Extension]{.title-ref} overloaded as required.</li> <li>The redefined functions should have adequate and appropriate     docstrings. The [Sklearn Extension API     :class:`openml.extensions.sklearn.SklearnExtension.html]{.title-ref}     is a good example to follow.</li> </ul>"},{"location":"extensions/#interfacing-with-openml-python","title":"Interfacing with OpenML-Python","text":"<p>Once the new extension class has been defined, the openml-python module to <code>openml.extensions.register_extension</code> must be called to allow OpenML-Python to interface the new extension.</p> <p>The following methods should get implemented. Although the documentation in the [Extension]{.title-ref} interface should always be leading, here we list some additional information and best practices. The [Sklearn Extension API :class:`openml.extensions.sklearn.SklearnExtension.html]{.title-ref} is a good example to follow. Note that most methods are relatively simple and can be implemented in several lines of code.</p> <ul> <li>General setup (required)<ul> <li><code>can_handle_flow</code>: Takes as     argument an OpenML flow, and checks whether this can be handled     by the current extension. The OpenML database consists of many     flows, from various workbenches (e.g., scikit-learn, Weka, mlr).     This method is called before a model is being deserialized.     Typically, the flow-dependency field is used to check whether     the specific library is present, and no unknown libraries are     present there.</li> <li><code>can_handle_model</code>: Similar as     <code>can_handle_flow</code>, except that in     this case a Python object is given. As such, in many cases, this     method can be implemented by checking whether this adheres to a     certain base class.</li> </ul> </li> <li>Serialization and De-serialization (required)<ul> <li><code>flow_to_model</code>: deserializes the     OpenML Flow into a model (if the library can indeed handle the     flow). This method has an important interplay with     <code>model_to_flow</code>. Running these     two methods in succession should result in exactly the same     model (or flow). This property can be used for unit testing     (e.g., build a model with hyperparameters, make predictions on a     task, serialize it to a flow, deserialize it back, make it     predict on the same task, and check whether the predictions are     exactly the same.) The example in the scikit-learn interface     might seem daunting, but note that here some complicated design     choices were made, that allow for all sorts of interesting     research questions. It is probably good practice to start easy.</li> <li><code>model_to_flow</code>: The inverse of     <code>flow_to_model</code>. Serializes a     model into an OpenML Flow. The flow should preserve the class,     the library version, and the tunable hyperparameters.</li> <li><code>get_version_information</code>: Return     a tuple with the version information of the important libraries.</li> <li><code>create_setup_string</code>: No longer     used, and will be deprecated soon.</li> </ul> </li> <li>Performing runs (required)<ul> <li><code>is_estimator</code>: Gets as input a     class, and checks whether it has the status of estimator in the     library (typically, whether it has a train method and a predict     method).</li> <li><code>seed_model</code>: Sets a random seed     to the model.</li> <li><code>_run_model_on_fold</code>: One of the     main requirements for a library to generate run objects for the     OpenML server. Obtains a train split (with labels) and a test     split (without labels) and the goal is to train a model on the     train split and return the predictions on the test split. On top     of the actual predictions, also the class probabilities should     be determined. For classifiers that do not return class     probabilities, this can just be the hot-encoded predicted label.     The predictions will be evaluated on the OpenML server. Also,     additional information can be returned, for example,     user-defined measures (such as runtime information, as this can     not be inferred on the server). Additionally, information about     a hyperparameter optimization trace can be provided.</li> <li><code>obtain_parameter_values</code>:     Obtains the hyperparameters of a given model and the current     values. Please note that in the case of a hyperparameter     optimization procedure (e.g., random search), you only should     return the hyperparameters of this procedure (e.g., the     hyperparameter grid, budget, etc) and that the chosen model will     be inferred from the optimization trace.</li> <li><code>check_if_model_fitted</code>: Check     whether the train method of the model has been called (and as     such, whether the predict method can be used).</li> </ul> </li> <li>Hyperparameter optimization (optional)<ul> <li><code>instantiate_model_from_hpo_class</code>{.interpreted-text     role=\"meth\"}: If a given run has recorded the hyperparameter     optimization trace, then this method can be used to     reinstantiate the model with hyperparameters of a given     hyperparameter optimization iteration. Has some similarities     with <code>flow_to_model</code> (as this     method also sets the hyperparameters of a model). Note that     although this method is required, it is not necessary to     implement any logic if hyperparameter optimization is not     implemented. Simply raise a [NotImplementedError]{.title-ref}     then.</li> </ul> </li> </ul>"},{"location":"extensions/#hosting-the-library","title":"Hosting the library","text":"<p>Each extension created should be a stand-alone repository, compatible with the OpenML-Python repository. The extension repository should work off-the-shelf with OpenML-Python installed.</p> <p>Create a public Github repo with the following directory structure:</p> <pre><code>| [repo name]\n|    |-- [extension name]\n|    |    |-- __init__.py\n|    |    |-- extension.py\n|    |    |-- config.py (optionally)\n</code></pre>"},{"location":"extensions/#recommended","title":"Recommended","text":"<ul> <li>Test cases to keep the extension up to date with the     [openml-python]{.title-ref} upstream changes.</li> <li>Documentation of the extension API, especially if any new     functionality added to OpenML-Python\\'s extension design.</li> <li>Examples to show how the new extension interfaces and works with     OpenML-Python.</li> <li>Create a PR to add the new extension to the OpenML-Python API     documentation.</li> </ul> <p>Happy contributing!</p>"},{"location":"progress/","title":"Changelog","text":""},{"location":"progress/#next","title":"next","text":"<ul> <li>MAINT #1340: Add Numpy 2.0 support. Update tests to work with     scikit-learn \\&lt;= 1.5.</li> <li>ADD #1342: Add HTTP header to requests to indicate they are from     openml-python.</li> </ul>"},{"location":"progress/#0142","title":"0.14.2","text":"<ul> <li>MAINT #1280: Use the server-provided <code>parquet_url</code> instead of     <code>minio_url</code> to determine the location of the parquet file.</li> <li>ADD #716: add documentation for remaining attributes of classes     and functions.</li> <li>ADD #1261: more annotations for type hints.</li> <li>MAINT #1294: update tests to new tag specification.</li> <li>FIX #1314: Update fetching a bucket from MinIO.</li> <li>FIX #1315: Make class label retrieval more lenient.</li> <li>ADD #1316: add feature descriptions ontologies support.</li> <li>MAINT #1310/#1307: switch to ruff and resolve all mypy errors.</li> </ul>"},{"location":"progress/#0141","title":"0.14.1","text":"<ul> <li>FIX: Fallback on downloading ARFF when failing to download parquet     from MinIO due to a ServerError.</li> </ul>"},{"location":"progress/#0140","title":"0.14.0","text":"<p>IMPORTANT: This release paves the way towards a breaking update of OpenML-Python. From version 0.15, functions that had the option to return a pandas DataFrame will return a pandas DataFrame by default. This version (0.14) emits a warning if you still use the old access functionality. More concretely:</p> <ul> <li>In 0.15 we will drop the ability to return dictionaries in listing     calls and only provide pandas DataFrames. To disable warnings in     0.14 you have to request a pandas DataFrame (using     <code>output_format=\"dataframe\"</code>).</li> <li>In 0.15 we will drop the ability to return datasets as numpy arrays     and only provide pandas DataFrames. To disable warnings in 0.14 you     have to request a pandas DataFrame (using     <code>dataset_format=\"dataframe\"</code>).</li> </ul> <p>Furthermore, from version 0.15, OpenML-Python will no longer download datasets and dataset metadata by default. This version (0.14) emits a warning if you don\\'t explicitly specifiy the desired behavior.</p> <p>Please see the pull requests #1258 and #1260 for further information.</p> <ul> <li>ADD #1081: New flag that allows disabling downloading dataset     features.</li> <li>ADD #1132: New flag that forces a redownload of cached data.</li> <li>FIX #1244: Fixes a rare bug where task listing could fail when the     server returned invalid data.</li> <li>DOC #1229: Fixes a comment string for the main example.</li> <li>DOC #1241: Fixes a comment in an example.</li> <li>MAINT #1124: Improve naming of helper functions that govern the     cache directories.</li> <li>MAINT #1223, #1250: Update tools used in pre-commit to the latest     versions (<code>black==23.30</code>, <code>mypy==1.3.0</code>, <code>flake8==6.0.0</code>).</li> <li>MAINT #1253: Update the citation request to the JMLR paper.</li> <li>MAINT #1246: Add a warning that warns the user that checking for     duplicate runs on the server cannot be done without an API key.</li> </ul>"},{"location":"progress/#0131","title":"0.13.1","text":"<ul> <li>ADD #1081 #1132: Add additional options for (not) downloading     datasets <code>openml.datasets.get_dataset</code> and cache management.</li> <li>ADD #1028: Add functions to delete runs, flows, datasets, and tasks     (e.g., <code>openml.datasets.delete_dataset</code>).</li> <li>ADD #1144: Add locally computed results to the <code>OpenMLRun</code> object\\'s     representation if the run was created locally and not downloaded     from the server.</li> <li>ADD #1180: Improve the error message when the checksum of a     downloaded dataset does not match the checksum provided by the API.</li> <li>ADD #1201: Make <code>OpenMLTraceIteration</code> a dataclass.</li> <li>DOC #1069: Add argument documentation for the <code>OpenMLRun</code> class.</li> <li>DOC #1241 #1229 #1231: Minor documentation fixes and resolve     documentation examples not working.</li> <li>FIX #1197 #559 #1131: Fix the order of ground truth and predictions     in the <code>OpenMLRun</code> object and in <code>format_prediction</code>.</li> <li>FIX #1198: Support numpy 1.24 and higher.</li> <li>FIX #1216: Allow unknown task types on the server. This is only     relevant when new task types are added to the test server.</li> <li>FIX #1223: Fix mypy errors for implicit optional typing.</li> <li>MAINT #1155: Add dependabot github action to automatically update     other github actions.</li> <li>MAINT #1199: Obtain pre-commit\\'s flake8 from github.com instead of     gitlab.com.</li> <li>MAINT #1215: Support latest numpy version.</li> <li>MAINT #1218: Test Python3.6 on Ubuntu 20.04 instead of the latest     Ubuntu (which is 22.04).</li> <li>MAINT #1221 #1212 #1206 #1211: Update github actions to the latest     versions.</li> </ul>"},{"location":"progress/#0130","title":"0.13.0","text":"<ul> <li>FIX #1030: <code>pre-commit</code> hooks now no longer should issue a     warning.</li> <li>FIX #1058, #1100: Avoid <code>NoneType</code> error when printing task     without <code>class_labels</code> attribute.</li> <li>FIX #1110: Make arguments to <code>create_study</code> and <code>create_suite</code>     that are defined as optional by the OpenML XSD actually optional.</li> <li>FIX #1147: <code>openml.flow.flow_exists</code> no longer requires an API     key.</li> <li>FIX #1184: Automatically resolve proxies when downloading from     minio. Turn this off by setting environment variable     <code>no_proxy=\"*\"</code>.</li> <li>MAINT #1088: Do CI for Windows on Github Actions instead of     Appveyor.</li> <li>MAINT #1104: Fix outdated docstring for <code>list_task</code>.</li> <li>MAINT #1146: Update the pre-commit dependencies.</li> <li>ADD #1103: Add a <code>predictions</code> property to OpenMLRun for easy     accessibility of prediction data.</li> <li>ADD #1188: EXPERIMENTAL. Allow downloading all files from a minio     bucket with <code>download_all_files=True</code> for <code>get_dataset</code>.</li> </ul>"},{"location":"progress/#0122","title":"0.12.2","text":"<ul> <li>ADD #1065: Add a <code>retry_policy</code> configuration option that determines     the frequency and number of times to attempt to retry server     requests.</li> <li>ADD #1075: A docker image is now automatically built on a push to     develop. It can be used to build docs or run tests in an isolated     environment.</li> <li>ADD: You can now avoid downloading \\'qualities\\' meta-data when     downloading a task with the <code>download_qualities</code> parameter of     <code>openml.tasks.get_task[s]</code> functions.</li> <li>DOC: Fixes a few broken links in the documentation.</li> <li>DOC #1061: Improve examples to always show a warning when they     switch to the test server.</li> <li>DOC #1067: Improve documentation on the scikit-learn extension     interface.</li> <li>DOC #1068: Create dedicated extensions page.</li> <li>FIX #1075: Correctly convert [y]{.title-ref} to a pandas series when     downloading sparse data.</li> <li>MAINT: Rename [master]{.title-ref} brach to [ main]{.title-ref}     branch.</li> <li>MAINT/DOC: Automatically check for broken external links when     building the documentation.</li> <li>MAINT/DOC: Fail documentation building on warnings. This will make     the documentation building fail if a reference cannot be found (i.e.     an internal link is broken).</li> </ul>"},{"location":"progress/#0121","title":"0.12.1","text":"<ul> <li>ADD #895/#1038: Measure runtimes of scikit-learn runs also for     models which are parallelized via the joblib.</li> <li>DOC #1050: Refer to the webpage instead of the XML file in the main     example.</li> <li>DOC #1051: Document existing extensions to OpenML-Python besides the     shipped scikit-learn extension.</li> <li>FIX #1035: Render class attributes and methods again.</li> <li>ADD #1049: Add a command line tool for configuration openml-python.</li> <li>FIX #1042: Fixes a rare concurrency issue with OpenML-Python and     joblib which caused the joblib worker pool to fail.</li> <li>FIX #1053: Fixes a bug which could prevent importing the package in     a docker container.</li> </ul>"},{"location":"progress/#0120","title":"0.12.0","text":"<ul> <li>ADD #964: Validate <code>ignore_attribute</code>, <code>default_target_attribute</code>,     <code>row_id_attribute</code> are set to attributes that exist on the dataset     when calling <code>create_dataset</code>.</li> <li>ADD #979: Dataset features and qualities are now also cached in     pickle format.</li> <li>ADD #982: Add helper functions for column transformers.</li> <li>ADD #989: <code>run_model_on_task</code> will now warn the user the the model     passed has already been fitted.</li> <li>ADD #1009 : Give possibility to not download the dataset qualities.     The cached version is used even so download attribute is false.</li> <li>ADD #1016: Add scikit-learn 0.24 support.</li> <li>ADD #1020: Add option to parallelize evaluation of tasks with     joblib.</li> <li>ADD #1022: Allow minimum version of dependencies to be listed for a     flow, use more accurate minimum versions for scikit-learn     dependencies.</li> <li>ADD #1023: Add admin-only calls for adding topics to datasets.</li> <li>ADD #1029: Add support for fetching dataset from a minio server in     parquet format.</li> <li>ADD #1031: Generally improve runtime measurements, add them for some     previously unsupported flows (e.g. BaseSearchCV derived flows).</li> <li>DOC #973 : Change the task used in the welcome page example so it no     longer fails using numerical dataset.</li> <li>MAINT #671: Improved the performance of <code>check_datasets_active</code> by     only querying the given list of datasets in contrast to querying all     datasets. Modified the corresponding unit test.</li> <li>MAINT #891: Changed the way that numerical features are stored.     Numerical features that range from 0 to 255 are now stored as uint8,     which reduces the storage space required as well as storing and     loading times.</li> <li>MAINT #975, #988: Add CI through Github Actions.</li> <li>MAINT #977: Allow <code>short</code> and <code>long</code> scenarios for unit tests.     Reduce the workload for some unit tests.</li> <li>MAINT #985, #1000: Improve unit test stability and output     readability, and adds load balancing.</li> <li>MAINT #1018: Refactor data loading and storage. Data is now     compressed on the first call to [get_data]{.title-ref}.</li> <li>MAINT #1024: Remove flaky decorator for study unit test.</li> <li>FIX #883 #884 #906 #972: Various improvements to the caching system.</li> <li>FIX #980: Speed up <code>check_datasets_active</code>.</li> <li>FIX #984: Add a retry mechanism when the server encounters a     database issue.</li> <li>FIX #1004: Fixed an issue that prevented installation on some     systems (e.g. Ubuntu).</li> <li>FIX #1013: Fixes a bug where <code>OpenMLRun.setup_string</code> was not     uploaded to the server, prepares for <code>run_details</code> being sent from     the server.</li> <li>FIX #1021: Fixes an issue that could occur when running unit tests     and openml-python was not in PATH.</li> <li>FIX #1037: Fixes a bug where a dataset could not be loaded if a     categorical value had listed nan-like as a possible category.</li> </ul>"},{"location":"progress/#0110","title":"0.11.0","text":"<ul> <li>ADD #753: Allows uploading custom flows to OpenML via OpenML-Python.</li> <li>ADD #777: Allows running a flow on pandas dataframes (in addition to     numpy arrays).</li> <li>ADD #888: Allow passing a [task_id]{.title-ref} to     [run_model_on_task]{.title-ref}.</li> <li>ADD #894: Support caching of datasets using feather format as an     option.</li> <li>ADD #929: Add <code>edit_dataset</code> and <code>fork_dataset</code> to allow editing and     forking of uploaded datasets.</li> <li>ADD #866, #943: Add support for scikit-learn\\'s     [passthrough]{.title-ref} and [drop]{.title-ref} when uploading     flows to OpenML.</li> <li>ADD #879: Add support for scikit-learn\\'s MLP hyperparameter     [layer_sizes]{.title-ref}.</li> <li>ADD #894: Support caching of datasets using feather format as an     option.</li> <li>ADD #945: PEP 561 compliance for distributing Type information.</li> <li>DOC #660: Remove nonexistent argument from docstring.</li> <li>DOC #901: The API reference now documents the config file and its     options.</li> <li>DOC #912: API reference now shows [create_task]{.title-ref}.</li> <li>DOC #954: Remove TODO text from documentation.</li> <li>DOC #960: document how to upload multiple ignore attributes.</li> <li>FIX #873: Fixes an issue which resulted in incorrect URLs when     printing OpenML objects after switching the server.</li> <li>FIX #885: Logger no longer registered by default. Added utility     functions to easily register logging to console and file.</li> <li>FIX #890: Correct the scaling of data in the SVM example.</li> <li>MAINT #371: <code>list_evaluations</code> default <code>size</code> changed from <code>None</code> to     <code>10_000</code>.</li> <li>MAINT #767: Source distribution installation is now unit-tested.</li> <li>MAINT #781: Add pre-commit and automated code formatting with black.</li> <li>MAINT #804: Rename arguments of list_evaluations to indicate they     expect lists of ids.</li> <li>MAINT #836: OpenML supports only pandas version 1.0.0 or above.</li> <li>MAINT #865: OpenML no longer bundles test files in the source     distribution.</li> <li>MAINT #881: Improve the error message for too-long URIs.</li> <li>MAINT #897: Dropping support for Python 3.5.</li> <li>MAINT #916: Adding support for Python 3.8.</li> <li>MAINT #920: Improve error messages for dataset upload.</li> <li>MAINT #921: Improve hangling of the OpenML server URL in the config     file.</li> <li>MAINT #925: Improve error handling and error message when loading     datasets.</li> <li>MAINT #928: Restructures the contributing documentation.</li> <li>MAINT #936: Adding support for scikit-learn 0.23.X.</li> <li>MAINT #945: Make OpenML-Python PEP562 compliant.</li> <li>MAINT #951: Converts TaskType class to a TaskType enum.</li> </ul>"},{"location":"progress/#0102","title":"0.10.2","text":"<ul> <li>ADD #857: Adds task type ID to list_runs</li> <li>DOC #862: Added license BSD 3-Clause to each of the source files.</li> </ul>"},{"location":"progress/#0101","title":"0.10.1","text":"<ul> <li>ADD #175: Automatically adds the docstring of scikit-learn objects     to flow and its parameters.</li> <li>ADD #737: New evaluation listing call that includes the     hyperparameter settings.</li> <li>ADD #744: It is now possible to only issue a warning and not raise     an exception if the package versions for a flow are not met when     deserializing it.</li> <li>ADD #783: The URL to download the predictions for a run is now     stored in the run object.</li> <li>ADD #790: Adds the uploader name and id as new filtering options for     <code>list_evaluations</code>.</li> <li>ADD #792: New convenience function <code>openml.flow.get_flow_id</code>.</li> <li>ADD #861: Debug-level log information now being written to a file in     the cache directory (at most 2 MB).</li> <li>DOC #778: Introduces instructions on how to publish an extension to     support other libraries than scikit-learn.</li> <li>DOC #785: The examples section is completely restructured into     simple simple examples, advanced examples and examples showcasing     the use of OpenML-Python to reproduce papers which were done with     OpenML-Python.</li> <li>DOC #788: New example on manually iterating through the split of a     task.</li> <li>DOC #789: Improve the usage of dataframes in the examples.</li> <li>DOC #791: New example for the paper Efficient and Robust Automated     Machine Learning by Feurer et al. (2015).</li> <li>DOC #803: New example for the paper Don't Rule Out Simple Models     Prematurely: A Large Scale Benchmark Comparing Linear and Non-linear     Classifiers in OpenML by Benjamin Strang et al. (2018).</li> <li>DOC #808: New example demonstrating basic use cases of a dataset.</li> <li>DOC #810: New example demonstrating the use of benchmarking studies     and suites.</li> <li>DOC #832: New example for the paper Scalable Hyperparameter     Transfer Learning by Valerio Perrone et al. (2019)</li> <li>DOC #834: New example showing how to plot the loss surface for a     support vector machine.</li> <li>FIX #305: Do not require the external version in the flow XML when     loading an object.</li> <li>FIX #734: Better handling of \\\"old\\\" flows.</li> <li>FIX #736: Attach a StreamHandler to the openml logger instead of the     root logger.</li> <li>FIX #758: Fixes an error which made the client API crash when     loading a sparse data with categorical variables.</li> <li>FIX #779: Do not fail on corrupt pickle</li> <li>FIX #782: Assign the study id to the correct class attribute.</li> <li>FIX #819: Automatically convert column names to type string when     uploading a dataset.</li> <li>FIX #820: Make <code>__repr__</code> work for datasets which do not have an id.</li> <li>MAINT #796: Rename an argument to make the function     <code>list_evaluations</code> more consistent.</li> <li>MAINT #811: Print the full error message given by the server.</li> <li>MAINT #828: Create base class for OpenML entity classes.</li> <li>MAINT #829: Reduce the number of data conversion warnings.</li> <li>MAINT #831: Warn if there\\'s an empty flow description when     publishing a flow.</li> <li>MAINT #837: Also print the flow XML if a flow fails to validate.</li> <li>FIX #838: Fix list_evaluations_setups to work when evaluations are     not a 100 multiple.</li> <li>FIX #847: Fixes an issue where the client API would crash when     trying to download a dataset when there are no qualities available     on the server.</li> <li>MAINT #849: Move logic of most different <code>publish</code> functions into     the base class.</li> <li>MAINt #850: Remove outdated test code.</li> </ul>"},{"location":"progress/#0100","title":"0.10.0","text":"<ul> <li>ADD #737: Add list_evaluations_setups to return hyperparameters     along with list of evaluations.</li> <li>FIX #261: Test server is cleared of all files uploaded during unit     testing.</li> <li>FIX #447: All files created by unit tests no longer persist in     local.</li> <li>FIX #608: Fixing dataset_id referenced before assignment error in     get_run function.</li> <li>FIX #447: All files created by unit tests are deleted after the     completion of all unit tests.</li> <li>FIX #589: Fixing a bug that did not successfully upload the columns     to ignore when creating and publishing a dataset.</li> <li>FIX #608: Fixing dataset_id referenced before assignment error in     get_run function.</li> <li>DOC #639: More descriptive documention for function to convert array     format.</li> <li>DOC #719: Add documentation on uploading tasks.</li> <li>ADD #687: Adds a function to retrieve the list of evaluation     measures available.</li> <li>ADD #695: A function to retrieve all the data quality measures     available.</li> <li>ADD #412: Add a function to trim flow names for scikit-learn flows.</li> <li>ADD #715: [list_evaluations]{.title-ref} now has an option to sort     evaluations by score (value).</li> <li>ADD #722: Automatic reinstantiation of flow in     [run_model_on_task]{.title-ref}. Clearer errors if that\\'s not     possible.</li> <li>ADD #412: The scikit-learn extension populates the short name field     for flows.</li> <li>MAINT #726: Update examples to remove deprecation warnings from     scikit-learn</li> <li>MAINT #752: Update OpenML-Python to be compatible with sklearn 0.21</li> <li>ADD #790: Add user ID and name to list_evaluations</li> </ul>"},{"location":"progress/#090","title":"0.9.0","text":"<ul> <li>ADD #560: OpenML-Python can now handle regression tasks as well.</li> <li>ADD #620, #628, #632, #649, #682: Full support for studies and     distinguishes suites from studies.</li> <li>ADD #607: Tasks can now be created and uploaded.</li> <li>ADD #647, #673: Introduced the extension interface. This provides an     easy way to create a hook for machine learning packages to perform     e.g. automated runs.</li> <li>ADD #548, #646, #676: Support for Pandas DataFrame and     SparseDataFrame</li> <li>ADD #662: Results of listing functions can now be returned as     pandas.DataFrame.</li> <li>ADD #59: Datasets can now also be retrieved by name.</li> <li>ADD #672: Add timing measurements for runs, when possible.</li> <li>ADD #661: Upload time and error messages now displayed with     [list_runs]{.title-ref}.</li> <li>ADD #644: Datasets can now be downloaded \\'lazily\\', retrieving only     metadata at first, and the full dataset only when necessary.</li> <li>ADD #659: Lazy loading of task splits.</li> <li>ADD #516: [run_flow_on_task]{.title-ref} flow uploading is now     optional.</li> <li>ADD #680: Adds     [openml.config.start_using_configuration_for_example]{.title-ref}     (and resp. stop) to easily connect to the test server.</li> <li>ADD #75, #653: Adds a pretty print for objects of the top-level     classes.</li> <li>FIX #642: [check_datasets_active]{.title-ref} now correctly also     returns active status of deactivated datasets.</li> <li>FIX #304, #636: Allow serialization of numpy datatypes and list of     lists of more types (e.g. bools, ints) for flows.</li> <li>FIX #651: Fixed a bug that would prevent openml-python from finding     the user\\'s config file.</li> <li>FIX #693: OpenML-Python uses liac-arff instead of scipy.io for     loading task splits now.</li> <li>DOC #678: Better color scheme for code examples in documentation.</li> <li>DOC #681: Small improvements and removing list of missing functions.</li> <li>DOC #684: Add notice to examples that connect to the test server.</li> <li>DOC #688: Add new example on retrieving evaluations.</li> <li>DOC #691: Update contributing guidelines to use Github draft feature     instead of tags in title.</li> <li>DOC #692: All functions are documented now.</li> <li>MAINT #184: Dropping Python2 support.</li> <li>MAINT #596: Fewer dependencies for regular pip install.</li> <li>MAINT #652: Numpy and Scipy are no longer required before     installation.</li> <li>MAINT #655: Lazy loading is now preferred in unit tests.</li> <li>MAINT #667: Different tag functions now share code.</li> <li>MAINT #666: More descriptive error message for     [TypeError]{.title-ref} in [list_runs]{.title-ref}.</li> <li>MAINT #668: Fix some type hints.</li> <li>MAINT #677: [dataset.get_data]{.title-ref} now has consistent     behavior in its return type.</li> <li>MAINT #686: Adds ignore directives for several [mypy]{.title-ref}     folders.</li> <li>MAINT #629, #630: Code now adheres to single PEP8 standard.</li> </ul>"},{"location":"progress/#080","title":"0.8.0","text":"<ul> <li>ADD #440: Improved dataset upload.</li> <li>ADD #545, #583: Allow uploading a dataset from a pandas DataFrame.</li> <li>ADD #528: New functions to update the status of a dataset.</li> <li>ADD #523: Support for scikit-learn 0.20\\'s new ColumnTransformer.</li> <li>ADD #459: Enhanced support to store runs on disk prior to uploading     them to OpenML.</li> <li>ADD #564: New helpers to access the structure of a flow (and find     its subflows).</li> <li>ADD #618: The software will from now on retry to connect to the     server if a connection failed. The number of retries can be     configured.</li> <li>FIX #538: Support loading clustering tasks.</li> <li>FIX #464: Fixes a bug related to listing functions (returns correct     listing size).</li> <li>FIX #580: Listing function now works properly when there are less     results than requested.</li> <li>FIX #571: Fixes an issue where tasks could not be downloaded in     parallel.</li> <li>FIX #536: Flows can now be printed when the flow name is None.</li> <li>FIX #504: Better support for hierarchical hyperparameters when     uploading scikit-learn\\'s grid and random search.</li> <li>FIX #569: Less strict checking of flow dependencies when loading     flows.</li> <li>FIX #431: Pickle of task splits are no longer cached.</li> <li>DOC #540: More examples for dataset uploading.</li> <li>DOC #554: Remove the doubled progress entry from the docs.</li> <li>MAINT #613: Utilize the latest updates in OpenML evaluation     listings.</li> <li>MAINT #482: Cleaner interface for handling search traces.</li> <li>MAINT #557: Continuous integration works for scikit-learn 0.18-0.20.</li> <li>MAINT #542: Continuous integration now runs python3.7 as well.</li> <li>MAINT #535: Continuous integration now enforces PEP8 compliance for     new code.</li> <li>MAINT #527: Replace deprecated nose by pytest.</li> <li>MAINT #510: Documentation is now built by travis-ci instead of     circle-ci.</li> <li>MAINT: Completely re-designed documentation built on sphinx gallery.</li> <li>MAINT #462: Appveyor CI support.</li> <li>MAINT #477: Improve error handling for issue     #479: the OpenML     connector fails earlier and with a better error message when failing     to create a flow from the OpenML description.</li> <li>MAINT #561: Improve documentation on running specific unit tests.</li> </ul>"},{"location":"progress/#04-07","title":"0.4.-0.7","text":"<p>There is no changelog for these versions.</p>"},{"location":"progress/#030","title":"0.3.0","text":"<ul> <li>Add this changelog</li> <li>2nd example notebook PyOpenML.ipynb</li> <li>Pagination support for list datasets and list tasks</li> </ul>"},{"location":"progress/#prior","title":"Prior","text":"<p>There is no changelog for prior versions.</p>"},{"location":"usage/","title":"User Guide","text":"<p>This document will guide you through the most important use cases, functions and classes in the OpenML Python API. Throughout this document, we will use pandas to format and filter tables.</p>"},{"location":"usage/#installation","title":"Installation","text":"<p>The OpenML Python package is a connector to OpenML. It allows you to use and share datasets and tasks, run machine learning algorithms on them and then share the results online.</p> <p>The \"intruduction tutorial and setup\" tutorial gives a short introduction on how to install and set up the OpenML Python connector, followed up by a simple example.</p>"},{"location":"usage/#configuration","title":"Configuration","text":"<p>The configuration file resides in a directory <code>.config/openml</code> in the home directory of the user and is called config (More specifically, it resides in the configuration directory specified by the XDGB Base Directory Specification). It consists of <code>key = value</code> pairs which are separated by newlines. The following keys are defined:</p> <ul> <li>apikey: required to access the server. The introduction tutorial describes how to obtain an API key.</li> <li>server: the server to connect to (default: <code>http://www.openml.org</code>).           For connection to the test server, set this to <code>test.openml.org</code>.</li> <li>cachedir: the root folder where the cache file directories should be created.     If not given, will default to <code>~/.openml/cache</code></li> <li>avoid_duplicate_runs: if set to <code>True</code> (default), when <code>run_flow_on_task</code> or similar methods             are called a lookup is performed to see if there already             exists such a run on the server. If so, download those             results instead.</li> <li> <p>retry_policy: Defines how to react when the server is unavailable or             experiencing high load. It determines both how often to             attempt to reconnect and how quickly to do so. Please don't             use <code>human</code> in an automated script that you run more than             one instance of, it might increase the time to complete your             jobs and that of others. One of:             -   human (default): For people running openml in interactive                 fashion. Try only a few times, but in quick succession.             -   robot: For people using openml in an automated fashion. Keep                 trying to reconnect for a longer time, quickly increasing                 the time between retries.</p> </li> <li> <p>connection_n_retries: number of times to retry a request if they fail.  Default depends on retry_policy (5 for <code>human</code>, 50 for <code>robot</code>)</p> </li> <li>verbosity: the level of output:<ul> <li>0: normal output</li> <li>1: info output</li> <li>2: debug output</li> </ul> </li> </ul> <p>This file is easily configurable by the <code>openml</code> command line interface. To see where the file is stored, and what its values are, use openml configure none. </p>"},{"location":"usage/#docker","title":"Docker","text":"<p>It is also possible to try out the latest development version of <code>openml-python</code> with docker:</p> <pre><code>docker run -it openml/openml-python\n</code></pre> <p>See the openml-python docker documentation for more information.</p>"},{"location":"usage/#key-concepts","title":"Key concepts","text":"<p>OpenML contains several key concepts which it needs to make machine learning research shareable. A machine learning experiment consists of one or several runs, which describe the performance of an algorithm (called a flow in OpenML), its hyperparameter settings (called a setup) on a task. A Task is the combination of a dataset, a split and an evaluation metric. In this user guide we will go through listing and exploring existing tasks to actually running machine learning algorithms on them. In a further user guide we will examine how to search through datasets in order to curate a list of tasks.</p> <p>A further explanation is given in the OpenML user guide.</p>"},{"location":"usage/#working-with-tasks","title":"Working with tasks","text":"<p>You can think of a task as an experimentation protocol, describing how to apply a machine learning model to a dataset in a way that is comparable with the results of others (more on how to do that further down). Tasks are containers, defining which dataset to use, what kind of task we\\'re solving (regression, classification, clustering, etc...) and which column to predict. Furthermore, it also describes how to split the dataset into a train and test set, whether to use several disjoint train and test splits (cross-validation) and whether this should be repeated several times. Also, the task defines a target metric for which a flow should be optimized.</p> <p>If you want to know more about tasks, try the \"Task tutorial\"</p>"},{"location":"usage/#running-machine-learning-algorithms-and-uploading-results","title":"Running machine learning algorithms and uploading results","text":"<p>In order to upload and share results of running a machine learning algorithm on a task, we need to create an openml.runs.OpenMLRun. A run object can be created by running a openml.flows.OpenMLFlow or a scikit-learn compatible model on a task. We will focus on the simpler example of running a scikit-learn model.</p> <p>Flows are descriptions of something runnable which does the machine learning. A flow contains all information to set up the necessary machine learning library and its dependencies as well as all possible parameters.</p> <p>A run is the outcome of running a flow on a task. It contains all parameter settings for the flow, a setup string (most likely a command line call) and all predictions of that run. When a run is uploaded to the server, the server automatically calculates several metrics which can be used to compare the performance of different flows to each other.</p> <p>So far, the OpenML Python connector works only with estimator objects following the scikit-learn estimator API. Those can be directly run on a task, and a flow will automatically be created or downloaded from the server if it already exists.</p> <p>See \"Simple Flows and Runs\" for a tutorial covers how to train different machine learning models, how to run machine learning models on OpenML data and how to share the results.</p>"},{"location":"usage/#datasets","title":"Datasets","text":"<p>OpenML provides a large collection of datasets and the benchmark OpenML100 which consists of a curated list of datasets.</p> <p>You can find the dataset that best fits your requirements by making use of the available metadata. The tutorial \"extended datasets\" which follows explains how to get a list of datasets, how to filter the list to find the dataset that suits your requirements and how to download a dataset.</p> <p>OpenML is about sharing machine learning results and the datasets they were obtained on. Learn how to share your datasets in the following tutorial \"Upload\" tutorial.</p>"},{"location":"usage/#extending-openml-python","title":"Extending OpenML-Python","text":"<p>OpenML-Python provides an extension interface to connect machine learning libraries directly to the API and ships a <code>scikit-learn</code> extension. Read more about them in the \"Extensions\" section.</p>"},{"location":"examples/SUMMARY/","title":"SUMMARY","text":"<ul> <li>20_basic<ul> <li>introduction_tutorial.py</li> <li>simple_datasets_tutorial.py</li> <li>simple_flows_and_runs_tutorial.py</li> <li>simple_suites_tutorial.py</li> </ul> </li> <li>30_extended<ul> <li>benchmark_with_optunahub.py</li> <li>configure_logging.py</li> <li>create_upload_tutorial.py</li> <li>custom_flow_.py</li> <li>datasets_tutorial.py</li> <li>fetch_evaluations_tutorial.py</li> <li>fetch_runtimes_tutorial.py</li> <li>flow_id_tutorial.py</li> <li>flows_and_runs_tutorial.py</li> <li>plot_svm_hyperparameters_tutorial.py</li> <li>run_setup_tutorial.py</li> <li>study_tutorial.py</li> <li>suites_tutorial.py</li> <li>task_manual_iteration_tutorial.py</li> <li>tasks_tutorial.py</li> </ul> </li> <li>40_paper<ul> <li>2015_neurips_feurer_example.py</li> <li>2018_ida_strang_example.py</li> <li>2018_kdd_rijn_example.py</li> <li>2018_neurips_perrone_example.py</li> </ul> </li> </ul>"},{"location":"examples/20_basic/introduction_tutorial/","title":"Introduction tutorial &amp; Setup","text":"<p>OpenML is an online collaboration platform for machine learning which allows you to:</p> <ul> <li>Find or share interesting, well-documented datasets</li> <li>Define research / modelling goals (tasks)</li> <li>Explore large amounts of machine learning algorithms, with APIs in Java, R, Python</li> <li>Log and share reproducible experiments, models, results</li> <li>Works seamlessly with scikit-learn and other libraries</li> <li>Large scale benchmarking, compare to state of the art</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import openml\nfrom sklearn import neighbors\n</pre>  import openml from sklearn import neighbors <p>Warning</p> <p>         This example uploads data. For that reason, this example connects to the         test server at test.openml.org.         This prevents the main server from becoming overloaded with example datasets, tasks,         runs, and other submissions.         Using this test server may affect the behavior and performance of the         OpenML-Python API.     </p> In\u00a0[\u00a0]: Copied! <pre># openml.config.start_using_configuration_for_example()\n</pre> # openml.config.start_using_configuration_for_example() <p>When using the main server instead, make sure your apikey is configured. This can be done with the following line of code (uncomment it!). Never share your apikey with others.</p> In\u00a0[\u00a0]: Copied! <pre># openml.config.apikey = 'YOURKEY'\n</pre> # openml.config.apikey = 'YOURKEY' In\u00a0[\u00a0]: Copied! <pre># Uncomment and set your OpenML cache directory\n# import os\n# openml.config.cache_directory = os.path.expanduser('YOURDIR')\nopenml.config.set_root_cache_directory(\"YOURDIR\")\n</pre> # Uncomment and set your OpenML cache directory # import os # openml.config.cache_directory = os.path.expanduser('YOURDIR') openml.config.set_root_cache_directory(\"YOURDIR\") In\u00a0[\u00a0]: Copied! <pre>task = openml.tasks.get_task(403)\nclf = neighbors.KNeighborsClassifier(n_neighbors=5)\nopenml.config.start_using_configuration_for_example()\n\nrun = openml.runs.run_model_on_task(clf, task, avoid_duplicate_runs=False)\n# Publish the experiment on OpenML (optional, requires an API key).\n# For this tutorial, our configuration publishes to the test server\n# as to not crowd the main server with runs created by examples.\nmyrun = run.publish()\n</pre> task = openml.tasks.get_task(403) clf = neighbors.KNeighborsClassifier(n_neighbors=5) openml.config.start_using_configuration_for_example()  run = openml.runs.run_model_on_task(clf, task, avoid_duplicate_runs=False) # Publish the experiment on OpenML (optional, requires an API key). # For this tutorial, our configuration publishes to the test server # as to not crowd the main server with runs created by examples. myrun = run.publish() In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n# License: BSD 3-Clause\n</pre> openml.config.stop_using_configuration_for_example() # License: BSD 3-Clause"},{"location":"examples/20_basic/introduction_tutorial/#introduction-tutorial-setup","title":"Introduction tutorial &amp; Setup\u00b6","text":"<p>An example how to set up OpenML-Python followed up by a simple example.</p>"},{"location":"examples/20_basic/introduction_tutorial/#installation","title":"Installation\u00b6","text":"<p>Installation is done via <code>pip</code>:</p> <pre>pip install openml\n</pre>"},{"location":"examples/20_basic/introduction_tutorial/#authentication","title":"Authentication\u00b6","text":"<p>The OpenML server can only be accessed by users who have signed up on the OpenML platform. If you don\u2019t have an account yet, sign up now. You will receive an API key, which will authenticate you to the server and allow you to download and upload datasets, tasks, runs and flows.</p> <ul> <li>Create an OpenML account (free) on https://www.openml.org.</li> <li>After logging in, open your account page (avatar on the top right)</li> <li>Open 'Account Settings', then 'API authentication' to find your API key.</li> </ul> <p>There are two ways to permanently authenticate:</p> <ul> <li>Use the <code>openml</code> CLI tool with <code>openml configure apikey MYKEY</code>, replacing MYKEY with your API key.</li> <li>Create a plain text file ~/.openml/config with the line 'apikey=MYKEY', replacing MYKEY with your API key. The config file must be in the directory ~/.openml/config and exist prior to importing the openml module.</li> </ul> <p>Alternatively, by running the code below and replacing 'YOURKEY' with your API key, you authenticate for the duration of the python process.</p>"},{"location":"examples/20_basic/introduction_tutorial/#caching","title":"Caching\u00b6","text":"<p>When downloading datasets, tasks, runs and flows, they will be cached to retrieve them without calling the server later. As with the API key, the cache directory can be either specified through the config file or through the API:</p> <ul> <li>Add the  line cachedir = 'MYDIR' to the config file, replacing 'MYDIR' with the path to the cache directory. By default, OpenML will use ~/.openml/cache as the cache directory.</li> <li>Run the code below, replacing 'YOURDIR' with the path to the cache directory.</li> </ul>"},{"location":"examples/20_basic/introduction_tutorial/#simple-example","title":"Simple Example\u00b6","text":"<p>Download the OpenML task for the eeg-eye-state.</p>"},{"location":"examples/20_basic/simple_datasets_tutorial/","title":"Datasets","text":"In\u00a0[\u00a0]: Copied! <pre>import openml\n</pre>  import openml In\u00a0[\u00a0]: Copied! <pre>datasets_df = openml.datasets.list_datasets(output_format=\"dataframe\")\nprint(datasets_df.head(n=10))\n</pre> datasets_df = openml.datasets.list_datasets(output_format=\"dataframe\") print(datasets_df.head(n=10)) In\u00a0[\u00a0]: Copied! <pre># Iris dataset https://www.openml.org/d/61\ndataset = openml.datasets.get_dataset(dataset_id=61, version=1)\n\n# Print a summary\nprint(\n    f\"This is dataset '{dataset.name}', the target feature is \"\n    f\"'{dataset.default_target_attribute}'\"\n)\nprint(f\"URL: {dataset.url}\")\nprint(dataset.description[:500])\n</pre> # Iris dataset https://www.openml.org/d/61 dataset = openml.datasets.get_dataset(dataset_id=61, version=1)  # Print a summary print(     f\"This is dataset '{dataset.name}', the target feature is \"     f\"'{dataset.default_target_attribute}'\" ) print(f\"URL: {dataset.url}\") print(dataset.description[:500]) In\u00a0[\u00a0]: Copied! <pre>X, y, categorical_indicator, attribute_names = dataset.get_data(\n    target=dataset.default_target_attribute\n)\n</pre> X, y, categorical_indicator, attribute_names = dataset.get_data(     target=dataset.default_target_attribute ) <p>Visualize the dataset</p> <p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; docs/mkdoc -- Incoming Change</p> In\u00a0[\u00a0]: Copied! <pre>=======\nimport matplotlib.pyplot as plt\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; develop -- Current Change\nimport pandas as pd\nimport seaborn as sns\n\nsns.set_style(\"darkgrid\")\n\n\ndef hide_current_axis(*args, **kwds):\n    plt.gca().set_visible(False)\n\n\n# We combine all the data so that we can map the different\n# examples to different colors according to the classes.\ncombined_data = pd.concat([X, y], axis=1)\niris_plot = sns.pairplot(combined_data, hue=\"class\")\niris_plot.map_upper(hide_current_axis)\nplt.show()\n\n# License: BSD 3-Clause\n</pre> ======= import matplotlib.pyplot as plt &gt;&gt;&gt;&gt;&gt;&gt;&gt; develop -- Current Change import pandas as pd import seaborn as sns  sns.set_style(\"darkgrid\")   def hide_current_axis(*args, **kwds):     plt.gca().set_visible(False)   # We combine all the data so that we can map the different # examples to different colors according to the classes. combined_data = pd.concat([X, y], axis=1) iris_plot = sns.pairplot(combined_data, hue=\"class\") iris_plot.map_upper(hide_current_axis) plt.show()  # License: BSD 3-Clause"},{"location":"examples/20_basic/simple_datasets_tutorial/#datasets","title":"Datasets\u00b6","text":"<p>A basic tutorial on how to list, load and visualize datasets.</p> <p>In general, we recommend working with tasks, so that the results can be easily reproduced. Furthermore, the results can be compared to existing results at OpenML. However, for the purposes of this tutorial, we are going to work with the datasets directly.</p>"},{"location":"examples/20_basic/simple_datasets_tutorial/#list-datasets","title":"List datasets\u00b6","text":""},{"location":"examples/20_basic/simple_datasets_tutorial/#download-a-dataset","title":"Download a dataset\u00b6","text":""},{"location":"examples/20_basic/simple_datasets_tutorial/#load-a-dataset","title":"Load a dataset\u00b6","text":"<p>X - An array/dataframe where each row represents one example with the corresponding feature values.</p> <p>y - the classes for each example</p> <p>categorical_indicator - an array that indicates which feature is categorical</p> <p>attribute_names - the names of the features for the examples (X) and target feature (y)</p>"},{"location":"examples/20_basic/simple_flows_and_runs_tutorial/","title":"Flows and Runs","text":"In\u00a0[\u00a0]: Copied! <pre>import openml\nfrom sklearn import ensemble, neighbors\n\nfrom openml.utils import thread_safe_if_oslo_installed\n</pre> import openml from sklearn import ensemble, neighbors  from openml.utils import thread_safe_if_oslo_installed <p>Warning</p> <p>         This example uploads data. For that reason, this example connects to the         test server at test.openml.org.         This prevents the main server from becoming overloaded with example datasets, tasks,         runs, and other submissions.         Using this test server may affect the behavior and performance of the         OpenML-Python API.     </p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() In\u00a0[\u00a0]: Copied! <pre>dataset = openml.datasets.get_dataset(20)\nX, y, categorical_indicator, attribute_names = dataset.get_data(\n    dataset_format=\"dataframe\", target=dataset.default_target_attribute\n)\nif y is None:\n    y = X[\"class\"]\n    X = X.drop(columns=[\"class\"], axis=1)\nclf = neighbors.KNeighborsClassifier(n_neighbors=3)\nclf.fit(X, y)\n</pre> dataset = openml.datasets.get_dataset(20) X, y, categorical_indicator, attribute_names = dataset.get_data(     dataset_format=\"dataframe\", target=dataset.default_target_attribute ) if y is None:     y = X[\"class\"]     X = X.drop(columns=[\"class\"], axis=1) clf = neighbors.KNeighborsClassifier(n_neighbors=3) clf.fit(X, y) In\u00a0[\u00a0]: Copied! <pre>task = openml.tasks.get_task(119)\n\nclf = ensemble.RandomForestClassifier()\nrun = openml.runs.run_model_on_task(clf, task)\nprint(run)\n</pre> task = openml.tasks.get_task(119)  clf = ensemble.RandomForestClassifier() run = openml.runs.run_model_on_task(clf, task) print(run) In\u00a0[\u00a0]: Copied! <pre>myrun = run.publish()\nprint(f\"Run was uploaded to {myrun.openml_url}\")\nprint(f\"The flow can be found at {myrun.flow.openml_url}\")\n</pre> myrun = run.publish() print(f\"Run was uploaded to {myrun.openml_url}\") print(f\"The flow can be found at {myrun.flow.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n# License: BSD 3-Clause\n</pre> openml.config.stop_using_configuration_for_example() # License: BSD 3-Clause"},{"location":"examples/20_basic/simple_flows_and_runs_tutorial/#flows-and-runs","title":"Flows and Runs\u00b6","text":"<p>A simple tutorial on how to train/run a model and how to upload the results.</p>"},{"location":"examples/20_basic/simple_flows_and_runs_tutorial/#train-a-machine-learning-model","title":"Train a machine learning model\u00b6","text":"<p>NOTE: We are using dataset 20 from the test server: https://test.openml.org/d/20</p>"},{"location":"examples/20_basic/simple_flows_and_runs_tutorial/#running-a-model-on-a-task","title":"Running a model on a task\u00b6","text":""},{"location":"examples/20_basic/simple_flows_and_runs_tutorial/#publishing-the-run","title":"Publishing the run\u00b6","text":""},{"location":"examples/20_basic/simple_suites_tutorial/","title":"Benchmark suites","text":"In\u00a0[\u00a0]: Copied! <pre>import openml\n</pre> import openml In\u00a0[\u00a0]: Copied! <pre>suite = openml.study.get_suite(99)\nprint(suite)\n</pre> suite = openml.study.get_suite(99) print(suite) <p>The benchmark suite does not download the included tasks and datasets itself, but only contains a list of which tasks constitute the study.</p> <p>Tasks can then be accessed via</p> In\u00a0[\u00a0]: Copied! <pre>tasks = suite.tasks\nprint(tasks)\n</pre> tasks = suite.tasks print(tasks) <p>and iterated over for benchmarking. For speed reasons we only iterate over the first three tasks:</p> In\u00a0[\u00a0]: Copied! <pre>for task_id in tasks[:3]:\n    task = openml.tasks.get_task(task_id)\n    print(task)\n</pre> for task_id in tasks[:3]:     task = openml.tasks.get_task(task_id)     print(task)"},{"location":"examples/20_basic/simple_suites_tutorial/#benchmark-suites","title":"Benchmark suites\u00b6","text":"<p>This is a brief showcase of OpenML benchmark suites, which were introduced by Bischl et al. (2019). Benchmark suites standardize the datasets and splits to be used in an experiment or paper. They are fully integrated into OpenML and simplify both the sharing of the setup and the results.</p>"},{"location":"examples/20_basic/simple_suites_tutorial/#openml-cc18","title":"OpenML-CC18\u00b6","text":"<p>As an example we have a look at the OpenML-CC18, which is a suite of 72 classification datasets from OpenML which were carefully selected to be usable by many algorithms and also represent datasets commonly used in machine learning research. These are all datasets from mid-2018 that satisfy a large set of clear requirements for thorough yet practical benchmarking:</p> <ol> <li>the number of observations are between 500 and 100,000 to focus on medium-sized datasets,</li> <li>the number of features does not exceed 5,000 features to keep the runtime of the algorithms low</li> <li>the target attribute has at least two classes with no class having less than 20 observations</li> <li>the ratio of the minority class and the majority class is above 0.05 (to eliminate highly imbalanced datasets which require special treatment for both algorithms and evaluation measures).</li> </ol> <p>A full description can be found in the OpenML benchmarking docs.</p> <p>In this example we'll focus on how to use benchmark suites in practice.</p>"},{"location":"examples/20_basic/simple_suites_tutorial/#downloading-benchmark-suites","title":"Downloading benchmark suites\u00b6","text":""},{"location":"examples/20_basic/simple_suites_tutorial/#further-examples","title":"Further examples\u00b6","text":"<ul> <li>Suites Tutorial</li> <li>Study Tutoral</li> <li>Paper example: Strang et al.</li> </ul> <p>License: BSD 3-Clause</p>"},{"location":"examples/30_extended/benchmark_with_optunahub/","title":"Prepare for preprocessors and an OpenML task","text":"In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline <p>Please make sure to install the dependencies with: <code>pip install \"openml&gt;=0.15.1\" plotly</code> Then we import all the necessary modules.</p> In\u00a0[\u00a0]: Copied! <pre># License: BSD 3-Clause\n\nimport logging\n\nimport optuna\n\nimport openml\nfrom openml.extensions.sklearn import cat\nfrom openml.extensions.sklearn import cont\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nlogger = logging.Logger(name=\"Experiment Logger\", level=1)\n\n# Set your openml api key if you want to upload your results to OpenML (eg:\n# https://openml.org/search?type=run&amp;sort=date) . To get one, simply make an\n# account (you don't need one for anything else, just to upload your results),\n# go to your profile and select the API-KEY.\n# Or log in, and navigate to https://www.openml.org/auth/api-key\nopenml.config.apikey = \"\"\n</pre> # License: BSD 3-Clause  import logging  import optuna  import openml from openml.extensions.sklearn import cat from openml.extensions.sklearn import cont from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestClassifier from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder   logger = logging.Logger(name=\"Experiment Logger\", level=1)  # Set your openml api key if you want to upload your results to OpenML (eg: # https://openml.org/search?type=run&amp;sort=date) . To get one, simply make an # account (you don't need one for anything else, just to upload your results), # go to your profile and select the API-KEY. # Or log in, and navigate to https://www.openml.org/auth/api-key openml.config.apikey = \"\" In\u00a0[\u00a0]: Copied! <pre># OpenML contains several key concepts which it needs to make machine learning research shareable.\n# A machine learning experiment consists of one or several runs, which describe the performance of\n# an algorithm (called a flow in OpenML), its hyperparameter settings (called a setup) on a task.\n# A Task is the combination of a dataset, a split and an evaluation metric We choose a dataset from\n# OpenML, (https://www.openml.org/d/1464) and a subsequent task (https://www.openml.org/t/10101) To\n# make your own dataset and task, please refer to\n# https://openml.github.io/openml-python/main/examples/30_extended/create_upload_tutorial.html\n\n# https://www.openml.org/search?type=study&amp;study_type=task&amp;id=218\ntask_id = 10101\nseed = 42\ncategorical_preproc = (\n    \"categorical\",\n    OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n    cat,\n)\nnumerical_preproc = (\"numerical\", SimpleImputer(strategy=\"median\"), cont)\npreproc = ColumnTransformer([categorical_preproc, numerical_preproc])\n</pre> # OpenML contains several key concepts which it needs to make machine learning research shareable. # A machine learning experiment consists of one or several runs, which describe the performance of # an algorithm (called a flow in OpenML), its hyperparameter settings (called a setup) on a task. # A Task is the combination of a dataset, a split and an evaluation metric We choose a dataset from # OpenML, (https://www.openml.org/d/1464) and a subsequent task (https://www.openml.org/t/10101) To # make your own dataset and task, please refer to # https://openml.github.io/openml-python/main/examples/30_extended/create_upload_tutorial.html  # https://www.openml.org/search?type=study&amp;study_type=task&amp;id=218 task_id = 10101 seed = 42 categorical_preproc = (     \"categorical\",     OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),     cat, ) numerical_preproc = (\"numerical\", SimpleImputer(strategy=\"median\"), cont) preproc = ColumnTransformer([categorical_preproc, numerical_preproc]) In\u00a0[\u00a0]: Copied! <pre># Optuna explanation\n# we follow the `Optuna &lt;https://github.com/optuna/optuna/&gt;`__ search space design.\n\n# OpenML runs\n# We can simply pass the parametrized classifier to `run_model_on_task` to obtain the performance\n# of the pipeline\n# on the specified OpenML task.\n# Do you want to share your results along with an easily reproducible pipeline, you can set an API\n# key and just upload your results.\n# You can find more examples on https://www.openml.org/\n\n\ndef objective(trial: optuna.Trial) -&gt; Pipeline:\n    clf = RandomForestClassifier(\n        max_depth=trial.suggest_int(\"max_depth\", 2, 32, log=True),\n        min_samples_leaf=trial.suggest_float(\"min_samples_leaf\", 0.0, 1.0),\n        random_state=seed,\n    )\n    pipe = Pipeline(steps=[(\"preproc\", preproc), (\"model\", clf)])\n    logger.log(1, f\"Running pipeline - {pipe}\")\n    run = openml.runs.run_model_on_task(pipe, task=task_id, avoid_duplicate_runs=False)\n\n    logger.log(1, f\"Model has been trained - {run}\")\n    if openml.config.apikey != \"\":\n        try:\n            run.publish()\n\n            logger.log(1, f\"Run was uploaded to - {run.openml_url}\")\n        except Exception as e:\n            logger.log(1, f\"Could not publish run - {e}\")\n    else:\n        logger.log(\n            0,\n            \"If you want to publish your results to OpenML, please set an apikey\",\n        )\n    accuracy = max(run.fold_evaluations[\"predictive_accuracy\"][0].values())\n    logger.log(0, f\"Accuracy {accuracy}\")\n\n    return accuracy\n</pre> # Optuna explanation # we follow the `Optuna `__ search space design.  # OpenML runs # We can simply pass the parametrized classifier to `run_model_on_task` to obtain the performance # of the pipeline # on the specified OpenML task. # Do you want to share your results along with an easily reproducible pipeline, you can set an API # key and just upload your results. # You can find more examples on https://www.openml.org/   def objective(trial: optuna.Trial) -&gt; Pipeline:     clf = RandomForestClassifier(         max_depth=trial.suggest_int(\"max_depth\", 2, 32, log=True),         min_samples_leaf=trial.suggest_float(\"min_samples_leaf\", 0.0, 1.0),         random_state=seed,     )     pipe = Pipeline(steps=[(\"preproc\", preproc), (\"model\", clf)])     logger.log(1, f\"Running pipeline - {pipe}\")     run = openml.runs.run_model_on_task(pipe, task=task_id, avoid_duplicate_runs=False)      logger.log(1, f\"Model has been trained - {run}\")     if openml.config.apikey != \"\":         try:             run.publish()              logger.log(1, f\"Run was uploaded to - {run.openml_url}\")         except Exception as e:             logger.log(1, f\"Could not publish run - {e}\")     else:         logger.log(             0,             \"If you want to publish your results to OpenML, please set an apikey\",         )     accuracy = max(run.fold_evaluations[\"predictive_accuracy\"][0].values())     logger.log(0, f\"Accuracy {accuracy}\")      return accuracy  In\u00a0[\u00a0]: Copied! <pre>study = optuna.create_study(direction=\"maximize\")\nlogger.log(0, f\"Study {study}\")\nstudy.optimize(objective, n_trials=15)\n</pre> study = optuna.create_study(direction=\"maximize\") logger.log(0, f\"Study {study}\") study.optimize(objective, n_trials=15) In\u00a0[\u00a0]: Copied! <pre>fig = optuna.visualization.plot_optimization_history(study)\nfig.show()\n</pre> fig = optuna.visualization.plot_optimization_history(study) fig.show()"},{"location":"examples/30_extended/benchmark_with_optunahub/#hyperparameter-optimization-benchmark-with-optunahub","title":"==================================================== Hyperparameter Optimization Benchmark with OptunaHub\u00b6","text":"<p>In this tutorial, we walk through how to conduct hyperparameter optimization experiments using OpenML and OptunaHub.</p>"},{"location":"examples/30_extended/benchmark_with_optunahub/#prepare-for-preprocessors-and-an-openml-task","title":"Prepare for preprocessors and an OpenML task\u00b6","text":""},{"location":"examples/30_extended/benchmark_with_optunahub/#define-a-pipeline-for-the-hyperparameter-optimization-this-is-standark-for-optuna","title":"Define a pipeline for the hyperparameter optimization (this is standark for Optuna)\u00b6","text":""},{"location":"examples/30_extended/benchmark_with_optunahub/#optimize-the-pipeline","title":"Optimize the pipeline\u00b6","text":""},{"location":"examples/30_extended/benchmark_with_optunahub/#visualize-the-optimization-history","title":"Visualize the optimization history\u00b6","text":""},{"location":"examples/30_extended/configure_logging/","title":"Logging","text":"In\u00a0[\u00a0]: Copied! <pre>import openml\n\nopenml.datasets.get_dataset(\"iris\", version=1)\n</pre> import openml  openml.datasets.get_dataset(\"iris\", version=1) <p>With default configuration, the above example will show no output to console. However, in your cache directory you should find a file named 'openml_python.log', which has a DEBUG message written to it. It should be either like \"[DEBUG] [10:46:19:openml.datasets.dataset] Saved dataset 61: iris to file ...\" or like \"[DEBUG] [10:49:38:openml.datasets.dataset] Data pickle file already exists and is up to date.\" , depending on whether or not you had downloaded iris before. The processed log levels can be configured programmatically:</p> In\u00a0[\u00a0]: Copied! <pre>import logging\n\nopenml.config.set_console_log_level(logging.DEBUG)\nopenml.config.set_file_log_level(logging.WARNING)\nopenml.datasets.get_dataset(\"iris\", version=1)\n</pre> import logging  openml.config.set_console_log_level(logging.DEBUG) openml.config.set_file_log_level(logging.WARNING) openml.datasets.get_dataset(\"iris\", version=1) <p>Now the log level that was previously written to file should also be shown in the console. The message is now no longer written to file as the <code>file_log</code> was set to level <code>WARNING</code>.</p> <p>It is also possible to specify the desired log levels through the configuration file. This way you will not need to set them on each script separately. Add the  line verbosity = NUMBER and/or file_verbosity = NUMBER to the config file, where 'NUMBER' should be one of:</p> <ul> <li>0: <code>logging.WARNING</code> and up.</li> <li>1: <code>logging.INFO</code> and up.</li> <li>2: <code>logging.DEBUG</code> and up (i.e. all messages).</li> </ul> <p>License: BSD 3-Clause</p>"},{"location":"examples/30_extended/configure_logging/#logging","title":"Logging\u00b6","text":"<p>This tutorial explains openml-python logging, and shows how to configure it. Openml-python uses the Python logging module to provide users with log messages. Each log message is assigned a level of importance, see the table in Python's logging tutorial here.</p> <p>By default, openml-python will print log messages of level <code>WARNING</code> and above to console. All log messages (including <code>DEBUG</code> and <code>INFO</code>) are also saved in a file, which can be found in your cache directory (see also the introduction tutorial. These file logs are automatically deleted if needed, and use at most 2MB of space.</p> <p>It is possible to configure what log levels to send to console and file. When downloading a dataset from OpenML, a <code>DEBUG</code>-level message is written:</p>"},{"location":"examples/30_extended/create_upload_tutorial/","title":"Dataset upload tutorial","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport sklearn.datasets\nfrom scipy.sparse import coo_matrix\n\nimport openml\nfrom openml.datasets.functions import create_dataset\n</pre> import numpy as np import pandas as pd import sklearn.datasets from scipy.sparse import coo_matrix  import openml from openml.datasets.functions import create_dataset <p>.. warning:: .. include:: ../../test_server_usage_warning.txt</p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() <p>Below we will cover the following cases of the dataset object:</p> <ul> <li>A numpy array</li> <li>A list</li> <li>A pandas dataframe</li> <li>A sparse matrix</li> <li>A pandas sparse dataframe</li> </ul> In\u00a0[\u00a0]: Copied! <pre>diabetes = sklearn.datasets.load_diabetes()\nname = \"Diabetes(scikit-learn)\"\nX = diabetes.data\ny = diabetes.target\nattribute_names = diabetes.feature_names\ndescription = diabetes.DESCR\n</pre> diabetes = sklearn.datasets.load_diabetes() name = \"Diabetes(scikit-learn)\" X = diabetes.data y = diabetes.target attribute_names = diabetes.feature_names description = diabetes.DESCR <p>OpenML does not distinguish between the attributes and targets on the data level and stores all data in a single matrix.</p> <p>The target feature is indicated as meta-data of the dataset (and tasks on that data).</p> In\u00a0[\u00a0]: Copied! <pre>data = np.concatenate((X, y.reshape((-1, 1))), axis=1)\nattribute_names = list(attribute_names)\nattributes = [(attribute_name, \"REAL\") for attribute_name in attribute_names] + [\n    (\"class\", \"INTEGER\")\n]\ncitation = (\n    \"Bradley Efron, Trevor Hastie, Iain Johnstone and \"\n    \"Robert Tibshirani (2004) (Least Angle Regression) \"\n    \"Annals of Statistics (with discussion), 407-499\"\n)\npaper_url = \"https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf\"\n</pre> data = np.concatenate((X, y.reshape((-1, 1))), axis=1) attribute_names = list(attribute_names) attributes = [(attribute_name, \"REAL\") for attribute_name in attribute_names] + [     (\"class\", \"INTEGER\") ] citation = (     \"Bradley Efron, Trevor Hastie, Iain Johnstone and \"     \"Robert Tibshirani (2004) (Least Angle Regression) \"     \"Annals of Statistics (with discussion), 407-499\" ) paper_url = \"https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf\" In\u00a0[\u00a0]: Copied! <pre>diabetes_dataset = create_dataset(\n    # The name of the dataset (needs to be unique).\n    # Must not be longer than 128 characters and only contain\n    # a-z, A-Z, 0-9 and the following special characters: _\\-\\.(),\n    name=name,\n    # Textual description of the dataset.\n    description=description,\n    # The person who created the dataset.\n    creator=\"Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani\",\n    # People who contributed to the current version of the dataset.\n    contributor=None,\n    # The date the data was originally collected, given by the uploader.\n    collection_date=\"09-01-2012\",\n    # Language in which the data is represented.\n    # Starts with 1 upper case letter, rest lower case, e.g. 'English'.\n    language=\"English\",\n    # License under which the data is/will be distributed.\n    licence=\"BSD (from scikit-learn)\",\n    # Name of the target. Can also have multiple values (comma-separated).\n    default_target_attribute=\"class\",\n    # The attribute that represents the row-id column, if present in the\n    # dataset.\n    row_id_attribute=None,\n    # Attribute or list of attributes that should be excluded in modelling, such as\n    # identifiers and indexes. E.g. \"feat1\" or [\"feat1\",\"feat2\"]\n    ignore_attribute=None,\n    # How to cite the paper.\n    citation=citation,\n    # Attributes of the data\n    attributes=attributes,\n    data=data,\n    # A version label which is provided by the user.\n    version_label=\"test\",\n    original_data_url=\"https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\",\n    paper_url=paper_url,\n)\n</pre> diabetes_dataset = create_dataset(     # The name of the dataset (needs to be unique).     # Must not be longer than 128 characters and only contain     # a-z, A-Z, 0-9 and the following special characters: _\\-\\.(),     name=name,     # Textual description of the dataset.     description=description,     # The person who created the dataset.     creator=\"Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani\",     # People who contributed to the current version of the dataset.     contributor=None,     # The date the data was originally collected, given by the uploader.     collection_date=\"09-01-2012\",     # Language in which the data is represented.     # Starts with 1 upper case letter, rest lower case, e.g. 'English'.     language=\"English\",     # License under which the data is/will be distributed.     licence=\"BSD (from scikit-learn)\",     # Name of the target. Can also have multiple values (comma-separated).     default_target_attribute=\"class\",     # The attribute that represents the row-id column, if present in the     # dataset.     row_id_attribute=None,     # Attribute or list of attributes that should be excluded in modelling, such as     # identifiers and indexes. E.g. \"feat1\" or [\"feat1\",\"feat2\"]     ignore_attribute=None,     # How to cite the paper.     citation=citation,     # Attributes of the data     attributes=attributes,     data=data,     # A version label which is provided by the user.     version_label=\"test\",     original_data_url=\"https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\",     paper_url=paper_url, ) In\u00a0[\u00a0]: Copied! <pre>diabetes_dataset.publish()\nprint(f\"URL for dataset: {diabetes_dataset.openml_url}\")\n</pre>  diabetes_dataset.publish() print(f\"URL for dataset: {diabetes_dataset.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>data = [\n    [\"sunny\", 85, 85, \"FALSE\", \"no\"],\n    [\"sunny\", 80, 90, \"TRUE\", \"no\"],\n    [\"overcast\", 83, 86, \"FALSE\", \"yes\"],\n    [\"rainy\", 70, 96, \"FALSE\", \"yes\"],\n    [\"rainy\", 68, 80, \"FALSE\", \"yes\"],\n    [\"rainy\", 65, 70, \"TRUE\", \"no\"],\n    [\"overcast\", 64, 65, \"TRUE\", \"yes\"],\n    [\"sunny\", 72, 95, \"FALSE\", \"no\"],\n    [\"sunny\", 69, 70, \"FALSE\", \"yes\"],\n    [\"rainy\", 75, 80, \"FALSE\", \"yes\"],\n    [\"sunny\", 75, 70, \"TRUE\", \"yes\"],\n    [\"overcast\", 72, 90, \"TRUE\", \"yes\"],\n    [\"overcast\", 81, 75, \"FALSE\", \"yes\"],\n    [\"rainy\", 71, 91, \"TRUE\", \"no\"],\n]\n\nattribute_names = [\n    (\"outlook\", [\"sunny\", \"overcast\", \"rainy\"]),\n    (\"temperature\", \"REAL\"),\n    (\"humidity\", \"REAL\"),\n    (\"windy\", [\"TRUE\", \"FALSE\"]),\n    (\"play\", [\"yes\", \"no\"]),\n]\n\ndescription = (\n    \"The weather problem is a tiny dataset that we will use repeatedly\"\n    \" to illustrate machine learning methods. Entirely fictitious, it \"\n    \"supposedly concerns the conditions that are suitable for playing \"\n    \"some unspecified game. In general, instances in a dataset are \"\n    \"characterized by the values of features, or attributes, that measure \"\n    \"different aspects of the instance. In this case there are four \"\n    \"attributes: outlook, temperature, humidity, and windy. \"\n    \"The outcome is whether to play or not.\"\n)\n\ncitation = (\n    \"I. H. Witten, E. Frank, M. A. Hall, and ITPro,\"\n    \"Data mining practical machine learning tools and techniques, \"\n    \"third edition. Burlington, Mass.: Morgan Kaufmann Publishers, 2011\"\n)\n\nweather_dataset = create_dataset(\n    name=\"Weather\",\n    description=description,\n    creator=\"I. H. Witten, E. Frank, M. A. Hall, and ITPro\",\n    contributor=None,\n    collection_date=\"01-01-2011\",\n    language=\"English\",\n    licence=None,\n    default_target_attribute=\"play\",\n    row_id_attribute=None,\n    ignore_attribute=None,\n    citation=citation,\n    attributes=attribute_names,\n    data=data,\n    version_label=\"example\",\n)\n</pre> data = [     [\"sunny\", 85, 85, \"FALSE\", \"no\"],     [\"sunny\", 80, 90, \"TRUE\", \"no\"],     [\"overcast\", 83, 86, \"FALSE\", \"yes\"],     [\"rainy\", 70, 96, \"FALSE\", \"yes\"],     [\"rainy\", 68, 80, \"FALSE\", \"yes\"],     [\"rainy\", 65, 70, \"TRUE\", \"no\"],     [\"overcast\", 64, 65, \"TRUE\", \"yes\"],     [\"sunny\", 72, 95, \"FALSE\", \"no\"],     [\"sunny\", 69, 70, \"FALSE\", \"yes\"],     [\"rainy\", 75, 80, \"FALSE\", \"yes\"],     [\"sunny\", 75, 70, \"TRUE\", \"yes\"],     [\"overcast\", 72, 90, \"TRUE\", \"yes\"],     [\"overcast\", 81, 75, \"FALSE\", \"yes\"],     [\"rainy\", 71, 91, \"TRUE\", \"no\"], ]  attribute_names = [     (\"outlook\", [\"sunny\", \"overcast\", \"rainy\"]),     (\"temperature\", \"REAL\"),     (\"humidity\", \"REAL\"),     (\"windy\", [\"TRUE\", \"FALSE\"]),     (\"play\", [\"yes\", \"no\"]), ]  description = (     \"The weather problem is a tiny dataset that we will use repeatedly\"     \" to illustrate machine learning methods. Entirely fictitious, it \"     \"supposedly concerns the conditions that are suitable for playing \"     \"some unspecified game. In general, instances in a dataset are \"     \"characterized by the values of features, or attributes, that measure \"     \"different aspects of the instance. In this case there are four \"     \"attributes: outlook, temperature, humidity, and windy. \"     \"The outcome is whether to play or not.\" )  citation = (     \"I. H. Witten, E. Frank, M. A. Hall, and ITPro,\"     \"Data mining practical machine learning tools and techniques, \"     \"third edition. Burlington, Mass.: Morgan Kaufmann Publishers, 2011\" )  weather_dataset = create_dataset(     name=\"Weather\",     description=description,     creator=\"I. H. Witten, E. Frank, M. A. Hall, and ITPro\",     contributor=None,     collection_date=\"01-01-2011\",     language=\"English\",     licence=None,     default_target_attribute=\"play\",     row_id_attribute=None,     ignore_attribute=None,     citation=citation,     attributes=attribute_names,     data=data,     version_label=\"example\", ) In\u00a0[\u00a0]: Copied! <pre>weather_dataset.publish()\nprint(f\"URL for dataset: {weather_dataset.openml_url}\")\n</pre> weather_dataset.publish() print(f\"URL for dataset: {weather_dataset.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(data, columns=[col_name for col_name, _ in attribute_names])\n\n# enforce the categorical column to have a categorical dtype\ndf[\"outlook\"] = df[\"outlook\"].astype(\"category\")\ndf[\"windy\"] = df[\"windy\"].astype(\"bool\")\ndf[\"play\"] = df[\"play\"].astype(\"category\")\nprint(df.info())\n</pre> df = pd.DataFrame(data, columns=[col_name for col_name, _ in attribute_names])  # enforce the categorical column to have a categorical dtype df[\"outlook\"] = df[\"outlook\"].astype(\"category\") df[\"windy\"] = df[\"windy\"].astype(\"bool\") df[\"play\"] = df[\"play\"].astype(\"category\") print(df.info()) <p>We enforce the column 'outlook' and 'play' to be a categorical dtype while the column 'windy' is kept as a boolean column. 'temperature' and 'humidity' are kept as numeric columns. Then, we can call :func:<code>openml.datasets.create_dataset</code> by passing the dataframe and fixing the parameter <code>attributes</code> to <code>'auto'</code>.</p> In\u00a0[\u00a0]: Copied! <pre>weather_dataset = create_dataset(\n    name=\"Weather\",\n    description=description,\n    creator=\"I. H. Witten, E. Frank, M. A. Hall, and ITPro\",\n    contributor=None,\n    collection_date=\"01-01-2011\",\n    language=\"English\",\n    licence=None,\n    default_target_attribute=\"play\",\n    row_id_attribute=None,\n    ignore_attribute=None,\n    citation=citation,\n    attributes=\"auto\",\n    data=df,\n    version_label=\"example\",\n)\n</pre> weather_dataset = create_dataset(     name=\"Weather\",     description=description,     creator=\"I. H. Witten, E. Frank, M. A. Hall, and ITPro\",     contributor=None,     collection_date=\"01-01-2011\",     language=\"English\",     licence=None,     default_target_attribute=\"play\",     row_id_attribute=None,     ignore_attribute=None,     citation=citation,     attributes=\"auto\",     data=df,     version_label=\"example\", ) In\u00a0[\u00a0]: Copied! <pre>weather_dataset.publish()\nprint(f\"URL for dataset: {weather_dataset.openml_url}\")\n</pre> weather_dataset.publish() print(f\"URL for dataset: {weather_dataset.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>sparse_data = coo_matrix(\n    ([0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], ([0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 2, 0, 1]))\n)\n\ncolumn_names = [\n    (\"input1\", \"REAL\"),\n    (\"input2\", \"REAL\"),\n    (\"y\", \"REAL\"),\n]\n\nxor_dataset = create_dataset(\n    name=\"XOR\",\n    description=\"Dataset representing the XOR operation\",\n    creator=None,\n    contributor=None,\n    collection_date=None,\n    language=\"English\",\n    licence=None,\n    default_target_attribute=\"y\",\n    row_id_attribute=None,\n    ignore_attribute=None,\n    citation=None,\n    attributes=column_names,\n    data=sparse_data,\n    version_label=\"example\",\n)\n</pre> sparse_data = coo_matrix(     ([0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], ([0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 2, 0, 1])) )  column_names = [     (\"input1\", \"REAL\"),     (\"input2\", \"REAL\"),     (\"y\", \"REAL\"), ]  xor_dataset = create_dataset(     name=\"XOR\",     description=\"Dataset representing the XOR operation\",     creator=None,     contributor=None,     collection_date=None,     language=\"English\",     licence=None,     default_target_attribute=\"y\",     row_id_attribute=None,     ignore_attribute=None,     citation=None,     attributes=column_names,     data=sparse_data,     version_label=\"example\", ) In\u00a0[\u00a0]: Copied! <pre>xor_dataset.publish()\nprint(f\"URL for dataset: {xor_dataset.openml_url}\")\n</pre> xor_dataset.publish() print(f\"URL for dataset: {xor_dataset.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>xor_dataset.publish()\nprint(f\"URL for dataset: {xor_dataset.openml_url}\")\n</pre>  xor_dataset.publish() print(f\"URL for dataset: {xor_dataset.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n# License: BSD 3-Clause\n</pre> openml.config.stop_using_configuration_for_example() # License: BSD 3-Clause"},{"location":"examples/30_extended/create_upload_tutorial/#dataset-upload-tutorial","title":"Dataset upload tutorial\u00b6","text":"<p>A tutorial on how to create and upload a dataset to OpenML.</p>"},{"location":"examples/30_extended/create_upload_tutorial/#dataset-is-a-numpy-array","title":"Dataset is a numpy array\u00b6","text":"<p>A numpy array can contain lists in the case of dense data or it can contain OrderedDicts in the case of sparse data.</p>"},{"location":"examples/30_extended/create_upload_tutorial/#prepare-dataset","title":"Prepare dataset\u00b6","text":"<p>Load an example dataset from scikit-learn which we will upload to OpenML.org via the API.</p>"},{"location":"examples/30_extended/create_upload_tutorial/#create-the-dataset-object","title":"Create the dataset object\u00b6","text":"<p>The definition of all fields can be found in the XSD files describing the expected format:</p> <p>https://github.com/openml/OpenML/blob/master/openml_OS/views/pages/api_new/v1/xsd/openml.data.upload.xsd</p>"},{"location":"examples/30_extended/create_upload_tutorial/#dataset-is-a-list","title":"Dataset is a list\u00b6","text":"<p>A list can contain lists in the case of dense data or it can contain OrderedDicts in the case of sparse data.</p> <p>Weather dataset: https://storm.cis.fordham.edu/~gweiss/data-mining/datasets.html</p>"},{"location":"examples/30_extended/create_upload_tutorial/#dataset-is-a-pandas-dataframe","title":"Dataset is a pandas DataFrame\u00b6","text":"<p>It might happen that your dataset is made of heterogeneous data which can usually be stored as a Pandas DataFrame. DataFrames offer the advantage of storing the type of data for each column as well as the attribute names. Therefore, when providing a Pandas DataFrame, OpenML can infer this information without needing to explicitly provide it when calling the function :func:<code>openml.datasets.create_dataset</code>. In this regard, you only need to pass <code>'auto'</code> to the <code>attributes</code> parameter.</p>"},{"location":"examples/30_extended/create_upload_tutorial/#dataset-is-a-sparse-matrix","title":"Dataset is a sparse matrix\u00b6","text":""},{"location":"examples/30_extended/create_upload_tutorial/#dataset-is-a-pandas-dataframe-with-sparse-columns","title":"Dataset is a pandas dataframe with sparse columns\u00b6","text":"<p>sparse_data = coo_matrix( ([1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0], ([0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 2, 0, 1])) ) column_names = [\"input1\", \"input2\", \"y\"] df = pd.DataFrame.sparse.from_spmatrix(sparse_data, columns=column_names) print(df.info())</p> <p>xor_dataset = create_dataset( name=\"XOR\", description=\"Dataset representing the XOR operation\", creator=None, contributor=None, collection_date=None, language=\"English\", licence=None, default_target_attribute=\"y\", row_id_attribute=None, ignore_attribute=None, citation=None, attributes=\"auto\", data=df, version_label=\"example\", )</p>"},{"location":"examples/30_extended/custom_flow_/","title":"Creating and Using a Custom Flow","text":"In\u00a0[\u00a0]: Copied! <pre>from collections import OrderedDict\nimport numpy as np\n\nimport openml\nfrom openml import OpenMLClassificationTask\nfrom openml.runs.functions import format_prediction\n</pre> from collections import OrderedDict import numpy as np  import openml from openml import OpenMLClassificationTask from openml.runs.functions import format_prediction <p>.. warning:: .. include:: ../../test_server_usage_warning.txt</p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() In\u00a0[\u00a0]: Copied! <pre>general = dict(\n    name=\"automlbenchmark_autosklearn\",\n    description=(\n        \"Auto-sklearn as set up by the AutoML Benchmark\"\n        \"Source: https://github.com/openml/automlbenchmark/releases/tag/v0.9\"\n    ),\n    external_version=\"amlb==0.9\",\n    language=\"English\",\n    tags=[\"amlb\", \"benchmark\", \"study_218\"],\n    dependencies=\"amlb==0.9\",\n)\n</pre> general = dict(     name=\"automlbenchmark_autosklearn\",     description=(         \"Auto-sklearn as set up by the AutoML Benchmark\"         \"Source: https://github.com/openml/automlbenchmark/releases/tag/v0.9\"     ),     external_version=\"amlb==0.9\",     language=\"English\",     tags=[\"amlb\", \"benchmark\", \"study_218\"],     dependencies=\"amlb==0.9\", ) <p>Next we define the flow hyperparameters. We define their name and default value in <code>parameters</code>, and provide meta-data for each hyperparameter through <code>parameters_meta_info</code>. Note that even though the argument name is <code>parameters</code> they describe the hyperparameters. The use of ordered dicts is required.</p> In\u00a0[\u00a0]: Copied! <pre>flow_hyperparameters = dict(\n    parameters=OrderedDict(time=\"240\", memory=\"32\", cores=\"8\"),\n    parameters_meta_info=OrderedDict(\n        cores=OrderedDict(description=\"number of available cores\", data_type=\"int\"),\n        memory=OrderedDict(description=\"memory in gigabytes\", data_type=\"int\"),\n        time=OrderedDict(description=\"time in minutes\", data_type=\"int\"),\n    ),\n)\n</pre> flow_hyperparameters = dict(     parameters=OrderedDict(time=\"240\", memory=\"32\", cores=\"8\"),     parameters_meta_info=OrderedDict(         cores=OrderedDict(description=\"number of available cores\", data_type=\"int\"),         memory=OrderedDict(description=\"memory in gigabytes\", data_type=\"int\"),         time=OrderedDict(description=\"time in minutes\", data_type=\"int\"),     ), ) <p>It is possible to build a flow which uses other flows. For example, the Random Forest Classifier is a flow, but you could also construct a flow which uses a Random Forest Classifier in a ML pipeline. When constructing the pipeline flow, you can use the Random Forest Classifier flow as a subflow. It allows for all hyperparameters of the Random Classifier Flow to also be specified in your pipeline flow.</p> <p>Note: you can currently only specific one subflow as part of the components.</p> <p>In this example, the auto-sklearn flow is a subflow: the auto-sklearn flow is entirely executed as part of this flow. This allows people to specify auto-sklearn hyperparameters used in this flow. In general, using a subflow is not required.</p> <p>Note: flow 9313 is not actually the right flow on the test server, but that does not matter for this demonstration.</p> In\u00a0[\u00a0]: Copied! <pre>autosklearn_flow = openml.flows.get_flow(9313)  # auto-sklearn 0.5.1\nsubflow = dict(\n    components=OrderedDict(automl_tool=autosklearn_flow),\n    # If you do not want to reference a subflow, you can use the following:\n    # components=OrderedDict(),\n)\n</pre> autosklearn_flow = openml.flows.get_flow(9313)  # auto-sklearn 0.5.1 subflow = dict(     components=OrderedDict(automl_tool=autosklearn_flow),     # If you do not want to reference a subflow, you can use the following:     # components=OrderedDict(), ) <p>With all parameters of the flow defined, we can now initialize the OpenMLFlow and publish. Because we provided all the details already, we do not need to provide a <code>model</code> to the flow.</p> <p>In our case, we don't even have a model. It is possible to have a model but still require to follow these steps when the model (python object) does not have an extensions from which to automatically extract the hyperparameters. So whether you have a model with no extension or no model at all, explicitly set the model of the flow to <code>None</code>.</p> In\u00a0[\u00a0]: Copied! <pre>autosklearn_amlb_flow = openml.flows.OpenMLFlow(\n    **general,\n    **flow_hyperparameters,\n    **subflow,\n    model=None,\n)\nautosklearn_amlb_flow.publish()\nprint(f\"autosklearn flow created: {autosklearn_amlb_flow.flow_id}\")\n</pre> autosklearn_amlb_flow = openml.flows.OpenMLFlow(     **general,     **flow_hyperparameters,     **subflow,     model=None, ) autosklearn_amlb_flow.publish() print(f\"autosklearn flow created: {autosklearn_amlb_flow.flow_id}\") In\u00a0[\u00a0]: Copied! <pre>flow_id = autosklearn_amlb_flow.flow_id\n\nparameters = [\n    OrderedDict([(\"oml:name\", \"cores\"), (\"oml:value\", 4), (\"oml:component\", flow_id)]),\n    OrderedDict([(\"oml:name\", \"memory\"), (\"oml:value\", 16), (\"oml:component\", flow_id)]),\n    OrderedDict([(\"oml:name\", \"time\"), (\"oml:value\", 120), (\"oml:component\", flow_id)]),\n]\n\ntask_id = 1200  # Iris Task\ntask = openml.tasks.get_task(task_id)\ndataset_id = task.get_dataset().dataset_id\n</pre> flow_id = autosklearn_amlb_flow.flow_id  parameters = [     OrderedDict([(\"oml:name\", \"cores\"), (\"oml:value\", 4), (\"oml:component\", flow_id)]),     OrderedDict([(\"oml:name\", \"memory\"), (\"oml:value\", 16), (\"oml:component\", flow_id)]),     OrderedDict([(\"oml:name\", \"time\"), (\"oml:value\", 120), (\"oml:component\", flow_id)]), ]  task_id = 1200  # Iris Task task = openml.tasks.get_task(task_id) dataset_id = task.get_dataset().dataset_id <p>The last bit of information for the run we need are the predicted values. The exact format of the predictions will depend on the task.</p> <p>The predictions should always be a list of lists, each list should contain:</p> <ul> <li>the repeat number: for repeated evaluation strategies. (e.g. repeated cross-validation)</li> <li>the fold number: for cross-validation. (what should this be for holdout?)</li> <li>0: this field is for backward compatibility.</li> <li>index: the row (of the original dataset) for which the prediction was made.</li> <li>p_1, ..., p_c: for each class the predicted probability of the sample belonging to that class. (no elements for regression tasks) Make sure the order of these elements follows the order of <code>task.class_labels</code>.</li> <li>the predicted class/value for the sample</li> <li>the true class/value for the sample</li> </ul> <p>When using openml-python extensions (such as through <code>run_model_on_task</code>), all of this formatting is automatic. Unfortunately we can not automate this procedure for custom flows, which means a little additional effort is required.</p> <p>Here we generated some random predictions in place. You can ignore this code, or use it to better understand the formatting of the predictions.</p> <p>Find the repeats/folds for this task:</p> In\u00a0[\u00a0]: Copied! <pre>n_repeats, n_folds, _ = task.get_split_dimensions()\nall_test_indices = [\n    (repeat, fold, index)\n    for repeat in range(n_repeats)\n    for fold in range(n_folds)\n    for index in task.get_train_test_split_indices(fold, repeat)[1]\n]\n\n# random class probabilities (Iris has 150 samples and 3 classes):\nr = np.random.rand(150 * n_repeats, 3)\n# scale the random values so that the probabilities of each sample sum to 1:\ny_proba = r / r.sum(axis=1).reshape(-1, 1)\ny_pred = y_proba.argmax(axis=1)\n\nclass_map = dict(zip(range(3), task.class_labels))\n_, y_true = task.get_X_and_y()\ny_true = [class_map[y] for y in y_true]\n\n# We format the predictions with the utility function `format_prediction`.\n# It will organize the relevant data in the expected format/order.\npredictions = []\nfor where, y, yp, proba in zip(all_test_indices, y_true, y_pred, y_proba):\n    repeat, fold, index = where\n\n    prediction = format_prediction(\n        task=task,\n        repeat=repeat,\n        fold=fold,\n        index=index,\n        prediction=class_map[yp],\n        truth=y,\n        proba={c: pb for (c, pb) in zip(task.class_labels, proba)},\n    )\n    predictions.append(prediction)\n</pre> n_repeats, n_folds, _ = task.get_split_dimensions() all_test_indices = [     (repeat, fold, index)     for repeat in range(n_repeats)     for fold in range(n_folds)     for index in task.get_train_test_split_indices(fold, repeat)[1] ]  # random class probabilities (Iris has 150 samples and 3 classes): r = np.random.rand(150 * n_repeats, 3) # scale the random values so that the probabilities of each sample sum to 1: y_proba = r / r.sum(axis=1).reshape(-1, 1) y_pred = y_proba.argmax(axis=1)  class_map = dict(zip(range(3), task.class_labels)) _, y_true = task.get_X_and_y() y_true = [class_map[y] for y in y_true]  # We format the predictions with the utility function `format_prediction`. # It will organize the relevant data in the expected format/order. predictions = [] for where, y, yp, proba in zip(all_test_indices, y_true, y_pred, y_proba):     repeat, fold, index = where      prediction = format_prediction(         task=task,         repeat=repeat,         fold=fold,         index=index,         prediction=class_map[yp],         truth=y,         proba={c: pb for (c, pb) in zip(task.class_labels, proba)},     )     predictions.append(prediction) <p>Finally we can create the OpenMLRun object and upload. We use the argument setup_string because the used flow was a script.</p> In\u00a0[\u00a0]: Copied! <pre>benchmark_command = f\"python3 runbenchmark.py auto-sklearn medium -m aws -t 119\"\nmy_run = openml.runs.OpenMLRun(\n    task_id=task_id,\n    flow_id=flow_id,\n    dataset_id=dataset_id,\n    parameter_settings=parameters,\n    setup_string=benchmark_command,\n    data_content=predictions,\n    tags=[\"study_218\"],\n    description_text=\"Run generated by the Custom Flow tutorial.\",\n)\nmy_run.publish()\nprint(\"run created:\", my_run.run_id)\n</pre> benchmark_command = f\"python3 runbenchmark.py auto-sklearn medium -m aws -t 119\" my_run = openml.runs.OpenMLRun(     task_id=task_id,     flow_id=flow_id,     dataset_id=dataset_id,     parameter_settings=parameters,     setup_string=benchmark_command,     data_content=predictions,     tags=[\"study_218\"],     description_text=\"Run generated by the Custom Flow tutorial.\", ) my_run.publish() print(\"run created:\", my_run.run_id) In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n# License: BSD 3-Clause\n</pre> openml.config.stop_using_configuration_for_example() # License: BSD 3-Clause"},{"location":"examples/30_extended/custom_flow_/#creating-and-using-a-custom-flow","title":"Creating and Using a Custom Flow\u00b6","text":"<p>The most convenient way to create a flow for your machine learning workflow is to generate it automatically as described in the \"Obtaining Flow IDs\" tutorial. However, there are scenarios where this is not possible, such as when the flow uses a framework without an extension or when the flow is described by a script.</p> <p>In those cases you can still create a custom flow by following the steps of this tutorial. As an example we will use the flows generated for the AutoML Benchmark, and also show how to link runs to the custom flow.</p>"},{"location":"examples/30_extended/custom_flow_/#1-defining-the-flow","title":"1. Defining the flow\u00b6","text":"<p>The first step is to define all the hyperparameters of your flow. The API pages feature a descriptions of each variable of the :class:<code>openml.flows.OpenMLFlow</code>. Note that <code>external version</code> and <code>name</code> together uniquely identify a flow.</p> <p>The AutoML Benchmark runs AutoML systems across a range of tasks. OpenML stores Flows for each AutoML system. However, the AutoML benchmark adds preprocessing to the flow, so should be described in a new flow.</p> <p>We will break down the flow arguments into several groups, for the tutorial. First we will define the name and version information. Make sure to leave enough information so others can determine exactly which version of the package/script is used. Use tags so users can find your flow easily.</p>"},{"location":"examples/30_extended/custom_flow_/#2-using-the-flow","title":"2. Using the flow\u00b6","text":"<p>This Section will show how to upload run data for your custom flow. Take care to change the values of parameters as well as the task id, to reflect the actual run. Task and parameter values in the example are fictional.</p>"},{"location":"examples/30_extended/datasets_tutorial/","title":"Datasets","text":"In\u00a0[\u00a0]: Copied! <pre>datalist[datalist.NumberOfInstances &gt; 10000].sort_values([\"NumberOfInstances\"]).head(n=20)\n</pre> datalist[datalist.NumberOfInstances &gt; 10000].sort_values([\"NumberOfInstances\"]).head(n=20) In\u00a0[\u00a0]: Copied! <pre>datalist.query('name == \"eeg-eye-state\"')\n</pre> datalist.query('name == \"eeg-eye-state\"') In\u00a0[\u00a0]: Copied! <pre>datalist.query(\"NumberOfClasses &gt; 50\")\n</pre> datalist.query(\"NumberOfClasses &gt; 50\") In\u00a0[\u00a0]: Copied! <pre># This is done based on the dataset ID.\ndataset = openml.datasets.get_dataset(dataset_id=\"eeg-eye-state\", version=1)\n\n# Print a summary\nprint(\n    f\"This is dataset '{dataset.name}', the target feature is \"\n    f\"'{dataset.default_target_attribute}'\"\n)\nprint(f\"URL: {dataset.url}\")\nprint(dataset.description[:500])\n</pre> # This is done based on the dataset ID. dataset = openml.datasets.get_dataset(dataset_id=\"eeg-eye-state\", version=1)  # Print a summary print(     f\"This is dataset '{dataset.name}', the target feature is \"     f\"'{dataset.default_target_attribute}'\" ) print(f\"URL: {dataset.url}\") print(dataset.description[:500]) <p>Get the actual data.</p> <p>openml-python returns data as pandas dataframes (stored in the <code>eeg</code> variable below), and also some additional metadata that we don't care about right now.</p> In\u00a0[\u00a0]: Copied! <pre>eeg, *_ = dataset.get_data()\n</pre> eeg, *_ = dataset.get_data() <p>You can optionally choose to have openml separate out a column from the dataset. In particular, many datasets for supervised problems have a set <code>default_target_attribute</code> which may help identify the target variable.</p> In\u00a0[\u00a0]: Copied! <pre>X, y, categorical_indicator, attribute_names = dataset.get_data(\n    target=dataset.default_target_attribute\n)\nprint(X.head())\nprint(X.info())\n</pre> X, y, categorical_indicator, attribute_names = dataset.get_data(     target=dataset.default_target_attribute ) print(X.head()) print(X.info()) <p>Sometimes you only need access to a dataset's metadata. In those cases, you can download the dataset without downloading the data file. The dataset object can be used as normal. Whenever you use any functionality that requires the data, such as <code>get_data</code>, the data will be downloaded. Starting from 0.15, not downloading data will be the default behavior instead. The data will be downloading automatically when you try to access it through openml objects, e.g., using <code>dataset.features</code>.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = openml.datasets.get_dataset(1471)\n</pre> dataset = openml.datasets.get_dataset(1471) In\u00a0[\u00a0]: Copied! <pre>eegs = eeg.sample(n=1000)\n_ = pd.plotting.scatter_matrix(\n    X.iloc[:100, :4],\n    c=y[:100],\n    figsize=(10, 10),\n    marker=\"o\",\n    hist_kwds={\"bins\": 20},\n    alpha=0.8,\n    cmap=\"plasma\",\n)\n</pre> eegs = eeg.sample(n=1000) _ = pd.plotting.scatter_matrix(     X.iloc[:100, :4],     c=y[:100],     figsize=(10, 10),     marker=\"o\",     hist_kwds={\"bins\": 20},     alpha=0.8,     cmap=\"plasma\", ) In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() <p>Edit non-critical fields, allowed for all authorized users: description, creator, contributor, collection_date, language, citation, original_data_url, paper_url</p> In\u00a0[\u00a0]: Copied! <pre>desc = (\n    \"This data sets consists of 3 different types of irises' \"\n    \"(Setosa, Versicolour, and Virginica) petal and sepal length,\"\n    \" stored in a 150x4 numpy.ndarray\"\n)\ndid = 128\ndata_id = edit_dataset(\n    did,\n    description=desc,\n    creator=\"R.A.Fisher\",\n    collection_date=\"1937\",\n    citation=\"The use of multiple measurements in taxonomic problems\",\n    language=\"English\",\n)\nedited_dataset = get_dataset(data_id)\nprint(f\"Edited dataset ID: {data_id}\")\n</pre> desc = (     \"This data sets consists of 3 different types of irises' \"     \"(Setosa, Versicolour, and Virginica) petal and sepal length,\"     \" stored in a 150x4 numpy.ndarray\" ) did = 128 data_id = edit_dataset(     did,     description=desc,     creator=\"R.A.Fisher\",     collection_date=\"1937\",     citation=\"The use of multiple measurements in taxonomic problems\",     language=\"English\", ) edited_dataset = get_dataset(data_id) print(f\"Edited dataset ID: {data_id}\") <p>Editing critical fields (default_target_attribute, row_id_attribute, ignore_attribute) is allowed only for the dataset owner. Further, critical fields cannot be edited if the dataset has any tasks associated with it. To edit critical fields of a dataset (without tasks) owned by you, configure the API key: openml.config.apikey = 'FILL_IN_OPENML_API_KEY' This example here only shows a failure when trying to work on a dataset not owned by you:</p> In\u00a0[\u00a0]: Copied! <pre>try:\n    data_id = edit_dataset(1, default_target_attribute=\"shape\")\nexcept openml.exceptions.OpenMLServerException as e:\n    print(e)\n</pre> try:     data_id = edit_dataset(1, default_target_attribute=\"shape\") except openml.exceptions.OpenMLServerException as e:     print(e) In\u00a0[\u00a0]: Copied! <pre>data_id = fork_dataset(1)\nprint(data_id)\ndata_id = edit_dataset(data_id, default_target_attribute=\"shape\")\nprint(f\"Forked dataset ID: {data_id}\")\n</pre> data_id = fork_dataset(1) print(data_id) data_id = edit_dataset(data_id, default_target_attribute=\"shape\") print(f\"Forked dataset ID: {data_id}\") In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n# License: BSD 3-Clauses\n</pre> openml.config.stop_using_configuration_for_example() # License: BSD 3-Clauses"},{"location":"examples/30_extended/datasets_tutorial/#datasets","title":"Datasets\u00b6","text":"<p>How to list and download datasets.</p> <p>import pandas as pd</p> <p>import openml from openml.datasets import edit_dataset, fork_dataset, get_dataset</p>"},{"location":"examples/30_extended/datasets_tutorial/#exercise-0","title":"Exercise 0\u00b6","text":"<ul> <li>List datasets and return a dataframe datalist = openml.datasets.list_datasets() datalist = datalist[[\"did\", \"name\", \"NumberOfInstances\", \"NumberOfFeatures\", \"NumberOfClasses\"]]</li> </ul> <p>print(f\"First 10 of {len(datalist)} datasets...\") datalist.head(n=10)</p> <p>The same can be done with lesser lines of code openml_df = openml.datasets.list_datasets() openml_df.head(n=10)</p>"},{"location":"examples/30_extended/datasets_tutorial/#exercise-1","title":"Exercise 1\u00b6","text":"<ul> <li>Find datasets with more than 10000 examples.</li> <li>Find a dataset called 'eeg_eye_state'.</li> <li>Find all datasets with more than 50 classes.</li> </ul>"},{"location":"examples/30_extended/datasets_tutorial/#download-datasets","title":"Download datasets\u00b6","text":""},{"location":"examples/30_extended/datasets_tutorial/#exercise-2","title":"Exercise 2\u00b6","text":"<ul> <li>Explore the data visually.</li> </ul>"},{"location":"examples/30_extended/datasets_tutorial/#edit-a-created-dataset","title":"Edit a created dataset\u00b6","text":"<p>This example uses the test server, to avoid editing a dataset on the main server.</p> <p>.. warning:: .. include:: ../../test_server_usage_warning.txt</p>"},{"location":"examples/30_extended/datasets_tutorial/#fork-dataset","title":"Fork dataset\u00b6","text":"<p>Used to create a copy of the dataset with you as the owner. Use this API only if you are unable to edit the critical fields (default_target_attribute, ignore_attribute, row_id_attribute) of a dataset through the edit_dataset API. After the dataset is forked, you can edit the new version of the dataset using edit_dataset.</p>"},{"location":"examples/30_extended/fetch_evaluations_tutorial/","title":"Fetching Evaluations","text":"In\u00a0[\u00a0]: Copied! <pre>import openml\n</pre> import openml In\u00a0[\u00a0]: Copied! <pre>openml.evaluations.list_evaluations(\n    function=\"predictive_accuracy\", size=10\n)\n\n# Using other evaluation metrics, 'precision' in this case\nevals = openml.evaluations.list_evaluations(\n    function=\"precision\", size=10, output_format=\"dataframe\"\n)\n\n# Querying the returned results for precision above 0.98\nprint(evals[evals.value &gt; 0.98])\n</pre> openml.evaluations.list_evaluations(     function=\"predictive_accuracy\", size=10 )  # Using other evaluation metrics, 'precision' in this case evals = openml.evaluations.list_evaluations(     function=\"precision\", size=10, output_format=\"dataframe\" )  # Querying the returned results for precision above 0.98 print(evals[evals.value &gt; 0.98]) In\u00a0[\u00a0]: Copied! <pre>task_id = 167140  # https://www.openml.org/t/167140\ntask = openml.tasks.get_task(task_id)\nprint(task)\n</pre> task_id = 167140  # https://www.openml.org/t/167140 task = openml.tasks.get_task(task_id) print(task) In\u00a0[\u00a0]: Copied! <pre>metric = \"predictive_accuracy\"\nevals = openml.evaluations.list_evaluations(\n    function=metric, tasks=[task_id], output_format=\"dataframe\"\n)\n# Displaying the first 10 rows\nprint(evals.head(n=10))\n# Sorting the evaluations in decreasing order of the metric chosen\nevals = evals.sort_values(by=\"value\", ascending=False)\nprint(\"\\nDisplaying head of sorted dataframe: \")\nprint(evals.head())\n</pre> metric = \"predictive_accuracy\" evals = openml.evaluations.list_evaluations(     function=metric, tasks=[task_id], output_format=\"dataframe\" ) # Displaying the first 10 rows print(evals.head(n=10)) # Sorting the evaluations in decreasing order of the metric chosen evals = evals.sort_values(by=\"value\", ascending=False) print(\"\\nDisplaying head of sorted dataframe: \") print(evals.head()) In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\n\n\ndef plot_cdf(values, metric=\"predictive_accuracy\"):\n    max_val = max(values)\n    n, bins, patches = plt.hist(values, density=True, histtype=\"step\", cumulative=True, linewidth=3)\n    patches[0].set_xy(patches[0].get_xy()[:-1])\n    plt.xlim(max(0, min(values) - 0.1), 1)\n    plt.title(\"CDF\")\n    plt.xlabel(metric)\n    plt.ylabel(\"Likelihood\")\n    plt.grid(visible=True, which=\"major\", linestyle=\"-\")\n    plt.minorticks_on()\n    plt.grid(visible=True, which=\"minor\", linestyle=\"--\")\n    plt.axvline(max_val, linestyle=\"--\", color=\"gray\")\n    plt.text(max_val, 0, f\"{max_val:.3f}\", fontsize=9)\n    plt.show()\n\n\nplot_cdf(evals.value, metric)\n</pre> from matplotlib import pyplot as plt   def plot_cdf(values, metric=\"predictive_accuracy\"):     max_val = max(values)     n, bins, patches = plt.hist(values, density=True, histtype=\"step\", cumulative=True, linewidth=3)     patches[0].set_xy(patches[0].get_xy()[:-1])     plt.xlim(max(0, min(values) - 0.1), 1)     plt.title(\"CDF\")     plt.xlabel(metric)     plt.ylabel(\"Likelihood\")     plt.grid(visible=True, which=\"major\", linestyle=\"-\")     plt.minorticks_on()     plt.grid(visible=True, which=\"minor\", linestyle=\"--\")     plt.axvline(max_val, linestyle=\"--\", color=\"gray\")     plt.text(max_val, 0, f\"{max_val:.3f}\", fontsize=9)     plt.show()   plot_cdf(evals.value, metric) <p>This CDF plot shows that for the given task, based on the results of the runs uploaded, it is almost certain to achieve an accuracy above 52%, i.e., with non-zero probability. While the maximum accuracy seen till now is 96.5%.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\n\n\ndef plot_flow_compare(evaluations, top_n=10, metric=\"predictive_accuracy\"):\n    # Collecting the top 10 performing unique flow_id\n    flow_ids = evaluations.flow_id.unique()[:top_n]\n\n    df = pd.DataFrame()\n    # Creating a data frame containing only the metric values of the selected flows\n    #   assuming evaluations is sorted in decreasing order of metric\n    for i in range(len(flow_ids)):\n        flow_values = evaluations[evaluations.flow_id == flow_ids[i]].value\n        df = pd.concat([df, flow_values], ignore_index=True, axis=1)\n    fig, axs = plt.subplots()\n    df.boxplot()\n    axs.set_title(\"Boxplot comparing \" + metric + \" for different flows\")\n    axs.set_ylabel(metric)\n    axs.set_xlabel(\"Flow ID\")\n    axs.set_xticklabels(flow_ids)\n    axs.grid(which=\"major\", linestyle=\"-\", linewidth=\"0.5\", color=\"gray\", axis=\"y\")\n    axs.minorticks_on()\n    axs.grid(which=\"minor\", linestyle=\"--\", linewidth=\"0.5\", color=\"gray\", axis=\"y\")\n    # Counting the number of entries for each flow in the data frame\n    #   which gives the number of runs for each flow\n    flow_freq = list(df.count(axis=0, numeric_only=True))\n    for i in range(len(flow_ids)):\n        axs.text(i + 1.05, np.nanmin(df.values), str(flow_freq[i]) + \"\\nrun(s)\", fontsize=7)\n    plt.show()\n\n\nplot_flow_compare(evals, metric=metric, top_n=10)\n</pre> import numpy as np import pandas as pd   def plot_flow_compare(evaluations, top_n=10, metric=\"predictive_accuracy\"):     # Collecting the top 10 performing unique flow_id     flow_ids = evaluations.flow_id.unique()[:top_n]      df = pd.DataFrame()     # Creating a data frame containing only the metric values of the selected flows     #   assuming evaluations is sorted in decreasing order of metric     for i in range(len(flow_ids)):         flow_values = evaluations[evaluations.flow_id == flow_ids[i]].value         df = pd.concat([df, flow_values], ignore_index=True, axis=1)     fig, axs = plt.subplots()     df.boxplot()     axs.set_title(\"Boxplot comparing \" + metric + \" for different flows\")     axs.set_ylabel(metric)     axs.set_xlabel(\"Flow ID\")     axs.set_xticklabels(flow_ids)     axs.grid(which=\"major\", linestyle=\"-\", linewidth=\"0.5\", color=\"gray\", axis=\"y\")     axs.minorticks_on()     axs.grid(which=\"minor\", linestyle=\"--\", linewidth=\"0.5\", color=\"gray\", axis=\"y\")     # Counting the number of entries for each flow in the data frame     #   which gives the number of runs for each flow     flow_freq = list(df.count(axis=0, numeric_only=True))     for i in range(len(flow_ids)):         axs.text(i + 1.05, np.nanmin(df.values), str(flow_freq[i]) + \"\\nrun(s)\", fontsize=7)     plt.show()   plot_flow_compare(evals, metric=metric, top_n=10) <p>The boxplots below show how the flows perform across multiple runs on the chosen task. The green horizontal lines represent the median accuracy of all the runs for that flow (number of runs denoted at the bottom of the boxplots). The higher the green line, the better the flow is for the task at hand. The ordering of the flows are in the descending order of the higest accuracy value seen under that flow.</p> <p>Printing the corresponding flow names for the top 10 performing flow IDs</p> In\u00a0[\u00a0]: Copied! <pre>top_n = 10\nflow_ids = evals.flow_id.unique()[:top_n]\nflow_names = evals.flow_name.unique()[:top_n]\nfor i in range(top_n):\n    print((flow_ids[i], flow_names[i]))\n</pre> top_n = 10 flow_ids = evals.flow_id.unique()[:top_n] flow_names = evals.flow_name.unique()[:top_n] for i in range(top_n):     print((flow_ids[i], flow_names[i])) In\u00a0[\u00a0]: Copied! <pre>evals_setups = openml.evaluations.list_evaluations_setups(\n    function=\"predictive_accuracy\",\n    tasks=[31],\n    size=100,\n    sort_order=\"desc\",\n)\n\nprint(evals_setups.head())\n</pre> evals_setups = openml.evaluations.list_evaluations_setups(     function=\"predictive_accuracy\",     tasks=[31],     size=100,     sort_order=\"desc\", )  print(evals_setups.head()) <p>Return evaluations for flow_id in descending order based on predictive_accuracy with hyperparameters. parameters_in_separate_columns returns parameters in separate columns</p> In\u00a0[\u00a0]: Copied! <pre>evals_setups = openml.evaluations.list_evaluations_setups(\n    function=\"predictive_accuracy\", flows=[6767], size=100, parameters_in_separate_columns=True\n)\n\nprint(evals_setups.head(10))\n\n# License: BSD 3-Clause\n</pre> evals_setups = openml.evaluations.list_evaluations_setups(     function=\"predictive_accuracy\", flows=[6767], size=100, parameters_in_separate_columns=True )  print(evals_setups.head(10))  # License: BSD 3-Clause"},{"location":"examples/30_extended/fetch_evaluations_tutorial/#fetching-evaluations","title":"Fetching Evaluations\u00b6","text":"<p>Evaluations contain a concise summary of the results of all runs made. Each evaluation provides information on the dataset used, the flow applied, the setup used, the metric evaluated, and the result obtained on the metric, for each such run made. These collection of results can be used for efficient benchmarking of an algorithm and also allow transparent reuse of results from previous experiments on similar parameters.</p> <p>In this example, we shall do the following:</p> <ul> <li>Retrieve evaluations based on different metrics</li> <li>Fetch evaluations pertaining to a specific task</li> <li>Sort the obtained results in descending order of the metric</li> <li>Plot a cumulative distribution function for the evaluations</li> <li>Compare the top 10 performing flows based on the evaluation performance</li> <li>Retrieve evaluations with hyperparameter settings</li> </ul>"},{"location":"examples/30_extended/fetch_evaluations_tutorial/#listing-evaluations","title":"Listing evaluations\u00b6","text":"<p>Evaluations can be retrieved from the database in the chosen output format. Required filters can be applied to retrieve results from runs as required.</p> <p>We shall retrieve a small set (only 10 entries) to test the listing function for evaluations</p>"},{"location":"examples/30_extended/fetch_evaluations_tutorial/#viewing-a-sample-task","title":"Viewing a sample task\u00b6","text":"<p>Over here we shall briefly take a look at the details of the task. We will start by displaying a simple supervised classification task:</p>"},{"location":"examples/30_extended/fetch_evaluations_tutorial/#obtaining-all-the-evaluations-for-the-task","title":"Obtaining all the evaluations for the task\u00b6","text":"<p>We'll now obtain all the evaluations that were uploaded for the task we displayed previously. Note that we now filter the evaluations based on another parameter 'task'.</p>"},{"location":"examples/30_extended/fetch_evaluations_tutorial/#obtaining-cdf-of-metric-for-chosen-task","title":"Obtaining CDF of metric for chosen task\u00b6","text":"<p>We shall now analyse how the performance of various flows have been on this task, by seeing the likelihood of the accuracy obtained across all runs. We shall now plot a cumulative distributive function (CDF) for the accuracies obtained.</p>"},{"location":"examples/30_extended/fetch_evaluations_tutorial/#comparing-top-10-performing-flows","title":"Comparing top 10 performing flows\u00b6","text":"<p>Let us now try to see which flows generally performed the best for this task. For this, we shall compare the top performing flows.</p>"},{"location":"examples/30_extended/fetch_evaluations_tutorial/#obtaining-evaluations-with-hyperparameter-settings","title":"Obtaining evaluations with hyperparameter settings\u00b6","text":"<p>We'll now obtain the evaluations of a task and a flow with the hyperparameters</p> <p>List evaluations in descending order based on predictive_accuracy with hyperparameters</p>"},{"location":"examples/30_extended/fetch_runtimes_tutorial/","title":"Preparing tasks and scikit-learn models","text":"<p>Measuring runtimes for Scikit-learn models</p> <p>The runtime of machine learning models on specific datasets can be a deciding factor on the choice of algorithms, especially for benchmarking and comparison purposes. OpenML's scikit-learn extension provides runtime data from runs of model fit and prediction on tasks or datasets, for both the CPU-clock as well as the actual wallclock-time incurred. The objective of this example is to illustrate how to retrieve such timing measures, and also offer some potential means of usage and interpretation of the same.</p> <p>It should be noted that there are multiple levels at which parallelism can occur.</p> <ul> <li><p>At the outermost level, OpenML tasks contain fixed data splits, on which the defined model/flow is executed. Thus, a model can be fit on each OpenML dataset fold in parallel using the <code>n_jobs</code> parameter to <code>run_model_on_task</code> or <code>run_flow_on_task</code> (illustrated under Case 2 &amp; 3 below).</p> </li> <li><p>The model/flow specified can also include scikit-learn models that perform their own parallelization. For instance, by specifying <code>n_jobs</code> in a Random Forest model definition (covered under Case 2 below).</p> </li> <li><p>The sklearn model can further be an HPO estimator and contain it's own parallelization. If the base estimator used also supports <code>parallelization</code>, then there's at least a 2-level nested definition for parallelization possible (covered under Case 3 below).</p> </li> </ul> <p>We shall cover these 5 representative scenarios for:</p> <ul> <li><p>(Case 1) Retrieving runtimes for Random Forest training and prediction on each of the cross-validation folds</p> </li> <li><p>(Case 2) Testing the above setting in a parallel setup and monitor the difference using runtimes retrieved</p> </li> <li><p>(Case 3) Comparing RandomSearchCV and GridSearchCV on the above task based on runtimes</p> </li> <li><p>(Case 4) Running models that don't run in parallel or models which scikit-learn doesn't parallelize</p> </li> <li><p>(Case 5) Running models that do not release the Python Global Interpreter Lock (GIL)</p> </li> </ul> <p>import openml import numpy as np from matplotlib import pyplot as plt from joblib.parallel import parallel_backend</p> <p>from sklearn.naive_bayes import GaussianNB from sklearn.tree import DecisionTreeClassifier from sklearn.neural_network import MLPClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV, RandomizedSearchCV</p> In\u00a0[\u00a0]: Copied! <pre>task_id = 167119\n\ntask = openml.tasks.get_task(task_id)\nprint(task)\n\n# Viewing associated data\nn_repeats, n_folds, n_samples = task.get_split_dimensions()\nprint(\n    \"Task {}: number of repeats: {}, number of folds: {}, number of samples {}.\".format(\n        task_id,\n        n_repeats,\n        n_folds,\n        n_samples,\n    )\n)\n\n\n# Creating utility function\ndef print_compare_runtimes(measures):\n    for repeat, val1 in measures[\"usercpu_time_millis_training\"].items():\n        for fold, val2 in val1.items():\n            print(\n                \"Repeat #{}-Fold #{}: CPU-{:.3f} vs Wall-{:.3f}\".format(\n                    repeat, fold, val2, measures[\"wall_clock_time_millis_training\"][repeat][fold]\n                )\n            )\n</pre> task_id = 167119  task = openml.tasks.get_task(task_id) print(task)  # Viewing associated data n_repeats, n_folds, n_samples = task.get_split_dimensions() print(     \"Task {}: number of repeats: {}, number of folds: {}, number of samples {}.\".format(         task_id,         n_repeats,         n_folds,         n_samples,     ) )   # Creating utility function def print_compare_runtimes(measures):     for repeat, val1 in measures[\"usercpu_time_millis_training\"].items():         for fold, val2 in val1.items():             print(                 \"Repeat #{}-Fold #{}: CPU-{:.3f} vs Wall-{:.3f}\".format(                     repeat, fold, val2, measures[\"wall_clock_time_millis_training\"][repeat][fold]                 )             ) In\u00a0[\u00a0]: Copied! <pre>clf = RandomForestClassifier(n_estimators=10)\n\nrun1 = openml.runs.run_model_on_task(\n    model=clf,\n    task=task,\n    upload_flow=False,\n    avoid_duplicate_runs=False,\n)\nmeasures = run1.fold_evaluations\n\nprint(\"The timing and performance metrics available: \")\nfor key in measures.keys():\n    print(key)\nprint()\n\nprint(\n    \"The performance metric is recorded under `predictive_accuracy` per \"\n    \"fold and can be retrieved as: \"\n)\nfor repeat, val1 in measures[\"predictive_accuracy\"].items():\n    for fold, val2 in val1.items():\n        print(f\"Repeat #{repeat}-Fold #{fold}: {val2:.4f}\")\n    print()\n</pre> clf = RandomForestClassifier(n_estimators=10)  run1 = openml.runs.run_model_on_task(     model=clf,     task=task,     upload_flow=False,     avoid_duplicate_runs=False, ) measures = run1.fold_evaluations  print(\"The timing and performance metrics available: \") for key in measures.keys():     print(key) print()  print(     \"The performance metric is recorded under `predictive_accuracy` per \"     \"fold and can be retrieved as: \" ) for repeat, val1 in measures[\"predictive_accuracy\"].items():     for fold, val2 in val1.items():         print(f\"Repeat #{repeat}-Fold #{fold}: {val2:.4f}\")     print() <p>The remaining entries recorded in <code>measures</code> are the runtime records related as:</p> <p>usercpu_time_millis = usercpu_time_millis_training + usercpu_time_millis_testing</p> <p>wall_clock_time_millis = wall_clock_time_millis_training + wall_clock_time_millis_testing</p> <p>The timing measures recorded as <code>*_millis_training</code> contain the per repeat-per fold timing incurred for the execution of the <code>.fit()</code> procedure of the model. For <code>usercpu_time_*</code> the time recorded using <code>time.process_time()</code> is converted to <code>milliseconds</code> and stored. Similarly, <code>time.time()</code> is used to record the time entry for <code>wall_clock_time_*</code>. The <code>*_millis_testing</code> entry follows the same procedure but for time taken for the <code>.predict()</code> procedure.</p> <p>Comparing the CPU and wall-clock training times of the Random Forest model</p> In\u00a0[\u00a0]: Copied! <pre>print_compare_runtimes(measures)\n</pre> print_compare_runtimes(measures) In\u00a0[\u00a0]: Copied! <pre>clf = RandomForestClassifier(n_estimators=10, n_jobs=2)\n\nrun2 = openml.runs.run_model_on_task(\n    model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False\n)\nmeasures = run2.fold_evaluations\n# The wall-clock time recorded per fold should be lesser than Case 1 above\nprint_compare_runtimes(measures)\n</pre> clf = RandomForestClassifier(n_estimators=10, n_jobs=2)  run2 = openml.runs.run_model_on_task(     model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False ) measures = run2.fold_evaluations # The wall-clock time recorded per fold should be lesser than Case 1 above print_compare_runtimes(measures) <p>Running a Random Forest model on an OpenML task in parallel (all cores available):</p> In\u00a0[\u00a0]: Copied! <pre># Redefining the model to use all available cores with `n_jobs=-1`\nclf = RandomForestClassifier(n_estimators=10, n_jobs=-1)\n\nrun3 = openml.runs.run_model_on_task(\n    model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False\n)\nmeasures = run3.fold_evaluations\n</pre> # Redefining the model to use all available cores with `n_jobs=-1` clf = RandomForestClassifier(n_estimators=10, n_jobs=-1)  run3 = openml.runs.run_model_on_task(     model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False ) measures = run3.fold_evaluations <p>The wall-clock time recorded per fold should be lesser than the case above, if more than 2 CPU cores are available. The speed-up is more pronounced for larger datasets. print_compare_runtimes(measures)</p> <p>We can now observe that the ratio of CPU time to wallclock time is lower than in case 1. This happens because joblib by default spawns subprocesses for the workloads for which CPU time cannot be tracked. Therefore, interpreting the reported CPU and wallclock time requires knowledge of the parallelization applied at runtime.</p> <p>Running the same task with a different parallel backend. Joblib provides multiple backends: {<code>loky</code> (default), <code>multiprocessing</code>, <code>dask</code>, <code>threading</code>, <code>sequential</code>}. The backend can be explicitly set using a joblib context manager. The behaviour of the job distribution can change and therefore the scale of runtimes recorded too.</p> In\u00a0[\u00a0]: Copied! <pre>with parallel_backend(backend=\"multiprocessing\", n_jobs=-1):\n    run3_ = openml.runs.run_model_on_task(\n        model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False\n    )\nmeasures = run3_.fold_evaluations\nprint_compare_runtimes(measures)\n</pre> with parallel_backend(backend=\"multiprocessing\", n_jobs=-1):     run3_ = openml.runs.run_model_on_task(         model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False     ) measures = run3_.fold_evaluations print_compare_runtimes(measures) <p>The CPU time interpretation becomes ambiguous when jobs are distributed over an unknown number of cores or when subprocesses are spawned for which the CPU time cannot be tracked, as in the examples above. It is impossible for OpenML-Python to capture the availability of the number of cores/threads, their eventual utilisation and whether workloads are executed in subprocesses, for various cases that can arise as demonstrated in the rest of the example. Therefore, the final interpretation of the runtimes is left to the <code>user</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import GridSearchCV\n\nclf = RandomForestClassifier(n_estimators=10, n_jobs=2)\n\n# GridSearchCV model\nn_iter = 5\ngrid_pipe = GridSearchCV(\n    estimator=clf,\n    param_grid={\"n_estimators\": np.linspace(start=1, stop=50, num=n_iter).astype(int).tolist()},\n    cv=2,\n    n_jobs=2,\n)\n\nrun4 = openml.runs.run_model_on_task(\n    model=grid_pipe, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2\n)\nmeasures = run4.fold_evaluations\nprint_compare_runtimes(measures)\n</pre> from sklearn.model_selection import GridSearchCV  clf = RandomForestClassifier(n_estimators=10, n_jobs=2)  # GridSearchCV model n_iter = 5 grid_pipe = GridSearchCV(     estimator=clf,     param_grid={\"n_estimators\": np.linspace(start=1, stop=50, num=n_iter).astype(int).tolist()},     cv=2,     n_jobs=2, )  run4 = openml.runs.run_model_on_task(     model=grid_pipe, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2 ) measures = run4.fold_evaluations print_compare_runtimes(measures) <p>Like any optimisation problem, scikit-learn's HPO estimators also generate a sequence of configurations which are evaluated, using which the best found configuration is tracked throughout the trace. The OpenML run object stores these traces as OpenMLRunTrace objects accessible using keys of the pattern (repeat, fold, iterations). Here <code>fold</code> implies the outer-cross validation fold as obtained from the task data splits in OpenML. GridSearchCV here performs grid search over the inner-cross validation folds as parameterized by the <code>cv</code> parameter. Since <code>GridSearchCV</code> in this example performs a <code>2-fold</code> cross validation, the runtime recorded per repeat-per fold in the run object is for the entire <code>fit()</code> procedure of GridSearchCV thus subsuming the runtimes of the 2-fold (inner) CV search performed.</p> In\u00a0[\u00a0]: Copied! <pre># We earlier extracted the number of repeats and folds for this task:\nprint(f\"# repeats: {n_repeats}\\n# folds: {n_folds}\")\n\n# To extract the training runtime of the first repeat, first fold:\nprint(run4.fold_evaluations[\"wall_clock_time_millis_training\"][0][0])\n</pre> # We earlier extracted the number of repeats and folds for this task: print(f\"# repeats: {n_repeats}\\n# folds: {n_folds}\")  # To extract the training runtime of the first repeat, first fold: print(run4.fold_evaluations[\"wall_clock_time_millis_training\"][0][0]) <p>To extract the training runtime of the 1-st repeat, 4-th (outer) fold and also to fetch the parameters and performance of the evaluations made during the 1-st repeat, 4-th fold evaluation by the Grid Search model.</p> In\u00a0[\u00a0]: Copied! <pre>_repeat = 0\n_fold = 3\nprint(\n    \"Total runtime for repeat {}'s fold {}: {:4f} ms\".format(\n        _repeat, _fold, run4.fold_evaluations[\"wall_clock_time_millis_training\"][_repeat][_fold]\n    )\n)\nfor i in range(n_iter):\n    key = (_repeat, _fold, i)\n    r = run4.trace.trace_iterations[key]\n    print(\n        \"n_estimators: {:&gt;2} - score: {:.3f}\".format(\n            r.parameters[\"parameter_n_estimators\"], r.evaluation\n        )\n    )\n</pre> _repeat = 0 _fold = 3 print(     \"Total runtime for repeat {}'s fold {}: {:4f} ms\".format(         _repeat, _fold, run4.fold_evaluations[\"wall_clock_time_millis_training\"][_repeat][_fold]     ) ) for i in range(n_iter):     key = (_repeat, _fold, i)     r = run4.trace.trace_iterations[key]     print(         \"n_estimators: {:&gt;2} - score: {:.3f}\".format(             r.parameters[\"parameter_n_estimators\"], r.evaluation         )     ) <p>Scikit-learn's HPO estimators also come with an argument <code>refit=True</code> as a default. In our previous model definition it was set to True by default, which meant that the best found hyperparameter configuration was used to refit or retrain the model without any inner cross validation. This extra refit time measure is provided by the scikit-learn model as the attribute <code>refit_time_</code>. This time is included in the <code>wall_clock_time_millis_training</code> measure.</p> <p>For non-HPO estimators, <code>wall_clock_time_millis = wall_clock_time_millis_training + wall_clock_time_millis_testing</code>.</p> <p>For HPO estimators, <code>wall_clock_time_millis = wall_clock_time_millis_training + wall_clock_time_millis_testing + refit_time</code>.</p> <p>This refit time can therefore be explicitly extracted in this manner:</p> In\u00a0[\u00a0]: Copied! <pre>def extract_refit_time(run, repeat, fold):\n    refit_time = (\n        run.fold_evaluations[\"wall_clock_time_millis\"][repeat][fold]\n        - run.fold_evaluations[\"wall_clock_time_millis_training\"][repeat][fold]\n        - run.fold_evaluations[\"wall_clock_time_millis_testing\"][repeat][fold]\n    )\n    return refit_time\n\n\nfor repeat in range(n_repeats):\n    for fold in range(n_folds):\n        print(\n            \"Repeat #{}-Fold #{}: {:.4f}\".format(\n                repeat, fold, extract_refit_time(run4, repeat, fold)\n            )\n        )\n</pre>  def extract_refit_time(run, repeat, fold):     refit_time = (         run.fold_evaluations[\"wall_clock_time_millis\"][repeat][fold]         - run.fold_evaluations[\"wall_clock_time_millis_training\"][repeat][fold]         - run.fold_evaluations[\"wall_clock_time_millis_testing\"][repeat][fold]     )     return refit_time   for repeat in range(n_repeats):     for fold in range(n_folds):         print(             \"Repeat #{}-Fold #{}: {:.4f}\".format(                 repeat, fold, extract_refit_time(run4, repeat, fold)             )         ) <p>Along with the GridSearchCV already used above, we demonstrate how such optimisation traces can be retrieved by showing an application of these traces - comparing the speed of finding the best configuration using RandomizedSearchCV and GridSearchCV available with scikit-learn.</p> In\u00a0[\u00a0]: Copied! <pre># RandomizedSearchCV model\nrs_pipe = RandomizedSearchCV(\n    estimator=clf,\n    param_distributions={\n        \"n_estimators\": np.linspace(start=1, stop=50, num=15).astype(int).tolist()\n    },\n    cv=2,\n    n_iter=n_iter,\n    n_jobs=2,\n)\nrun5 = openml.runs.run_model_on_task(\n    model=rs_pipe, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2\n)\n</pre> # RandomizedSearchCV model rs_pipe = RandomizedSearchCV(     estimator=clf,     param_distributions={         \"n_estimators\": np.linspace(start=1, stop=50, num=15).astype(int).tolist()     },     cv=2,     n_iter=n_iter,     n_jobs=2, ) run5 = openml.runs.run_model_on_task(     model=rs_pipe, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2 ) <p>Since for the call to <code>openml.runs.run_model_on_task</code> the parameter <code>n_jobs</code> is set to its default <code>None</code>, the evaluations across the OpenML folds are not parallelized. Hence, the time recorded is agnostic to the <code>n_jobs</code> being set at both the HPO estimator <code>GridSearchCV</code> as well as the base estimator <code>RandomForestClassifier</code> in this case. The OpenML extension only records the time taken for the completion of the complete <code>fit()</code> call, per-repeat per-fold.</p> <p>This notion can be used to extract and plot the best found performance per fold by the HPO model and the corresponding time taken for search across that fold. Moreover, since <code>n_jobs=None</code> for <code>openml.runs.run_model_on_task</code> the runtimes per fold can be cumulatively added to plot the trace against time.</p> In\u00a0[\u00a0]: Copied! <pre>def extract_trace_data(run, n_repeats, n_folds, n_iter, key=None):\n    key = \"wall_clock_time_millis_training\" if key is None else key\n    data = {\"score\": [], \"runtime\": []}\n    for i_r in range(n_repeats):\n        for i_f in range(n_folds):\n            data[\"runtime\"].append(run.fold_evaluations[key][i_r][i_f])\n            for i_i in range(n_iter):\n                r = run.trace.trace_iterations[(i_r, i_f, i_i)]\n                if r.selected:\n                    data[\"score\"].append(r.evaluation)\n                    break\n    return data\n\n\ndef get_incumbent_trace(trace):\n    best_score = 1\n    inc_trace = []\n    for i, r in enumerate(trace):\n        if i == 0 or (1 - r) &lt; best_score:\n            best_score = 1 - r\n        inc_trace.append(best_score)\n    return inc_trace\n\n\ngrid_data = extract_trace_data(run4, n_repeats, n_folds, n_iter)\nrs_data = extract_trace_data(run5, n_repeats, n_folds, n_iter)\n\nplt.clf()\nplt.plot(\n    np.cumsum(grid_data[\"runtime\"]), get_incumbent_trace(grid_data[\"score\"]), label=\"Grid Search\"\n)\nplt.plot(\n    np.cumsum(rs_data[\"runtime\"]), get_incumbent_trace(rs_data[\"score\"]), label=\"Random Search\"\n)\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"Wallclock time (in milliseconds)\")\nplt.ylabel(\"1 - Accuracy\")\nplt.title(\"Optimisation Trace Comparison\")\nplt.legend()\nplt.show()\n</pre> def extract_trace_data(run, n_repeats, n_folds, n_iter, key=None):     key = \"wall_clock_time_millis_training\" if key is None else key     data = {\"score\": [], \"runtime\": []}     for i_r in range(n_repeats):         for i_f in range(n_folds):             data[\"runtime\"].append(run.fold_evaluations[key][i_r][i_f])             for i_i in range(n_iter):                 r = run.trace.trace_iterations[(i_r, i_f, i_i)]                 if r.selected:                     data[\"score\"].append(r.evaluation)                     break     return data   def get_incumbent_trace(trace):     best_score = 1     inc_trace = []     for i, r in enumerate(trace):         if i == 0 or (1 - r) &lt; best_score:             best_score = 1 - r         inc_trace.append(best_score)     return inc_trace   grid_data = extract_trace_data(run4, n_repeats, n_folds, n_iter) rs_data = extract_trace_data(run5, n_repeats, n_folds, n_iter)  plt.clf() plt.plot(     np.cumsum(grid_data[\"runtime\"]), get_incumbent_trace(grid_data[\"score\"]), label=\"Grid Search\" ) plt.plot(     np.cumsum(rs_data[\"runtime\"]), get_incumbent_trace(rs_data[\"score\"]), label=\"Random Search\" ) plt.xscale(\"log\") plt.yscale(\"log\") plt.xlabel(\"Wallclock time (in milliseconds)\") plt.ylabel(\"1 - Accuracy\") plt.title(\"Optimisation Trace Comparison\") plt.legend() plt.show() In\u00a0[\u00a0]: Copied! <pre>dt = DecisionTreeClassifier()\n\nrun6 = openml.runs.run_model_on_task(\n    model=dt, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2\n)\nmeasures = run6.fold_evaluations\nprint_compare_runtimes(measures)\n</pre> dt = DecisionTreeClassifier()  run6 = openml.runs.run_model_on_task(     model=dt, task=task, upload_flow=False, avoid_duplicate_runs=False, n_jobs=2 ) measures = run6.fold_evaluations print_compare_runtimes(measures) <p>Although the decision tree does not run in parallel, it can release the <code>Python GIL &lt;https://docs.python.org/dev/glossary.html#term-global-interpreter-lock&gt;</code>_. This can result in surprising runtime measures as demonstrated below:</p> In\u00a0[\u00a0]: Copied! <pre>with parallel_backend(\"threading\", n_jobs=-1):\n    run7 = openml.runs.run_model_on_task(\n        model=dt, task=task, upload_flow=False, avoid_duplicate_runs=False\n    )\nmeasures = run7.fold_evaluations\nprint_compare_runtimes(measures)\n</pre> with parallel_backend(\"threading\", n_jobs=-1):     run7 = openml.runs.run_model_on_task(         model=dt, task=task, upload_flow=False, avoid_duplicate_runs=False     ) measures = run7.fold_evaluations print_compare_runtimes(measures) <p>Running a Neural Network from scikit-learn that uses scikit-learn independent parallelism using libraries such as MKL, OpenBLAS or BLIS.</p> In\u00a0[\u00a0]: Copied! <pre>mlp = MLPClassifier(max_iter=10)\n\nrun8 = openml.runs.run_model_on_task(\n    model=mlp, task=task, upload_flow=False, avoid_duplicate_runs=False\n)\nmeasures = run8.fold_evaluations\nprint_compare_runtimes(measures)\n</pre> mlp = MLPClassifier(max_iter=10)  run8 = openml.runs.run_model_on_task(     model=mlp, task=task, upload_flow=False, avoid_duplicate_runs=False ) measures = run8.fold_evaluations print_compare_runtimes(measures) In\u00a0[\u00a0]: Copied! <pre>clf = GaussianNB()\n\nwith parallel_backend(\"multiprocessing\", n_jobs=-1):\n    run9 = openml.runs.run_model_on_task(\n        model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False\n    )\nmeasures = run9.fold_evaluations\nprint_compare_runtimes(measures)\n</pre> clf = GaussianNB()  with parallel_backend(\"multiprocessing\", n_jobs=-1):     run9 = openml.runs.run_model_on_task(         model=clf, task=task, upload_flow=False, avoid_duplicate_runs=False     ) measures = run9.fold_evaluations print_compare_runtimes(measures)"},{"location":"examples/30_extended/fetch_runtimes_tutorial/#preparing-tasks-and-scikit-learn-models","title":"Preparing tasks and scikit-learn models\u00b6","text":""},{"location":"examples/30_extended/fetch_runtimes_tutorial/#case-1-running-a-random-forest-model-on-an-openml-task","title":"Case 1: Running a Random Forest model on an OpenML task\u00b6","text":"<p>We'll run a Random Forest model and obtain an OpenML run object. We can see the evaluations recorded per fold for the dataset and the information available for this run.</p>"},{"location":"examples/30_extended/fetch_runtimes_tutorial/#case-2-running-scikit-learn-model-on-an-openml-task-in-parallel","title":"Case 2: Running Scikit-learn model on an OpenML task in parallel\u00b6","text":"<p>Redefining the model to allow parallelism with <code>n_jobs=2</code> (2 cores)</p>"},{"location":"examples/30_extended/fetch_runtimes_tutorial/#case-3-running-and-benchmarking-hpo-algorithms-with-their-runtimes","title":"Case 3: Running and benchmarking HPO algorithms with their runtimes\u00b6","text":"<p>We shall now optimize a similar RandomForest model for the same task using scikit-learn's HPO support by using GridSearchCV to optimize our earlier RandomForest model's hyperparameter <code>n_estimators</code>. Scikit-learn also provides a <code>refit_time_</code> for such HPO models, i.e., the time incurred by training and evaluating the model on the best found parameter setting. This is included in the <code>wall_clock_time_millis_training</code> measure recorded.</p>"},{"location":"examples/30_extended/fetch_runtimes_tutorial/#case-4-running-models-that-scikit-learn-doesnt-parallelize","title":"Case 4: Running models that scikit-learn doesn't parallelize\u00b6","text":"<p>Both scikit-learn and OpenML depend on parallelism implemented through <code>joblib</code>. However, there can be cases where either models cannot be parallelized or don't depend on joblib for its parallelism. 2 such cases are illustrated below.</p> <p>Running a Decision Tree model that doesn't support parallelism implicitly, but using OpenML to parallelize evaluations for the outer-cross validation folds.</p>"},{"location":"examples/30_extended/fetch_runtimes_tutorial/#case-5-running-scikit-learn-models-that-dont-release-gil","title":"Case 5: Running Scikit-learn models that don't release GIL\u00b6","text":"<p>Certain Scikit-learn models do not release the Python GIL and are also not executed in parallel via a BLAS library. In such cases, the CPU times and wallclock times are most likely trustworthy. Note however that only very few models such as naive Bayes models are of this kind.</p>"},{"location":"examples/30_extended/fetch_runtimes_tutorial/#summmary","title":"Summmary\u00b6","text":"<p>The scikit-learn extension for OpenML-Python records model runtimes for the CPU-clock and the wall-clock times. The above examples illustrated how these recorded runtimes can be extracted when using a scikit-learn model and under parallel setups too. To summarize, the scikit-learn extension measures the:</p> <ul> <li><p><code>CPU-time</code> &amp; <code>wallclock-time</code> for the whole run</p> <ul> <li>A run here corresponds to a call to <code>run_model_on_task</code> or <code>run_flow_on_task</code></li> <li>The recorded time is for the model fit for each of the outer-cross validations folds, i.e., the OpenML data splits</li> </ul> </li> <li><p>Python's <code>time</code> module is used to compute the runtimes</p> <ul> <li><code>CPU-time</code> is recorded using the responses of <code>time.process_time()</code></li> <li><code>wallclock-time</code> is recorded using the responses of <code>time.time()</code></li> </ul> </li> <li><p>The timings recorded by OpenML per outer-cross validation fold is agnostic to model parallelisation</p> <ul> <li>The wallclock times reported in Case 2 above highlights the speed-up on using <code>n_jobs=-1</code> in comparison to <code>n_jobs=2</code>, since the timing recorded by OpenML is for the entire <code>fit()</code> procedure, whereas the parallelisation is performed inside <code>fit()</code> by scikit-learn</li> <li>The CPU-time for models that are run in parallel can be difficult to interpret</li> </ul> </li> <li><p><code>CPU-time</code> &amp; <code>wallclock-time</code> for each search per outer fold in an HPO run</p> <ul> <li>Reports the total time for performing search on each of the OpenML data split, subsuming any sort of parallelism that happened as part of the HPO estimator or the underlying base estimator</li> <li>Also allows extraction of the <code>refit_time</code> that scikit-learn measures using <code>time.time()</code> for retraining the model per outer fold, for the best found configuration</li> </ul> </li> <li><p><code>CPU-time</code> &amp; <code>wallclock-time</code> for models that scikit-learn doesn't parallelize</p> <ul> <li>Models like Decision Trees or naive Bayes don't parallelize and thus both the wallclock and CPU times are similar in runtime for the OpenML call</li> <li>However, models implemented in Cython, such as the Decision Trees can release the GIL and still run in parallel if a <code>threading</code> backend is used by joblib.</li> <li>Scikit-learn Neural Networks can undergo parallelization implicitly owing to thread-level parallelism involved in the linear algebraic operations and thus the wallclock-time and CPU-time can differ.</li> </ul> </li> </ul> <p>Because of all the cases mentioned above it is crucial to understand which case is triggered when reporting runtimes for scikit-learn models measured with OpenML-Python! License: BSD 3-Clause</p>"},{"location":"examples/30_extended/flow_id_tutorial/","title":"Obtaining Flow IDs","text":"In\u00a0[\u00a0]: Copied! <pre>import sklearn.tree\n\nimport openml\n</pre> import sklearn.tree  import openml <p>.. warning:: .. include:: ../../test_server_usage_warning.txt</p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\nopenml.config.server = \"https://api.openml.org/api/v1/xml\"\n</pre> openml.config.start_using_configuration_for_example() openml.config.server = \"https://api.openml.org/api/v1/xml\" In\u00a0[\u00a0]: Copied! <pre># Defining a classifier\nclf = sklearn.tree.DecisionTreeClassifier()\n</pre> # Defining a classifier clf = sklearn.tree.DecisionTreeClassifier() In\u00a0[\u00a0]: Copied! <pre>flow = openml.extensions.get_extension_by_model(clf).model_to_flow(clf).publish()\nflow_id = flow.flow_id\nprint(flow_id)\n</pre> flow = openml.extensions.get_extension_by_model(clf).model_to_flow(clf).publish() flow_id = flow.flow_id print(flow_id) <p>This piece of code is rather involved. First, it retrieves a :class:<code>~openml.extensions.Extension</code> which is registered and can handle the given model, in our case it is :class:<code>openml.extensions.sklearn.SklearnExtension</code>. Second, the extension converts the classifier into an instance of :class:<code>openml.OpenMLFlow</code>. Third and finally, the publish method checks whether the current flow is already present on OpenML. If not, it uploads the flow, otherwise, it updates the current instance with all information computed by the server (which is obviously also done when uploading/publishing a flow).</p> <p>To simplify the usage we have created a helper function which automates all these steps:</p> In\u00a0[\u00a0]: Copied! <pre>flow_id = openml.flows.get_flow_id(model=clf)\nprint(flow_id)\n</pre> flow_id = openml.flows.get_flow_id(model=clf) print(flow_id) In\u00a0[\u00a0]: Copied! <pre>print(flow.name, flow.external_version)\n</pre> print(flow.name, flow.external_version) <p>The name and external version are automatically added to a flow when constructing it from a model. We can then use them to retrieve the flow id as follows:</p> In\u00a0[\u00a0]: Copied! <pre>flow_id = openml.flows.flow_exists(name=flow.name, external_version=flow.external_version)\nprint(flow_id)\n</pre> flow_id = openml.flows.flow_exists(name=flow.name, external_version=flow.external_version) print(flow_id) <p>We can also retrieve all flows for a given name:</p> In\u00a0[\u00a0]: Copied! <pre>flow_ids = openml.flows.get_flow_id(name=flow.name)\nprint(flow_ids)\n</pre> flow_ids = openml.flows.get_flow_id(name=flow.name) print(flow_ids) <p>This also works with the actual model (generalizing the first part of this example):</p> In\u00a0[\u00a0]: Copied! <pre>flow_ids = openml.flows.get_flow_id(model=clf, exact_version=False)\nprint(flow_ids)\n</pre> flow_ids = openml.flows.get_flow_id(model=clf, exact_version=False) print(flow_ids) In\u00a0[\u00a0]: Copied! <pre># Deactivating test configuration\nopenml.config.stop_using_configuration_for_example()\n# License: BSD 3-Clause\n</pre> # Deactivating test configuration openml.config.stop_using_configuration_for_example() # License: BSD 3-Clause"},{"location":"examples/30_extended/flow_id_tutorial/#obtaining-flow-ids","title":"Obtaining Flow IDs\u00b6","text":"<p>This tutorial discusses different ways to obtain the ID of a flow in order to perform further analysis.</p>"},{"location":"examples/30_extended/flow_id_tutorial/#1-obtaining-a-flow-given-a-classifier","title":"1. Obtaining a flow given a classifier\u00b6","text":""},{"location":"examples/30_extended/flow_id_tutorial/#2-obtaining-a-flow-given-its-name","title":"2. Obtaining a flow given its name\u00b6","text":"<p>The schema of a flow is given in XSD ( here).  # noqa E501 Only two fields are required, a unique name, and an external version. While it should be pretty obvious why we need a name, the need for the additional external version information might not be immediately clear. However, this information is very important as it allows to have multiple flows with the same name for different versions of a software. This might be necessary if an algorithm or implementation introduces, renames or drop hyperparameters over time.</p>"},{"location":"examples/30_extended/flows_and_runs_tutorial/","title":"Flows and Runs","text":"In\u00a0[\u00a0]: Copied! <pre>import openml\nfrom sklearn import compose, ensemble, impute, neighbors, preprocessing, pipeline, tree\n\nimport openml\n</pre> import openml from sklearn import compose, ensemble, impute, neighbors, preprocessing, pipeline, tree  import openml <p>We'll use the test server for the rest of this tutorial.</p> <p>.. warning:: .. include:: ../../test_server_usage_warning.txt</p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() In\u00a0[\u00a0]: Copied! <pre># NOTE: We are using dataset 68 from the test server: https://test.openml.org/d/68\ndataset = openml.datasets.get_dataset(dataset_id=\"eeg-eye-state\", version=1)\nX, y, categorical_indicator, attribute_names = dataset.get_data(\n    target=dataset.default_target_attribute\n)\nclf = neighbors.KNeighborsClassifier(n_neighbors=1)\nclf.fit(X, y)\n</pre> # NOTE: We are using dataset 68 from the test server: https://test.openml.org/d/68 dataset = openml.datasets.get_dataset(dataset_id=\"eeg-eye-state\", version=1) X, y, categorical_indicator, attribute_names = dataset.get_data(     target=dataset.default_target_attribute ) clf = neighbors.KNeighborsClassifier(n_neighbors=1) clf.fit(X, y) <p>You can also ask for meta-data to automatically preprocess the data.</p> <ul> <li>e.g. categorical features -&gt; do feature encoding</li> </ul> In\u00a0[\u00a0]: Copied! <pre>dataset = openml.datasets.get_dataset(17)\nX, y, categorical_indicator, attribute_names = dataset.get_data(\n    target=dataset.default_target_attribute\n)\nprint(f\"Categorical features: {categorical_indicator}\")\ntransformer = compose.ColumnTransformer(\n    [(\"one_hot_encoder\", preprocessing.OneHotEncoder(categories=\"auto\"), categorical_indicator)]\n)\nX = transformer.fit_transform(X)\nclf.fit(X, y)\n</pre> dataset = openml.datasets.get_dataset(17) X, y, categorical_indicator, attribute_names = dataset.get_data(     target=dataset.default_target_attribute ) print(f\"Categorical features: {categorical_indicator}\") transformer = compose.ColumnTransformer(     [(\"one_hot_encoder\", preprocessing.OneHotEncoder(categories=\"auto\"), categorical_indicator)] ) X = transformer.fit_transform(X) clf.fit(X, y) In\u00a0[\u00a0]: Copied! <pre># Get a task\ntask = openml.tasks.get_task(403)\n\n# Build any classifier or pipeline\nclf = tree.DecisionTreeClassifier()\n\n# Run the flow\nrun = openml.runs.run_model_on_task(clf, task)\n\nprint(run)\n</pre> # Get a task task = openml.tasks.get_task(403)  # Build any classifier or pipeline clf = tree.DecisionTreeClassifier()  # Run the flow run = openml.runs.run_model_on_task(clf, task)  print(run) <p>Share the run on the OpenML server</p> <p>So far the run is only available locally. By calling the publish function, the run is sent to the OpenML server:</p> In\u00a0[\u00a0]: Copied! <pre>myrun = run.publish()\n# For this tutorial, our configuration publishes to the test server\n# as to not pollute the main server.\nprint(f\"Uploaded to {myrun.openml_url}\")\n</pre> myrun = run.publish() # For this tutorial, our configuration publishes to the test server # as to not pollute the main server. print(f\"Uploaded to {myrun.openml_url}\") <p>We can now also inspect the flow object which was automatically created:</p> In\u00a0[\u00a0]: Copied! <pre>flow = openml.flows.get_flow(run.flow_id)\nprint(flow)\n</pre> flow = openml.flows.get_flow(run.flow_id) print(flow) In\u00a0[\u00a0]: Copied! <pre>task = openml.tasks.get_task(96)\n\n# OpenML helper functions for sklearn can be plugged in directly for complicated pipelines\nfrom openml.extensions.sklearn import cat, cont\n\npipe = pipeline.Pipeline(\n    steps=[\n        (\n            \"Preprocessing\",\n            compose.ColumnTransformer(\n                [\n                    (\n                        \"categorical\",\n                        preprocessing.OneHotEncoder(handle_unknown=\"ignore\"),\n                        cat,  # returns the categorical feature indices\n                    ),\n                    (\n                        \"continuous\",\n                        impute.SimpleImputer(strategy=\"median\"),\n                        cont,\n                    ),  # returns the numeric feature indices\n                ]\n            ),\n        ),\n        (\"Classifier\", ensemble.RandomForestClassifier(n_estimators=10)),\n    ]\n)\n\nrun = openml.runs.run_model_on_task(pipe, task, avoid_duplicate_runs=False)\nmyrun = run.publish()\nprint(f\"Uploaded to {myrun.openml_url}\")\n</pre> task = openml.tasks.get_task(96)  # OpenML helper functions for sklearn can be plugged in directly for complicated pipelines from openml.extensions.sklearn import cat, cont  pipe = pipeline.Pipeline(     steps=[         (             \"Preprocessing\",             compose.ColumnTransformer(                 [                     (                         \"categorical\",                         preprocessing.OneHotEncoder(handle_unknown=\"ignore\"),                         cat,  # returns the categorical feature indices                     ),                     (                         \"continuous\",                         impute.SimpleImputer(strategy=\"median\"),                         cont,                     ),  # returns the numeric feature indices                 ]             ),         ),         (\"Classifier\", ensemble.RandomForestClassifier(n_estimators=10)),     ] )  run = openml.runs.run_model_on_task(pipe, task, avoid_duplicate_runs=False) myrun = run.publish() print(f\"Uploaded to {myrun.openml_url}\") <p>The above pipeline works with the helper functions that internally deal with pandas DataFrame. In the case, pandas is not available, or a NumPy based data processing is the requirement, the above pipeline is presented below to work with NumPy.</p> In\u00a0[\u00a0]: Copied! <pre># Extracting the indices of the categorical columns\nfeatures = task.get_dataset().features\ncategorical_feature_indices = []\nnumeric_feature_indices = []\nfor i in range(len(features)):\n    if features[i].name == task.target_name:\n        continue\n    if features[i].data_type == \"nominal\":\n        categorical_feature_indices.append(i)\n    else:\n        numeric_feature_indices.append(i)\n\npipe = pipeline.Pipeline(\n    steps=[\n        (\n            \"Preprocessing\",\n            compose.ColumnTransformer(\n                [\n                    (\n                        \"categorical\",\n                        preprocessing.OneHotEncoder(handle_unknown=\"ignore\"),\n                        categorical_feature_indices,\n                    ),\n                    (\n                        \"continuous\",\n                        impute.SimpleImputer(strategy=\"median\"),\n                        numeric_feature_indices,\n                    ),\n                ]\n            ),\n        ),\n        (\"Classifier\", ensemble.RandomForestClassifier(n_estimators=10)),\n    ]\n)\n\nrun = openml.runs.run_model_on_task(pipe, task, avoid_duplicate_runs=False)\nmyrun = run.publish()\nprint(f\"Uploaded to {myrun.openml_url}\")\n</pre> # Extracting the indices of the categorical columns features = task.get_dataset().features categorical_feature_indices = [] numeric_feature_indices = [] for i in range(len(features)):     if features[i].name == task.target_name:         continue     if features[i].data_type == \"nominal\":         categorical_feature_indices.append(i)     else:         numeric_feature_indices.append(i)  pipe = pipeline.Pipeline(     steps=[         (             \"Preprocessing\",             compose.ColumnTransformer(                 [                     (                         \"categorical\",                         preprocessing.OneHotEncoder(handle_unknown=\"ignore\"),                         categorical_feature_indices,                     ),                     (                         \"continuous\",                         impute.SimpleImputer(strategy=\"median\"),                         numeric_feature_indices,                     ),                 ]             ),         ),         (\"Classifier\", ensemble.RandomForestClassifier(n_estimators=10)),     ] )  run = openml.runs.run_model_on_task(pipe, task, avoid_duplicate_runs=False) myrun = run.publish() print(f\"Uploaded to {myrun.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>task = openml.tasks.get_task(96)\n\n# The following lines can then be executed offline:\nrun = openml.runs.run_model_on_task(\n    pipe,\n    task,\n    avoid_duplicate_runs=False,\n    upload_flow=False,\n)\n\n# The run may be stored offline, and the flow will be stored along with it:\nrun.to_filesystem(directory=\"myrun\")\n\n# They may be loaded and uploaded at a later time\nrun = openml.runs.OpenMLRun.from_filesystem(directory=\"myrun\")\nrun.publish()\n\n# Publishing the run will automatically upload the related flow if\n# it does not yet exist on the server.\n</pre> task = openml.tasks.get_task(96)  # The following lines can then be executed offline: run = openml.runs.run_model_on_task(     pipe,     task,     avoid_duplicate_runs=False,     upload_flow=False, )  # The run may be stored offline, and the flow will be stored along with it: run.to_filesystem(directory=\"myrun\")  # They may be loaded and uploaded at a later time run = openml.runs.OpenMLRun.from_filesystem(directory=\"myrun\") run.publish()  # Publishing the run will automatically upload the related flow if # it does not yet exist on the server. <p>Alternatively, one can also directly run flows.</p> In\u00a0[\u00a0]: Copied! <pre># Get a task\ntask = openml.tasks.get_task(403)\n\n# Build any classifier or pipeline\nclf = tree.ExtraTreeClassifier()\n\n# Obtain the scikit-learn extension interface to convert the classifier\n# into a flow object.\nextension = openml.extensions.get_extension_by_model(clf)\nflow = extension.model_to_flow(clf)\n\nrun = openml.runs.run_flow_on_task(flow, task)\n</pre> # Get a task task = openml.tasks.get_task(403)  # Build any classifier or pipeline clf = tree.ExtraTreeClassifier()  # Obtain the scikit-learn extension interface to convert the classifier # into a flow object. extension = openml.extensions.get_extension_by_model(clf) flow = extension.model_to_flow(clf)  run = openml.runs.run_flow_on_task(flow, task) In\u00a0[\u00a0]: Copied! <pre># Easy benchmarking:\nfor task_id in [115]:  # Add further tasks. Disclaimer: they might take some time\n    task = openml.tasks.get_task(task_id)\n    data = openml.datasets.get_dataset(task.dataset_id)\n    clf = neighbors.KNeighborsClassifier(n_neighbors=5)\n\n    run = openml.runs.run_model_on_task(clf, task, avoid_duplicate_runs=False)\n    myrun = run.publish()\n    print(f\"kNN on {data.name}: {myrun.openml_url}\")\n</pre> # Easy benchmarking: for task_id in [115]:  # Add further tasks. Disclaimer: they might take some time     task = openml.tasks.get_task(task_id)     data = openml.datasets.get_dataset(task.dataset_id)     clf = neighbors.KNeighborsClassifier(n_neighbors=5)      run = openml.runs.run_model_on_task(clf, task, avoid_duplicate_runs=False)     myrun = run.publish()     print(f\"kNN on {data.name}: {myrun.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n# License: BSD 3-Clause\n</pre> openml.config.stop_using_configuration_for_example() # License: BSD 3-Clause"},{"location":"examples/30_extended/flows_and_runs_tutorial/#flows-and-runs","title":"Flows and Runs\u00b6","text":"<p>This tutorial covers how to train/run a model and how to upload the results.</p>"},{"location":"examples/30_extended/flows_and_runs_tutorial/#train-machine-learning-models","title":"Train machine learning models\u00b6","text":"<p>Train a scikit-learn model on the data manually.</p>"},{"location":"examples/30_extended/flows_and_runs_tutorial/#runs-easily-explore-models","title":"Runs: Easily explore models\u00b6","text":"<p>We can run (many) scikit-learn algorithms on (many) OpenML tasks.</p>"},{"location":"examples/30_extended/flows_and_runs_tutorial/#it-also-works-with-pipelines","title":"It also works with pipelines\u00b6","text":"<p>When you need to handle 'dirty' data, build pipelines to model then automatically. To demonstrate this using the dataset <code>credit-a &lt;https://test.openml.org/d/16&gt;</code>_ via <code>task &lt;https://test.openml.org/t/96&gt;</code>_ as it contains both numerical and categorical variables and missing values in both.</p>"},{"location":"examples/30_extended/flows_and_runs_tutorial/#running-flows-on-tasks-offline-for-later-upload","title":"Running flows on tasks offline for later upload\u00b6","text":"<p>For those scenarios where there is no access to internet, it is possible to run a model on a task without uploading results or flows to the server immediately.</p> <p>To perform the following line offline, it is required to have been called before such that the task is cached on the local openml cache directory:</p>"},{"location":"examples/30_extended/flows_and_runs_tutorial/#challenge","title":"Challenge\u00b6","text":"<p>Try to build the best possible models on several OpenML tasks, compare your results with the rest of the class and learn from them. Some tasks you could try (or browse openml.org):</p> <ul> <li>EEG eye state: data_id:<code>1471 &lt;https://www.openml.org/d/1471&gt;</code>, task_id:<code>14951 &lt;https://www.openml.org/t/14951&gt;</code></li> <li>Volcanoes on Venus: data_id:<code>1527 &lt;https://www.openml.org/d/1527&gt;</code>, task_id:<code>10103 &lt;https://www.openml.org/t/10103&gt;</code></li> <li>Walking activity: data_id:<code>1509 &lt;https://www.openml.org/d/1509&gt;</code>, task_id:<code>9945 &lt;https://www.openml.org/t/9945&gt;</code>, 150k instances.</li> <li>Covertype (Satellite): data_id:<code>150 &lt;https://www.openml.org/d/150&gt;</code>, task_id:<code>218 &lt;https://www.openml.org/t/218&gt;</code>, 500k instances.</li> <li>Higgs (Physics): data_id:<code>23512 &lt;https://www.openml.org/d/23512&gt;</code>, task_id:<code>52950 &lt;https://www.openml.org/t/52950&gt;</code>, 100k instances, missing values.</li> </ul>"},{"location":"examples/30_extended/plot_svm_hyperparameters_tutorial/","title":"Plotting hyperparameter surfaces","text":"In\u00a0[\u00a0]: Copied! <pre>import openml\nimport numpy as np\n</pre> import openml import numpy as np In\u00a0[\u00a0]: Copied! <pre>df = openml.evaluations.list_evaluations_setups(\n    function=\"predictive_accuracy\",\n    flows=[8353],\n    tasks=[6],\n    # Using this flag incorporates the hyperparameters into the returned dataframe. Otherwise,\n    # the dataframe would contain a field ``paramaters`` containing an unparsed dictionary.\n    parameters_in_separate_columns=True,\n)\nprint(df.head(n=10))\n</pre> df = openml.evaluations.list_evaluations_setups(     function=\"predictive_accuracy\",     flows=[8353],     tasks=[6],     # Using this flag incorporates the hyperparameters into the returned dataframe. Otherwise,     # the dataframe would contain a field ``paramaters`` containing an unparsed dictionary.     parameters_in_separate_columns=True, ) print(df.head(n=10)) <p>We can see all the hyperparameter names in the columns of the dataframe:</p> In\u00a0[\u00a0]: Copied! <pre>for name in df.columns:\n    print(name)\n</pre> for name in df.columns:     print(name) <p>Next, we cast and transform the hyperparameters of interest (<code>C</code> and <code>gamma</code>) so that we can nicely plot them.</p> In\u00a0[\u00a0]: Copied! <pre>hyperparameters = [\"sklearn.svm.classes.SVC(16)_C\", \"sklearn.svm.classes.SVC(16)_gamma\"]\ndf[hyperparameters] = df[hyperparameters].astype(float).apply(np.log10)\n</pre> hyperparameters = [\"sklearn.svm.classes.SVC(16)_C\", \"sklearn.svm.classes.SVC(16)_gamma\"] df[hyperparameters] = df[hyperparameters].astype(float).apply(np.log10) In\u00a0[\u00a0]: Copied! <pre>df.plot.hexbin(\n    x=\"sklearn.svm.classes.SVC(16)_C\",\n    y=\"sklearn.svm.classes.SVC(16)_gamma\",\n    C=\"value\",\n    reduce_C_function=np.mean,\n    gridsize=25,\n    title=\"SVM performance landscape\",\n)\n</pre> df.plot.hexbin(     x=\"sklearn.svm.classes.SVC(16)_C\",     y=\"sklearn.svm.classes.SVC(16)_gamma\",     C=\"value\",     reduce_C_function=np.mean,     gridsize=25,     title=\"SVM performance landscape\", ) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nC = df[\"sklearn.svm.classes.SVC(16)_C\"]\ngamma = df[\"sklearn.svm.classes.SVC(16)_gamma\"]\nscore = df[\"value\"]\n\n# Plotting all evaluations:\nax.plot(C, gamma, \"ko\", ms=1)\n# Create a contour plot\ncntr = ax.tricontourf(C, gamma, score, levels=12, cmap=\"RdBu_r\")\n# Adjusting the colorbar\nfig.colorbar(cntr, ax=ax, label=\"accuracy\")\n# Adjusting the axis limits\nax.set(\n    xlim=(min(C), max(C)),\n    ylim=(min(gamma), max(gamma)),\n    xlabel=\"C (log10)\",\n    ylabel=\"gamma (log10)\",\n)\nax.set_title(\"SVM performance landscape\")\n# License: BSD 3-Clause\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots()  C = df[\"sklearn.svm.classes.SVC(16)_C\"] gamma = df[\"sklearn.svm.classes.SVC(16)_gamma\"] score = df[\"value\"]  # Plotting all evaluations: ax.plot(C, gamma, \"ko\", ms=1) # Create a contour plot cntr = ax.tricontourf(C, gamma, score, levels=12, cmap=\"RdBu_r\") # Adjusting the colorbar fig.colorbar(cntr, ax=ax, label=\"accuracy\") # Adjusting the axis limits ax.set(     xlim=(min(C), max(C)),     ylim=(min(gamma), max(gamma)),     xlabel=\"C (log10)\",     ylabel=\"gamma (log10)\", ) ax.set_title(\"SVM performance landscape\") # License: BSD 3-Clause"},{"location":"examples/30_extended/plot_svm_hyperparameters_tutorial/#plotting-hyperparameter-surfaces","title":"Plotting hyperparameter surfaces\u00b6","text":""},{"location":"examples/30_extended/plot_svm_hyperparameters_tutorial/#first-step-obtaining-the-data","title":"First step - obtaining the data\u00b6","text":"<p>First, we need to choose an SVM flow, for example 8353, and a task. Finding the IDs of them are not part of this tutorial, this could for example be done via the website.</p> <p>For this we use the function <code>list_evaluations_setup</code> which can automatically join evaluations conducted by the server with the hyperparameter settings extracted from the uploaded runs (called setup).</p>"},{"location":"examples/30_extended/plot_svm_hyperparameters_tutorial/#option-1-plotting-via-the-pandas-helper-functions","title":"Option 1 - plotting via the pandas helper functions\u00b6","text":""},{"location":"examples/30_extended/plot_svm_hyperparameters_tutorial/#option-2-plotting-via-matplotlib","title":"Option 2 - plotting via matplotlib\u00b6","text":""},{"location":"examples/30_extended/run_setup_tutorial/","title":"Run Setup","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport openml\nfrom openml.extensions.sklearn import cat, cont\n\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import TruncatedSVD\n</pre>  import numpy as np import openml from openml.extensions.sklearn import cat, cont  from sklearn.pipeline import make_pipeline, Pipeline from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder, FunctionTransformer from sklearn.ensemble import RandomForestClassifier from sklearn.decomposition import TruncatedSVD <p>.. warning:: .. include:: ../../test_server_usage_warning.txt</p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() <ol> <li>Create a flow and use it to solve a task</li> </ol> <p>First, let's download the task that we are interested in</p> In\u00a0[\u00a0]: Copied! <pre>task = openml.tasks.get_task(6)\n</pre> task = openml.tasks.get_task(6) <p>we will create a fairly complex model, with many preprocessing components and many potential hyperparameters. Of course, the model can be as complex and as easy as you want it to be</p> In\u00a0[\u00a0]: Copied! <pre>cat_imp = make_pipeline(\n    OneHotEncoder(handle_unknown=\"ignore\"),\n    TruncatedSVD(),\n)\ncont_imp = SimpleImputer(strategy=\"median\")\nct = ColumnTransformer([(\"cat\", cat_imp, cat), (\"cont\", cont_imp, cont)])\nmodel_original = Pipeline(\n    steps=[\n        (\"transform\", ct),\n        (\"estimator\", RandomForestClassifier()),\n    ]\n)\n</pre> cat_imp = make_pipeline(     OneHotEncoder(handle_unknown=\"ignore\"),     TruncatedSVD(), ) cont_imp = SimpleImputer(strategy=\"median\") ct = ColumnTransformer([(\"cat\", cat_imp, cat), (\"cont\", cont_imp, cont)]) model_original = Pipeline(     steps=[         (\"transform\", ct),         (\"estimator\", RandomForestClassifier()),     ] ) <p>Let's change some hyperparameters. Of course, in any good application we would tune them using, e.g., Random Search or Bayesian Optimization, but for the purpose of this tutorial we set them to some specific values that might or might not be optimal</p> In\u00a0[\u00a0]: Copied! <pre>hyperparameters_original = {\n    \"estimator__criterion\": \"gini\",\n    \"estimator__n_estimators\": 50,\n    \"estimator__max_depth\": 10,\n    \"estimator__min_samples_leaf\": 1,\n}\nmodel_original.set_params(**hyperparameters_original)\n\n# solve the task and upload the result (this implicitly creates the flow)\nrun = openml.runs.run_model_on_task(model_original, task, avoid_duplicate_runs=False)\nrun_original = run.publish()  # this implicitly uploads the flow\n</pre> hyperparameters_original = {     \"estimator__criterion\": \"gini\",     \"estimator__n_estimators\": 50,     \"estimator__max_depth\": 10,     \"estimator__min_samples_leaf\": 1, } model_original.set_params(**hyperparameters_original)  # solve the task and upload the result (this implicitly creates the flow) run = openml.runs.run_model_on_task(model_original, task, avoid_duplicate_runs=False) run_original = run.publish()  # this implicitly uploads the flow In\u00a0[\u00a0]: Copied! <pre># obtain setup id (note that the setup id is assigned by the OpenML server -\n# therefore it was not yet available in our local copy of the run)\nrun_downloaded = openml.runs.get_run(run_original.run_id)\nsetup_id = run_downloaded.setup_id\n\n# after this, we can easily reinstantiate the model\nmodel_duplicate = openml.setups.initialize_model(setup_id)\n# it will automatically have all the hyperparameters set\n\n# and run the task again\nrun_duplicate = openml.runs.run_model_on_task(model_duplicate, task, avoid_duplicate_runs=False)\n</pre> # obtain setup id (note that the setup id is assigned by the OpenML server - # therefore it was not yet available in our local copy of the run) run_downloaded = openml.runs.get_run(run_original.run_id) setup_id = run_downloaded.setup_id  # after this, we can easily reinstantiate the model model_duplicate = openml.setups.initialize_model(setup_id) # it will automatically have all the hyperparameters set  # and run the task again run_duplicate = openml.runs.run_model_on_task(model_duplicate, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre># the run has stored all predictions in the field data content\nnp.testing.assert_array_equal(run_original.data_content, run_duplicate.data_content)\n</pre> # the run has stored all predictions in the field data content np.testing.assert_array_equal(run_original.data_content, run_duplicate.data_content) In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n\n# By: Jan N. van Rijn\n# License: BSD 3-Clause\n</pre> openml.config.stop_using_configuration_for_example()  # By: Jan N. van Rijn # License: BSD 3-Clause"},{"location":"examples/30_extended/run_setup_tutorial/#run-setup","title":"Run Setup\u00b6","text":"<p>One of the key features of the openml-python library is that is allows to reinstantiate flows with hyperparameter settings that were uploaded before. This tutorial uses the concept of setups. Although setups are not extensively described in the OpenML documentation (because most users will not directly use them), they form a important concept within OpenML distinguishing between hyperparameter configurations. A setup is the combination of a flow with all its hyperparameters set.</p> <p>A key requirement for reinstantiating a flow is to have the same scikit-learn version as the flow that was uploaded. However, this tutorial will upload the flow (that will later be reinstantiated) itself, so it can be ran with any scikit-learn version that is supported by this library. In this case, the requirement of the corresponding scikit-learn versions is automatically met.</p> <p>In this tutorial we will 1) Create a flow and use it to solve a task; 2) Download the flow, reinstantiate the model with same hyperparameters, and solve the same task again; 3) We will verify that the obtained results are exactly the same.</p>"},{"location":"examples/30_extended/run_setup_tutorial/#2-download-the-flow-and-solve-the-same-task-again","title":"2) Download the flow and solve the same task again.\u00b6","text":""},{"location":"examples/30_extended/run_setup_tutorial/#3-we-will-verify-that-the-obtained-results-are-exactly-the-same","title":"3) We will verify that the obtained results are exactly the same.\u00b6","text":""},{"location":"examples/30_extended/study_tutorial/","title":"Benchmark studies","text":"In\u00a0[\u00a0]: Copied! <pre>import uuid\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport openml\n</pre> import uuid  from sklearn.ensemble import RandomForestClassifier  import openml In\u00a0[\u00a0]: Copied! <pre>studies = openml.study.list_studies(output_format=\"dataframe\", status=\"all\")\nprint(studies.head(n=10))\n</pre> studies = openml.study.list_studies(output_format=\"dataframe\", status=\"all\") print(studies.head(n=10)) <p>This is done based on the study ID.</p> In\u00a0[\u00a0]: Copied! <pre>study = openml.study.get_study(123)\nprint(study)\n</pre> study = openml.study.get_study(123) print(study) <p>Studies also features a description:</p> In\u00a0[\u00a0]: Copied! <pre>print(study.description)\n</pre> print(study.description) <p>Studies are a container for runs:</p> In\u00a0[\u00a0]: Copied! <pre>print(study.runs)\n</pre> print(study.runs) <p>And we can use the evaluation listing functionality to learn more about the evaluations available for the conducted runs:</p> In\u00a0[\u00a0]: Copied! <pre>evaluations = openml.evaluations.list_evaluations(\n    function=\"predictive_accuracy\",\n    study=study.study_id,\n    output_format=\"dataframe\",\n)\nprint(evaluations.head())\n</pre> evaluations = openml.evaluations.list_evaluations(     function=\"predictive_accuracy\",     study=study.study_id,     output_format=\"dataframe\", ) print(evaluations.head()) <p>We'll use the test server for the rest of this tutorial.</p> <p>.. warning:: .. include:: ../../test_server_usage_warning.txt</p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() In\u00a0[\u00a0]: Copied! <pre># Model to be used\nclf = RandomForestClassifier()\n\n# We'll create a study with one run on 3 datasets present in the suite\ntasks = [115, 259, 307]\n\n# To verify\n# https://test.openml.org/api/v1/study/1\nsuite = openml.study.get_suite(\"OpenML100\")\nprint(all(t_id in suite.tasks for t_id in tasks))\n\nrun_ids = []\nfor task_id in tasks:\n    task = openml.tasks.get_task(task_id)\n    run = openml.runs.run_model_on_task(clf, task)\n    run.publish()\n    run_ids.append(run.run_id)\n\n# The study needs a machine-readable and unique alias. To obtain this,\n# we simply generate a random uuid.\nalias = uuid.uuid4().hex\n\nnew_study = openml.study.create_study(\n    name=\"Test-Study\",\n    description=\"Test study for the Python tutorial on studies\",\n    run_ids=run_ids,\n    alias=alias,\n    benchmark_suite=suite.study_id,\n)\nnew_study.publish()\nprint(new_study)\n</pre> # Model to be used clf = RandomForestClassifier()  # We'll create a study with one run on 3 datasets present in the suite tasks = [115, 259, 307]  # To verify # https://test.openml.org/api/v1/study/1 suite = openml.study.get_suite(\"OpenML100\") print(all(t_id in suite.tasks for t_id in tasks))  run_ids = [] for task_id in tasks:     task = openml.tasks.get_task(task_id)     run = openml.runs.run_model_on_task(clf, task)     run.publish()     run_ids.append(run.run_id)  # The study needs a machine-readable and unique alias. To obtain this, # we simply generate a random uuid. alias = uuid.uuid4().hex  new_study = openml.study.create_study(     name=\"Test-Study\",     description=\"Test study for the Python tutorial on studies\",     run_ids=run_ids,     alias=alias,     benchmark_suite=suite.study_id, ) new_study.publish() print(new_study) In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n# License: BSD 3-Clause\n</pre> openml.config.stop_using_configuration_for_example() # License: BSD 3-Clause"},{"location":"examples/30_extended/study_tutorial/#benchmark-studies","title":"Benchmark studies\u00b6","text":"<p>How to list, download and upload benchmark studies. In contrast to benchmark suites which hold a list of tasks, studies hold a list of runs. As runs contain all information on flows and tasks, all required information about a study can be retrieved.</p>"},{"location":"examples/30_extended/study_tutorial/#listing-studies","title":"Listing studies\u00b6","text":"<ul> <li>Use the output_format parameter to select output type</li> <li>Default gives <code>dict</code>, but we'll use <code>dataframe</code> to obtain an easier-to-work-with data structure</li> </ul>"},{"location":"examples/30_extended/study_tutorial/#downloading-studies","title":"Downloading studies\u00b6","text":""},{"location":"examples/30_extended/study_tutorial/#uploading-studies","title":"Uploading studies\u00b6","text":"<p>Creating a study is as simple as creating any kind of other OpenML entity. In this examples we'll create a few runs for the OpenML-100 benchmark suite which is available on the OpenML test server.</p>"},{"location":"examples/30_extended/suites_tutorial/","title":"Benchmark suites","text":"In\u00a0[\u00a0]: Copied! <pre>import uuid\n\nimport numpy as np\n\nimport openml\n</pre> import uuid  import numpy as np  import openml In\u00a0[\u00a0]: Copied! <pre>suites = openml.study.list_suites(output_format=\"dataframe\", status=\"all\")\nprint(suites.head(n=10))\n</pre> suites = openml.study.list_suites(output_format=\"dataframe\", status=\"all\") print(suites.head(n=10)) <p>This is done based on the dataset ID.</p> In\u00a0[\u00a0]: Copied! <pre>suite = openml.study.get_suite(99)\nprint(suite)\n</pre> suite = openml.study.get_suite(99) print(suite) <p>Suites also feature a description:</p> In\u00a0[\u00a0]: Copied! <pre>print(suite.description)\n</pre> print(suite.description) <p>Suites are a container for tasks:</p> In\u00a0[\u00a0]: Copied! <pre>print(suite.tasks)\n</pre> print(suite.tasks) <p>And we can use the task listing functionality to learn more about them:</p> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks(output_format=\"dataframe\")\n</pre> tasks = openml.tasks.list_tasks(output_format=\"dataframe\") <p>Using <code>@</code> in pd.DataFrame.query accesses variables outside of the current dataframe.</p> In\u00a0[\u00a0]: Copied! <pre>tasks = tasks.query(\"tid in @suite.tasks\")\nprint(tasks.describe().transpose())\n</pre> tasks = tasks.query(\"tid in @suite.tasks\") print(tasks.describe().transpose()) <p>We'll use the test server for the rest of this tutorial.</p> <p>.. warning:: .. include:: ../../test_server_usage_warning.txt</p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() In\u00a0[\u00a0]: Copied! <pre>all_tasks = list(openml.tasks.list_tasks(output_format=\"dataframe\")[\"tid\"])\ntask_ids_for_suite = sorted(np.random.choice(all_tasks, replace=False, size=20))\n\n# The study needs a machine-readable and unique alias. To obtain this,\n# we simply generate a random uuid.\n\nalias = uuid.uuid4().hex\n\nnew_suite = openml.study.create_benchmark_suite(\n    name=\"Test-Suite\",\n    description=\"Test suite for the Python tutorial on benchmark suites\",\n    task_ids=task_ids_for_suite,\n    alias=alias,\n)\nnew_suite.publish()\nprint(new_suite)\n</pre> all_tasks = list(openml.tasks.list_tasks(output_format=\"dataframe\")[\"tid\"]) task_ids_for_suite = sorted(np.random.choice(all_tasks, replace=False, size=20))  # The study needs a machine-readable and unique alias. To obtain this, # we simply generate a random uuid.  alias = uuid.uuid4().hex  new_suite = openml.study.create_benchmark_suite(     name=\"Test-Suite\",     description=\"Test suite for the Python tutorial on benchmark suites\",     task_ids=task_ids_for_suite,     alias=alias, ) new_suite.publish() print(new_suite) In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n# License: BSD 3-Clause\n</pre> openml.config.stop_using_configuration_for_example() # License: BSD 3-Clause"},{"location":"examples/30_extended/suites_tutorial/#benchmark-suites","title":"Benchmark suites\u00b6","text":"<p>How to list, download and upload benchmark suites.</p> <p>If you want to learn more about benchmark suites, check out our brief introductory tutorial \"Simple suites tutorial\" or the OpenML benchmark docs.</p>"},{"location":"examples/30_extended/suites_tutorial/#listing-suites","title":"Listing suites\u00b6","text":"<ul> <li>Use the output_format parameter to select output type</li> <li>Default gives <code>dict</code>, but we'll use <code>dataframe</code> to obtain an easier-to-work-with data structure</li> </ul>"},{"location":"examples/30_extended/suites_tutorial/#downloading-suites","title":"Downloading suites\u00b6","text":""},{"location":"examples/30_extended/suites_tutorial/#uploading-suites","title":"Uploading suites\u00b6","text":"<p>Uploading suites is as simple as uploading any kind of other OpenML entity - the only reason why we need so much code in this example is because we upload some random data.</p> <p>We'll take a random subset of at least ten tasks of all available tasks on the test server:</p>"},{"location":"examples/30_extended/task_manual_iteration_tutorial/","title":"Tasks: retrieving splits","text":"In\u00a0[\u00a0]: Copied! <pre>import openml\n</pre> import openml <p>For this tutorial we will use the famous King+Rook versus King+Pawn on A7 dataset, which has the dataset ID 3 (dataset on OpenML), and for which there exist tasks with all important estimation procedures. It is small enough (less than 5000 samples) to efficiently use it in an example.</p> <p>We will first start with (task 233), which is a task with a holdout estimation procedure.</p> In\u00a0[\u00a0]: Copied! <pre>task_id = 233\ntask = openml.tasks.get_task(task_id)\n</pre> task_id = 233 task = openml.tasks.get_task(task_id) <p>Now that we have a task object we can obtain the number of repetitions, folds and samples as defined by the task:</p> In\u00a0[\u00a0]: Copied! <pre>n_repeats, n_folds, n_samples = task.get_split_dimensions()\n</pre> n_repeats, n_folds, n_samples = task.get_split_dimensions() <ul> <li><code>n_repeats</code>: Number of times the model quality estimation is performed</li> <li><code>n_folds</code>: Number of folds per repeat</li> <li><code>n_samples</code>: How many data points to use. This is only relevant for learning curve tasks</li> </ul> <p>A list of all available estimation procedures is available here.</p> <p>Task <code>233</code> is a simple task using the holdout estimation procedure and therefore has only a single repeat, a single fold and a single sample size:</p> In\u00a0[\u00a0]: Copied! <pre>print(\n    \"Task {}: number of repeats: {}, number of folds: {}, number of samples {}.\".format(\n        task_id,\n        n_repeats,\n        n_folds,\n        n_samples,\n    )\n)\n</pre> print(     \"Task {}: number of repeats: {}, number of folds: {}, number of samples {}.\".format(         task_id,         n_repeats,         n_folds,         n_samples,     ) ) <p>We can now retrieve the train/test split for this combination of repeats, folds and number of samples (indexing is zero-based). Usually, one would loop over all repeats, folds and sample sizes, but we can neglect this here as there is only a single repetition.</p> In\u00a0[\u00a0]: Copied! <pre>train_indices, test_indices = task.get_train_test_split_indices(\n    repeat=0,\n    fold=0,\n    sample=0,\n)\n\nprint(train_indices.shape, train_indices.dtype)\nprint(test_indices.shape, test_indices.dtype)\n</pre> train_indices, test_indices = task.get_train_test_split_indices(     repeat=0,     fold=0,     sample=0, )  print(train_indices.shape, train_indices.dtype) print(test_indices.shape, test_indices.dtype) <p>And then split the data based on this:</p> In\u00a0[\u00a0]: Copied! <pre>X, y = task.get_X_and_y(dataset_format=\"dataframe\")\nX_train = X.iloc[train_indices]\ny_train = y.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_test = y.iloc[test_indices]\n\nprint(\n    \"X_train.shape: {}, y_train.shape: {}, X_test.shape: {}, y_test.shape: {}\".format(\n        X_train.shape,\n        y_train.shape,\n        X_test.shape,\n        y_test.shape,\n    )\n)\n</pre> X, y = task.get_X_and_y(dataset_format=\"dataframe\") X_train = X.iloc[train_indices] y_train = y.iloc[train_indices] X_test = X.iloc[test_indices] y_test = y.iloc[test_indices]  print(     \"X_train.shape: {}, y_train.shape: {}, X_test.shape: {}, y_test.shape: {}\".format(         X_train.shape,         y_train.shape,         X_test.shape,         y_test.shape,     ) ) <p>Obviously, we can also retrieve cross-validation versions of the dataset used in task <code>233</code>:</p> In\u00a0[\u00a0]: Copied! <pre>task_id = 3\ntask = openml.tasks.get_task(task_id)\nX, y = task.get_X_and_y()\nn_repeats, n_folds, n_samples = task.get_split_dimensions()\nprint(\n    \"Task {}: number of repeats: {}, number of folds: {}, number of samples {}.\".format(\n        task_id,\n        n_repeats,\n        n_folds,\n        n_samples,\n    )\n)\n</pre> task_id = 3 task = openml.tasks.get_task(task_id) X, y = task.get_X_and_y() n_repeats, n_folds, n_samples = task.get_split_dimensions() print(     \"Task {}: number of repeats: {}, number of folds: {}, number of samples {}.\".format(         task_id,         n_repeats,         n_folds,         n_samples,     ) ) <p>And then perform the aforementioned iteration over all splits:</p> In\u00a0[\u00a0]: Copied! <pre>for repeat_idx in range(n_repeats):\n    for fold_idx in range(n_folds):\n        for sample_idx in range(n_samples):\n            train_indices, test_indices = task.get_train_test_split_indices(\n                repeat=repeat_idx,\n                fold=fold_idx,\n                sample=sample_idx,\n            )\n            X_train = X.iloc[train_indices]\n            y_train = y.iloc[train_indices]\n            X_test = X.iloc[test_indices]\n            y_test = y.iloc[test_indices]\n\n            print(\n                \"Repeat #{}, fold #{}, samples {}: X_train.shape: {}, \"\n                \"y_train.shape {}, X_test.shape {}, y_test.shape {}\".format(\n                    repeat_idx,\n                    fold_idx,\n                    sample_idx,\n                    X_train.shape,\n                    y_train.shape,\n                    X_test.shape,\n                    y_test.shape,\n                )\n            )\n</pre> for repeat_idx in range(n_repeats):     for fold_idx in range(n_folds):         for sample_idx in range(n_samples):             train_indices, test_indices = task.get_train_test_split_indices(                 repeat=repeat_idx,                 fold=fold_idx,                 sample=sample_idx,             )             X_train = X.iloc[train_indices]             y_train = y.iloc[train_indices]             X_test = X.iloc[test_indices]             y_test = y.iloc[test_indices]              print(                 \"Repeat #{}, fold #{}, samples {}: X_train.shape: {}, \"                 \"y_train.shape {}, X_test.shape {}, y_test.shape {}\".format(                     repeat_idx,                     fold_idx,                     sample_idx,                     X_train.shape,                     y_train.shape,                     X_test.shape,                     y_test.shape,                 )             ) <p>And also versions with multiple repeats:</p> In\u00a0[\u00a0]: Copied! <pre>task_id = 1767\ntask = openml.tasks.get_task(task_id)\nX, y = task.get_X_and_y()\nn_repeats, n_folds, n_samples = task.get_split_dimensions()\nprint(\n    \"Task {}: number of repeats: {}, number of folds: {}, number of samples {}.\".format(\n        task_id,\n        n_repeats,\n        n_folds,\n        n_samples,\n    )\n)\n</pre> task_id = 1767 task = openml.tasks.get_task(task_id) X, y = task.get_X_and_y() n_repeats, n_folds, n_samples = task.get_split_dimensions() print(     \"Task {}: number of repeats: {}, number of folds: {}, number of samples {}.\".format(         task_id,         n_repeats,         n_folds,         n_samples,     ) ) <p>And then again perform the aforementioned iteration over all splits:</p> In\u00a0[\u00a0]: Copied! <pre>for repeat_idx in range(n_repeats):\n    for fold_idx in range(n_folds):\n        for sample_idx in range(n_samples):\n            train_indices, test_indices = task.get_train_test_split_indices(\n                repeat=repeat_idx,\n                fold=fold_idx,\n                sample=sample_idx,\n            )\n            X_train = X.iloc[train_indices]\n            y_train = y.iloc[train_indices]\n            X_test = X.iloc[test_indices]\n            y_test = y.iloc[test_indices]\n\n            print(\n                \"Repeat #{}, fold #{}, samples {}: X_train.shape: {}, \"\n                \"y_train.shape {}, X_test.shape {}, y_test.shape {}\".format(\n                    repeat_idx,\n                    fold_idx,\n                    sample_idx,\n                    X_train.shape,\n                    y_train.shape,\n                    X_test.shape,\n                    y_test.shape,\n                )\n            )\n</pre> for repeat_idx in range(n_repeats):     for fold_idx in range(n_folds):         for sample_idx in range(n_samples):             train_indices, test_indices = task.get_train_test_split_indices(                 repeat=repeat_idx,                 fold=fold_idx,                 sample=sample_idx,             )             X_train = X.iloc[train_indices]             y_train = y.iloc[train_indices]             X_test = X.iloc[test_indices]             y_test = y.iloc[test_indices]              print(                 \"Repeat #{}, fold #{}, samples {}: X_train.shape: {}, \"                 \"y_train.shape {}, X_test.shape {}, y_test.shape {}\".format(                     repeat_idx,                     fold_idx,                     sample_idx,                     X_train.shape,                     y_train.shape,                     X_test.shape,                     y_test.shape,                 )             ) <p>And finally a task based on learning curves:</p> In\u00a0[\u00a0]: Copied! <pre>task_id = 1702\ntask = openml.tasks.get_task(task_id)\nX, y = task.get_X_and_y()\nn_repeats, n_folds, n_samples = task.get_split_dimensions()\nprint(\n    \"Task {}: number of repeats: {}, number of folds: {}, number of samples {}.\".format(\n        task_id,\n        n_repeats,\n        n_folds,\n        n_samples,\n    )\n)\n</pre> task_id = 1702 task = openml.tasks.get_task(task_id) X, y = task.get_X_and_y() n_repeats, n_folds, n_samples = task.get_split_dimensions() print(     \"Task {}: number of repeats: {}, number of folds: {}, number of samples {}.\".format(         task_id,         n_repeats,         n_folds,         n_samples,     ) ) <p>And then again perform the aforementioned iteration over all splits:</p> In\u00a0[\u00a0]: Copied! <pre>for repeat_idx in range(n_repeats):\n    for fold_idx in range(n_folds):\n        for sample_idx in range(n_samples):\n            train_indices, test_indices = task.get_train_test_split_indices(\n                repeat=repeat_idx,\n                fold=fold_idx,\n                sample=sample_idx,\n            )\n            X_train = X.iloc[train_indices]\n            y_train = y.iloc[train_indices]\n            X_test = X.iloc[test_indices]\n            y_test = y.iloc[test_indices]\n\n            print(\n                \"Repeat #{}, fold #{}, samples {}: X_train.shape: {}, \"\n                \"y_train.shape {}, X_test.shape {}, y_test.shape {}\".format(\n                    repeat_idx,\n                    fold_idx,\n                    sample_idx,\n                    X_train.shape,\n                    y_train.shape,\n                    X_test.shape,\n                    y_test.shape,\n                )\n            )\n# License: BSD 3-Clause\n</pre> for repeat_idx in range(n_repeats):     for fold_idx in range(n_folds):         for sample_idx in range(n_samples):             train_indices, test_indices = task.get_train_test_split_indices(                 repeat=repeat_idx,                 fold=fold_idx,                 sample=sample_idx,             )             X_train = X.iloc[train_indices]             y_train = y.iloc[train_indices]             X_test = X.iloc[test_indices]             y_test = y.iloc[test_indices]              print(                 \"Repeat #{}, fold #{}, samples {}: X_train.shape: {}, \"                 \"y_train.shape {}, X_test.shape {}, y_test.shape {}\".format(                     repeat_idx,                     fold_idx,                     sample_idx,                     X_train.shape,                     y_train.shape,                     X_test.shape,                     y_test.shape,                 )             ) # License: BSD 3-Clause"},{"location":"examples/30_extended/task_manual_iteration_tutorial/#tasks-retrieving-splits","title":"Tasks: retrieving splits\u00b6","text":"<p>Tasks define a target and a train/test split. Normally, they are the input to the function <code>openml.runs.run_model_on_task</code> which automatically runs the model on all splits of the task. However, sometimes it is necessary to manually split a dataset to perform experiments outside of the functions provided by OpenML. One such example is in the benchmark library HPOBench which extensively uses data from OpenML, but not OpenML's functionality to conduct runs.</p>"},{"location":"examples/30_extended/tasks_tutorial/","title":"Tasks","text":"In\u00a0[\u00a0]: Copied! <pre>import openml\nfrom openml.tasks import TaskType\n</pre> import openml from openml.tasks import TaskType <p>Tasks are identified by IDs and can be accessed in two different ways:</p> <ol> <li>In a list providing basic information on all tasks available on OpenML. This function will not download the actual tasks, but will instead download meta data that can be used to filter the tasks and retrieve a set of IDs. We can filter this list, for example, we can only list tasks having a special tag or only tasks for a specific target such as supervised classification.</li> <li>A single task by its ID. It contains all meta information, the target metric, the splits and an iterator which can be used to access the splits in a useful manner.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks(\n    task_type=TaskType.SUPERVISED_CLASSIFICATION, output_format=\"dataframe\"\n)\nprint(tasks.columns)\nprint(f\"First 5 of {len(tasks)} tasks:\")\nprint(tasks.head())\n</pre> tasks = openml.tasks.list_tasks(     task_type=TaskType.SUPERVISED_CLASSIFICATION, output_format=\"dataframe\" ) print(tasks.columns) print(f\"First 5 of {len(tasks)} tasks:\") print(tasks.head()) <p>We can filter the list of tasks to only contain datasets with more than 500 samples, but less than 1000 samples:</p> In\u00a0[\u00a0]: Copied! <pre>filtered_tasks = tasks.query(\"NumberOfInstances &gt; 500 and NumberOfInstances &lt; 1000\")\nprint(list(filtered_tasks.index))\n</pre> filtered_tasks = tasks.query(\"NumberOfInstances &gt; 500 and NumberOfInstances &lt; 1000\") print(list(filtered_tasks.index)) In\u00a0[\u00a0]: Copied! <pre># Number of tasks\nprint(len(filtered_tasks))\n</pre> # Number of tasks print(len(filtered_tasks)) <p>Then, we can further restrict the tasks to all have the same resampling strategy:</p> In\u00a0[\u00a0]: Copied! <pre>filtered_tasks = filtered_tasks.query('estimation_procedure == \"10-fold Crossvalidation\"')\nprint(list(filtered_tasks.index))\n</pre> filtered_tasks = filtered_tasks.query('estimation_procedure == \"10-fold Crossvalidation\"') print(list(filtered_tasks.index)) In\u00a0[\u00a0]: Copied! <pre># Number of tasks\nprint(len(filtered_tasks))\n</pre> # Number of tasks print(len(filtered_tasks)) <p>Resampling strategies can be found on the OpenML Website.</p> <p>Similar to listing tasks by task type, we can list tasks by tags:</p> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks(tag=\"OpenML100\", output_format=\"dataframe\")\nprint(f\"First 5 of {len(tasks)} tasks:\")\nprint(tasks.head())\n</pre> tasks = openml.tasks.list_tasks(tag=\"OpenML100\", output_format=\"dataframe\") print(f\"First 5 of {len(tasks)} tasks:\") print(tasks.head()) <p>Furthermore, we can list tasks based on the dataset id:</p> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks(data_id=1471, output_format=\"dataframe\")\nprint(f\"First 5 of {len(tasks)} tasks:\")\nprint(tasks.head())\n</pre> tasks = openml.tasks.list_tasks(data_id=1471, output_format=\"dataframe\") print(f\"First 5 of {len(tasks)} tasks:\") print(tasks.head()) <p>In addition, a size limit and an offset can be applied both separately and simultaneously:</p> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks(size=10, offset=50, output_format=\"dataframe\")\nprint(tasks)\n</pre> tasks = openml.tasks.list_tasks(size=10, offset=50, output_format=\"dataframe\") print(tasks) <p>OpenML 100 is a curated list of 100 tasks to start using OpenML. They are all supervised classification tasks with more than 500 instances and less than 50000 instances per task. To make things easier, the tasks do not contain highly unbalanced data and sparse data. However, the tasks include missing values and categorical features. You can find out more about the OpenML 100 on the OpenML benchmarking page.</p> <p>Finally, it is also possible to list all tasks on OpenML with:</p> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks(output_format=\"dataframe\")\nprint(len(tasks))\n</pre> tasks = openml.tasks.list_tasks(output_format=\"dataframe\") print(len(tasks)) In\u00a0[\u00a0]: Copied! <pre>tasks.query('name==\"eeg-eye-state\"')\n</pre> tasks.query('name==\"eeg-eye-state\"') In\u00a0[\u00a0]: Copied! <pre>task_id = 31\ntask = openml.tasks.get_task(task_id)\n</pre> task_id = 31 task = openml.tasks.get_task(task_id) In\u00a0[\u00a0]: Copied! <pre># Properties of the task are stored as member variables:\nprint(task)\n</pre> # Properties of the task are stored as member variables: print(task) In\u00a0[\u00a0]: Copied! <pre># And:\n\nids = [2, 1891, 31, 9983]\ntasks = openml.tasks.get_tasks(ids)\nprint(tasks[0])\n</pre> # And:  ids = [2, 1891, 31, 9983] tasks = openml.tasks.get_tasks(ids) print(tasks[0]) <p>We'll use the test server for the rest of this tutorial.</p> <p>.. warning:: .. include:: ../../test_server_usage_warning.txt</p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() In\u00a0[\u00a0]: Copied! <pre>try:\n    my_task = openml.tasks.create_task(\n        task_type=TaskType.SUPERVISED_CLASSIFICATION,\n        dataset_id=128,\n        target_name=\"class\",\n        evaluation_measure=\"predictive_accuracy\",\n        estimation_procedure_id=1,\n    )\n    my_task.publish()\nexcept openml.exceptions.OpenMLServerException as e:\n    # Error code for 'task already exists'\n    if e.code == 614:\n        # Lookup task\n        tasks = openml.tasks.list_tasks(data_id=128)\n        tasks = tasks.query(\n            'task_type == \"Supervised Classification\" '\n            'and estimation_procedure == \"10-fold Crossvalidation\" '\n            'and evaluation_measures == \"predictive_accuracy\"'\n        )\n        task_id = tasks.loc[:, \"tid\"].values[0]\n        print(\"Task already exists. Task ID is\", task_id)\n</pre> try:     my_task = openml.tasks.create_task(         task_type=TaskType.SUPERVISED_CLASSIFICATION,         dataset_id=128,         target_name=\"class\",         evaluation_measure=\"predictive_accuracy\",         estimation_procedure_id=1,     )     my_task.publish() except openml.exceptions.OpenMLServerException as e:     # Error code for 'task already exists'     if e.code == 614:         # Lookup task         tasks = openml.tasks.list_tasks(data_id=128)         tasks = tasks.query(             'task_type == \"Supervised Classification\" '             'and estimation_procedure == \"10-fold Crossvalidation\" '             'and evaluation_measures == \"predictive_accuracy\"'         )         task_id = tasks.loc[:, \"tid\"].values[0]         print(\"Task already exists. Task ID is\", task_id) In\u00a0[\u00a0]: Copied! <pre># reverting to prod server\nopenml.config.stop_using_configuration_for_example()\n</pre> # reverting to prod server openml.config.stop_using_configuration_for_example() <ul> <li>Complete list of task types.</li> <li>Complete list of model estimation procedures.</li> <li>Complete list of evaluation measures.</li> </ul> <p>License: BSD 3-Clause</p>"},{"location":"examples/30_extended/tasks_tutorial/#tasks","title":"Tasks\u00b6","text":"<p>A tutorial on how to list and download tasks.</p>"},{"location":"examples/30_extended/tasks_tutorial/#listing-tasks","title":"Listing tasks\u00b6","text":"<p>We will start by simply listing only supervised classification tasks.</p> <p>openml.tasks.list_tasks() returns a dictionary of dictionaries by default, but we request a pandas dataframe instead to have better visualization capabilities and easier access:</p>"},{"location":"examples/30_extended/tasks_tutorial/#exercise","title":"Exercise\u00b6","text":"<p>Search for the tasks on the 'eeg-eye-state' dataset.</p>"},{"location":"examples/30_extended/tasks_tutorial/#downloading-tasks","title":"Downloading tasks\u00b6","text":"<p>We provide two functions to download tasks, one which downloads only a single task by its ID, and one which takes a list of IDs and downloads all of these tasks:</p>"},{"location":"examples/30_extended/tasks_tutorial/#creating-tasks","title":"Creating tasks\u00b6","text":"<p>You can also create new tasks. Take the following into account:</p> <ul> <li>You can only create tasks on active datasets</li> <li>For now, only the following tasks are supported: classification, regression, clustering, and learning curve analysis.</li> <li>For now, tasks can only be created on a single dataset.</li> <li>The exact same task must not already exist.</li> </ul> <p>Creating a task requires the following input:</p> <ul> <li>task_type: The task type ID, required (see below). Required.</li> <li>dataset_id: The dataset ID. Required.</li> <li>target_name: The name of the attribute you aim to predict. Optional.</li> <li>estimation_procedure_id : The ID of the estimation procedure used to create train-test splits. Optional.</li> <li>evaluation_measure: The name of the evaluation measure. Optional.</li> <li>Any additional inputs for specific tasks</li> </ul> <p>It is best to leave the evaluation measure open if there is no strong prerequisite for a specific measure. OpenML will always compute all appropriate measures and you can filter or sort results on your favourite measure afterwards. Only add an evaluation measure if necessary (e.g. when other measure make no sense), since it will create a new task, which scatters results across tasks.</p>"},{"location":"examples/30_extended/tasks_tutorial/#example","title":"Example\u00b6","text":"<p>Let's create a classification task on a dataset. In this example we will do this on the Iris dataset (ID=128 (on test server)). We'll use 10-fold cross-validation (ID=1), and predictive accuracy as the predefined measure (this can also be left open). If a task with these parameters exists, we will get an appropriate exception. If such a task doesn't exist, a task will be created and the corresponding task_id will be returned.</p>"},{"location":"examples/40_paper/2015_neurips_feurer_example/","title":"Feurer et al. (2015)","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\nimport openml\n</pre> import pandas as pd  import openml <p>List of dataset IDs given in the supplementary material of Feurer et al.: https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning-supplemental.zip</p> In\u00a0[\u00a0]: Copied! <pre>dataset_ids = [\n    3, 6, 12, 14, 16, 18, 21, 22, 23, 24, 26, 28, 30, 31, 32, 36, 38, 44, 46,\n    57, 60, 179, 180, 181, 182, 184, 185, 273, 293, 300, 351, 354, 357, 389,\n    390, 391, 392, 393, 395, 396, 398, 399, 401, 554, 679, 715, 718, 720, 722,\n    723, 727, 728, 734, 735, 737, 740, 741, 743, 751, 752, 761, 772, 797, 799,\n    803, 806, 807, 813, 816, 819, 821, 822, 823, 833, 837, 843, 845, 846, 847,\n    849, 866, 871, 881, 897, 901, 903, 904, 910, 912, 913, 914, 917, 923, 930,\n    934, 953, 958, 959, 962, 966, 971, 976, 977, 978, 979, 980, 991, 993, 995,\n    1000, 1002, 1018, 1019, 1020, 1021, 1036, 1040, 1041, 1049, 1050, 1053,\n    1056, 1067, 1068, 1069, 1111, 1112, 1114, 1116, 1119, 1120, 1128, 1130,\n    1134, 1138, 1139, 1142, 1146, 1161, 1166,\n]\n</pre> dataset_ids = [     3, 6, 12, 14, 16, 18, 21, 22, 23, 24, 26, 28, 30, 31, 32, 36, 38, 44, 46,     57, 60, 179, 180, 181, 182, 184, 185, 273, 293, 300, 351, 354, 357, 389,     390, 391, 392, 393, 395, 396, 398, 399, 401, 554, 679, 715, 718, 720, 722,     723, 727, 728, 734, 735, 737, 740, 741, 743, 751, 752, 761, 772, 797, 799,     803, 806, 807, 813, 816, 819, 821, 822, 823, 833, 837, 843, 845, 846, 847,     849, 866, 871, 881, 897, 901, 903, 904, 910, 912, 913, 914, 917, 923, 930,     934, 953, 958, 959, 962, 966, 971, 976, 977, 978, 979, 980, 991, 993, 995,     1000, 1002, 1018, 1019, 1020, 1021, 1036, 1040, 1041, 1049, 1050, 1053,     1056, 1067, 1068, 1069, 1111, 1112, 1114, 1116, 1119, 1120, 1128, 1130,     1134, 1138, 1139, 1142, 1146, 1161, 1166, ] <p>The dataset IDs could be used directly to load the dataset and split the data into a training set and a test set. However, to be reproducible, we will first obtain the respective tasks from OpenML, which define both the target feature and the train/test split.</p> <p>.. note:: It is discouraged to work directly on datasets and only provide dataset IDs in a paper as this does not allow reproducibility (unclear splitting). Please do not use datasets but the respective tasks as basis for a paper and publish task IDS. This example is only given to showcase the use of OpenML-Python for a published paper and as a warning on how not to do it. Please check the <code>OpenML documentation of tasks &lt;https://docs.openml.org/concepts/tasks/&gt;</code>_ if you want to learn more about them.</p> <p>This lists both active and inactive tasks (because of <code>status='all'</code>). Unfortunately, this is necessary as some of the datasets contain issues found after the publication and became deactivated, which also deactivated the tasks on them. More information on active or inactive datasets can be found in the online docs.</p> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks(\n    task_type=openml.tasks.TaskType.SUPERVISED_CLASSIFICATION,\n    status=\"all\",\n)\n\n# Query only those with holdout as the resampling startegy.\ntasks = tasks.query('estimation_procedure == \"33% Holdout set\"')\n\ntask_ids = []\nfor did in dataset_ids:\n    tasks_ = list(tasks.query(f\"did == {did}\").tid)\n    if len(tasks_) &gt;= 1:  # if there are multiple task, take the one with lowest ID (oldest).\n        task_id = min(tasks_)\n    else:\n        raise ValueError(did)\n\n    # Optional - Check that the task has the same target attribute as the\n    # dataset default target attribute\n    # (disabled for this example as it needs to run fast to be rendered online)\n    # task = openml.tasks.get_task(task_id)\n    # dataset = task.get_dataset()\n    # if task.target_name != dataset.default_target_attribute:\n    #     raise ValueError(\n    #         (task.target_name, dataset.default_target_attribute)\n    #     )\n\n    task_ids.append(task_id)\n\nassert len(task_ids) == 140\ntask_ids.sort()\n\n# These are the tasks to work with:\nprint(task_ids)\n\n# License: BSD 3-Clause\n</pre> tasks = openml.tasks.list_tasks(     task_type=openml.tasks.TaskType.SUPERVISED_CLASSIFICATION,     status=\"all\", )  # Query only those with holdout as the resampling startegy. tasks = tasks.query('estimation_procedure == \"33% Holdout set\"')  task_ids = [] for did in dataset_ids:     tasks_ = list(tasks.query(f\"did == {did}\").tid)     if len(tasks_) &gt;= 1:  # if there are multiple task, take the one with lowest ID (oldest).         task_id = min(tasks_)     else:         raise ValueError(did)      # Optional - Check that the task has the same target attribute as the     # dataset default target attribute     # (disabled for this example as it needs to run fast to be rendered online)     # task = openml.tasks.get_task(task_id)     # dataset = task.get_dataset()     # if task.target_name != dataset.default_target_attribute:     #     raise ValueError(     #         (task.target_name, dataset.default_target_attribute)     #     )      task_ids.append(task_id)  assert len(task_ids) == 140 task_ids.sort()  # These are the tasks to work with: print(task_ids)  # License: BSD 3-Clause"},{"location":"examples/40_paper/2015_neurips_feurer_example/#feurer-et-al-2015","title":"Feurer et al. (2015)\u00b6","text":"<p>A tutorial on how to get the datasets used in the paper introducing Auto-sklearn by Feurer et al..</p> <p>Auto-sklearn website: https://automl.github.io/auto-sklearn/</p>"},{"location":"examples/40_paper/2015_neurips_feurer_example/#publication","title":"Publication\u00b6","text":"<p>| Efficient and Robust Automated Machine Learning | Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum and Frank Hutter | In Advances in Neural Information Processing Systems 28, 2015 | Available at https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf</p>"},{"location":"examples/40_paper/2018_ida_strang_example/","title":"Strang et al. (2018)","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nimport openml\n</pre> import matplotlib.pyplot as plt  import openml <p>A basic step for each data-mining or machine learning task is to determine which model to choose based on the problem and the data at hand. In this work we investigate when non-linear classifiers outperform linear classifiers by means of a large scale experiment.</p> <p>The paper is accompanied with a study object, containing all relevant tasks and runs (<code>study_id=123</code>). The paper features three experiment classes: Support Vector Machines (SVM), Neural Networks (NN) and Decision Trees (DT). This example demonstrates how to reproduce the plots, comparing two classifiers given the OpenML flow ids. Note that this allows us to reproduce the SVM and NN experiment, but not the DT experiment, as this requires a bit more effort to distinguish the same flow with different hyperparameter values.</p> In\u00a0[\u00a0]: Copied! <pre>study_id = 123\n# for comparing svms: flow_ids = [7754, 7756]\n# for comparing nns: flow_ids = [7722, 7729]\n# for comparing dts: flow_ids = [7725], differentiate on hyper-parameter value\nclassifier_family = \"SVM\"\nflow_ids = [7754, 7756]\nmeasure = \"predictive_accuracy\"\nmeta_features = [\"NumberOfInstances\", \"NumberOfFeatures\"]\nclass_values = [\"non-linear better\", \"linear better\", \"equal\"]\n\n# Downloads all evaluation records related to this study\nevaluations = openml.evaluations.list_evaluations(\n    measure,\n    size=None,\n    flows=flow_ids,\n    study=study_id,\n    output_format=\"dataframe\",\n)\n# gives us a table with columns data_id, flow1_value, flow2_value\nevaluations = evaluations.pivot(index=\"data_id\", columns=\"flow_id\", values=\"value\").dropna()\n# downloads all data qualities (for scatter plot)\ndata_qualities = openml.datasets.list_datasets(\n    data_id=list(evaluations.index.values),\n)\n# removes irrelevant data qualities\ndata_qualities = data_qualities[meta_features]\n# makes a join between evaluation table and data qualities table,\n# now we have columns data_id, flow1_value, flow2_value, meta_feature_1,\n# meta_feature_2\nevaluations = evaluations.join(data_qualities, how=\"inner\")\n\n# adds column that indicates the difference between the two classifiers\nevaluations[\"diff\"] = evaluations[flow_ids[0]] - evaluations[flow_ids[1]]\n</pre> study_id = 123 # for comparing svms: flow_ids = [7754, 7756] # for comparing nns: flow_ids = [7722, 7729] # for comparing dts: flow_ids = [7725], differentiate on hyper-parameter value classifier_family = \"SVM\" flow_ids = [7754, 7756] measure = \"predictive_accuracy\" meta_features = [\"NumberOfInstances\", \"NumberOfFeatures\"] class_values = [\"non-linear better\", \"linear better\", \"equal\"]  # Downloads all evaluation records related to this study evaluations = openml.evaluations.list_evaluations(     measure,     size=None,     flows=flow_ids,     study=study_id,     output_format=\"dataframe\", ) # gives us a table with columns data_id, flow1_value, flow2_value evaluations = evaluations.pivot(index=\"data_id\", columns=\"flow_id\", values=\"value\").dropna() # downloads all data qualities (for scatter plot) data_qualities = openml.datasets.list_datasets(     data_id=list(evaluations.index.values), ) # removes irrelevant data qualities data_qualities = data_qualities[meta_features] # makes a join between evaluation table and data qualities table, # now we have columns data_id, flow1_value, flow2_value, meta_feature_1, # meta_feature_2 evaluations = evaluations.join(data_qualities, how=\"inner\")  # adds column that indicates the difference between the two classifiers evaluations[\"diff\"] = evaluations[flow_ids[0]] - evaluations[flow_ids[1]] <p>makes the s-plot</p> In\u00a0[\u00a0]: Copied! <pre>fig_splot, ax_splot = plt.subplots()\nax_splot.plot(range(len(evaluations)), sorted(evaluations[\"diff\"]))\nax_splot.set_title(classifier_family)\nax_splot.set_xlabel(\"Dataset (sorted)\")\nax_splot.set_ylabel(\"difference between linear and non-linear classifier\")\nax_splot.grid(linestyle=\"--\", axis=\"y\")\nplt.show()\n</pre> fig_splot, ax_splot = plt.subplots() ax_splot.plot(range(len(evaluations)), sorted(evaluations[\"diff\"])) ax_splot.set_title(classifier_family) ax_splot.set_xlabel(\"Dataset (sorted)\") ax_splot.set_ylabel(\"difference between linear and non-linear classifier\") ax_splot.grid(linestyle=\"--\", axis=\"y\") plt.show() <p>adds column that indicates the difference between the two classifiers, needed for the scatter plot</p> In\u00a0[\u00a0]: Copied! <pre>def determine_class(val_lin, val_nonlin):\n    if val_lin &lt; val_nonlin:\n        return class_values[0]\n    if val_nonlin &lt; val_lin:\n        return class_values[1]\n    return class_values[2]\n\n\nevaluations[\"class\"] = evaluations.apply(\n    lambda row: determine_class(row[flow_ids[0]], row[flow_ids[1]]), axis=1\n)\n\n# does the plotting and formatting\nfig_scatter, ax_scatter = plt.subplots()\nfor class_val in class_values:\n    df_class = evaluations[evaluations[\"class\"] == class_val]\n    plt.scatter(df_class[meta_features[0]], df_class[meta_features[1]], label=class_val)\nax_scatter.set_title(classifier_family)\nax_scatter.set_xlabel(meta_features[0])\nax_scatter.set_ylabel(meta_features[1])\nax_scatter.legend()\nax_scatter.set_xscale(\"log\")\nax_scatter.set_yscale(\"log\")\nplt.show()\n</pre> def determine_class(val_lin, val_nonlin):     if val_lin &lt; val_nonlin:         return class_values[0]     if val_nonlin &lt; val_lin:         return class_values[1]     return class_values[2]   evaluations[\"class\"] = evaluations.apply(     lambda row: determine_class(row[flow_ids[0]], row[flow_ids[1]]), axis=1 )  # does the plotting and formatting fig_scatter, ax_scatter = plt.subplots() for class_val in class_values:     df_class = evaluations[evaluations[\"class\"] == class_val]     plt.scatter(df_class[meta_features[0]], df_class[meta_features[1]], label=class_val) ax_scatter.set_title(classifier_family) ax_scatter.set_xlabel(meta_features[0]) ax_scatter.set_ylabel(meta_features[1]) ax_scatter.legend() ax_scatter.set_xscale(\"log\") ax_scatter.set_yscale(\"log\") plt.show() <p>makes a scatter plot where each data point represents the performance of the two algorithms on various axis (not in the paper)</p> In\u00a0[\u00a0]: Copied! <pre>fig_diagplot, ax_diagplot = plt.subplots()\nax_diagplot.grid(linestyle=\"--\")\nax_diagplot.plot([0, 1], ls=\"-\", color=\"black\")\nax_diagplot.plot([0.2, 1.2], ls=\"--\", color=\"black\")\nax_diagplot.plot([-0.2, 0.8], ls=\"--\", color=\"black\")\nax_diagplot.scatter(evaluations[flow_ids[0]], evaluations[flow_ids[1]])\nax_diagplot.set_xlabel(measure)\nax_diagplot.set_ylabel(measure)\nplt.show()\n# License: BSD 3-Clause\n</pre> fig_diagplot, ax_diagplot = plt.subplots() ax_diagplot.grid(linestyle=\"--\") ax_diagplot.plot([0, 1], ls=\"-\", color=\"black\") ax_diagplot.plot([0.2, 1.2], ls=\"--\", color=\"black\") ax_diagplot.plot([-0.2, 0.8], ls=\"--\", color=\"black\") ax_diagplot.scatter(evaluations[flow_ids[0]], evaluations[flow_ids[1]]) ax_diagplot.set_xlabel(measure) ax_diagplot.set_ylabel(measure) plt.show() # License: BSD 3-Clause"},{"location":"examples/40_paper/2018_ida_strang_example/#strang-et-al-2018","title":"Strang et al. (2018)\u00b6","text":"<p>A tutorial on how to reproduce the analysis conducted for Don't Rule Out Simple Models Prematurely: A Large Scale Benchmark Comparing Linear and Non-linear Classifiers in OpenML.</p>"},{"location":"examples/40_paper/2018_ida_strang_example/#publication","title":"Publication\u00b6","text":"<p>| Don't Rule Out Simple Models Prematurely: A Large Scale Benchmark Comparing Linear and Non-linear Classifiers in OpenML | Benjamin Strang, Peter van der Putten, Jan N. van Rijn and Frank Hutter | In Advances in Intelligent Data Analysis XVII 17th International Symposium, 2018 | Available at https://link.springer.com/chapter/10.1007%2F978-3-030-01768-2_25</p>"},{"location":"examples/40_paper/2018_kdd_rijn_example/","title":"van Rijn and Hutter (2018)","text":"<p>With the advent of automated machine learning, automated hyperparameter optimization methods are by now routinely used in data mining. However, this progress is not yet matched by equal progress on automatic analyses that yield information beyond performance-optimizing hyperparameter settings. In this example, we aim to answer the following two questions: Given an algorithm, what are generally its most important hyperparameters?</p> <p>This work is carried out on the OpenML-100 benchmark suite, which can be obtained by <code>openml.study.get_suite('OpenML100')</code>. In this example, we conduct the experiment on the Support Vector Machine (<code>flow_id=7707</code>) with specific kernel (we will perform a post-process filter operation for this). We should set some other experimental parameters (number of results per task, evaluation measure and the number of trees of the internal functional Anova) before the fun can begin.</p> <p>Note that we simplify the example in several ways:</p> <ol> <li>We only consider numerical hyperparameters</li> <li>We consider all hyperparameters that are numerical (in reality, some hyperparameters might be inactive (e.g., <code>degree</code>) or irrelevant (e.g., <code>random_state</code>)</li> <li>We assume all hyperparameters to be on uniform scale</li> </ol> <p>Any difference in conclusion between the actual paper and the presented results is most likely due to one of these simplifications. For example, the hyperparameter C looks rather insignificant, whereas it is quite important when it is put on a log-scale. All these simplifications can be addressed by defining a ConfigSpace. For a more elaborated example that uses this, please see: https://github.com/janvanrijn/openml-pimp/blob/d0a14f3eb480f2a90008889f00041bdccc7b9265/examples/plot/plot_fanova_aggregates.py # noqa F401</p> In\u00a0[\u00a0]: Copied! <pre>    suite = openml.study.get_suite(\"OpenML100\")\n    flow_id = 7707\n    parameter_filters = {\"sklearn.svm.classes.SVC(17)_kernel\": \"sigmoid\"}\n    evaluation_measure = \"predictive_accuracy\"\n    limit_per_task = 500\n    limit_nr_tasks = 15\n    n_trees = 16\n\n    fanova_results = []\n    # we will obtain all results from OpenML per task. Practice has shown that this places the bottleneck on the\n    # communication with OpenML, and for iterated experimenting it is better to cache the results in a local file.\n    for idx, task_id in enumerate(suite.tasks):\n        if limit_nr_tasks is not None and idx &gt;= limit_nr_tasks:\n            continue\n        print(\n            \"Starting with task %d (%d/%d)\"\n            % (task_id, idx + 1, len(suite.tasks) if limit_nr_tasks is None else limit_nr_tasks)\n        )\n        # note that we explicitly only include tasks from the benchmark suite that was specified (as per the for-loop)\n        evals = openml.evaluations.list_evaluations_setups(\n            evaluation_measure,\n            flows=[flow_id],\n            tasks=[task_id],\n            size=limit_per_task,\n            output_format=\"dataframe\",\n        )\n    except json.decoder.JSONDecodeError as e:\n        print(\"Task %d error: %s\" % (task_id, e))\n        continue\n    # apply our filters, to have only the setups that comply to the hyperparameters we want\n    for filter_key, filter_value in parameter_filters.items():\n        setups_evals = setups_evals[setups_evals[filter_key] == filter_value]\n    # in this simplified example, we only display numerical and float hyperparameters. For categorical hyperparameters,\n    # the fanova library needs to be informed by using a configspace object.\n    setups_evals = setups_evals.select_dtypes(include=[\"int64\", \"float64\"])\n    # drop rows with unique values. These are by definition not an interesting hyperparameter, e.g., ``axis``,\n    # ``verbose``.\n    setups_evals = setups_evals[\n        [\n            c\n            for c in list(setups_evals)\n            if len(setups_evals[c].unique()) &gt; 1 or c == performance_column\n        ]\n    ]\n    # We are done with processing ``setups_evals``. Note that we still might have some irrelevant hyperparameters, e.g.,\n    # ``random_state``. We have dropped some relevant hyperparameters, i.e., several categoricals. Let's check it out:\n\n    # determine x values to pass to fanova library\n    parameter_names = [\n        pname for pname in setups_evals.columns.to_numpy() if pname != performance_column\n    ]\n    evaluator = fanova.fanova.fANOVA(\n        X=setups_evals[parameter_names].to_numpy(),\n        Y=setups_evals[performance_column].to_numpy(),\n        n_trees=n_trees,\n    )\n    for idx, pname in enumerate(parameter_names):\n        try:\n            fanova_results.append(\n                {\n                    \"hyperparameter\": pname.split(\".\")[-1],\n                    \"fanova\": evaluator.quantify_importance([idx])[(idx,)][\n                        \"individual importance\"\n                    ],\n                }\n            )\n        except RuntimeError as e:\n            # functional ANOVA sometimes crashes with a RuntimeError, e.g., on tasks where the performance is constant\n            # for all configurations (there is no variance). We will skip these tasks (like the authors did in the\n            # paper).\n            print(\"Task %d error: %s\" % (task_id, e))\n            continue\n\n    # transform ``fanova_results`` from a list of dicts into a DataFrame\n    fanova_results = pd.DataFrame(fanova_results)\n</pre>     suite = openml.study.get_suite(\"OpenML100\")     flow_id = 7707     parameter_filters = {\"sklearn.svm.classes.SVC(17)_kernel\": \"sigmoid\"}     evaluation_measure = \"predictive_accuracy\"     limit_per_task = 500     limit_nr_tasks = 15     n_trees = 16      fanova_results = []     # we will obtain all results from OpenML per task. Practice has shown that this places the bottleneck on the     # communication with OpenML, and for iterated experimenting it is better to cache the results in a local file.     for idx, task_id in enumerate(suite.tasks):         if limit_nr_tasks is not None and idx &gt;= limit_nr_tasks:             continue         print(             \"Starting with task %d (%d/%d)\"             % (task_id, idx + 1, len(suite.tasks) if limit_nr_tasks is None else limit_nr_tasks)         )         # note that we explicitly only include tasks from the benchmark suite that was specified (as per the for-loop)         evals = openml.evaluations.list_evaluations_setups(             evaluation_measure,             flows=[flow_id],             tasks=[task_id],             size=limit_per_task,             output_format=\"dataframe\",         )     except json.decoder.JSONDecodeError as e:         print(\"Task %d error: %s\" % (task_id, e))         continue     # apply our filters, to have only the setups that comply to the hyperparameters we want     for filter_key, filter_value in parameter_filters.items():         setups_evals = setups_evals[setups_evals[filter_key] == filter_value]     # in this simplified example, we only display numerical and float hyperparameters. For categorical hyperparameters,     # the fanova library needs to be informed by using a configspace object.     setups_evals = setups_evals.select_dtypes(include=[\"int64\", \"float64\"])     # drop rows with unique values. These are by definition not an interesting hyperparameter, e.g., ``axis``,     # ``verbose``.     setups_evals = setups_evals[         [             c             for c in list(setups_evals)             if len(setups_evals[c].unique()) &gt; 1 or c == performance_column         ]     ]     # We are done with processing ``setups_evals``. Note that we still might have some irrelevant hyperparameters, e.g.,     # ``random_state``. We have dropped some relevant hyperparameters, i.e., several categoricals. Let's check it out:      # determine x values to pass to fanova library     parameter_names = [         pname for pname in setups_evals.columns.to_numpy() if pname != performance_column     ]     evaluator = fanova.fanova.fANOVA(         X=setups_evals[parameter_names].to_numpy(),         Y=setups_evals[performance_column].to_numpy(),         n_trees=n_trees,     )     for idx, pname in enumerate(parameter_names):         try:             fanova_results.append(                 {                     \"hyperparameter\": pname.split(\".\")[-1],                     \"fanova\": evaluator.quantify_importance([idx])[(idx,)][                         \"individual importance\"                     ],                 }             )         except RuntimeError as e:             # functional ANOVA sometimes crashes with a RuntimeError, e.g., on tasks where the performance is constant             # for all configurations (there is no variance). We will skip these tasks (like the authors did in the             # paper).             print(\"Task %d error: %s\" % (task_id, e))             continue      # transform ``fanova_results`` from a list of dicts into a DataFrame     fanova_results = pd.DataFrame(fanova_results) <p>make the boxplot of the variance contribution. Obviously, we can also use this data to make the Nemenyi plot, but this relies on the rather complex <code>Orange</code> dependency (<code>pip install Orange3</code>). For the complete example, the reader is referred to the more elaborate script (referred to earlier)</p> In\u00a0[\u00a0]: Copied! <pre>    fig, ax = plt.subplots()\n    sns.boxplot(x=\"hyperparameter\", y=\"fanova\", data=fanova_results, ax=ax)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    ax.set_ylabel(\"Variance Contribution\")\n    ax.set_xlabel(None)\n    plt.tight_layout()\n    plt.show()\n    # License: BSD 3-Clause\n</pre>     fig, ax = plt.subplots()     sns.boxplot(x=\"hyperparameter\", y=\"fanova\", data=fanova_results, ax=ax)     ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")     ax.set_ylabel(\"Variance Contribution\")     ax.set_xlabel(None)     plt.tight_layout()     plt.show()     # License: BSD 3-Clause"},{"location":"examples/40_paper/2018_kdd_rijn_example/#van-rijn-and-hutter-2018","title":"van Rijn and Hutter (2018)\u00b6","text":"<p>A tutorial on how to reproduce the paper Hyperparameter Importance Across Datasets.</p> <p>This is a Unix-only tutorial, as the requirements can not be satisfied on a Windows machine (Untested on other systems).</p>"},{"location":"examples/40_paper/2018_kdd_rijn_example/#publication","title":"Publication\u00b6","text":"<p>| Hyperparameter importance across datasets | Jan N. van Rijn and Frank Hutter | In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 2018 | Available at https://dl.acm.org/doi/10.1145/3219819.3220058</p> <p>import sys DEPRECATED EXAMPLE -- Avoid running this code in our CI/CD pipeline print(\"This example is deprecated, remove this code to use it manually.\") if not run_code: print(\"Exiting...\") sys.exit()</p> <p>import json</p> <p>import fanova import matplotlib.pyplot as plt import pandas as pd import seaborn as sns</p> <p>import openml</p> <p>############################################################################# With the advent of automated machine learning, automated hyperparameter optimization methods are by now routinely used in data mining. However, this progress is not yet matched by equal progress on automatic analyses that yield information beyond performance-optimizing hyperparameter settings. In this example, we aim to answer the following two questions: Given an algorithm, what are generally its most important hyperparameters?</p> <p>This work is carried out on the OpenML-100 benchmark suite, which can be obtained by <code>openml.study.get_suite('OpenML100')</code>. In this example, we conduct the experiment on the Support Vector Machine (<code>flow_id=7707</code>) with specific kernel (we will perform a post-process filter operation for this). We should set some other experimental parameters (number of results per task, evaluation measure and the number of trees of the internal functional Anova) before the fun can begin.</p> <p>Note that we simplify the example in several ways:</p> <ol> <li>We only consider numerical hyperparameters</li> <li>We consider all hyperparameters that are numerical (in reality, some hyperparameters might be inactive (e.g., <code>degree</code>) or irrelevant (e.g., <code>random_state</code>)</li> <li>We assume all hyperparameters to be on uniform scale</li> </ol> <p>Any difference in conclusion between the actual paper and the presented results is most likely due to one of these simplifications. For example, the hyperparameter C looks rather insignificant, whereas it is quite important when it is put on a log-scale. All these simplifications can be addressed by defining a ConfigSpace. For a more elaborated example that uses this, please see: https://github.com/janvanrijn/openml-pimp/blob/d0a14f3eb480f2a90008889f00041bdccc7b9265/examples/plot/plot_fanova_aggregates.py</p> <p>suite = openml.study.get_suite(\"OpenML100\") flow_id = 7707 parameter_filters = {\"sklearn.svm.classes.SVC(17)_kernel\": \"sigmoid\"} evaluation_measure = \"predictive_accuracy\" limit_per_task = 500 limit_nr_tasks = 15 n_trees = 16</p> <p>fanova_results = [] we will obtain all results from OpenML per task. Practice has shown that this places the bottleneck on the communication with OpenML, and for iterated experimenting it is better to cache the results in a local file. for idx, task_id in enumerate(suite.tasks): if limit_nr_tasks is not None and idx &gt;= limit_nr_tasks: continue print( \"Starting with task %d (%d/%d)\" % (task_id, idx + 1, len(suite.tasks) if limit_nr_tasks is None else limit_nr_tasks) ) # note that we explicitly only include tasks from the benchmark suite that was specified (as per the for-loop) evals = openml.evaluations.list_evaluations_setups( evaluation_measure, flows=[flow_id], tasks=[task_id], size=limit_per_task, )</p>"},{"location":"examples/40_paper/2018_neurips_perrone_example/","title":"Perrone et al. (2018)","text":"In\u00a0[\u00a0]: Copied! <pre>import openml\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport openml\n\nflow_type = \"svm\"  # this example will use the smaller svm flow evaluations\n</pre> import openml import numpy as np import pandas as pd from matplotlib import pyplot as plt from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestRegressor from sklearn.impute import SimpleImputer from sklearn.metrics import mean_squared_error from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder  import openml  flow_type = \"svm\"  # this example will use the smaller svm flow evaluations <p>The subsequent functions are defined to fetch tasks, flows, evaluations and preprocess them into a tabular format that can be used to build models.</p> In\u00a0[\u00a0]: Copied! <pre>def fetch_evaluations(run_full=False, flow_type=\"svm\", metric=\"area_under_roc_curve\"):\n    \"\"\"\n    Fetch a list of evaluations based on the flows and tasks used in the experiments.\n\n    Parameters\n    ----------\n    run_full : boolean\n        If True, use the full list of tasks used in the paper\n        If False, use 5 tasks with the smallest number of evaluations available\n    flow_type : str, {'svm', 'xgboost'}\n        To select whether svm or xgboost experiments are to be run\n    metric : str\n        The evaluation measure that is passed to openml.evaluations.list_evaluations\n\n    Returns\n    -------\n    eval_df : dataframe\n    task_ids : list\n    flow_id : int\n    \"\"\"\n    # Collecting task IDs as used by the experiments from the paper\n    # fmt: off\n    if flow_type == \"svm\" and run_full:\n        task_ids = [\n            10101, 145878, 146064, 14951, 34537, 3485, 3492, 3493, 3494,\n            37, 3889, 3891, 3899, 3902, 3903, 3913, 3918, 3950, 9889,\n            9914, 9946, 9952, 9967, 9971, 9976, 9978, 9980, 9983,\n        ]\n    elif flow_type == \"svm\" and not run_full:\n        task_ids = [9983, 3485, 3902, 3903, 145878]\n    elif flow_type == \"xgboost\" and run_full:\n        task_ids = [\n            10093, 10101, 125923, 145847, 145857, 145862, 145872, 145878,\n            145953, 145972, 145976, 145979, 146064, 14951, 31, 3485,\n            3492, 3493, 37, 3896, 3903, 3913, 3917, 3918, 3, 49, 9914,\n            9946, 9952, 9967,\n        ]\n    else:  # flow_type == 'xgboost' and not run_full:\n        task_ids = [3903, 37, 3485, 49, 3913]\n    # fmt: on\n\n    # Fetching the relevant flow\n    flow_id = 5891 if flow_type == \"svm\" else 6767\n\n    # Fetching evaluations\n    eval_df = openml.evaluations.list_evaluations_setups(\n        function=metric,\n        tasks=task_ids,\n        flows=[flow_id],\n        uploaders=[2702],\n        parameters_in_separate_columns=True,\n    )\n    return eval_df, task_ids, flow_id\n\n\ndef create_table_from_evaluations(\n    eval_df, flow_type=\"svm\", run_count=np.iinfo(np.int64).max, task_ids=None\n):\n    \"\"\"\n    Create a tabular data with its ground truth from a dataframe of evaluations.\n    Optionally, can filter out records based on task ids.\n\n    Parameters\n    ----------\n    eval_df : dataframe\n        Containing list of runs as obtained from list_evaluations()\n    flow_type : str, {'svm', 'xgboost'}\n        To select whether svm or xgboost experiments are to be run\n    run_count : int\n        Maximum size of the table created, or number of runs included in the table\n    task_ids : list, (optional)\n        List of integers specifying the tasks to be retained from the evaluations dataframe\n\n    Returns\n    -------\n    eval_table : dataframe\n    values : list\n    \"\"\"\n    if task_ids is not None:\n        eval_df = eval_df[eval_df[\"task_id\"].isin(task_ids)]\n    if flow_type == \"svm\":\n        colnames = [\"cost\", \"degree\", \"gamma\", \"kernel\"]\n    else:\n        colnames = [\n            \"alpha\",\n            \"booster\",\n            \"colsample_bylevel\",\n            \"colsample_bytree\",\n            \"eta\",\n            \"lambda\",\n            \"max_depth\",\n            \"min_child_weight\",\n            \"nrounds\",\n            \"subsample\",\n        ]\n    eval_df = eval_df.sample(frac=1)  # shuffling rows\n    eval_df = eval_df.iloc[:run_count, :]\n    eval_df.columns = [column.split(\"_\")[-1] for column in eval_df.columns]\n    eval_table = eval_df.loc[:, colnames]\n    value = eval_df.loc[:, \"value\"]\n    return eval_table, value\n\n\ndef list_categorical_attributes(flow_type=\"svm\"):\n    if flow_type == \"svm\":\n        return [\"kernel\"]\n    return [\"booster\"]\n</pre> def fetch_evaluations(run_full=False, flow_type=\"svm\", metric=\"area_under_roc_curve\"):     \"\"\"     Fetch a list of evaluations based on the flows and tasks used in the experiments.      Parameters     ----------     run_full : boolean         If True, use the full list of tasks used in the paper         If False, use 5 tasks with the smallest number of evaluations available     flow_type : str, {'svm', 'xgboost'}         To select whether svm or xgboost experiments are to be run     metric : str         The evaluation measure that is passed to openml.evaluations.list_evaluations      Returns     -------     eval_df : dataframe     task_ids : list     flow_id : int     \"\"\"     # Collecting task IDs as used by the experiments from the paper     # fmt: off     if flow_type == \"svm\" and run_full:         task_ids = [             10101, 145878, 146064, 14951, 34537, 3485, 3492, 3493, 3494,             37, 3889, 3891, 3899, 3902, 3903, 3913, 3918, 3950, 9889,             9914, 9946, 9952, 9967, 9971, 9976, 9978, 9980, 9983,         ]     elif flow_type == \"svm\" and not run_full:         task_ids = [9983, 3485, 3902, 3903, 145878]     elif flow_type == \"xgboost\" and run_full:         task_ids = [             10093, 10101, 125923, 145847, 145857, 145862, 145872, 145878,             145953, 145972, 145976, 145979, 146064, 14951, 31, 3485,             3492, 3493, 37, 3896, 3903, 3913, 3917, 3918, 3, 49, 9914,             9946, 9952, 9967,         ]     else:  # flow_type == 'xgboost' and not run_full:         task_ids = [3903, 37, 3485, 49, 3913]     # fmt: on      # Fetching the relevant flow     flow_id = 5891 if flow_type == \"svm\" else 6767      # Fetching evaluations     eval_df = openml.evaluations.list_evaluations_setups(         function=metric,         tasks=task_ids,         flows=[flow_id],         uploaders=[2702],         parameters_in_separate_columns=True,     )     return eval_df, task_ids, flow_id   def create_table_from_evaluations(     eval_df, flow_type=\"svm\", run_count=np.iinfo(np.int64).max, task_ids=None ):     \"\"\"     Create a tabular data with its ground truth from a dataframe of evaluations.     Optionally, can filter out records based on task ids.      Parameters     ----------     eval_df : dataframe         Containing list of runs as obtained from list_evaluations()     flow_type : str, {'svm', 'xgboost'}         To select whether svm or xgboost experiments are to be run     run_count : int         Maximum size of the table created, or number of runs included in the table     task_ids : list, (optional)         List of integers specifying the tasks to be retained from the evaluations dataframe      Returns     -------     eval_table : dataframe     values : list     \"\"\"     if task_ids is not None:         eval_df = eval_df[eval_df[\"task_id\"].isin(task_ids)]     if flow_type == \"svm\":         colnames = [\"cost\", \"degree\", \"gamma\", \"kernel\"]     else:         colnames = [             \"alpha\",             \"booster\",             \"colsample_bylevel\",             \"colsample_bytree\",             \"eta\",             \"lambda\",             \"max_depth\",             \"min_child_weight\",             \"nrounds\",             \"subsample\",         ]     eval_df = eval_df.sample(frac=1)  # shuffling rows     eval_df = eval_df.iloc[:run_count, :]     eval_df.columns = [column.split(\"_\")[-1] for column in eval_df.columns]     eval_table = eval_df.loc[:, colnames]     value = eval_df.loc[:, \"value\"]     return eval_table, value   def list_categorical_attributes(flow_type=\"svm\"):     if flow_type == \"svm\":         return [\"kernel\"]     return [\"booster\"] <p>Fetching the data from OpenML</p> <p>Now, we read all the tasks and evaluations for them and collate into a table. Here, we are reading all the tasks and evaluations for the SVM flow and pre-processing all retrieved evaluations.</p> In\u00a0[\u00a0]: Copied! <pre>eval_df, task_ids, flow_id = fetch_evaluations(run_full=False, flow_type=flow_type)\nX, y = create_table_from_evaluations(eval_df, flow_type=flow_type)\nprint(X.head())\nprint(\"Y : \", y[:5])\n</pre> eval_df, task_ids, flow_id = fetch_evaluations(run_full=False, flow_type=flow_type) X, y = create_table_from_evaluations(eval_df, flow_type=flow_type) print(X.head()) print(\"Y : \", y[:5]) In\u00a0[\u00a0]: Copied! <pre># Separating data into categorical and non-categorical (numeric for this example) columns\ncat_cols = list_categorical_attributes(flow_type=flow_type)\nnum_cols = list(set(X.columns) - set(cat_cols))\n\n# Missing value imputers for numeric columns\nnum_imputer = SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=-1)\n\n# Creating the one-hot encoder for numerical representation of categorical columns\nenc = Pipeline(\n    [\n        (\n            \"cat_si\",\n            SimpleImputer(\n                strategy=\"constant\",\n                fill_value=\"missing\",\n            ),\n        ),\n        (\"cat_ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n    ],\n)\n# Combining column transformers\nct = ColumnTransformer([(\"cat\", enc, cat_cols), (\"num\", num_imputer, num_cols)])\n\n# Creating the full pipeline with the surrogate model\nclf = RandomForestRegressor(n_estimators=50)\nmodel = Pipeline(steps=[(\"preprocess\", ct), (\"surrogate\", clf)])\n</pre> # Separating data into categorical and non-categorical (numeric for this example) columns cat_cols = list_categorical_attributes(flow_type=flow_type) num_cols = list(set(X.columns) - set(cat_cols))  # Missing value imputers for numeric columns num_imputer = SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=-1)  # Creating the one-hot encoder for numerical representation of categorical columns enc = Pipeline(     [         (             \"cat_si\",             SimpleImputer(                 strategy=\"constant\",                 fill_value=\"missing\",             ),         ),         (\"cat_ohe\", OneHotEncoder(handle_unknown=\"ignore\")),     ], ) # Combining column transformers ct = ColumnTransformer([(\"cat\", enc, cat_cols), (\"num\", num_imputer, num_cols)])  # Creating the full pipeline with the surrogate model clf = RandomForestRegressor(n_estimators=50) model = Pipeline(steps=[(\"preprocess\", ct), (\"surrogate\", clf)]) In\u00a0[\u00a0]: Copied! <pre># Selecting a task for the surrogate\ntask_id = task_ids[-1]\nprint(\"Task ID : \", task_id)\nX, y = create_table_from_evaluations(eval_df, task_ids=[task_id], flow_type=\"svm\")\n\nmodel.fit(X, y)\ny_pred = model.predict(X)\n\nprint(f\"Training RMSE : {mean_squared_error(y, y_pred):.5}\")\n</pre> # Selecting a task for the surrogate task_id = task_ids[-1] print(\"Task ID : \", task_id) X, y = create_table_from_evaluations(eval_df, task_ids=[task_id], flow_type=\"svm\")  model.fit(X, y) y_pred = model.predict(X)  print(f\"Training RMSE : {mean_squared_error(y, y_pred):.5}\") In\u00a0[\u00a0]: Copied! <pre># Sampling random configurations\ndef random_sample_configurations(num_samples=100):\n    colnames = [\"cost\", \"degree\", \"gamma\", \"kernel\"]\n    ranges = [\n        (0.000986, 998.492437),\n        (2.0, 5.0),\n        (0.000988, 913.373845),\n        ([\"linear\", \"polynomial\", \"radial\", \"sigmoid\"]),\n    ]\n    X = pd.DataFrame(np.nan, index=range(num_samples), columns=colnames)\n    for i in range(len(colnames)):\n        if len(ranges[i]) == 2:\n            col_val = np.random.uniform(low=ranges[i][0], high=ranges[i][1], size=num_samples)\n        else:\n            col_val = np.random.choice(ranges[i], size=num_samples)\n        X.iloc[:, i] = col_val\n    return X\n\n\nconfigs = random_sample_configurations(num_samples=1000)\nprint(configs)\n</pre> # Sampling random configurations def random_sample_configurations(num_samples=100):     colnames = [\"cost\", \"degree\", \"gamma\", \"kernel\"]     ranges = [         (0.000986, 998.492437),         (2.0, 5.0),         (0.000988, 913.373845),         ([\"linear\", \"polynomial\", \"radial\", \"sigmoid\"]),     ]     X = pd.DataFrame(np.nan, index=range(num_samples), columns=colnames)     for i in range(len(colnames)):         if len(ranges[i]) == 2:             col_val = np.random.uniform(low=ranges[i][0], high=ranges[i][1], size=num_samples)         else:             col_val = np.random.choice(ranges[i], size=num_samples)         X.iloc[:, i] = col_val     return X   configs = random_sample_configurations(num_samples=1000) print(configs) In\u00a0[\u00a0]: Copied! <pre>preds = model.predict(configs)\n\n# tracking the maximum AUC obtained over the functions evaluations\npreds = np.maximum.accumulate(preds)\n# computing regret (1 - predicted_auc)\nregret = 1 - preds\n\n# plotting the regret curve\nplt.plot(regret)\nplt.title(\"AUC regret for Random Search on surrogate\")\nplt.xlabel(\"Numbe of function evaluations\")\nplt.ylabel(\"Regret\")\n# License: BSD 3-Clause\n</pre> preds = model.predict(configs)  # tracking the maximum AUC obtained over the functions evaluations preds = np.maximum.accumulate(preds) # computing regret (1 - predicted_auc) regret = 1 - preds  # plotting the regret curve plt.plot(regret) plt.title(\"AUC regret for Random Search on surrogate\") plt.xlabel(\"Numbe of function evaluations\") plt.ylabel(\"Regret\") # License: BSD 3-Clause"},{"location":"examples/40_paper/2018_neurips_perrone_example/#perrone-et-al-2018","title":"Perrone et al. (2018)\u00b6","text":"<p>A tutorial on how to build a surrogate model based on OpenML data as done for Scalable Hyperparameter Transfer Learning by Perrone et al..</p>"},{"location":"examples/40_paper/2018_neurips_perrone_example/#publication","title":"Publication\u00b6","text":"<p>| Scalable Hyperparameter Transfer Learning | Valerio Perrone and Rodolphe Jenatton and Matthias Seeger and Cedric Archambeau | In Advances in Neural Information Processing Systems 31, 2018 | Available at https://papers.nips.cc/paper/7917-scalable-hyperparameter-transfer-learning.pdf</p> <p>This example demonstrates how OpenML runs can be used to construct a surrogate model.</p> <p>In the following section, we shall do the following:</p> <ul> <li>Retrieve tasks and flows as used in the experiments by Perrone et al. (2018).</li> <li>Build a tabular data by fetching the evaluations uploaded to OpenML.</li> <li>Impute missing values and handle categorical data before building a Random Forest model that maps hyperparameter values to the area under curve score.</li> </ul>"},{"location":"examples/40_paper/2018_neurips_perrone_example/#creating-pre-processing-and-modelling-pipelines","title":"Creating pre-processing and modelling pipelines\u00b6","text":"<p>The two primary tasks are to impute the missing values, that is, account for the hyperparameters that are not available with the runs from OpenML. And secondly, to handle categorical variables using One-hot encoding prior to modelling.</p>"},{"location":"examples/40_paper/2018_neurips_perrone_example/#building-a-surrogate-model-on-a-tasks-evaluation","title":"Building a surrogate model on a task's evaluation\u00b6","text":"<p>The same set of functions can be used for a single task to retrieve a singular table which can be used for the surrogate model construction. We shall use the SVM flow here to keep execution time simple and quick.</p>"},{"location":"examples/40_paper/2018_neurips_perrone_example/#evaluating-the-surrogate-model","title":"Evaluating the surrogate model\u00b6","text":"<p>The surrogate model built from a task's evaluations fetched from OpenML will be put into trivial action here, where we shall randomly sample configurations and observe the trajectory of the area under curve (auc) we can obtain from the surrogate we've built.</p> <p>NOTE: This section is written exclusively for the SVM flow</p>"},{"location":"reference/","title":"openml","text":""},{"location":"reference/#openml","title":"openml","text":"<p>The OpenML module implements a python interface to <code>OpenML &lt;https://www.openml.org&gt;</code>_, a collaborative platform for machine learning. OpenML can be used to</p> <ul> <li>store, download and analyze datasets</li> <li>make experiments and their results (e.g. models, predictions)   accesible and reproducible for everybody</li> <li>analyze experiments (uploaded by you and other collaborators) and conduct   meta studies</li> </ul> <p>In particular, this module implements a python interface for the <code>OpenML REST API &lt;https://www.openml.org/guide#!rest_services&gt;</code> (<code>REST on wikipedia &lt;https://en.wikipedia.org/wiki/Representational_state_transfer&gt;</code>).</p>"},{"location":"reference/#openml.OpenMLBenchmarkSuite","title":"OpenMLBenchmarkSuite","text":"<pre><code>OpenMLBenchmarkSuite(suite_id: int | None, alias: str | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None)\n</code></pre> <p>               Bases: <code>BaseStudy</code></p> <p>An OpenMLBenchmarkSuite represents the OpenML concept of a suite (a collection of tasks).</p> <p>It contains the following information: name, id, description, creation date, creator id and the task ids.</p> <p>According to this list of task ids, the suite object receives a list of OpenML object ids (datasets).</p>"},{"location":"reference/#openml.OpenMLBenchmarkSuite--parameters","title":"Parameters","text":"<p>suite_id : int     the study id alias : str (optional)     a string ID, unique on server (url-friendly) main_entity_type : str     the entity type (e.g., task, run) that is core in this study.     only entities of this type can be added explicitly name : str     the name of the study (meta-info) description : str     brief description (meta-info) status : str     Whether the study is in preparation, active or deactivated creation_date : str     date of creation (meta-info) creator : int     openml user id of the owner / creator tags : list(dict)     The list of tags shows which tags are associated with the study.     Each tag is a dict of (tag) name, window_start and write_access. data : list     a list of data ids associated with this study tasks : list     a list of task ids associated with this study</p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    suite_id: int | None,\n    alias: str | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n):\n    super().__init__(\n        study_id=suite_id,\n        alias=alias,\n        main_entity_type=\"task\",\n        benchmark_suite=None,\n        name=name,\n        description=description,\n        status=status,\n        creation_date=creation_date,\n        creator=creator,\n        tags=tags,\n        data=data,\n        tasks=tasks,\n        flows=None,\n        runs=None,\n        setups=None,\n    )\n</code></pre>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask","title":"OpenMLClassificationTask","text":"<pre><code>OpenMLClassificationTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None, task_id: int | None = None, class_labels: list[str] | None = None, cost_matrix: ndarray | None = None)\n</code></pre> <p>               Bases: <code>OpenMLSupervisedTask</code></p> <p>OpenML Classification object.</p>"},{"location":"reference/#openml.OpenMLClassificationTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     ID of the Classification task type. task_type : str     Name of the Classification task type. data_set_id : int     ID of the OpenML dataset associated with the Classification task. target_name : str     Name of the target variable. estimation_procedure_id : int, default=None     ID of the estimation procedure for the Classification task. estimation_procedure_type : str, default=None     Type of the estimation procedure. estimation_parameters : dict, default=None     Estimation parameters for the Classification task. evaluation_measure : str, default=None     Name of the evaluation measure. data_splits_url : str, default=None     URL of the data splits for the Classification task. task_id : Union[int, None]     ID of the Classification task (if it already exists on OpenML). class_labels : List of str, default=None     A list of class labels (for classification tasks). cost_matrix : array, default=None     A cost matrix (for classification tasks).</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    class_labels: list[str] | None = None,\n    cost_matrix: np.ndarray | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n    )\n    self.class_labels = class_labels\n    self.cost_matrix = cost_matrix\n\n    if cost_matrix is not None:\n        raise NotImplementedError(\"Costmatrix\")\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/#openml.OpenMLClassificationTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/#openml.OpenMLClassificationTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLClassificationTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/#openml.OpenMLClassificationTask.get_X_and_y--returns","title":"Returns","text":"<p>tuple - X and y</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/#openml.OpenMLClassificationTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/#openml.OpenMLClassificationTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask","title":"OpenMLClusteringTask","text":"<pre><code>OpenMLClusteringTask(task_type_id: TaskType, task_type: str, data_set_id: int, estimation_procedure_id: int = 17, task_id: int | None = None, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, evaluation_measure: str | None = None, target_name: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLTask</code></p> <p>OpenML Clustering object.</p>"},{"location":"reference/#openml.OpenMLClusteringTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     Task type ID of the OpenML clustering task. task_type : str     Task type of the OpenML clustering task. data_set_id : int     ID of the OpenML dataset used in clustering the task. estimation_procedure_id : int, default=None     ID of the OpenML estimation procedure. task_id : Union[int, None]     ID of the OpenML clustering task. estimation_procedure_type : str, default=None     Type of the OpenML estimation procedure used in the clustering task. estimation_parameters : dict, default=None     Parameters used by the OpenML estimation procedure. data_splits_url : str, default=None     URL of the OpenML data splits for the clustering task. evaluation_measure : str, default=None     Evaluation measure used in the clustering task. target_name : str, default=None     Name of the target feature (class) that is not part of the     feature set for the clustering task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    estimation_procedure_id: int = 17,\n    task_id: int | None = None,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    evaluation_measure: str | None = None,\n    target_name: str | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        evaluation_measure=evaluation_measure,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        data_splits_url=data_splits_url,\n    )\n\n    self.target_name = target_name\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/#openml.OpenMLClusteringTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLClusteringTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.get_X","title":"get_X","text":"<pre><code>get_X() -&gt; DataFrame\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/#openml.OpenMLClusteringTask.get_X--returns","title":"Returns","text":"<p>The X data as a dataframe</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X(self) -&gt; pd.DataFrame:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    The X data as a dataframe\n    \"\"\"\n    dataset = self.get_dataset()\n    data, *_ = dataset.get_data(target=None)\n    return data\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/#openml.OpenMLClusteringTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/#openml.OpenMLClusteringTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLDataFeature","title":"OpenMLDataFeature","text":"<pre><code>OpenMLDataFeature(index: int, name: str, data_type: str, nominal_values: list[str], number_missing_values: int, ontologies: list[str] | None = None)\n</code></pre> <p>Data Feature (a.k.a. Attribute) object.</p>"},{"location":"reference/#openml.OpenMLDataFeature--parameters","title":"Parameters","text":"<p>index : int     The index of this feature name : str     Name of the feature data_type : str     can be nominal, numeric, string, date (corresponds to arff) nominal_values : list(str)     list of the possible values, in case of nominal attribute number_missing_values : int     Number of rows that have a missing value for this feature. ontologies : list(str)     list of ontologies attached to this feature. An ontology describes the     concept that are described in a feature. An ontology is defined by an     URL where the information is provided.</p> Source code in <code>openml/datasets/data_feature.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    index: int,\n    name: str,\n    data_type: str,\n    nominal_values: list[str],\n    number_missing_values: int,\n    ontologies: list[str] | None = None,\n):\n    if not isinstance(index, int):\n        raise TypeError(f\"Index must be `int` but is {type(index)}\")\n\n    if data_type not in self.LEGAL_DATA_TYPES:\n        raise ValueError(\n            f\"data type should be in {self.LEGAL_DATA_TYPES!s}, found: {data_type}\",\n        )\n\n    if data_type == \"nominal\":\n        if nominal_values is None:\n            raise TypeError(\n                \"Dataset features require attribute `nominal_values` for nominal \"\n                \"feature type.\",\n            )\n\n        if not isinstance(nominal_values, list):\n            raise TypeError(\n                \"Argument `nominal_values` is of wrong datatype, should be list, \"\n                f\"but is {type(nominal_values)}\",\n            )\n    elif nominal_values is not None:\n        raise TypeError(\"Argument `nominal_values` must be None for non-nominal feature.\")\n\n    if not isinstance(number_missing_values, int):\n        msg = f\"number_missing_values must be int but is {type(number_missing_values)}\"\n        raise TypeError(msg)\n\n    self.index = index\n    self.name = str(name)\n    self.data_type = str(data_type)\n    self.nominal_values = nominal_values\n    self.number_missing_values = number_missing_values\n    self.ontologies = ontologies\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset","title":"OpenMLDataset","text":"<pre><code>OpenMLDataset(name: str, description: str | None, data_format: Literal['arff', 'sparse_arff'] = 'arff', cache_format: Literal['feather', 'pickle'] = 'pickle', dataset_id: int | None = None, version: int | None = None, creator: str | None = None, contributor: str | None = None, collection_date: str | None = None, upload_date: str | None = None, language: str | None = None, licence: str | None = None, url: str | None = None, default_target_attribute: str | None = None, row_id_attribute: str | None = None, ignore_attribute: str | list[str] | None = None, version_label: str | None = None, citation: str | None = None, tag: str | None = None, visibility: str | None = None, original_data_url: str | None = None, paper_url: str | None = None, update_comment: str | None = None, md5_checksum: str | None = None, data_file: str | None = None, features_file: str | None = None, qualities_file: str | None = None, dataset: str | None = None, parquet_url: str | None = None, parquet_file: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>Dataset object.</p> <p>Allows fetching and uploading datasets to OpenML.</p>"},{"location":"reference/#openml.OpenMLDataset--parameters","title":"Parameters","text":"<p>name : str     Name of the dataset. description : str     Description of the dataset. data_format : str     Format of the dataset which can be either 'arff' or 'sparse_arff'. cache_format : str     Format for caching the dataset which can be either 'feather' or 'pickle'. dataset_id : int, optional     Id autogenerated by the server. version : int, optional     Version of this dataset. '1' for original version.     Auto-incremented by server. creator : str, optional     The person who created the dataset. contributor : str, optional     People who contributed to the current version of the dataset. collection_date : str, optional     The date the data was originally collected, given by the uploader. upload_date : str, optional     The date-time when the dataset was uploaded, generated by server. language : str, optional     Language in which the data is represented.     Starts with 1 upper case letter, rest lower case, e.g. 'English'. licence : str, optional     License of the data. url : str, optional     Valid URL, points to actual data file.     The file can be on the OpenML server or another dataset repository. default_target_attribute : str, optional     The default target attribute, if it exists.     Can have multiple values, comma separated. row_id_attribute : str, optional     The attribute that represents the row-id column,     if present in the dataset. ignore_attribute : str | list, optional     Attributes that should be excluded in modelling,     such as identifiers and indexes. version_label : str, optional     Version label provided by user.     Can be a date, hash, or some other type of id. citation : str, optional     Reference(s) that should be cited when building on this data. tag : str, optional     Tags, describing the algorithms. visibility : str, optional     Who can see the dataset.     Typical values: 'Everyone','All my friends','Only me'.     Can also be any of the user's circles. original_data_url : str, optional     For derived data, the url to the original dataset. paper_url : str, optional     Link to a paper describing the dataset. update_comment : str, optional     An explanation for when the dataset is uploaded. md5_checksum : str, optional     MD5 checksum to check if the dataset is downloaded without corruption. data_file : str, optional     Path to where the dataset is located. features_file : dict, optional     A dictionary of dataset features,     which maps a feature index to a OpenMLDataFeature. qualities_file : dict, optional     A dictionary of dataset qualities,     which maps a quality name to a quality value. dataset: string, optional     Serialized arff dataset string. parquet_url: string, optional     This is the URL to the storage location where the dataset files are hosted.     This can be a MinIO bucket URL. If specified, the data will be accessed     from this URL when reading the files. parquet_file: string, optional     Path to the local file.</p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def __init__(  # noqa: C901, PLR0912, PLR0913, PLR0915\n    self,\n    name: str,\n    description: str | None,\n    data_format: Literal[\"arff\", \"sparse_arff\"] = \"arff\",\n    cache_format: Literal[\"feather\", \"pickle\"] = \"pickle\",\n    dataset_id: int | None = None,\n    version: int | None = None,\n    creator: str | None = None,\n    contributor: str | None = None,\n    collection_date: str | None = None,\n    upload_date: str | None = None,\n    language: str | None = None,\n    licence: str | None = None,\n    url: str | None = None,\n    default_target_attribute: str | None = None,\n    row_id_attribute: str | None = None,\n    ignore_attribute: str | list[str] | None = None,\n    version_label: str | None = None,\n    citation: str | None = None,\n    tag: str | None = None,\n    visibility: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n    update_comment: str | None = None,\n    md5_checksum: str | None = None,\n    data_file: str | None = None,\n    features_file: str | None = None,\n    qualities_file: str | None = None,\n    dataset: str | None = None,\n    parquet_url: str | None = None,\n    parquet_file: str | None = None,\n):\n    if cache_format not in [\"feather\", \"pickle\"]:\n        raise ValueError(\n            \"cache_format must be one of 'feather' or 'pickle. \"\n            f\"Invalid format specified: {cache_format}\",\n        )\n\n    def find_invalid_characters(string: str, pattern: str) -&gt; str:\n        invalid_chars = set()\n        regex = re.compile(pattern)\n        for char in string:\n            if not regex.match(char):\n                invalid_chars.add(char)\n        return \",\".join(\n            [f\"'{char}'\" if char != \"'\" else f'\"{char}\"' for char in invalid_chars],\n        )\n\n    if dataset_id is None:\n        pattern = \"^[\\x00-\\x7f]*$\"\n        if description and not re.match(pattern, description):\n            # not basiclatin (XSD complains)\n            invalid_characters = find_invalid_characters(description, pattern)\n            raise ValueError(\n                f\"Invalid symbols {invalid_characters} in description: {description}\",\n            )\n        pattern = \"^[\\x00-\\x7f]*$\"\n        if citation and not re.match(pattern, citation):\n            # not basiclatin (XSD complains)\n            invalid_characters = find_invalid_characters(citation, pattern)\n            raise ValueError(\n                f\"Invalid symbols {invalid_characters} in citation: {citation}\",\n            )\n        pattern = \"^[a-zA-Z0-9_\\\\-\\\\.\\\\(\\\\),]+$\"\n        if not re.match(pattern, name):\n            # regex given by server in error message\n            invalid_characters = find_invalid_characters(name, pattern)\n            raise ValueError(f\"Invalid symbols {invalid_characters} in name: {name}\")\n\n    self.ignore_attribute: list[str] | None = None\n    if isinstance(ignore_attribute, str):\n        self.ignore_attribute = [ignore_attribute]\n    elif isinstance(ignore_attribute, list) or ignore_attribute is None:\n        self.ignore_attribute = ignore_attribute\n    else:\n        raise ValueError(\"Wrong data type for ignore_attribute. Should be list.\")\n\n    # TODO add function to check if the name is casual_string128\n    # Attributes received by querying the RESTful API\n    self.dataset_id = int(dataset_id) if dataset_id is not None else None\n    self.name = name\n    self.version = int(version) if version is not None else None\n    self.description = description\n    self.cache_format = cache_format\n    # Has to be called format, otherwise there will be an XML upload error\n    self.format = data_format\n    self.creator = creator\n    self.contributor = contributor\n    self.collection_date = collection_date\n    self.upload_date = upload_date\n    self.language = language\n    self.licence = licence\n    self.url = url\n    self.default_target_attribute = default_target_attribute\n    self.row_id_attribute = row_id_attribute\n\n    self.version_label = version_label\n    self.citation = citation\n    self.tag = tag\n    self.visibility = visibility\n    self.original_data_url = original_data_url\n    self.paper_url = paper_url\n    self.update_comment = update_comment\n    self.md5_checksum = md5_checksum\n    self.data_file = data_file\n    self.parquet_file = parquet_file\n    self._dataset = dataset\n    self._parquet_url = parquet_url\n\n    self._features: dict[int, OpenMLDataFeature] | None = None\n    self._qualities: dict[str, float] | None = None\n    self._no_qualities_found = False\n\n    if features_file is not None:\n        self._features = _read_features(Path(features_file))\n\n    # \"\" was the old default value by `get_dataset` and maybe still used by some\n    if qualities_file == \"\":\n        # TODO(0.15): to switch to \"qualities_file is not None\" below and remove warning\n        warnings.warn(\n            \"Starting from Version 0.15 `qualities_file` must be None and not an empty string \"\n            \"to avoid reading the qualities from file. Set `qualities_file` to None to avoid \"\n            \"this warning.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        qualities_file = None\n\n    if qualities_file is not None:\n        self._qualities = _read_qualities(Path(qualities_file))\n\n    if data_file is not None:\n        data_pickle, data_feather, feather_attribute = self._compressed_cache_file_paths(\n            Path(data_file)\n        )\n        self.data_pickle_file = data_pickle if Path(data_pickle).exists() else None\n        self.data_feather_file = data_feather if Path(data_feather).exists() else None\n        self.feather_attribute_file = feather_attribute if Path(feather_attribute) else None\n    else:\n        self.data_pickle_file = None\n        self.data_feather_file = None\n        self.feather_attribute_file = None\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.features","title":"features  <code>property</code>","text":"<pre><code>features: dict[int, OpenMLDataFeature]\n</code></pre> <p>Get the features of this dataset.</p>"},{"location":"reference/#openml.OpenMLDataset.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Get the dataset numeric id.</p>"},{"location":"reference/#openml.OpenMLDataset.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLDataset.qualities","title":"qualities  <code>property</code>","text":"<pre><code>qualities: dict[str, float] | None\n</code></pre> <p>Get the qualities of this dataset.</p>"},{"location":"reference/#openml.OpenMLDataset.get_data","title":"get_data","text":"<pre><code>get_data(target: list[str] | str | None = None, include_row_id: bool = False, include_ignore_attribute: bool = False) -&gt; tuple[DataFrame, Series | None, list[bool], list[str]]\n</code></pre> <p>Returns dataset content as dataframes.</p>"},{"location":"reference/#openml.OpenMLDataset.get_data--parameters","title":"Parameters","text":"<p>target : string, List[str] or None (default=None)     Name of target column to separate from the data.     Splitting multiple columns is currently not supported. include_row_id : boolean (default=False)     Whether to include row ids in the returned dataset. include_ignore_attribute : boolean (default=False)     Whether to include columns that are marked as \"ignore\"     on the server in the dataset.</p>"},{"location":"reference/#openml.OpenMLDataset.get_data--returns","title":"Returns","text":"<p>X : dataframe, shape (n_samples, n_columns)     Dataset, may have sparse dtypes in the columns if required. y : pd.Series, shape (n_samples, ) or None     Target column categorical_indicator : list[bool]     Mask that indicate categorical features. attribute_names : list[str]     List of attribute names.</p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def get_data(  # noqa: C901\n    self,\n    target: list[str] | str | None = None,\n    include_row_id: bool = False,  # noqa: FBT001, FBT002\n    include_ignore_attribute: bool = False,  # noqa: FBT001, FBT002\n) -&gt; tuple[pd.DataFrame, pd.Series | None, list[bool], list[str]]:\n    \"\"\"Returns dataset content as dataframes.\n\n    Parameters\n    ----------\n    target : string, List[str] or None (default=None)\n        Name of target column to separate from the data.\n        Splitting multiple columns is currently not supported.\n    include_row_id : boolean (default=False)\n        Whether to include row ids in the returned dataset.\n    include_ignore_attribute : boolean (default=False)\n        Whether to include columns that are marked as \"ignore\"\n        on the server in the dataset.\n\n\n    Returns\n    -------\n    X : dataframe, shape (n_samples, n_columns)\n        Dataset, may have sparse dtypes in the columns if required.\n    y : pd.Series, shape (n_samples, ) or None\n        Target column\n    categorical_indicator : list[bool]\n        Mask that indicate categorical features.\n    attribute_names : list[str]\n        List of attribute names.\n    \"\"\"\n    data, categorical_mask, attribute_names = self._load_data()\n\n    to_exclude = []\n    if not include_row_id and self.row_id_attribute is not None:\n        if isinstance(self.row_id_attribute, str):\n            to_exclude.append(self.row_id_attribute)\n        elif isinstance(self.row_id_attribute, Iterable):\n            to_exclude.extend(self.row_id_attribute)\n\n    if not include_ignore_attribute and self.ignore_attribute is not None:\n        if isinstance(self.ignore_attribute, str):\n            to_exclude.append(self.ignore_attribute)\n        elif isinstance(self.ignore_attribute, Iterable):\n            to_exclude.extend(self.ignore_attribute)\n\n    if len(to_exclude) &gt; 0:\n        logger.info(f\"Going to remove the following attributes: {to_exclude}\")\n        keep = np.array([column not in to_exclude for column in attribute_names])\n        data = data.drop(columns=to_exclude)\n        categorical_mask = [cat for cat, k in zip(categorical_mask, keep) if k]\n        attribute_names = [att for att, k in zip(attribute_names, keep) if k]\n\n    if target is None:\n        return data, None, categorical_mask, attribute_names\n\n    if isinstance(target, str):\n        target_names = target.split(\",\") if \",\" in target else [target]\n    else:\n        target_names = target\n\n    # All the assumptions below for the target are dependant on the number of targets being 1\n    n_targets = len(target_names)\n    if n_targets &gt; 1:\n        raise NotImplementedError(f\"Number of targets {n_targets} not implemented.\")\n\n    target_name = target_names[0]\n    x = data.drop(columns=[target_name])\n    y = data[target_name].squeeze()\n\n    # Finally, remove the target from the list of attributes and categorical mask\n    target_index = attribute_names.index(target_name)\n    categorical_mask.pop(target_index)\n    attribute_names.remove(target_name)\n\n    assert isinstance(y, pd.Series)\n    return x, y, categorical_mask, attribute_names\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.get_features_by_type","title":"get_features_by_type","text":"<pre><code>get_features_by_type(data_type: str, exclude: list[str] | None = None, exclude_ignore_attribute: bool = True, exclude_row_id_attribute: bool = True) -&gt; list[int]\n</code></pre> <p>Return indices of features of a given type, e.g. all nominal features. Optional parameters to exclude various features by index or ontology.</p>"},{"location":"reference/#openml.OpenMLDataset.get_features_by_type--parameters","title":"Parameters","text":"<p>data_type : str     The data type to return (e.g., nominal, numeric, date, string) exclude : list(int)     List of columns to exclude from the return value exclude_ignore_attribute : bool     Whether to exclude the defined ignore attributes (and adapt the     return values as if these indices are not present) exclude_row_id_attribute : bool     Whether to exclude the defined row id attributes (and adapt the     return values as if these indices are not present)</p>"},{"location":"reference/#openml.OpenMLDataset.get_features_by_type--returns","title":"Returns","text":"<p>result : list     a list of indices that have the specified data type</p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def get_features_by_type(  # noqa: C901\n    self,\n    data_type: str,\n    exclude: list[str] | None = None,\n    exclude_ignore_attribute: bool = True,  # noqa: FBT002, FBT001\n    exclude_row_id_attribute: bool = True,  # noqa: FBT002, FBT001\n) -&gt; list[int]:\n    \"\"\"\n    Return indices of features of a given type, e.g. all nominal features.\n    Optional parameters to exclude various features by index or ontology.\n\n    Parameters\n    ----------\n    data_type : str\n        The data type to return (e.g., nominal, numeric, date, string)\n    exclude : list(int)\n        List of columns to exclude from the return value\n    exclude_ignore_attribute : bool\n        Whether to exclude the defined ignore attributes (and adapt the\n        return values as if these indices are not present)\n    exclude_row_id_attribute : bool\n        Whether to exclude the defined row id attributes (and adapt the\n        return values as if these indices are not present)\n\n    Returns\n    -------\n    result : list\n        a list of indices that have the specified data type\n    \"\"\"\n    if data_type not in OpenMLDataFeature.LEGAL_DATA_TYPES:\n        raise TypeError(\"Illegal feature type requested\")\n    if self.ignore_attribute is not None and not isinstance(self.ignore_attribute, list):\n        raise TypeError(\"ignore_attribute should be a list\")\n    if self.row_id_attribute is not None and not isinstance(self.row_id_attribute, str):\n        raise TypeError(\"row id attribute should be a str\")\n    if exclude is not None and not isinstance(exclude, list):\n        raise TypeError(\"Exclude should be a list\")\n        # assert all(isinstance(elem, str) for elem in exclude),\n        #            \"Exclude should be a list of strings\"\n    to_exclude = []\n    if exclude is not None:\n        to_exclude.extend(exclude)\n    if exclude_ignore_attribute and self.ignore_attribute is not None:\n        to_exclude.extend(self.ignore_attribute)\n    if exclude_row_id_attribute and self.row_id_attribute is not None:\n        to_exclude.append(self.row_id_attribute)\n\n    result = []\n    offset = 0\n    # this function assumes that everything in to_exclude will\n    # be 'excluded' from the dataset (hence the offset)\n    for idx in self.features:\n        name = self.features[idx].name\n        if name in to_exclude:\n            offset += 1\n        elif self.features[idx].data_type == data_type:\n            result.append(idx - offset)\n    return result\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/#openml.OpenMLDataset.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/#openml.OpenMLDataset.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.retrieve_class_labels","title":"retrieve_class_labels","text":"<pre><code>retrieve_class_labels(target_name: str = 'class') -&gt; None | list[str]\n</code></pre> <p>Reads the datasets arff to determine the class-labels.</p> <p>If the task has no class labels (for example a regression problem) it returns None. Necessary because the data returned by get_data only contains the indices of the classes, while OpenML needs the real classname when uploading the results of a run.</p>"},{"location":"reference/#openml.OpenMLDataset.retrieve_class_labels--parameters","title":"Parameters","text":"<p>target_name : str     Name of the target attribute</p>"},{"location":"reference/#openml.OpenMLDataset.retrieve_class_labels--returns","title":"Returns","text":"<p>list</p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def retrieve_class_labels(self, target_name: str = \"class\") -&gt; None | list[str]:\n    \"\"\"Reads the datasets arff to determine the class-labels.\n\n    If the task has no class labels (for example a regression problem)\n    it returns None. Necessary because the data returned by get_data\n    only contains the indices of the classes, while OpenML needs the real\n    classname when uploading the results of a run.\n\n    Parameters\n    ----------\n    target_name : str\n        Name of the target attribute\n\n    Returns\n    -------\n    list\n    \"\"\"\n    for feature in self.features.values():\n        if feature.name == target_name:\n            if feature.data_type == \"nominal\":\n                return feature.nominal_values\n\n            if feature.data_type == \"string\":\n                # Rel.: #1311\n                # The target is invalid for a classification task if the feature type is string\n                # and not nominal. For such miss-configured tasks, we silently fix it here as\n                # we can safely interpreter string as nominal.\n                df, *_ = self.get_data()\n                return list(df[feature.name].unique())\n\n    return None\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLEvaluation","title":"OpenMLEvaluation","text":"<pre><code>OpenMLEvaluation(run_id: int, task_id: int, setup_id: int, flow_id: int, flow_name: str, data_id: int, data_name: str, function: str, upload_time: str, uploader: int, uploader_name: str, value: float | None, values: list[float] | None, array_data: str | None = None)\n</code></pre> <p>Contains all meta-information about a run / evaluation combination, according to the evaluation/list function</p>"},{"location":"reference/#openml.OpenMLEvaluation--parameters","title":"Parameters","text":"<p>run_id : int     Refers to the run. task_id : int     Refers to the task. setup_id : int     Refers to the setup. flow_id : int     Refers to the flow. flow_name : str     Name of the referred flow. data_id : int     Refers to the dataset. data_name : str     The name of the dataset. function : str     The evaluation metric of this item (e.g., accuracy). upload_time : str     The time of evaluation. uploader: int     Uploader ID (user ID) upload_name : str     Name of the uploader of this evaluation value : float     The value (score) of this evaluation. values : List[float]     The values (scores) per repeat and fold (if requested) array_data : str     list of information per class.     (e.g., in case of precision, auroc, recall)</p> Source code in <code>openml/evaluations/evaluation.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    run_id: int,\n    task_id: int,\n    setup_id: int,\n    flow_id: int,\n    flow_name: str,\n    data_id: int,\n    data_name: str,\n    function: str,\n    upload_time: str,\n    uploader: int,\n    uploader_name: str,\n    value: float | None,\n    values: list[float] | None,\n    array_data: str | None = None,\n):\n    self.run_id = run_id\n    self.task_id = task_id\n    self.setup_id = setup_id\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.data_id = data_id\n    self.data_name = data_name\n    self.function = function\n    self.upload_time = upload_time\n    self.uploader = uploader\n    self.uploader_name = uploader_name\n    self.value = value\n    self.values = values\n    self.array_data = array_data\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow","title":"OpenMLFlow","text":"<pre><code>OpenMLFlow(name: str, description: str, model: object, components: dict, parameters: dict, parameters_meta_info: dict, external_version: str, tags: list, language: str, dependencies: str, class_name: str | None = None, custom_name: str | None = None, binary_url: str | None = None, binary_format: str | None = None, binary_md5: str | None = None, uploader: str | None = None, upload_date: str | None = None, flow_id: int | None = None, extension: Extension | None = None, version: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Flow. Stores machine learning models.</p> <p>Flows should not be generated manually, but by the function :meth:<code>openml.flows.create_flow_from_model</code>. Using this helper function ensures that all relevant fields are filled in.</p> <p>Implements <code>openml.implementation.upload.xsd &lt;https://github.com/openml/openml/blob/master/openml_OS/views/pages/api_new/v1/xsd/ openml.implementation.upload.xsd&gt;</code>_.</p>"},{"location":"reference/#openml.OpenMLFlow--parameters","title":"Parameters","text":"<p>name : str     Name of the flow. Is used together with the attribute     <code>external_version</code> as a unique identifier of the flow. description : str     Human-readable description of the flow (free text). model : object     ML model which is described by this flow. components : OrderedDict     Mapping from component identifier to an OpenMLFlow object. Components     are usually subfunctions of an algorithm (e.g. kernels), base learners     in ensemble algorithms (decision tree in adaboost) or building blocks     of a machine learning pipeline. Components are modeled as independent     flows and can be shared between flows (different pipelines can use     the same components). parameters : OrderedDict     Mapping from parameter name to the parameter default value. The     parameter default value must be of type <code>str</code>, so that the respective     toolbox plugin can take care of casting the parameter default value to     the correct type. parameters_meta_info : OrderedDict     Mapping from parameter name to <code>dict</code>. Stores additional information     for each parameter. Required keys are <code>data_type</code> and <code>description</code>. external_version : str     Version number of the software the flow is implemented in. Is used     together with the attribute <code>name</code> as a uniquer identifier of the flow. tags : list     List of tags. Created on the server by other API calls. language : str     Natural language the flow is described in (not the programming     language). dependencies : str     A list of dependencies necessary to run the flow. This field should     contain all libraries the flow depends on. To allow reproducibility     it should also specify the exact version numbers. class_name : str, optional     The development language name of the class which is described by this     flow. custom_name : str, optional     Custom name of the flow given by the owner. binary_url : str, optional     Url from which the binary can be downloaded. Added by the server.     Ignored when uploaded manually. Will not be used by the python API     because binaries aren't compatible across machines. binary_format : str, optional     Format in which the binary code was uploaded. Will not be used by the     python API because binaries aren't compatible across machines. binary_md5 : str, optional     MD5 checksum to check if the binary code was correctly downloaded. Will     not be used by the python API because binaries aren't compatible across     machines. uploader : str, optional     OpenML user ID of the uploader. Filled in by the server. upload_date : str, optional     Date the flow was uploaded. Filled in by the server. flow_id : int, optional     Flow ID. Assigned by the server. extension : Extension, optional     The extension for a flow (e.g., sklearn). version : str, optional     OpenML version of the flow. Assigned by the server.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    name: str,\n    description: str,\n    model: object,\n    components: dict,\n    parameters: dict,\n    parameters_meta_info: dict,\n    external_version: str,\n    tags: list,\n    language: str,\n    dependencies: str,\n    class_name: str | None = None,\n    custom_name: str | None = None,\n    binary_url: str | None = None,\n    binary_format: str | None = None,\n    binary_md5: str | None = None,\n    uploader: str | None = None,\n    upload_date: str | None = None,\n    flow_id: int | None = None,\n    extension: Extension | None = None,\n    version: str | None = None,\n):\n    self.name = name\n    self.description = description\n    self.model = model\n\n    for variable, variable_name in [\n        [components, \"components\"],\n        [parameters, \"parameters\"],\n        [parameters_meta_info, \"parameters_meta_info\"],\n    ]:\n        if not isinstance(variable, (OrderedDict, dict)):\n            raise TypeError(\n                f\"{variable_name} must be of type OrderedDict or dict, \"\n                f\"but is {type(variable)}.\",\n            )\n\n    self.components = components\n    self.parameters = parameters\n    self.parameters_meta_info = parameters_meta_info\n    self.class_name = class_name\n\n    keys_parameters = set(parameters.keys())\n    keys_parameters_meta_info = set(parameters_meta_info.keys())\n    if len(keys_parameters.difference(keys_parameters_meta_info)) &gt; 0:\n        raise ValueError(\n            f\"Parameter {keys_parameters.difference(keys_parameters_meta_info)!s} only in \"\n            \"parameters, but not in parameters_meta_info.\",\n        )\n    if len(keys_parameters_meta_info.difference(keys_parameters)) &gt; 0:\n        raise ValueError(\n            f\"Parameter {keys_parameters_meta_info.difference(keys_parameters)!s} only in \"\n            \" parameters_meta_info, but not in parameters.\",\n        )\n\n    self.external_version = external_version\n    self.uploader = uploader\n\n    self.custom_name = custom_name\n    self.tags = tags if tags is not None else []\n    self.binary_url = binary_url\n    self.binary_format = binary_format\n    self.binary_md5 = binary_md5\n    self.version = version\n    self.upload_date = upload_date\n    self.language = language\n    self.dependencies = dependencies\n    self.flow_id = flow_id\n    if extension is None:\n        self._extension = get_extension_by_flow(self)\n    else:\n        self._extension = extension\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.extension","title":"extension  <code>property</code>","text":"<pre><code>extension: Extension\n</code></pre> <p>The extension of the flow (e.g., sklearn).</p>"},{"location":"reference/#openml.OpenMLFlow.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The ID of the flow.</p>"},{"location":"reference/#openml.OpenMLFlow.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLFlow.from_filesystem","title":"from_filesystem  <code>classmethod</code>","text":"<pre><code>from_filesystem(input_directory: str | Path) -&gt; OpenMLFlow\n</code></pre> <p>Read a flow from an XML in input_directory on the filesystem.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>@classmethod\ndef from_filesystem(cls, input_directory: str | Path) -&gt; OpenMLFlow:\n    \"\"\"Read a flow from an XML in input_directory on the filesystem.\"\"\"\n    input_directory = Path(input_directory) / \"flow.xml\"\n    with input_directory.open() as f:\n        xml_string = f.read()\n    return OpenMLFlow._from_dict(xmltodict.parse(xml_string))\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.get_structure","title":"get_structure","text":"<pre><code>get_structure(key_item: str) -&gt; dict[str, list[str]]\n</code></pre> <p>Returns for each sub-component of the flow the path of identifiers that should be traversed to reach this component. The resulting dict maps a key (identifying a flow by either its id, name or fullname) to the parameter prefix.</p>"},{"location":"reference/#openml.OpenMLFlow.get_structure--parameters","title":"Parameters","text":"<p>key_item: str     The flow attribute that will be used to identify flows in the     structure. Allowed values {flow_id, name}</p>"},{"location":"reference/#openml.OpenMLFlow.get_structure--returns","title":"Returns","text":"<p>dict[str, List[str]]     The flow structure</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def get_structure(self, key_item: str) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Returns for each sub-component of the flow the path of identifiers\n    that should be traversed to reach this component. The resulting dict\n    maps a key (identifying a flow by either its id, name or fullname) to\n    the parameter prefix.\n\n    Parameters\n    ----------\n    key_item: str\n        The flow attribute that will be used to identify flows in the\n        structure. Allowed values {flow_id, name}\n\n    Returns\n    -------\n    dict[str, List[str]]\n        The flow structure\n    \"\"\"\n    if key_item not in [\"flow_id\", \"name\"]:\n        raise ValueError(\"key_item should be in {flow_id, name}\")\n    structure = {}\n    for key, sub_flow in self.components.items():\n        sub_structure = sub_flow.get_structure(key_item)\n        for flow_name, flow_sub_structure in sub_structure.items():\n            structure[flow_name] = [key, *flow_sub_structure]\n    structure[getattr(self, key_item)] = []\n    return structure\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.get_subflow","title":"get_subflow","text":"<pre><code>get_subflow(structure: list[str]) -&gt; OpenMLFlow\n</code></pre> <p>Returns a subflow from the tree of dependencies.</p>"},{"location":"reference/#openml.OpenMLFlow.get_subflow--parameters","title":"Parameters","text":"<p>structure: list[str]     A list of strings, indicating the location of the subflow</p>"},{"location":"reference/#openml.OpenMLFlow.get_subflow--returns","title":"Returns","text":"<p>OpenMLFlow     The OpenMLFlow that corresponds to the structure</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def get_subflow(self, structure: list[str]) -&gt; OpenMLFlow:\n    \"\"\"\n    Returns a subflow from the tree of dependencies.\n\n    Parameters\n    ----------\n    structure: list[str]\n        A list of strings, indicating the location of the subflow\n\n    Returns\n    -------\n    OpenMLFlow\n        The OpenMLFlow that corresponds to the structure\n    \"\"\"\n    # make a copy of structure, as we don't want to change it in the\n    # outer scope\n    structure = list(structure)\n    if len(structure) &lt; 1:\n        raise ValueError(\"Please provide a structure list of size &gt;= 1\")\n    sub_identifier = structure[0]\n    if sub_identifier not in self.components:\n        raise ValueError(\n            f\"Flow {self.name} does not contain component with \" f\"identifier {sub_identifier}\",\n        )\n    if len(structure) == 1:\n        return self.components[sub_identifier]  # type: ignore\n\n    structure.pop(0)\n    return self.components[sub_identifier].get_subflow(structure)  # type: ignore\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.publish","title":"publish","text":"<pre><code>publish(raise_error_if_exists: bool = False) -&gt; OpenMLFlow\n</code></pre> <p>Publish this flow to OpenML server.</p> <p>Raises a PyOpenMLError if the flow exists on the server, but <code>self.flow_id</code> does not match the server known flow id.</p>"},{"location":"reference/#openml.OpenMLFlow.publish--parameters","title":"Parameters","text":"<p>raise_error_if_exists : bool, optional (default=False)     If True, raise PyOpenMLError if the flow exists on the server.     If False, update the local flow to match the server flow.</p>"},{"location":"reference/#openml.OpenMLFlow.publish--returns","title":"Returns","text":"<p>self : OpenMLFlow</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def publish(self, raise_error_if_exists: bool = False) -&gt; OpenMLFlow:  # noqa: FBT001, FBT002\n    \"\"\"Publish this flow to OpenML server.\n\n    Raises a PyOpenMLError if the flow exists on the server, but\n    `self.flow_id` does not match the server known flow id.\n\n    Parameters\n    ----------\n    raise_error_if_exists : bool, optional (default=False)\n        If True, raise PyOpenMLError if the flow exists on the server.\n        If False, update the local flow to match the server flow.\n\n    Returns\n    -------\n    self : OpenMLFlow\n\n    \"\"\"\n    # Import at top not possible because of cyclic dependencies. In\n    # particular, flow.py tries to import functions.py in order to call\n    # get_flow(), while functions.py tries to import flow.py in order to\n    # instantiate an OpenMLFlow.\n    import openml.flows.functions\n\n    flow_id = openml.flows.functions.flow_exists(self.name, self.external_version)\n    if not flow_id:\n        if self.flow_id:\n            raise openml.exceptions.PyOpenMLError(\n                \"Flow does not exist on the server, \" \"but 'flow.flow_id' is not None.\",\n            )\n        super().publish()\n        assert self.flow_id is not None  # for mypy\n        flow_id = self.flow_id\n    elif raise_error_if_exists:\n        error_message = f\"This OpenMLFlow already exists with id: {flow_id}.\"\n        raise openml.exceptions.PyOpenMLError(error_message)\n    elif self.flow_id is not None and self.flow_id != flow_id:\n        raise openml.exceptions.PyOpenMLError(\n            \"Local flow_id does not match server flow_id: \" f\"'{self.flow_id}' vs '{flow_id}'\",\n        )\n\n    flow = openml.flows.functions.get_flow(flow_id)\n    _copy_server_fields(flow, self)\n    try:\n        openml.flows.functions.assert_flows_equal(\n            self,\n            flow,\n            flow.upload_date,\n            ignore_parameter_values=True,\n            ignore_custom_name_if_none=True,\n        )\n    except ValueError as e:\n        message = e.args[0]\n        raise ValueError(\n            \"The flow on the server is inconsistent with the local flow. \"\n            f\"The server flow ID is {flow_id}. Please check manually and remove \"\n            f\"the flow if necessary! Error is:\\n'{message}'\",\n        ) from e\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/#openml.OpenMLFlow.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/#openml.OpenMLFlow.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.to_filesystem","title":"to_filesystem","text":"<pre><code>to_filesystem(output_directory: str | Path) -&gt; None\n</code></pre> <p>Write a flow to the filesystem as XML to output_directory.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def to_filesystem(self, output_directory: str | Path) -&gt; None:\n    \"\"\"Write a flow to the filesystem as XML to output_directory.\"\"\"\n    output_directory = Path(output_directory)\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    output_path = output_directory / \"flow.xml\"\n    if output_path.exists():\n        raise ValueError(\"Output directory already contains a flow.xml file.\")\n\n    run_xml = self._to_xml()\n    with output_path.open(\"w\") as f:\n        f.write(run_xml)\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask","title":"OpenMLLearningCurveTask","text":"<pre><code>OpenMLLearningCurveTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 13, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, task_id: int | None = None, evaluation_measure: str | None = None, class_labels: list[str] | None = None, cost_matrix: ndarray | None = None)\n</code></pre> <p>               Bases: <code>OpenMLClassificationTask</code></p> <p>OpenML Learning Curve object.</p>"},{"location":"reference/#openml.OpenMLLearningCurveTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     ID of the Learning Curve task. task_type : str     Name of the Learning Curve task. data_set_id : int     ID of the dataset that this task is associated with. target_name : str     Name of the target feature in the dataset. estimation_procedure_id : int, default=None     ID of the estimation procedure to use for evaluating models. estimation_procedure_type : str, default=None     Type of the estimation procedure. estimation_parameters : dict, default=None     Additional parameters for the estimation procedure. data_splits_url : str, default=None     URL of the file containing the data splits for Learning Curve task. task_id : Union[int, None]     ID of the Learning Curve task. evaluation_measure : str, default=None     Name of the evaluation measure to use for evaluating models. class_labels : list of str, default=None     Class labels for Learning Curve tasks. cost_matrix : numpy array, default=None     Cost matrix for Learning Curve tasks.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 13,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    evaluation_measure: str | None = None,\n    class_labels: list[str] | None = None,\n    cost_matrix: np.ndarray | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n        class_labels=class_labels,\n        cost_matrix=cost_matrix,\n    )\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/#openml.OpenMLLearningCurveTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/#openml.OpenMLLearningCurveTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLLearningCurveTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/#openml.OpenMLLearningCurveTask.get_X_and_y--returns","title":"Returns","text":"<p>tuple - X and y</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/#openml.OpenMLLearningCurveTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/#openml.OpenMLLearningCurveTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLParameter","title":"OpenMLParameter","text":"<pre><code>OpenMLParameter(input_id: int, flow_id: int, flow_name: str, full_name: str, parameter_name: str, data_type: str, default_value: str, value: str)\n</code></pre> <p>Parameter object (used in setup).</p>"},{"location":"reference/#openml.OpenMLParameter--parameters","title":"Parameters","text":"<p>input_id : int     The input id from the openml database flow id : int     The flow to which this parameter is associated flow name : str     The name of the flow (no version number) to which this parameter     is associated full_name : str     The name of the flow and parameter combined parameter_name : str     The name of the parameter data_type : str     The datatype of the parameter. generally unused for sklearn flows default_value : str     The default value. For sklearn parameters, this is unknown and a     default value is selected arbitrarily value : str     If the parameter was set, the value that it was set to.</p> Source code in <code>openml/setups/setup.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    input_id: int,\n    flow_id: int,\n    flow_name: str,\n    full_name: str,\n    parameter_name: str,\n    data_type: str,\n    default_value: str,\n    value: str,\n):\n    self.id = input_id\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.full_name = full_name\n    self.parameter_name = parameter_name\n    self.data_type = data_type\n    self.default_value = default_value\n    self.value = value\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask","title":"OpenMLRegressionTask","text":"<pre><code>OpenMLRegressionTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 7, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, task_id: int | None = None, evaluation_measure: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLSupervisedTask</code></p> <p>OpenML Regression object.</p>"},{"location":"reference/#openml.OpenMLRegressionTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     Task type ID of the OpenML Regression task. task_type : str     Task type of the OpenML Regression task. data_set_id : int     ID of the OpenML dataset. target_name : str     Name of the target feature used in the Regression task. estimation_procedure_id : int, default=None     ID of the OpenML estimation procedure. estimation_procedure_type : str, default=None     Type of the OpenML estimation procedure. estimation_parameters : dict, default=None     Parameters used by the OpenML estimation procedure. data_splits_url : str, default=None     URL of the OpenML data splits for the Regression task. task_id : Union[int, None]     ID of the OpenML Regression task. evaluation_measure : str, default=None     Evaluation measure used in the Regression task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 7,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    evaluation_measure: str | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n    )\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/#openml.OpenMLRegressionTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/#openml.OpenMLRegressionTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLRegressionTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/#openml.OpenMLRegressionTask.get_X_and_y--returns","title":"Returns","text":"<p>tuple - X and y</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/#openml.OpenMLRegressionTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/#openml.OpenMLRegressionTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLRun","title":"OpenMLRun","text":"<pre><code>OpenMLRun(task_id: int, flow_id: int | None, dataset_id: int | None, setup_string: str | None = None, output_files: dict[str, int] | None = None, setup_id: int | None = None, tags: list[str] | None = None, uploader: int | None = None, uploader_name: str | None = None, evaluations: dict | None = None, fold_evaluations: dict | None = None, sample_evaluations: dict | None = None, data_content: list[list] | None = None, trace: OpenMLRunTrace | None = None, model: object | None = None, task_type: str | None = None, task_evaluation_measure: str | None = None, flow_name: str | None = None, parameter_settings: list[dict[str, Any]] | None = None, predictions_url: str | None = None, task: OpenMLTask | None = None, flow: OpenMLFlow | None = None, run_id: int | None = None, description_text: str | None = None, run_details: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Run: result of running a model on an OpenML dataset.</p>"},{"location":"reference/#openml.OpenMLRun--parameters","title":"Parameters","text":"<p>task_id: int     The ID of the OpenML task associated with the run. flow_id: int     The ID of the OpenML flow associated with the run. dataset_id: int     The ID of the OpenML dataset used for the run. setup_string: str     The setup string of the run. output_files: Dict[str, int]     Specifies where each related file can be found. setup_id: int     An integer representing the ID of the setup used for the run. tags: List[str]     Representing the tags associated with the run. uploader: int     User ID of the uploader. uploader_name: str     The name of the person who uploaded the run. evaluations: Dict     Representing the evaluations of the run. fold_evaluations: Dict     The evaluations of the run for each fold. sample_evaluations: Dict     The evaluations of the run for each sample. data_content: List[List]     The predictions generated from executing this run. trace: OpenMLRunTrace     The trace containing information on internal model evaluations of this run. model: object     The untrained model that was evaluated in the run. task_type: str     The type of the OpenML task associated with the run. task_evaluation_measure: str     The evaluation measure used for the task. flow_name: str     The name of the OpenML flow associated with the run. parameter_settings: list[OrderedDict]     Representing the parameter settings used for the run. predictions_url: str     The URL of the predictions file. task: OpenMLTask     An instance of the OpenMLTask class, representing the OpenML task associated     with the run. flow: OpenMLFlow     An instance of the OpenMLFlow class, representing the OpenML flow associated     with the run. run_id: int     The ID of the run. description_text: str, optional     Description text to add to the predictions file. If left None, is set to the     time the arff file is generated. run_details: str, optional (default=None)     Description of the run stored in the run meta-data.</p> Source code in <code>openml/runs/run.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_id: int,\n    flow_id: int | None,\n    dataset_id: int | None,\n    setup_string: str | None = None,\n    output_files: dict[str, int] | None = None,\n    setup_id: int | None = None,\n    tags: list[str] | None = None,\n    uploader: int | None = None,\n    uploader_name: str | None = None,\n    evaluations: dict | None = None,\n    fold_evaluations: dict | None = None,\n    sample_evaluations: dict | None = None,\n    data_content: list[list] | None = None,\n    trace: OpenMLRunTrace | None = None,\n    model: object | None = None,\n    task_type: str | None = None,\n    task_evaluation_measure: str | None = None,\n    flow_name: str | None = None,\n    parameter_settings: list[dict[str, Any]] | None = None,\n    predictions_url: str | None = None,\n    task: OpenMLTask | None = None,\n    flow: OpenMLFlow | None = None,\n    run_id: int | None = None,\n    description_text: str | None = None,\n    run_details: str | None = None,\n):\n    self.uploader = uploader\n    self.uploader_name = uploader_name\n    self.task_id = task_id\n    self.task_type = task_type\n    self.task_evaluation_measure = task_evaluation_measure\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.setup_id = setup_id\n    self.setup_string = setup_string\n    self.parameter_settings = parameter_settings\n    self.dataset_id = dataset_id\n    self.evaluations = evaluations\n    self.fold_evaluations = fold_evaluations\n    self.sample_evaluations = sample_evaluations\n    self.data_content = data_content\n    self.output_files = output_files\n    self.trace = trace\n    self.error_message = None\n    self.task = task\n    self.flow = flow\n    self.run_id = run_id\n    self.model = model\n    self.tags = tags\n    self.predictions_url = predictions_url\n    self.description_text = description_text\n    self.run_details = run_details\n    self._predictions = None\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The ID of the run, None if not uploaded to the server yet.</p>"},{"location":"reference/#openml.OpenMLRun.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLRun.predictions","title":"predictions  <code>property</code>","text":"<pre><code>predictions: DataFrame\n</code></pre> <p>Return a DataFrame with predictions for this run</p>"},{"location":"reference/#openml.OpenMLRun.from_filesystem","title":"from_filesystem  <code>classmethod</code>","text":"<pre><code>from_filesystem(directory: str | Path, expect_model: bool = True) -&gt; OpenMLRun\n</code></pre> <p>The inverse of the to_filesystem method. Instantiates an OpenMLRun object based on files stored on the file system.</p>"},{"location":"reference/#openml.OpenMLRun.from_filesystem--parameters","title":"Parameters","text":"<p>directory : str     a path leading to the folder where the results     are stored</p> bool <p>if True, it requires the model pickle to be present, and an error will be thrown if not. Otherwise, the model might or might not be present.</p>"},{"location":"reference/#openml.OpenMLRun.from_filesystem--returns","title":"Returns","text":"<p>run : OpenMLRun     the re-instantiated run object</p> Source code in <code>openml/runs/run.py</code> <pre><code>@classmethod\ndef from_filesystem(cls, directory: str | Path, expect_model: bool = True) -&gt; OpenMLRun:  # noqa: FBT001, FBT002\n    \"\"\"\n    The inverse of the to_filesystem method. Instantiates an OpenMLRun\n    object based on files stored on the file system.\n\n    Parameters\n    ----------\n    directory : str\n        a path leading to the folder where the results\n        are stored\n\n    expect_model : bool\n        if True, it requires the model pickle to be present, and an error\n        will be thrown if not. Otherwise, the model might or might not\n        be present.\n\n    Returns\n    -------\n    run : OpenMLRun\n        the re-instantiated run object\n    \"\"\"\n    # Avoiding cyclic imports\n    import openml.runs.functions\n\n    directory = Path(directory)\n    if not directory.is_dir():\n        raise ValueError(\"Could not find folder\")\n\n    description_path = directory / \"description.xml\"\n    predictions_path = directory / \"predictions.arff\"\n    trace_path = directory / \"trace.arff\"\n    model_path = directory / \"model.pkl\"\n\n    if not description_path.is_file():\n        raise ValueError(\"Could not find description.xml\")\n    if not predictions_path.is_file():\n        raise ValueError(\"Could not find predictions.arff\")\n    if (not model_path.is_file()) and expect_model:\n        raise ValueError(\"Could not find model.pkl\")\n\n    with description_path.open() as fht:\n        xml_string = fht.read()\n    run = openml.runs.functions._create_run_from_xml(xml_string, from_server=False)\n\n    if run.flow_id is None:\n        flow = openml.flows.OpenMLFlow.from_filesystem(directory)\n        run.flow = flow\n        run.flow_name = flow.name\n\n    with predictions_path.open() as fht:\n        predictions = arff.load(fht)\n        run.data_content = predictions[\"data\"]\n\n    if model_path.is_file():\n        # note that it will load the model if the file exists, even if\n        # expect_model is False\n        with model_path.open(\"rb\") as fhb:\n            run.model = pickle.load(fhb)  # noqa: S301\n\n    if trace_path.is_file():\n        run.trace = openml.runs.OpenMLRunTrace._from_filesystem(trace_path)\n\n    return run\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.get_metric_fn","title":"get_metric_fn","text":"<pre><code>get_metric_fn(sklearn_fn: Callable, kwargs: dict | None = None) -&gt; ndarray\n</code></pre> <p>Calculates metric scores based on predicted values. Assumes the run has been executed locally (and contains run_data). Furthermore, it assumes that the 'correct' or 'truth' attribute is specified in the arff (which is an optional field, but always the case for openml-python runs)</p>"},{"location":"reference/#openml.OpenMLRun.get_metric_fn--parameters","title":"Parameters","text":"<p>sklearn_fn : function     a function pointer to a sklearn function that     accepts <code>y_true</code>, <code>y_pred</code> and <code>**kwargs</code> kwargs : dict     kwargs for the function</p>"},{"location":"reference/#openml.OpenMLRun.get_metric_fn--returns","title":"Returns","text":"<p>scores : ndarray of scores of length num_folds * num_repeats     metric results</p> Source code in <code>openml/runs/run.py</code> <pre><code>def get_metric_fn(self, sklearn_fn: Callable, kwargs: dict | None = None) -&gt; np.ndarray:  # noqa: PLR0915, PLR0912, C901\n    \"\"\"Calculates metric scores based on predicted values. Assumes the\n    run has been executed locally (and contains run_data). Furthermore,\n    it assumes that the 'correct' or 'truth' attribute is specified in\n    the arff (which is an optional field, but always the case for\n    openml-python runs)\n\n    Parameters\n    ----------\n    sklearn_fn : function\n        a function pointer to a sklearn function that\n        accepts ``y_true``, ``y_pred`` and ``**kwargs``\n    kwargs : dict\n        kwargs for the function\n\n    Returns\n    -------\n    scores : ndarray of scores of length num_folds * num_repeats\n        metric results\n    \"\"\"\n    kwargs = kwargs if kwargs else {}\n    if self.data_content is not None and self.task_id is not None:\n        predictions_arff = self._generate_arff_dict()\n    elif (self.output_files is not None) and (\"predictions\" in self.output_files):\n        predictions_file_url = openml._api_calls._file_id_to_url(\n            self.output_files[\"predictions\"],\n            \"predictions.arff\",\n        )\n        response = openml._api_calls._download_text_file(predictions_file_url)\n        predictions_arff = arff.loads(response)\n        # TODO: make this a stream reader\n    else:\n        raise ValueError(\n            \"Run should have been locally executed or \" \"contain outputfile reference.\",\n        )\n\n    # Need to know more about the task to compute scores correctly\n    task = get_task(self.task_id)\n\n    attribute_names = [att[0] for att in predictions_arff[\"attributes\"]]\n    if (\n        task.task_type_id in [TaskType.SUPERVISED_CLASSIFICATION, TaskType.LEARNING_CURVE]\n        and \"correct\" not in attribute_names\n    ):\n        raise ValueError('Attribute \"correct\" should be set for ' \"classification task runs\")\n    if task.task_type_id == TaskType.SUPERVISED_REGRESSION and \"truth\" not in attribute_names:\n        raise ValueError('Attribute \"truth\" should be set for ' \"regression task runs\")\n    if task.task_type_id != TaskType.CLUSTERING and \"prediction\" not in attribute_names:\n        raise ValueError('Attribute \"predict\" should be set for ' \"supervised task runs\")\n\n    def _attribute_list_to_dict(attribute_list):  # type: ignore\n        # convenience function: Creates a mapping to map from the name of\n        # attributes present in the arff prediction file to their index.\n        # This is necessary because the number of classes can be different\n        # for different tasks.\n        res = OrderedDict()\n        for idx in range(len(attribute_list)):\n            res[attribute_list[idx][0]] = idx\n        return res\n\n    attribute_dict = _attribute_list_to_dict(predictions_arff[\"attributes\"])\n\n    repeat_idx = attribute_dict[\"repeat\"]\n    fold_idx = attribute_dict[\"fold\"]\n    predicted_idx = attribute_dict[\"prediction\"]  # Assume supervised task\n\n    if task.task_type_id in (TaskType.SUPERVISED_CLASSIFICATION, TaskType.LEARNING_CURVE):\n        correct_idx = attribute_dict[\"correct\"]\n    elif task.task_type_id == TaskType.SUPERVISED_REGRESSION:\n        correct_idx = attribute_dict[\"truth\"]\n    has_samples = False\n    if \"sample\" in attribute_dict:\n        sample_idx = attribute_dict[\"sample\"]\n        has_samples = True\n\n    if (\n        predictions_arff[\"attributes\"][predicted_idx][1]\n        != predictions_arff[\"attributes\"][correct_idx][1]\n    ):\n        pred = predictions_arff[\"attributes\"][predicted_idx][1]\n        corr = predictions_arff[\"attributes\"][correct_idx][1]\n        raise ValueError(\n            \"Predicted and Correct do not have equal values:\" f\" {pred!s} Vs. {corr!s}\",\n        )\n\n    # TODO: these could be cached\n    values_predict: dict[int, dict[int, dict[int, list[float]]]] = {}\n    values_correct: dict[int, dict[int, dict[int, list[float]]]] = {}\n    for _line_idx, line in enumerate(predictions_arff[\"data\"]):\n        rep = line[repeat_idx]\n        fold = line[fold_idx]\n        samp = line[sample_idx] if has_samples else 0\n\n        if task.task_type_id in [\n            TaskType.SUPERVISED_CLASSIFICATION,\n            TaskType.LEARNING_CURVE,\n        ]:\n            prediction = predictions_arff[\"attributes\"][predicted_idx][1].index(\n                line[predicted_idx],\n            )\n            correct = predictions_arff[\"attributes\"][predicted_idx][1].index(line[correct_idx])\n        elif task.task_type_id == TaskType.SUPERVISED_REGRESSION:\n            prediction = line[predicted_idx]\n            correct = line[correct_idx]\n        if rep not in values_predict:\n            values_predict[rep] = OrderedDict()\n            values_correct[rep] = OrderedDict()\n        if fold not in values_predict[rep]:\n            values_predict[rep][fold] = OrderedDict()\n            values_correct[rep][fold] = OrderedDict()\n        if samp not in values_predict[rep][fold]:\n            values_predict[rep][fold][samp] = []\n            values_correct[rep][fold][samp] = []\n\n        values_predict[rep][fold][samp].append(prediction)\n        values_correct[rep][fold][samp].append(correct)\n\n    scores = []\n    for rep in values_predict:\n        for fold in values_predict[rep]:\n            last_sample = len(values_predict[rep][fold]) - 1\n            y_pred = values_predict[rep][fold][last_sample]\n            y_true = values_correct[rep][fold][last_sample]\n            scores.append(sklearn_fn(y_true, y_pred, **kwargs))\n    return np.array(scores)\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/#openml.OpenMLRun.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/#openml.OpenMLRun.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.to_filesystem","title":"to_filesystem","text":"<pre><code>to_filesystem(directory: str | Path, store_model: bool = True) -&gt; None\n</code></pre> <p>The inverse of the from_filesystem method. Serializes a run on the filesystem, to be uploaded later.</p>"},{"location":"reference/#openml.OpenMLRun.to_filesystem--parameters","title":"Parameters","text":"<p>directory : str     a path leading to the folder where the results     will be stored. Should be empty</p> bool, optional (default=True) <p>if True, a model will be pickled as well. As this is the most storage expensive part, it is often desirable to not store the model.</p> Source code in <code>openml/runs/run.py</code> <pre><code>def to_filesystem(\n    self,\n    directory: str | Path,\n    store_model: bool = True,  # noqa: FBT001, FBT002\n) -&gt; None:\n    \"\"\"\n    The inverse of the from_filesystem method. Serializes a run\n    on the filesystem, to be uploaded later.\n\n    Parameters\n    ----------\n    directory : str\n        a path leading to the folder where the results\n        will be stored. Should be empty\n\n    store_model : bool, optional (default=True)\n        if True, a model will be pickled as well. As this is the most\n        storage expensive part, it is often desirable to not store the\n        model.\n    \"\"\"\n    if self.data_content is None or self.model is None:\n        raise ValueError(\"Run should have been executed (and contain \" \"model / predictions)\")\n    directory = Path(directory)\n    directory.mkdir(exist_ok=True, parents=True)\n\n    if any(directory.iterdir()):\n        raise ValueError(f\"Output directory {directory.expanduser().resolve()} should be empty\")\n\n    run_xml = self._to_xml()\n    predictions_arff = arff.dumps(self._generate_arff_dict())\n\n    # It seems like typing does not allow to define the same variable multiple times\n    with (directory / \"description.xml\").open(\"w\") as fh:\n        fh.write(run_xml)\n    with (directory / \"predictions.arff\").open(\"w\") as fh:\n        fh.write(predictions_arff)\n    if store_model:\n        with (directory / \"model.pkl\").open(\"wb\") as fh_b:\n            pickle.dump(self.model, fh_b)\n\n    if self.flow_id is None and self.flow is not None:\n        self.flow.to_filesystem(directory)\n\n    if self.trace is not None:\n        self.trace._to_filesystem(directory)\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLSetup","title":"OpenMLSetup","text":"<pre><code>OpenMLSetup(setup_id: int, flow_id: int, parameters: dict[int, Any] | None)\n</code></pre> <p>Setup object (a.k.a. Configuration).</p>"},{"location":"reference/#openml.OpenMLSetup--parameters","title":"Parameters","text":"<p>setup_id : int     The OpenML setup id flow_id : int     The flow that it is build upon parameters : dict     The setting of the parameters</p> Source code in <code>openml/setups/setup.py</code> <pre><code>def __init__(self, setup_id: int, flow_id: int, parameters: dict[int, Any] | None):\n    if not isinstance(setup_id, int):\n        raise ValueError(\"setup id should be int\")\n\n    if not isinstance(flow_id, int):\n        raise ValueError(\"flow id should be int\")\n\n    if parameters is not None and not isinstance(parameters, dict):\n        raise ValueError(\"parameters should be dict\")\n\n    self.setup_id = setup_id\n    self.flow_id = flow_id\n    self.parameters = parameters\n</code></pre>"},{"location":"reference/#openml.OpenMLSplit","title":"OpenMLSplit","text":"<pre><code>OpenMLSplit(name: int | str, description: str, split: dict[int, dict[int, dict[int, tuple[ndarray, ndarray]]]])\n</code></pre> <p>OpenML Split object.</p> <p>This class manages train-test splits for a dataset across multiple repetitions, folds, and samples.</p>"},{"location":"reference/#openml.OpenMLSplit--parameters","title":"Parameters","text":"<p>name : int or str     The name or ID of the split. description : str     A description of the split. split : dict     A dictionary containing the splits organized by repetition, fold,     and sample.</p> Source code in <code>openml/tasks/split.py</code> <pre><code>def __init__(\n    self,\n    name: int | str,\n    description: str,\n    split: dict[int, dict[int, dict[int, tuple[np.ndarray, np.ndarray]]]],\n):\n    self.description = description\n    self.name = name\n    self.split: dict[int, dict[int, dict[int, tuple[np.ndarray, np.ndarray]]]] = {}\n\n    # Add splits according to repetition\n    for repetition in split:\n        _rep = int(repetition)\n        self.split[_rep] = OrderedDict()\n        for fold in split[_rep]:\n            self.split[_rep][fold] = OrderedDict()\n            for sample in split[_rep][fold]:\n                self.split[_rep][fold][sample] = split[_rep][fold][sample]\n\n    self.repeats = len(self.split)\n\n    # TODO(eddiebergman): Better error message\n    if any(len(self.split[0]) != len(self.split[i]) for i in range(self.repeats)):\n        raise ValueError(\"\")\n\n    self.folds = len(self.split[0])\n    self.samples = len(self.split[0][0])\n</code></pre>"},{"location":"reference/#openml.OpenMLSplit.get","title":"get","text":"<pre><code>get(repeat: int = 0, fold: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns the specified data split from the CrossValidationSplit object.</p>"},{"location":"reference/#openml.OpenMLSplit.get--parameters","title":"Parameters","text":"<p>repeat : int     Index of the repeat to retrieve. fold : int     Index of the fold to retrieve. sample : int     Index of the sample to retrieve.</p>"},{"location":"reference/#openml.OpenMLSplit.get--returns","title":"Returns","text":"<p>numpy.ndarray     The data split for the specified repeat, fold, and sample.</p>"},{"location":"reference/#openml.OpenMLSplit.get--raises","title":"Raises","text":"<p>ValueError     If the specified repeat, fold, or sample is not known.</p> Source code in <code>openml/tasks/split.py</code> <pre><code>def get(self, repeat: int = 0, fold: int = 0, sample: int = 0) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns the specified data split from the CrossValidationSplit object.\n\n    Parameters\n    ----------\n    repeat : int\n        Index of the repeat to retrieve.\n    fold : int\n        Index of the fold to retrieve.\n    sample : int\n        Index of the sample to retrieve.\n\n    Returns\n    -------\n    numpy.ndarray\n        The data split for the specified repeat, fold, and sample.\n\n    Raises\n    ------\n    ValueError\n        If the specified repeat, fold, or sample is not known.\n    \"\"\"\n    if repeat not in self.split:\n        raise ValueError(f\"Repeat {repeat!s} not known\")\n    if fold not in self.split[repeat]:\n        raise ValueError(f\"Fold {fold!s} not known\")\n    if sample not in self.split[repeat][fold]:\n        raise ValueError(f\"Sample {sample!s} not known\")\n    return self.split[repeat][fold][sample]\n</code></pre>"},{"location":"reference/#openml.OpenMLStudy","title":"OpenMLStudy","text":"<pre><code>OpenMLStudy(study_id: int | None, alias: str | None, benchmark_suite: int | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None, flows: list[int] | None, runs: list[int] | None, setups: list[int] | None)\n</code></pre> <p>               Bases: <code>BaseStudy</code></p> <p>An OpenMLStudy represents the OpenML concept of a study (a collection of runs).</p> <p>It contains the following information: name, id, description, creation date, creator id and a list of run ids.</p> <p>According to this list of run ids, the study object receives a list of OpenML object ids (datasets, flows, tasks and setups).</p>"},{"location":"reference/#openml.OpenMLStudy--parameters","title":"Parameters","text":"<p>study_id : int     the study id alias : str (optional)     a string ID, unique on server (url-friendly) benchmark_suite : int (optional)     the benchmark suite (another study) upon which this study is ran.     can only be active if main entity type is runs. name : str     the name of the study (meta-info) description : str     brief description (meta-info) status : str     Whether the study is in preparation, active or deactivated creation_date : str     date of creation (meta-info) creator : int     openml user id of the owner / creator tags : list(dict)     The list of tags shows which tags are associated with the study.     Each tag is a dict of (tag) name, window_start and write_access. data : list     a list of data ids associated with this study tasks : list     a list of task ids associated with this study flows : list     a list of flow ids associated with this study runs : list     a list of run ids associated with this study setups : list     a list of setup ids associated with this study</p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    study_id: int | None,\n    alias: str | None,\n    benchmark_suite: int | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n    flows: list[int] | None,\n    runs: list[int] | None,\n    setups: list[int] | None,\n):\n    super().__init__(\n        study_id=study_id,\n        alias=alias,\n        main_entity_type=\"run\",\n        benchmark_suite=benchmark_suite,\n        name=name,\n        description=description,\n        status=status,\n        creation_date=creation_date,\n        creator=creator,\n        tags=tags,\n        data=data,\n        tasks=tasks,\n        flows=flows,\n        runs=runs,\n        setups=setups,\n    )\n</code></pre>"},{"location":"reference/#openml.OpenMLStudy.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/#openml.OpenMLStudy.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLStudy.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLStudy.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLStudy.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/#openml.OpenMLStudy.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/#openml.OpenMLStudy.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask","title":"OpenMLSupervisedTask","text":"<pre><code>OpenMLSupervisedTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None, task_id: int | None = None)\n</code></pre> <p>               Bases: <code>OpenMLTask</code>, <code>ABC</code></p> <p>OpenML Supervised Classification object.</p>"},{"location":"reference/#openml.OpenMLSupervisedTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     ID of the task type. task_type : str     Name of the task type. data_set_id : int     ID of the OpenML dataset associated with the task. target_name : str     Name of the target feature (the class variable). estimation_procedure_id : int, default=None     ID of the estimation procedure for the task. estimation_procedure_type : str, default=None     Type of the estimation procedure for the task. estimation_parameters : dict, default=None     Estimation parameters for the task. evaluation_measure : str, default=None     Name of the evaluation measure for the task. data_splits_url : str, default=None     URL of the data splits for the task. task_id: Union[int, None]     Refers to the unique identifier of task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        data_splits_url=data_splits_url,\n    )\n\n    self.target_name = target_name\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/#openml.OpenMLSupervisedTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/#openml.OpenMLSupervisedTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLSupervisedTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/#openml.OpenMLSupervisedTask.get_X_and_y--returns","title":"Returns","text":"<p>tuple - X and y</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/#openml.OpenMLSupervisedTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/#openml.OpenMLSupervisedTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLTask","title":"OpenMLTask","text":"<pre><code>OpenMLTask(task_id: int | None, task_type_id: TaskType, task_type: str, data_set_id: int, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Task object.</p>"},{"location":"reference/#openml.OpenMLTask--parameters","title":"Parameters","text":"<p>task_id: Union[int, None]     Refers to the unique identifier of OpenML task. task_type_id: TaskType     Refers to the type of OpenML task. task_type: str     Refers to the OpenML task. data_set_id: int     Refers to the data. estimation_procedure_id: int     Refers to the type of estimates used. estimation_procedure_type: str, default=None     Refers to the type of estimation procedure used for the OpenML task. estimation_parameters: [Dict[str, str]], default=None     Estimation parameters used for the OpenML task. evaluation_measure: str, default=None     Refers to the evaluation measure. data_splits_url: str, default=None     Refers to the URL of the data splits used for the OpenML task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_id: int | None,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n):\n    self.task_id = int(task_id) if task_id is not None else None\n    self.task_type_id = task_type_id\n    self.task_type = task_type\n    self.dataset_id = int(data_set_id)\n    self.evaluation_measure = evaluation_measure\n    self.estimation_procedure: _EstimationProcedure = {\n        \"type\": estimation_procedure_type,\n        \"parameters\": estimation_parameters,\n        \"data_splits_url\": data_splits_url,\n    }\n    self.estimation_procedure_id = estimation_procedure_id\n    self.split: OpenMLSplit | None = None\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/#openml.OpenMLTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/#openml.OpenMLTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/#openml.OpenMLTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.populate_cache","title":"populate_cache","text":"<pre><code>populate_cache(task_ids: list[int] | None = None, dataset_ids: list[int | str] | None = None, flow_ids: list[int] | None = None, run_ids: list[int] | None = None) -&gt; None\n</code></pre> <p>Populate a cache for offline and parallel usage of the OpenML connector.</p>"},{"location":"reference/#openml.populate_cache--parameters","title":"Parameters","text":"<p>task_ids : iterable</p> <p>dataset_ids : iterable</p> <p>flow_ids : iterable</p> <p>run_ids : iterable</p>"},{"location":"reference/#openml.populate_cache--returns","title":"Returns","text":"<p>None</p> Source code in <code>openml/__init__.py</code> <pre><code>def populate_cache(\n    task_ids: list[int] | None = None,\n    dataset_ids: list[int | str] | None = None,\n    flow_ids: list[int] | None = None,\n    run_ids: list[int] | None = None,\n) -&gt; None:\n    \"\"\"\n    Populate a cache for offline and parallel usage of the OpenML connector.\n\n    Parameters\n    ----------\n    task_ids : iterable\n\n    dataset_ids : iterable\n\n    flow_ids : iterable\n\n    run_ids : iterable\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if task_ids is not None:\n        for task_id in task_ids:\n            tasks.functions.get_task(task_id)\n\n    if dataset_ids is not None:\n        for dataset_id in dataset_ids:\n            datasets.functions.get_dataset(dataset_id)\n\n    if flow_ids is not None:\n        for flow_id in flow_ids:\n            flows.functions.get_flow(flow_id)\n\n    if run_ids is not None:\n        for run_id in run_ids:\n            runs.functions.get_run(run_id)\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>openml<ul> <li>__version__</li> <li>_api_calls</li> <li>base</li> <li>cli</li> <li>config</li> <li>datasets<ul> <li>data_feature</li> <li>dataset</li> <li>functions</li> </ul> </li> <li>evaluations<ul> <li>evaluation</li> <li>functions</li> </ul> </li> <li>exceptions</li> <li>extensions<ul> <li>extension_interface</li> <li>functions</li> <li>sklearn<ul> <li>extension</li> </ul> </li> </ul> </li> <li>flows<ul> <li>flow</li> <li>functions</li> </ul> </li> <li>runs<ul> <li>functions</li> <li>run</li> <li>trace</li> </ul> </li> <li>setups<ul> <li>functions</li> <li>setup</li> </ul> </li> <li>study<ul> <li>functions</li> <li>study</li> </ul> </li> <li>tasks<ul> <li>functions</li> <li>split</li> <li>task</li> </ul> </li> <li>testing</li> <li>utils</li> </ul> </li> </ul>"},{"location":"reference/__version__/","title":"__version__","text":""},{"location":"reference/__version__/#openml.__version__","title":"openml.__version__","text":"<p>Version information.</p>"},{"location":"reference/_api_calls/","title":"_api_calls","text":""},{"location":"reference/_api_calls/#openml._api_calls","title":"openml._api_calls","text":""},{"location":"reference/_api_calls/#openml._api_calls.resolve_env_proxies","title":"resolve_env_proxies","text":"<pre><code>resolve_env_proxies(url: str) -&gt; str | None\n</code></pre> <p>Attempt to find a suitable proxy for this url.</p> <p>Relies on <code>requests</code> internals to remain consistent. To disable this from the environment, please set the enviornment varialbe <code>no_proxy=\"*\"</code>.</p>"},{"location":"reference/_api_calls/#openml._api_calls.resolve_env_proxies--parameters","title":"Parameters","text":"<p>url : str     The url endpoint</p>"},{"location":"reference/_api_calls/#openml._api_calls.resolve_env_proxies--returns","title":"Returns","text":"<p>Optional[str]     The proxy url if found, else None</p> Source code in <code>openml/_api_calls.py</code> <pre><code>def resolve_env_proxies(url: str) -&gt; str | None:\n    \"\"\"Attempt to find a suitable proxy for this url.\n\n    Relies on ``requests`` internals to remain consistent. To disable this from the\n    environment, please set the enviornment varialbe ``no_proxy=\"*\"``.\n\n    Parameters\n    ----------\n    url : str\n        The url endpoint\n\n    Returns\n    -------\n    Optional[str]\n        The proxy url if found, else None\n    \"\"\"\n    resolved_proxies = requests.utils.get_environ_proxies(url)\n    return requests.utils.select_proxy(url, resolved_proxies)  # type: ignore\n</code></pre>"},{"location":"reference/base/","title":"base","text":""},{"location":"reference/base/#openml.base","title":"openml.base","text":""},{"location":"reference/base/#openml.base.OpenMLBase","title":"OpenMLBase","text":"<p>               Bases: <code>ABC</code></p> <p>Base object for functionality that is shared across entities.</p>"},{"location":"reference/base/#openml.base.OpenMLBase.id","title":"id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The id of the entity, it is unique for its entity type.</p>"},{"location":"reference/base/#openml.base.OpenMLBase.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/base/#openml.base.OpenMLBase.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/base/#openml.base.OpenMLBase.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/base/#openml.base.OpenMLBase.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/base/#openml.base.OpenMLBase.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/base/#openml.base.OpenMLBase.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/base/#openml.base.OpenMLBase.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/base/#openml.base.OpenMLBase.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/cli/","title":"cli","text":""},{"location":"reference/cli/#openml.cli","title":"openml.cli","text":"<p>Command Line Interface for <code>openml</code> to configure its settings.</p>"},{"location":"reference/cli/#openml.cli.configure","title":"configure","text":"<pre><code>configure(args: Namespace) -&gt; None\n</code></pre> <p>Calls the right submenu(s) to edit <code>args.field</code> in the configuration file.</p> Source code in <code>openml/cli.py</code> <pre><code>def configure(args: argparse.Namespace) -&gt; None:\n    \"\"\"Calls the right submenu(s) to edit `args.field` in the configuration file.\"\"\"\n    set_functions = {\n        \"apikey\": configure_apikey,\n        \"server\": configure_server,\n        \"cachedir\": configure_cachedir,\n        \"retry_policy\": configure_retry_policy,\n        \"connection_n_retries\": configure_connection_n_retries,\n        \"avoid_duplicate_runs\": configure_avoid_duplicate_runs,\n        \"verbosity\": configure_verbosity,\n    }\n\n    def not_supported_yet(_: str) -&gt; None:\n        print(f\"Setting '{args.field}' is not supported yet.\")\n\n    if args.field not in [\"all\", \"none\"]:\n        set_functions.get(args.field, not_supported_yet)(args.value)\n    else:\n        if args.value is not None:\n            print(f\"Can not set value ('{args.value}') when field is specified as '{args.field}'.\")\n            sys.exit()\n        print_configuration()\n\n    if args.field == \"all\":\n        for set_field_function in set_functions.values():\n            set_field_function(args.value)\n</code></pre>"},{"location":"reference/cli/#openml.cli.configure_field","title":"configure_field","text":"<pre><code>configure_field(field: str, value: None | str, check_with_message: Callable[[str], str], intro_message: str, input_message: str, sanitize: Callable[[str], str] | None = None) -&gt; None\n</code></pre> <p>Configure <code>field</code> with <code>value</code>. If <code>value</code> is None ask the user for input.</p> <p><code>value</code> and user input are first corrected/auto-completed with <code>convert_value</code> if provided, then validated with <code>check_with_message</code> function. If the user input a wrong value in interactive mode, the user gets to input a new value. The new valid value is saved in the openml configuration file. In case an invalid <code>value</code> is supplied directly (non-interactive), no changes are made.</p>"},{"location":"reference/cli/#openml.cli.configure_field--parameters","title":"Parameters","text":"<p>field: str     Field to set. value: str, None     Value to field to. If <code>None</code> will ask user for input. check_with_message: Callable[[str], str]     Function which validates <code>value</code> or user input, and returns either an error message if it     is invalid, or a False-like value if <code>value</code> is valid. intro_message: str     Message that is printed once if user input is requested (e.g. instructions). input_message: str     Message that comes with the input prompt. sanitize: Union[Callable[[str], str], None]     A function to convert user input to 'more acceptable' input, e.g. for auto-complete.     If no correction of user input is possible, return the original value.     If no function is provided, don't attempt to correct/auto-complete input.</p> Source code in <code>openml/cli.py</code> <pre><code>def configure_field(  # noqa: PLR0913\n    field: str,\n    value: None | str,\n    check_with_message: Callable[[str], str],\n    intro_message: str,\n    input_message: str,\n    sanitize: Callable[[str], str] | None = None,\n) -&gt; None:\n    \"\"\"Configure `field` with `value`. If `value` is None ask the user for input.\n\n    `value` and user input are first corrected/auto-completed with `convert_value` if provided,\n    then validated with `check_with_message` function.\n    If the user input a wrong value in interactive mode, the user gets to input a new value.\n    The new valid value is saved in the openml configuration file.\n    In case an invalid `value` is supplied directly (non-interactive), no changes are made.\n\n    Parameters\n    ----------\n    field: str\n        Field to set.\n    value: str, None\n        Value to field to. If `None` will ask user for input.\n    check_with_message: Callable[[str], str]\n        Function which validates `value` or user input, and returns either an error message if it\n        is invalid, or a False-like value if `value` is valid.\n    intro_message: str\n        Message that is printed once if user input is requested (e.g. instructions).\n    input_message: str\n        Message that comes with the input prompt.\n    sanitize: Union[Callable[[str], str], None]\n        A function to convert user input to 'more acceptable' input, e.g. for auto-complete.\n        If no correction of user input is possible, return the original value.\n        If no function is provided, don't attempt to correct/auto-complete input.\n    \"\"\"\n    if value is not None:\n        if sanitize:\n            value = sanitize(value)\n        malformed_input = check_with_message(value)\n        if malformed_input:\n            print(malformed_input)\n            sys.exit()\n    else:\n        print(intro_message)\n        value = wait_until_valid_input(\n            prompt=input_message,\n            check=check_with_message,\n            sanitize=sanitize,\n        )\n    verbose_set(field, value)\n</code></pre>"},{"location":"reference/cli/#openml.cli.wait_until_valid_input","title":"wait_until_valid_input","text":"<pre><code>wait_until_valid_input(prompt: str, check: Callable[[str], str], sanitize: Callable[[str], str] | None) -&gt; str\n</code></pre> <p>Asks <code>prompt</code> until an input is received which returns True for <code>check</code>.</p>"},{"location":"reference/cli/#openml.cli.wait_until_valid_input--parameters","title":"Parameters","text":"<p>prompt: str     message to display check: Callable[[str], str]     function to call with the given input, that provides an error message if the input is not     valid otherwise, and False-like otherwise. sanitize: Callable[[str], str], optional     A function which attempts to sanitize the user input (e.g. auto-complete).</p>"},{"location":"reference/cli/#openml.cli.wait_until_valid_input--returns","title":"Returns","text":"<p>valid input</p> Source code in <code>openml/cli.py</code> <pre><code>def wait_until_valid_input(\n    prompt: str,\n    check: Callable[[str], str],\n    sanitize: Callable[[str], str] | None,\n) -&gt; str:\n    \"\"\"Asks `prompt` until an input is received which returns True for `check`.\n\n    Parameters\n    ----------\n    prompt: str\n        message to display\n    check: Callable[[str], str]\n        function to call with the given input, that provides an error message if the input is not\n        valid otherwise, and False-like otherwise.\n    sanitize: Callable[[str], str], optional\n        A function which attempts to sanitize the user input (e.g. auto-complete).\n\n    Returns\n    -------\n    valid input\n\n    \"\"\"\n    while True:\n        response = input(prompt)\n        if sanitize:\n            response = sanitize(response)\n        error_message = check(response)\n        if error_message:\n            print(error_message, end=\"\\n\\n\")\n        else:\n            return response\n</code></pre>"},{"location":"reference/config/","title":"config","text":""},{"location":"reference/config/#openml.config","title":"openml.config","text":"<p>Store module level information like the API key, cache directory and the server</p>"},{"location":"reference/config/#openml.config.ConfigurationForExamples","title":"ConfigurationForExamples","text":"<p>Allows easy switching to and from a test configuration, used for examples.</p>"},{"location":"reference/config/#openml.config.ConfigurationForExamples.start_using_configuration_for_example","title":"start_using_configuration_for_example  <code>classmethod</code>","text":"<pre><code>start_using_configuration_for_example() -&gt; None\n</code></pre> <p>Sets the configuration to connect to the test server with valid apikey.</p> <p>To configuration as was before this call is stored, and can be recovered by using the <code>stop_use_example_configuration</code> method.</p> Source code in <code>openml/config.py</code> <pre><code>@classmethod\ndef start_using_configuration_for_example(cls) -&gt; None:\n    \"\"\"Sets the configuration to connect to the test server with valid apikey.\n\n    To configuration as was before this call is stored, and can be recovered\n    by using the `stop_use_example_configuration` method.\n    \"\"\"\n    global server  # noqa: PLW0603\n    global apikey  # noqa: PLW0603\n\n    if cls._start_last_called and server == cls._test_server and apikey == cls._test_apikey:\n        # Method is called more than once in a row without modifying the server or apikey.\n        # We don't want to save the current test configuration as a last used configuration.\n        return\n\n    cls._last_used_server = server\n    cls._last_used_key = apikey\n    cls._start_last_called = True\n\n    # Test server key for examples\n    server = cls._test_server\n    apikey = cls._test_apikey\n    warnings.warn(\n        f\"Switching to the test server {server} to not upload results to the live server. \"\n        \"Using the test server may result in reduced performance of the API!\",\n        stacklevel=2,\n    )\n</code></pre>"},{"location":"reference/config/#openml.config.ConfigurationForExamples.stop_using_configuration_for_example","title":"stop_using_configuration_for_example  <code>classmethod</code>","text":"<pre><code>stop_using_configuration_for_example() -&gt; None\n</code></pre> <p>Return to configuration as it was before <code>start_use_example_configuration</code>.</p> Source code in <code>openml/config.py</code> <pre><code>@classmethod\ndef stop_using_configuration_for_example(cls) -&gt; None:\n    \"\"\"Return to configuration as it was before `start_use_example_configuration`.\"\"\"\n    if not cls._start_last_called:\n        # We don't want to allow this because it will (likely) result in the `server` and\n        # `apikey` variables being set to None.\n        raise RuntimeError(\n            \"`stop_use_example_configuration` called without a saved config.\"\n            \"`start_use_example_configuration` must be called first.\",\n        )\n\n    global server  # noqa: PLW0603\n    global apikey  # noqa: PLW0603\n\n    server = cast(str, cls._last_used_server)\n    apikey = cast(str, cls._last_used_key)\n    cls._start_last_called = False\n</code></pre>"},{"location":"reference/config/#openml.config.get_cache_directory","title":"get_cache_directory","text":"<pre><code>get_cache_directory() -&gt; str\n</code></pre> <p>Get the current cache directory.</p> <p>This gets the cache directory for the current server relative to the root cache directory that can be set via <code>set_root_cache_directory()</code>. The cache directory is the <code>root_cache_directory</code> with additional information on which subdirectory to use based on the server name. By default it is <code>root_cache_directory / org / openml / www</code> for the standard OpenML.org server and is defined as <code>root_cache_directory / top-level domain / second-level domain / hostname</code> ```</p>"},{"location":"reference/config/#openml.config.get_cache_directory--returns","title":"Returns","text":"<p>cachedir : string     The current cache directory.</p> Source code in <code>openml/config.py</code> <pre><code>def get_cache_directory() -&gt; str:\n    \"\"\"Get the current cache directory.\n\n    This gets the cache directory for the current server relative\n    to the root cache directory that can be set via\n    ``set_root_cache_directory()``. The cache directory is the\n    ``root_cache_directory`` with additional information on which\n    subdirectory to use based on the server name. By default it is\n    ``root_cache_directory / org / openml / www`` for the standard\n    OpenML.org server and is defined as\n    ``root_cache_directory / top-level domain / second-level domain /\n    hostname``\n    ```\n\n    Returns\n    -------\n    cachedir : string\n        The current cache directory.\n\n    \"\"\"\n    url_suffix = urlparse(server).netloc\n    reversed_url_suffix = os.sep.join(url_suffix.split(\".\")[::-1])  # noqa: PTH118\n    return os.path.join(_root_cache_directory, reversed_url_suffix)  # noqa: PTH118\n</code></pre>"},{"location":"reference/config/#openml.config.get_server_base_url","title":"get_server_base_url","text":"<pre><code>get_server_base_url() -&gt; str\n</code></pre> <p>Return the base URL of the currently configured server.</p> <p>Turns <code>\"https://api.openml.org/api/v1/xml\"</code> in <code>\"https://www.openml.org/\"</code> and <code>\"https://test.openml.org/api/v1/xml\"</code> in <code>\"https://test.openml.org/\"</code></p>"},{"location":"reference/config/#openml.config.get_server_base_url--returns","title":"Returns","text":"<p>str</p> Source code in <code>openml/config.py</code> <pre><code>def get_server_base_url() -&gt; str:\n    \"\"\"Return the base URL of the currently configured server.\n\n    Turns ``\"https://api.openml.org/api/v1/xml\"`` in ``\"https://www.openml.org/\"``\n    and ``\"https://test.openml.org/api/v1/xml\"`` in ``\"https://test.openml.org/\"``\n\n    Returns\n    -------\n    str\n    \"\"\"\n    domain, path = server.split(\"/api\", maxsplit=1)\n    return domain.replace(\"api\", \"www\")\n</code></pre>"},{"location":"reference/config/#openml.config.overwrite_config_context","title":"overwrite_config_context","text":"<pre><code>overwrite_config_context(config: dict[str, Any]) -&gt; Iterator[_Config]\n</code></pre> <p>A context manager to temporarily override variables in the configuration.</p> Source code in <code>openml/config.py</code> <pre><code>@contextmanager\ndef overwrite_config_context(config: dict[str, Any]) -&gt; Iterator[_Config]:\n    \"\"\"A context manager to temporarily override variables in the configuration.\"\"\"\n    existing_config = get_config_as_dict()\n    merged_config = {**existing_config, **config}\n\n    _setup(merged_config)  # type: ignore\n    yield merged_config  # type: ignore\n\n    _setup(existing_config)\n</code></pre>"},{"location":"reference/config/#openml.config.set_console_log_level","title":"set_console_log_level","text":"<pre><code>set_console_log_level(console_output_level: int) -&gt; None\n</code></pre> <p>Set console output to the desired level and register it with openml logger if needed.</p> Source code in <code>openml/config.py</code> <pre><code>def set_console_log_level(console_output_level: int) -&gt; None:\n    \"\"\"Set console output to the desired level and register it with openml logger if needed.\"\"\"\n    global console_handler  # noqa: PLW0602\n    assert console_handler is not None\n    _set_level_register_and_store(console_handler, console_output_level)\n</code></pre>"},{"location":"reference/config/#openml.config.set_field_in_config_file","title":"set_field_in_config_file","text":"<pre><code>set_field_in_config_file(field: str, value: Any) -&gt; None\n</code></pre> <p>Overwrites the <code>field</code> in the configuration file with the new <code>value</code>.</p> Source code in <code>openml/config.py</code> <pre><code>def set_field_in_config_file(field: str, value: Any) -&gt; None:\n    \"\"\"Overwrites the `field` in the configuration file with the new `value`.\"\"\"\n    if field not in _defaults:\n        raise ValueError(f\"Field '{field}' is not valid and must be one of '{_defaults.keys()}'.\")\n\n    # TODO(eddiebergman): This use of globals has gone too far\n    globals()[field] = value\n    config_file = determine_config_file_path()\n    config = _parse_config(config_file)\n    with config_file.open(\"w\") as fh:\n        for f in _defaults:\n            # We can't blindly set all values based on globals() because when the user\n            # sets it through config.FIELD it should not be stored to file.\n            # There doesn't seem to be a way to avoid writing defaults to file with configparser,\n            # because it is impossible to distinguish from an explicitly set value that matches\n            # the default value, to one that was set to its default because it was omitted.\n            value = globals()[f] if f == field else config.get(f)  # type: ignore\n            if value is not None:\n                fh.write(f\"{f} = {value}\\n\")\n</code></pre>"},{"location":"reference/config/#openml.config.set_file_log_level","title":"set_file_log_level","text":"<pre><code>set_file_log_level(file_output_level: int) -&gt; None\n</code></pre> <p>Set file output to the desired level and register it with openml logger if needed.</p> Source code in <code>openml/config.py</code> <pre><code>def set_file_log_level(file_output_level: int) -&gt; None:\n    \"\"\"Set file output to the desired level and register it with openml logger if needed.\"\"\"\n    global file_handler  # noqa: PLW0602\n    assert file_handler is not None\n    _set_level_register_and_store(file_handler, file_output_level)\n</code></pre>"},{"location":"reference/config/#openml.config.set_root_cache_directory","title":"set_root_cache_directory","text":"<pre><code>set_root_cache_directory(root_cache_directory: str | Path) -&gt; None\n</code></pre> <p>Set module-wide base cache directory.</p> <p>Sets the root cache directory, wherin the cache directories are created to store content from different OpenML servers. For example, by default, cached data for the standard OpenML.org server is stored at <code>root_cache_directory / org / openml / www</code>, and the general pattern is <code>root_cache_directory / top-level domain / second-level domain / hostname</code>.</p>"},{"location":"reference/config/#openml.config.set_root_cache_directory--parameters","title":"Parameters","text":"<p>root_cache_directory : string      Path to use as cache directory.</p>"},{"location":"reference/config/#openml.config.set_root_cache_directory--see-also","title":"See Also","text":"<p>get_cache_directory</p> Source code in <code>openml/config.py</code> <pre><code>def set_root_cache_directory(root_cache_directory: str | Path) -&gt; None:\n    \"\"\"Set module-wide base cache directory.\n\n    Sets the root cache directory, wherin the cache directories are\n    created to store content from different OpenML servers. For example,\n    by default, cached data for the standard OpenML.org server is stored\n    at ``root_cache_directory / org / openml / www``, and the general\n    pattern is ``root_cache_directory / top-level domain / second-level\n    domain / hostname``.\n\n    Parameters\n    ----------\n    root_cache_directory : string\n         Path to use as cache directory.\n\n    See Also\n    --------\n    get_cache_directory\n    \"\"\"\n    global _root_cache_directory  # noqa: PLW0603\n    _root_cache_directory = Path(root_cache_directory)\n</code></pre>"},{"location":"reference/exceptions/","title":"exceptions","text":""},{"location":"reference/exceptions/#openml.exceptions","title":"openml.exceptions","text":""},{"location":"reference/exceptions/#openml.exceptions.ObjectNotPublishedError","title":"ObjectNotPublishedError","text":"<pre><code>ObjectNotPublishedError(message: str)\n</code></pre> <p>               Bases: <code>PyOpenMLError</code></p> <p>Indicates an object has not been published yet.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLCacheException","title":"OpenMLCacheException","text":"<pre><code>OpenMLCacheException(message: str)\n</code></pre> <p>               Bases: <code>PyOpenMLError</code></p> <p>Dataset / task etc not found in cache</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLHashException","title":"OpenMLHashException","text":"<pre><code>OpenMLHashException(message: str)\n</code></pre> <p>               Bases: <code>PyOpenMLError</code></p> <p>Locally computed hash is different than hash announced by the server.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLNotAuthorizedError","title":"OpenMLNotAuthorizedError","text":"<pre><code>OpenMLNotAuthorizedError(message: str)\n</code></pre> <p>               Bases: <code>OpenMLServerError</code></p> <p>Indicates an authenticated user is not authorized to execute the requested action.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLPrivateDatasetError","title":"OpenMLPrivateDatasetError","text":"<pre><code>OpenMLPrivateDatasetError(message: str)\n</code></pre> <p>               Bases: <code>PyOpenMLError</code></p> <p>Exception thrown when the user has no rights to access the dataset.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLRunsExistError","title":"OpenMLRunsExistError","text":"<pre><code>OpenMLRunsExistError(run_ids: set[int], message: str)\n</code></pre> <p>               Bases: <code>PyOpenMLError</code></p> <p>Indicates run(s) already exists on the server when they should not be duplicated.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, run_ids: set[int], message: str) -&gt; None:\n    if len(run_ids) &lt; 1:\n        raise ValueError(\"Set of run ids must be non-empty.\")\n    self.run_ids = run_ids\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLServerError","title":"OpenMLServerError","text":"<pre><code>OpenMLServerError(message: str)\n</code></pre> <p>               Bases: <code>PyOpenMLError</code></p> <p>class for when something is really wrong on the server (result did not parse to dict), contains unparsed error.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLServerException","title":"OpenMLServerException","text":"<pre><code>OpenMLServerException(message: str, code: int | None = None, url: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLServerError</code></p> <p>exception for when the result of the server was not 200 (e.g., listing call w/o results).</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str, code: int | None = None, url: str | None = None):\n    self.message = message\n    self.code = code\n    self.url = url\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLServerNoResult","title":"OpenMLServerNoResult","text":"<pre><code>OpenMLServerNoResult(message: str, code: int | None = None, url: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLServerException</code></p> <p>Exception for when the result of the server is empty.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str, code: int | None = None, url: str | None = None):\n    self.message = message\n    self.code = code\n    self.url = url\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.PyOpenMLError","title":"PyOpenMLError","text":"<pre><code>PyOpenMLError(message: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Base class for all exceptions in OpenML-Python.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/testing/","title":"testing","text":""},{"location":"reference/testing/#openml.testing","title":"openml.testing","text":""},{"location":"reference/testing/#openml.testing.CustomImputer","title":"CustomImputer","text":"<p>               Bases: <code>Imputer</code></p> <p>Duplicate class alias for sklearn's SimpleImputer</p> <p>Helps bypass the sklearn extension duplicate operation check</p>"},{"location":"reference/testing/#openml.testing.TestBase","title":"TestBase","text":"<p>               Bases: <code>TestCase</code></p> <p>Base class for tests</p>"},{"location":"reference/testing/#openml.testing.TestBase--note","title":"Note","text":"<p>Currently hard-codes a read-write key. Hopefully soon allows using a test server, not the production server.</p>"},{"location":"reference/testing/#openml.testing.TestBase.setUp","title":"setUp","text":"<pre><code>setUp(n_levels: int = 1, tmpdir_suffix: str = '') -&gt; None\n</code></pre> <p>Setup variables and temporary directories.</p> <p>In particular, this methods:</p> <ul> <li>creates a temporary working directory</li> <li>figures out a path to a few static test files</li> <li>set the default server to be the test server</li> <li>set a static API key for the test server</li> <li>increases the maximal number of retries</li> </ul>"},{"location":"reference/testing/#openml.testing.TestBase.setUp--parameters","title":"Parameters","text":"<p>n_levels : int     Number of nested directories the test is in. Necessary to resolve the path to the     <code>files</code> directory, which is located directly under the <code>tests</code> directory.</p> Source code in <code>openml/testing.py</code> <pre><code>def setUp(self, n_levels: int = 1, tmpdir_suffix: str = \"\") -&gt; None:\n    \"\"\"Setup variables and temporary directories.\n\n    In particular, this methods:\n\n    * creates a temporary working directory\n    * figures out a path to a few static test files\n    * set the default server to be the test server\n    * set a static API key for the test server\n    * increases the maximal number of retries\n\n    Parameters\n    ----------\n    n_levels : int\n        Number of nested directories the test is in. Necessary to resolve the path to the\n        ``files`` directory, which is located directly under the ``tests`` directory.\n    \"\"\"\n    # This cache directory is checked in to git to simulate a populated\n    # cache\n    self.maxDiff = None\n    abspath_this_file = Path(inspect.getfile(self.__class__)).absolute()\n    static_cache_dir = abspath_this_file.parent\n    for _ in range(n_levels):\n        static_cache_dir = static_cache_dir.parent.absolute()\n\n    content = os.listdir(static_cache_dir)\n    if \"files\" in content:\n        static_cache_dir = static_cache_dir / \"files\"\n    else:\n        raise ValueError(\n            f\"Cannot find test cache dir, expected it to be {static_cache_dir}!\",\n        )\n\n    self.static_cache_dir = static_cache_dir\n    self.cwd = Path.cwd()\n    workdir = Path(__file__).parent.absolute()\n    tmp_dir_name = self.id() + tmpdir_suffix\n    self.workdir = workdir / tmp_dir_name\n    shutil.rmtree(self.workdir, ignore_errors=True)\n\n    self.workdir.mkdir(exist_ok=True)\n    os.chdir(self.workdir)\n\n    self.cached = True\n    openml.config.apikey = TestBase.apikey\n    self.production_server = \"https://www.openml.org/api/v1/xml\"\n    openml.config.avoid_duplicate_runs = False\n    openml.config.set_root_cache_directory(str(self.workdir))\n\n    # Increase the number of retries to avoid spurious server failures\n    self.retry_policy = openml.config.retry_policy\n    self.connection_n_retries = openml.config.connection_n_retries\n    openml.config.set_retry_policy(\"robot\", n_retries=20)\n</code></pre>"},{"location":"reference/testing/#openml.testing.TestBase.tearDown","title":"tearDown","text":"<pre><code>tearDown() -&gt; None\n</code></pre> <p>Tear down the test</p> Source code in <code>openml/testing.py</code> <pre><code>def tearDown(self) -&gt; None:\n    \"\"\"Tear down the test\"\"\"\n    os.chdir(self.cwd)\n    try:\n        shutil.rmtree(self.workdir)\n    except PermissionError as e:\n        if os.name != \"nt\":\n            # one of the files may still be used by another process\n            raise e\n\n    openml.config.connection_n_retries = self.connection_n_retries\n    openml.config.retry_policy = self.retry_policy\n</code></pre>"},{"location":"reference/testing/#openml.testing.check_task_existence","title":"check_task_existence","text":"<pre><code>check_task_existence(task_type: TaskType, dataset_id: int, target_name: str, **kwargs: dict[str, str | int | dict[str, str | int | TaskType]]) -&gt; int | None\n</code></pre> <p>Checks if any task with exists on test server that matches the meta data.</p>"},{"location":"reference/testing/#openml.testing.check_task_existence--parameter","title":"Parameter","text":"<p>task_type : openml.tasks.TaskType dataset_id : int target_name : str</p>"},{"location":"reference/testing/#openml.testing.check_task_existence--return","title":"Return","text":"<p>int, None</p> Source code in <code>openml/testing.py</code> <pre><code>def check_task_existence(\n    task_type: TaskType,\n    dataset_id: int,\n    target_name: str,\n    **kwargs: dict[str, str | int | dict[str, str | int | openml.tasks.TaskType]],\n) -&gt; int | None:\n    \"\"\"Checks if any task with exists on test server that matches the meta data.\n\n    Parameter\n    ---------\n    task_type : openml.tasks.TaskType\n    dataset_id : int\n    target_name : str\n\n    Return\n    ------\n    int, None\n    \"\"\"\n    return_val = None\n    tasks = openml.tasks.list_tasks(task_type=task_type)\n    if len(tasks) == 0:\n        return None\n    tasks = tasks.loc[tasks[\"did\"] == dataset_id]\n    if len(tasks) == 0:\n        return None\n    tasks = tasks.loc[tasks[\"target_feature\"] == target_name]\n    if len(tasks) == 0:\n        return None\n    task_match = []\n    for task_id in tasks[\"tid\"].to_list():\n        task_match.append(task_id)\n        try:\n            task = openml.tasks.get_task(task_id)\n        except OpenMLServerException:\n            # can fail if task_id deleted by another parallely run unit test\n            task_match.pop(-1)\n            return_val = None\n            continue\n        for k, v in kwargs.items():\n            if getattr(task, k) != v:\n                # even if one of the meta-data key mismatches, then task_id is not a match\n                task_match.pop(-1)\n                break\n        # if task_id is retained in the task_match list, it passed all meta key-value matches\n        if len(task_match) == 1:\n            return_val = task_id\n            break\n    if len(task_match) == 0:\n        return_val = None\n    return return_val\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#openml.utils","title":"openml.utils","text":""},{"location":"reference/utils/#openml.utils.ProgressBar","title":"ProgressBar","text":"<pre><code>ProgressBar()\n</code></pre> <p>               Bases: <code>ProgressType</code></p> <p>Progressbar for MinIO function's <code>progress</code> parameter.</p> Source code in <code>openml/utils.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._object_name = \"\"\n    self._progress_bar: tqdm | None = None\n</code></pre>"},{"location":"reference/utils/#openml.utils.ProgressBar.set_meta","title":"set_meta","text":"<pre><code>set_meta(object_name: str, total_length: int) -&gt; None\n</code></pre> <p>Initializes the progress bar.</p>"},{"location":"reference/utils/#openml.utils.ProgressBar.set_meta--parameters","title":"Parameters","text":"<p>object_name: str   Not used.</p> int <p>File size of the object in bytes.</p> Source code in <code>openml/utils.py</code> <pre><code>def set_meta(self, object_name: str, total_length: int) -&gt; None:\n    \"\"\"Initializes the progress bar.\n\n    Parameters\n    ----------\n    object_name: str\n      Not used.\n\n    total_length: int\n      File size of the object in bytes.\n    \"\"\"\n    self._object_name = object_name\n    self._progress_bar = tqdm(total=total_length, unit_scale=True, unit=\"B\")\n</code></pre>"},{"location":"reference/utils/#openml.utils.ProgressBar.update","title":"update","text":"<pre><code>update(length: int) -&gt; None\n</code></pre> <p>Updates the progress bar.</p>"},{"location":"reference/utils/#openml.utils.ProgressBar.update--parameters","title":"Parameters","text":"<p>length: int   Number of bytes downloaded since last <code>update</code> call.</p> Source code in <code>openml/utils.py</code> <pre><code>def update(self, length: int) -&gt; None:\n    \"\"\"Updates the progress bar.\n\n    Parameters\n    ----------\n    length: int\n      Number of bytes downloaded since last `update` call.\n    \"\"\"\n    if not self._progress_bar:\n        raise RuntimeError(\"Call `set_meta` before calling `update`.\")\n    self._progress_bar.update(length)\n    if self._progress_bar.total &lt;= self._progress_bar.n:\n        self._progress_bar.close()\n</code></pre>"},{"location":"reference/utils/#openml.utils.extract_xml_tags","title":"extract_xml_tags","text":"<pre><code>extract_xml_tags(xml_tag_name: str, node: Mapping[str, Any], *, allow_none: bool = True) -&gt; Any | None\n</code></pre> <p>Helper to extract xml tags from xmltodict.</p>"},{"location":"reference/utils/#openml.utils.extract_xml_tags--parameters","title":"Parameters","text":"<p>xml_tag_name : str     Name of the xml tag to extract from the node.</p> Mapping[str, Any] <p>Node object returned by <code>xmltodict</code> from which <code>xml_tag_name</code> should be extracted.</p> bool <p>If <code>False</code>, the tag needs to exist in the node. Will raise a <code>ValueError</code> if it does not.</p>"},{"location":"reference/utils/#openml.utils.extract_xml_tags--returns","title":"Returns","text":"<p>object</p> Source code in <code>openml/utils.py</code> <pre><code>def extract_xml_tags(\n    xml_tag_name: str,\n    node: Mapping[str, Any],\n    *,\n    allow_none: bool = True,\n) -&gt; Any | None:\n    \"\"\"Helper to extract xml tags from xmltodict.\n\n    Parameters\n    ----------\n    xml_tag_name : str\n        Name of the xml tag to extract from the node.\n\n    node : Mapping[str, Any]\n        Node object returned by ``xmltodict`` from which ``xml_tag_name``\n        should be extracted.\n\n    allow_none : bool\n        If ``False``, the tag needs to exist in the node. Will raise a\n        ``ValueError`` if it does not.\n\n    Returns\n    -------\n    object\n    \"\"\"\n    if xml_tag_name in node and node[xml_tag_name] is not None:\n        if isinstance(node[xml_tag_name], (dict, str)):\n            return [node[xml_tag_name]]\n        if isinstance(node[xml_tag_name], list):\n            return node[xml_tag_name]\n\n        raise ValueError(\"Received not string and non list as tag item\")\n\n    if allow_none:\n        return None\n\n    raise ValueError(f\"Could not find tag '{xml_tag_name}' in node '{node!s}'\")\n</code></pre>"},{"location":"reference/datasets/","title":"datasets","text":""},{"location":"reference/datasets/#openml.datasets","title":"openml.datasets","text":""},{"location":"reference/datasets/#openml.datasets.OpenMLDataFeature","title":"OpenMLDataFeature","text":"<pre><code>OpenMLDataFeature(index: int, name: str, data_type: str, nominal_values: list[str], number_missing_values: int, ontologies: list[str] | None = None)\n</code></pre> <p>Data Feature (a.k.a. Attribute) object.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataFeature--parameters","title":"Parameters","text":"<p>index : int     The index of this feature name : str     Name of the feature data_type : str     can be nominal, numeric, string, date (corresponds to arff) nominal_values : list(str)     list of the possible values, in case of nominal attribute number_missing_values : int     Number of rows that have a missing value for this feature. ontologies : list(str)     list of ontologies attached to this feature. An ontology describes the     concept that are described in a feature. An ontology is defined by an     URL where the information is provided.</p> Source code in <code>openml/datasets/data_feature.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    index: int,\n    name: str,\n    data_type: str,\n    nominal_values: list[str],\n    number_missing_values: int,\n    ontologies: list[str] | None = None,\n):\n    if not isinstance(index, int):\n        raise TypeError(f\"Index must be `int` but is {type(index)}\")\n\n    if data_type not in self.LEGAL_DATA_TYPES:\n        raise ValueError(\n            f\"data type should be in {self.LEGAL_DATA_TYPES!s}, found: {data_type}\",\n        )\n\n    if data_type == \"nominal\":\n        if nominal_values is None:\n            raise TypeError(\n                \"Dataset features require attribute `nominal_values` for nominal \"\n                \"feature type.\",\n            )\n\n        if not isinstance(nominal_values, list):\n            raise TypeError(\n                \"Argument `nominal_values` is of wrong datatype, should be list, \"\n                f\"but is {type(nominal_values)}\",\n            )\n    elif nominal_values is not None:\n        raise TypeError(\"Argument `nominal_values` must be None for non-nominal feature.\")\n\n    if not isinstance(number_missing_values, int):\n        msg = f\"number_missing_values must be int but is {type(number_missing_values)}\"\n        raise TypeError(msg)\n\n    self.index = index\n    self.name = str(name)\n    self.data_type = str(data_type)\n    self.nominal_values = nominal_values\n    self.number_missing_values = number_missing_values\n    self.ontologies = ontologies\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset","title":"OpenMLDataset","text":"<pre><code>OpenMLDataset(name: str, description: str | None, data_format: Literal['arff', 'sparse_arff'] = 'arff', cache_format: Literal['feather', 'pickle'] = 'pickle', dataset_id: int | None = None, version: int | None = None, creator: str | None = None, contributor: str | None = None, collection_date: str | None = None, upload_date: str | None = None, language: str | None = None, licence: str | None = None, url: str | None = None, default_target_attribute: str | None = None, row_id_attribute: str | None = None, ignore_attribute: str | list[str] | None = None, version_label: str | None = None, citation: str | None = None, tag: str | None = None, visibility: str | None = None, original_data_url: str | None = None, paper_url: str | None = None, update_comment: str | None = None, md5_checksum: str | None = None, data_file: str | None = None, features_file: str | None = None, qualities_file: str | None = None, dataset: str | None = None, parquet_url: str | None = None, parquet_file: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>Dataset object.</p> <p>Allows fetching and uploading datasets to OpenML.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset--parameters","title":"Parameters","text":"<p>name : str     Name of the dataset. description : str     Description of the dataset. data_format : str     Format of the dataset which can be either 'arff' or 'sparse_arff'. cache_format : str     Format for caching the dataset which can be either 'feather' or 'pickle'. dataset_id : int, optional     Id autogenerated by the server. version : int, optional     Version of this dataset. '1' for original version.     Auto-incremented by server. creator : str, optional     The person who created the dataset. contributor : str, optional     People who contributed to the current version of the dataset. collection_date : str, optional     The date the data was originally collected, given by the uploader. upload_date : str, optional     The date-time when the dataset was uploaded, generated by server. language : str, optional     Language in which the data is represented.     Starts with 1 upper case letter, rest lower case, e.g. 'English'. licence : str, optional     License of the data. url : str, optional     Valid URL, points to actual data file.     The file can be on the OpenML server or another dataset repository. default_target_attribute : str, optional     The default target attribute, if it exists.     Can have multiple values, comma separated. row_id_attribute : str, optional     The attribute that represents the row-id column,     if present in the dataset. ignore_attribute : str | list, optional     Attributes that should be excluded in modelling,     such as identifiers and indexes. version_label : str, optional     Version label provided by user.     Can be a date, hash, or some other type of id. citation : str, optional     Reference(s) that should be cited when building on this data. tag : str, optional     Tags, describing the algorithms. visibility : str, optional     Who can see the dataset.     Typical values: 'Everyone','All my friends','Only me'.     Can also be any of the user's circles. original_data_url : str, optional     For derived data, the url to the original dataset. paper_url : str, optional     Link to a paper describing the dataset. update_comment : str, optional     An explanation for when the dataset is uploaded. md5_checksum : str, optional     MD5 checksum to check if the dataset is downloaded without corruption. data_file : str, optional     Path to where the dataset is located. features_file : dict, optional     A dictionary of dataset features,     which maps a feature index to a OpenMLDataFeature. qualities_file : dict, optional     A dictionary of dataset qualities,     which maps a quality name to a quality value. dataset: string, optional     Serialized arff dataset string. parquet_url: string, optional     This is the URL to the storage location where the dataset files are hosted.     This can be a MinIO bucket URL. If specified, the data will be accessed     from this URL when reading the files. parquet_file: string, optional     Path to the local file.</p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def __init__(  # noqa: C901, PLR0912, PLR0913, PLR0915\n    self,\n    name: str,\n    description: str | None,\n    data_format: Literal[\"arff\", \"sparse_arff\"] = \"arff\",\n    cache_format: Literal[\"feather\", \"pickle\"] = \"pickle\",\n    dataset_id: int | None = None,\n    version: int | None = None,\n    creator: str | None = None,\n    contributor: str | None = None,\n    collection_date: str | None = None,\n    upload_date: str | None = None,\n    language: str | None = None,\n    licence: str | None = None,\n    url: str | None = None,\n    default_target_attribute: str | None = None,\n    row_id_attribute: str | None = None,\n    ignore_attribute: str | list[str] | None = None,\n    version_label: str | None = None,\n    citation: str | None = None,\n    tag: str | None = None,\n    visibility: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n    update_comment: str | None = None,\n    md5_checksum: str | None = None,\n    data_file: str | None = None,\n    features_file: str | None = None,\n    qualities_file: str | None = None,\n    dataset: str | None = None,\n    parquet_url: str | None = None,\n    parquet_file: str | None = None,\n):\n    if cache_format not in [\"feather\", \"pickle\"]:\n        raise ValueError(\n            \"cache_format must be one of 'feather' or 'pickle. \"\n            f\"Invalid format specified: {cache_format}\",\n        )\n\n    def find_invalid_characters(string: str, pattern: str) -&gt; str:\n        invalid_chars = set()\n        regex = re.compile(pattern)\n        for char in string:\n            if not regex.match(char):\n                invalid_chars.add(char)\n        return \",\".join(\n            [f\"'{char}'\" if char != \"'\" else f'\"{char}\"' for char in invalid_chars],\n        )\n\n    if dataset_id is None:\n        pattern = \"^[\\x00-\\x7f]*$\"\n        if description and not re.match(pattern, description):\n            # not basiclatin (XSD complains)\n            invalid_characters = find_invalid_characters(description, pattern)\n            raise ValueError(\n                f\"Invalid symbols {invalid_characters} in description: {description}\",\n            )\n        pattern = \"^[\\x00-\\x7f]*$\"\n        if citation and not re.match(pattern, citation):\n            # not basiclatin (XSD complains)\n            invalid_characters = find_invalid_characters(citation, pattern)\n            raise ValueError(\n                f\"Invalid symbols {invalid_characters} in citation: {citation}\",\n            )\n        pattern = \"^[a-zA-Z0-9_\\\\-\\\\.\\\\(\\\\),]+$\"\n        if not re.match(pattern, name):\n            # regex given by server in error message\n            invalid_characters = find_invalid_characters(name, pattern)\n            raise ValueError(f\"Invalid symbols {invalid_characters} in name: {name}\")\n\n    self.ignore_attribute: list[str] | None = None\n    if isinstance(ignore_attribute, str):\n        self.ignore_attribute = [ignore_attribute]\n    elif isinstance(ignore_attribute, list) or ignore_attribute is None:\n        self.ignore_attribute = ignore_attribute\n    else:\n        raise ValueError(\"Wrong data type for ignore_attribute. Should be list.\")\n\n    # TODO add function to check if the name is casual_string128\n    # Attributes received by querying the RESTful API\n    self.dataset_id = int(dataset_id) if dataset_id is not None else None\n    self.name = name\n    self.version = int(version) if version is not None else None\n    self.description = description\n    self.cache_format = cache_format\n    # Has to be called format, otherwise there will be an XML upload error\n    self.format = data_format\n    self.creator = creator\n    self.contributor = contributor\n    self.collection_date = collection_date\n    self.upload_date = upload_date\n    self.language = language\n    self.licence = licence\n    self.url = url\n    self.default_target_attribute = default_target_attribute\n    self.row_id_attribute = row_id_attribute\n\n    self.version_label = version_label\n    self.citation = citation\n    self.tag = tag\n    self.visibility = visibility\n    self.original_data_url = original_data_url\n    self.paper_url = paper_url\n    self.update_comment = update_comment\n    self.md5_checksum = md5_checksum\n    self.data_file = data_file\n    self.parquet_file = parquet_file\n    self._dataset = dataset\n    self._parquet_url = parquet_url\n\n    self._features: dict[int, OpenMLDataFeature] | None = None\n    self._qualities: dict[str, float] | None = None\n    self._no_qualities_found = False\n\n    if features_file is not None:\n        self._features = _read_features(Path(features_file))\n\n    # \"\" was the old default value by `get_dataset` and maybe still used by some\n    if qualities_file == \"\":\n        # TODO(0.15): to switch to \"qualities_file is not None\" below and remove warning\n        warnings.warn(\n            \"Starting from Version 0.15 `qualities_file` must be None and not an empty string \"\n            \"to avoid reading the qualities from file. Set `qualities_file` to None to avoid \"\n            \"this warning.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        qualities_file = None\n\n    if qualities_file is not None:\n        self._qualities = _read_qualities(Path(qualities_file))\n\n    if data_file is not None:\n        data_pickle, data_feather, feather_attribute = self._compressed_cache_file_paths(\n            Path(data_file)\n        )\n        self.data_pickle_file = data_pickle if Path(data_pickle).exists() else None\n        self.data_feather_file = data_feather if Path(data_feather).exists() else None\n        self.feather_attribute_file = feather_attribute if Path(feather_attribute) else None\n    else:\n        self.data_pickle_file = None\n        self.data_feather_file = None\n        self.feather_attribute_file = None\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.features","title":"features  <code>property</code>","text":"<pre><code>features: dict[int, OpenMLDataFeature]\n</code></pre> <p>Get the features of this dataset.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Get the dataset numeric id.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.qualities","title":"qualities  <code>property</code>","text":"<pre><code>qualities: dict[str, float] | None\n</code></pre> <p>Get the qualities of this dataset.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.get_data","title":"get_data","text":"<pre><code>get_data(target: list[str] | str | None = None, include_row_id: bool = False, include_ignore_attribute: bool = False) -&gt; tuple[DataFrame, Series | None, list[bool], list[str]]\n</code></pre> <p>Returns dataset content as dataframes.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.get_data--parameters","title":"Parameters","text":"<p>target : string, List[str] or None (default=None)     Name of target column to separate from the data.     Splitting multiple columns is currently not supported. include_row_id : boolean (default=False)     Whether to include row ids in the returned dataset. include_ignore_attribute : boolean (default=False)     Whether to include columns that are marked as \"ignore\"     on the server in the dataset.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.get_data--returns","title":"Returns","text":"<p>X : dataframe, shape (n_samples, n_columns)     Dataset, may have sparse dtypes in the columns if required. y : pd.Series, shape (n_samples, ) or None     Target column categorical_indicator : list[bool]     Mask that indicate categorical features. attribute_names : list[str]     List of attribute names.</p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def get_data(  # noqa: C901\n    self,\n    target: list[str] | str | None = None,\n    include_row_id: bool = False,  # noqa: FBT001, FBT002\n    include_ignore_attribute: bool = False,  # noqa: FBT001, FBT002\n) -&gt; tuple[pd.DataFrame, pd.Series | None, list[bool], list[str]]:\n    \"\"\"Returns dataset content as dataframes.\n\n    Parameters\n    ----------\n    target : string, List[str] or None (default=None)\n        Name of target column to separate from the data.\n        Splitting multiple columns is currently not supported.\n    include_row_id : boolean (default=False)\n        Whether to include row ids in the returned dataset.\n    include_ignore_attribute : boolean (default=False)\n        Whether to include columns that are marked as \"ignore\"\n        on the server in the dataset.\n\n\n    Returns\n    -------\n    X : dataframe, shape (n_samples, n_columns)\n        Dataset, may have sparse dtypes in the columns if required.\n    y : pd.Series, shape (n_samples, ) or None\n        Target column\n    categorical_indicator : list[bool]\n        Mask that indicate categorical features.\n    attribute_names : list[str]\n        List of attribute names.\n    \"\"\"\n    data, categorical_mask, attribute_names = self._load_data()\n\n    to_exclude = []\n    if not include_row_id and self.row_id_attribute is not None:\n        if isinstance(self.row_id_attribute, str):\n            to_exclude.append(self.row_id_attribute)\n        elif isinstance(self.row_id_attribute, Iterable):\n            to_exclude.extend(self.row_id_attribute)\n\n    if not include_ignore_attribute and self.ignore_attribute is not None:\n        if isinstance(self.ignore_attribute, str):\n            to_exclude.append(self.ignore_attribute)\n        elif isinstance(self.ignore_attribute, Iterable):\n            to_exclude.extend(self.ignore_attribute)\n\n    if len(to_exclude) &gt; 0:\n        logger.info(f\"Going to remove the following attributes: {to_exclude}\")\n        keep = np.array([column not in to_exclude for column in attribute_names])\n        data = data.drop(columns=to_exclude)\n        categorical_mask = [cat for cat, k in zip(categorical_mask, keep) if k]\n        attribute_names = [att for att, k in zip(attribute_names, keep) if k]\n\n    if target is None:\n        return data, None, categorical_mask, attribute_names\n\n    if isinstance(target, str):\n        target_names = target.split(\",\") if \",\" in target else [target]\n    else:\n        target_names = target\n\n    # All the assumptions below for the target are dependant on the number of targets being 1\n    n_targets = len(target_names)\n    if n_targets &gt; 1:\n        raise NotImplementedError(f\"Number of targets {n_targets} not implemented.\")\n\n    target_name = target_names[0]\n    x = data.drop(columns=[target_name])\n    y = data[target_name].squeeze()\n\n    # Finally, remove the target from the list of attributes and categorical mask\n    target_index = attribute_names.index(target_name)\n    categorical_mask.pop(target_index)\n    attribute_names.remove(target_name)\n\n    assert isinstance(y, pd.Series)\n    return x, y, categorical_mask, attribute_names\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.get_features_by_type","title":"get_features_by_type","text":"<pre><code>get_features_by_type(data_type: str, exclude: list[str] | None = None, exclude_ignore_attribute: bool = True, exclude_row_id_attribute: bool = True) -&gt; list[int]\n</code></pre> <p>Return indices of features of a given type, e.g. all nominal features. Optional parameters to exclude various features by index or ontology.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.get_features_by_type--parameters","title":"Parameters","text":"<p>data_type : str     The data type to return (e.g., nominal, numeric, date, string) exclude : list(int)     List of columns to exclude from the return value exclude_ignore_attribute : bool     Whether to exclude the defined ignore attributes (and adapt the     return values as if these indices are not present) exclude_row_id_attribute : bool     Whether to exclude the defined row id attributes (and adapt the     return values as if these indices are not present)</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.get_features_by_type--returns","title":"Returns","text":"<p>result : list     a list of indices that have the specified data type</p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def get_features_by_type(  # noqa: C901\n    self,\n    data_type: str,\n    exclude: list[str] | None = None,\n    exclude_ignore_attribute: bool = True,  # noqa: FBT002, FBT001\n    exclude_row_id_attribute: bool = True,  # noqa: FBT002, FBT001\n) -&gt; list[int]:\n    \"\"\"\n    Return indices of features of a given type, e.g. all nominal features.\n    Optional parameters to exclude various features by index or ontology.\n\n    Parameters\n    ----------\n    data_type : str\n        The data type to return (e.g., nominal, numeric, date, string)\n    exclude : list(int)\n        List of columns to exclude from the return value\n    exclude_ignore_attribute : bool\n        Whether to exclude the defined ignore attributes (and adapt the\n        return values as if these indices are not present)\n    exclude_row_id_attribute : bool\n        Whether to exclude the defined row id attributes (and adapt the\n        return values as if these indices are not present)\n\n    Returns\n    -------\n    result : list\n        a list of indices that have the specified data type\n    \"\"\"\n    if data_type not in OpenMLDataFeature.LEGAL_DATA_TYPES:\n        raise TypeError(\"Illegal feature type requested\")\n    if self.ignore_attribute is not None and not isinstance(self.ignore_attribute, list):\n        raise TypeError(\"ignore_attribute should be a list\")\n    if self.row_id_attribute is not None and not isinstance(self.row_id_attribute, str):\n        raise TypeError(\"row id attribute should be a str\")\n    if exclude is not None and not isinstance(exclude, list):\n        raise TypeError(\"Exclude should be a list\")\n        # assert all(isinstance(elem, str) for elem in exclude),\n        #            \"Exclude should be a list of strings\"\n    to_exclude = []\n    if exclude is not None:\n        to_exclude.extend(exclude)\n    if exclude_ignore_attribute and self.ignore_attribute is not None:\n        to_exclude.extend(self.ignore_attribute)\n    if exclude_row_id_attribute and self.row_id_attribute is not None:\n        to_exclude.append(self.row_id_attribute)\n\n    result = []\n    offset = 0\n    # this function assumes that everything in to_exclude will\n    # be 'excluded' from the dataset (hence the offset)\n    for idx in self.features:\n        name = self.features[idx].name\n        if name in to_exclude:\n            offset += 1\n        elif self.features[idx].data_type == data_type:\n            result.append(idx - offset)\n    return result\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.retrieve_class_labels","title":"retrieve_class_labels","text":"<pre><code>retrieve_class_labels(target_name: str = 'class') -&gt; None | list[str]\n</code></pre> <p>Reads the datasets arff to determine the class-labels.</p> <p>If the task has no class labels (for example a regression problem) it returns None. Necessary because the data returned by get_data only contains the indices of the classes, while OpenML needs the real classname when uploading the results of a run.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.retrieve_class_labels--parameters","title":"Parameters","text":"<p>target_name : str     Name of the target attribute</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.retrieve_class_labels--returns","title":"Returns","text":"<p>list</p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def retrieve_class_labels(self, target_name: str = \"class\") -&gt; None | list[str]:\n    \"\"\"Reads the datasets arff to determine the class-labels.\n\n    If the task has no class labels (for example a regression problem)\n    it returns None. Necessary because the data returned by get_data\n    only contains the indices of the classes, while OpenML needs the real\n    classname when uploading the results of a run.\n\n    Parameters\n    ----------\n    target_name : str\n        Name of the target attribute\n\n    Returns\n    -------\n    list\n    \"\"\"\n    for feature in self.features.values():\n        if feature.name == target_name:\n            if feature.data_type == \"nominal\":\n                return feature.nominal_values\n\n            if feature.data_type == \"string\":\n                # Rel.: #1311\n                # The target is invalid for a classification task if the feature type is string\n                # and not nominal. For such miss-configured tasks, we silently fix it here as\n                # we can safely interpreter string as nominal.\n                df, *_ = self.get_data()\n                return list(df[feature.name].unique())\n\n    return None\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.attributes_arff_from_df","title":"attributes_arff_from_df","text":"<pre><code>attributes_arff_from_df(df: DataFrame) -&gt; list[tuple[str, list[str] | str]]\n</code></pre> <p>Describe attributes of the dataframe according to ARFF specification.</p>"},{"location":"reference/datasets/#openml.datasets.attributes_arff_from_df--parameters","title":"Parameters","text":"<p>df : DataFrame, shape (n_samples, n_features)     The dataframe containing the data set.</p>"},{"location":"reference/datasets/#openml.datasets.attributes_arff_from_df--returns","title":"Returns","text":"<p>attributes_arff : list[str]     The data set attributes as required by the ARFF format.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def attributes_arff_from_df(df: pd.DataFrame) -&gt; list[tuple[str, list[str] | str]]:\n    \"\"\"Describe attributes of the dataframe according to ARFF specification.\n\n    Parameters\n    ----------\n    df : DataFrame, shape (n_samples, n_features)\n        The dataframe containing the data set.\n\n    Returns\n    -------\n    attributes_arff : list[str]\n        The data set attributes as required by the ARFF format.\n    \"\"\"\n    PD_DTYPES_TO_ARFF_DTYPE = {\"integer\": \"INTEGER\", \"floating\": \"REAL\", \"string\": \"STRING\"}\n    attributes_arff: list[tuple[str, list[str] | str]] = []\n\n    if not all(isinstance(column_name, str) for column_name in df.columns):\n        logger.warning(\"Converting non-str column names to str.\")\n        df.columns = [str(column_name) for column_name in df.columns]\n\n    for column_name in df:\n        # skipna=True does not infer properly the dtype. The NA values are\n        # dropped before the inference instead.\n        column_dtype = pd.api.types.infer_dtype(df[column_name].dropna(), skipna=False)\n\n        if column_dtype == \"categorical\":\n            # for categorical feature, arff expects a list string. However, a\n            # categorical column can contain mixed type and should therefore\n            # raise an error asking to convert all entries to string.\n            categories = df[column_name].cat.categories\n            categories_dtype = pd.api.types.infer_dtype(categories)\n            if categories_dtype not in (\"string\", \"unicode\"):\n                raise ValueError(\n                    f\"The column '{column_name}' of the dataframe is of \"\n                    \"'category' dtype. Therefore, all values in \"\n                    \"this columns should be string. Please \"\n                    \"convert the entries which are not string. \"\n                    f\"Got {categories_dtype} dtype in this column.\",\n                )\n            attributes_arff.append((column_name, categories.tolist()))\n        elif column_dtype == \"boolean\":\n            # boolean are encoded as categorical.\n            attributes_arff.append((column_name, [\"True\", \"False\"]))\n        elif column_dtype in PD_DTYPES_TO_ARFF_DTYPE:\n            attributes_arff.append((column_name, PD_DTYPES_TO_ARFF_DTYPE[column_dtype]))\n        else:\n            raise ValueError(\n                f\"The dtype '{column_dtype}' of the column '{column_name}' is not \"\n                \"currently supported by liac-arff. Supported \"\n                \"dtypes are categorical, string, integer, \"\n                \"floating, and boolean.\",\n            )\n    return attributes_arff\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.check_datasets_active","title":"check_datasets_active","text":"<pre><code>check_datasets_active(dataset_ids: list[int], raise_error_if_not_exist: bool = True) -&gt; dict[int, bool]\n</code></pre> <p>Check if the dataset ids provided are active.</p> <p>Raises an error if a dataset_id in the given list of dataset_ids does not exist on the server and <code>raise_error_if_not_exist</code> is set to True (default).</p>"},{"location":"reference/datasets/#openml.datasets.check_datasets_active--parameters","title":"Parameters","text":"<p>dataset_ids : List[int]     A list of integers representing dataset ids. raise_error_if_not_exist : bool (default=True)     Flag that if activated can raise an error, if one or more of the     given dataset ids do not exist on the server.</p>"},{"location":"reference/datasets/#openml.datasets.check_datasets_active--returns","title":"Returns","text":"<p>dict     A dictionary with items {did: bool}</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def check_datasets_active(\n    dataset_ids: list[int],\n    raise_error_if_not_exist: bool = True,  # noqa: FBT001, FBT002\n) -&gt; dict[int, bool]:\n    \"\"\"\n    Check if the dataset ids provided are active.\n\n    Raises an error if a dataset_id in the given list\n    of dataset_ids does not exist on the server and\n    `raise_error_if_not_exist` is set to True (default).\n\n    Parameters\n    ----------\n    dataset_ids : List[int]\n        A list of integers representing dataset ids.\n    raise_error_if_not_exist : bool (default=True)\n        Flag that if activated can raise an error, if one or more of the\n        given dataset ids do not exist on the server.\n\n    Returns\n    -------\n    dict\n        A dictionary with items {did: bool}\n    \"\"\"\n    datasets = list_datasets(status=\"all\", data_id=dataset_ids)\n    missing = set(dataset_ids) - set(datasets.index)\n    if raise_error_if_not_exist and missing:\n        missing_str = \", \".join(str(did) for did in missing)\n        raise ValueError(f\"Could not find dataset(s) {missing_str} in OpenML dataset list.\")\n    mask = datasets[\"status\"] == \"active\"\n    return dict(mask)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(name: str, description: str | None, creator: str | None, contributor: str | None, collection_date: str | None, language: str | None, licence: str | None, attributes: list[tuple[str, str | list[str]]] | dict[str, str | list[str]] | Literal['auto'], data: DataFrame | ndarray | coo_matrix, default_target_attribute: str, ignore_attribute: str | list[str] | None, citation: str, row_id_attribute: str | None = None, original_data_url: str | None = None, paper_url: str | None = None, update_comment: str | None = None, version_label: str | None = None) -&gt; OpenMLDataset\n</code></pre> <p>Create a dataset.</p> <p>This function creates an OpenMLDataset object. The OpenMLDataset object contains information related to the dataset and the actual data file.</p>"},{"location":"reference/datasets/#openml.datasets.create_dataset--parameters","title":"Parameters","text":"<p>name : str     Name of the dataset. description : str     Description of the dataset. creator : str     The person who created the dataset. contributor : str     People who contributed to the current version of the dataset. collection_date : str     The date the data was originally collected, given by the uploader. language : str     Language in which the data is represented.     Starts with 1 upper case letter, rest lower case, e.g. 'English'. licence : str     License of the data. attributes : list, dict, or 'auto'     A list of tuples. Each tuple consists of the attribute name and type.     If passing a pandas DataFrame, the attributes can be automatically     inferred by passing <code>'auto'</code>. Specific attributes can be manually     specified by a passing a dictionary where the key is the name of the     attribute and the value is the data type of the attribute. data : ndarray, list, dataframe, coo_matrix, shape (n_samples, n_features)     An array that contains both the attributes and the targets. When     providing a dataframe, the attribute names and type can be inferred by     passing <code>attributes='auto'</code>.     The target feature is indicated as meta-data of the dataset. default_target_attribute : str     The default target attribute, if it exists.     Can have multiple values, comma separated. ignore_attribute : str | list     Attributes that should be excluded in modelling,     such as identifiers and indexes.     Can have multiple values, comma separated. citation : str     Reference(s) that should be cited when building on this data. version_label : str, optional     Version label provided by user.      Can be a date, hash, or some other type of id. row_id_attribute : str, optional     The attribute that represents the row-id column, if present in the     dataset. If <code>data</code> is a dataframe and <code>row_id_attribute</code> is not     specified, the index of the dataframe will be used as the     <code>row_id_attribute</code>. If the name of the index is <code>None</code>, it will     be discarded.</p> <pre><code>.. versionadded: 0.8\n    Inference of ``row_id_attribute`` from a dataframe.\n</code></pre> <p>original_data_url : str, optional     For derived data, the url to the original dataset. paper_url : str, optional     Link to a paper describing the dataset. update_comment : str, optional     An explanation for when the dataset is uploaded.</p>"},{"location":"reference/datasets/#openml.datasets.create_dataset--returns","title":"Returns","text":"<p>class:<code>openml.OpenMLDataset</code> Dataset description.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def create_dataset(  # noqa: C901, PLR0912, PLR0915\n    name: str,\n    description: str | None,\n    creator: str | None,\n    contributor: str | None,\n    collection_date: str | None,\n    language: str | None,\n    licence: str | None,\n    # TODO(eddiebergman): Docstring says `type` but I don't know what this is other than strings\n    # Edit: Found it could also be like [\"True\", \"False\"]\n    attributes: list[tuple[str, str | list[str]]] | dict[str, str | list[str]] | Literal[\"auto\"],\n    data: pd.DataFrame | np.ndarray | scipy.sparse.coo_matrix,\n    # TODO(eddiebergman): Function requires `default_target_attribute` exist but API allows None\n    default_target_attribute: str,\n    ignore_attribute: str | list[str] | None,\n    citation: str,\n    row_id_attribute: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n    update_comment: str | None = None,\n    version_label: str | None = None,\n) -&gt; OpenMLDataset:\n    \"\"\"Create a dataset.\n\n    This function creates an OpenMLDataset object.\n    The OpenMLDataset object contains information related to the dataset\n    and the actual data file.\n\n    Parameters\n    ----------\n    name : str\n        Name of the dataset.\n    description : str\n        Description of the dataset.\n    creator : str\n        The person who created the dataset.\n    contributor : str\n        People who contributed to the current version of the dataset.\n    collection_date : str\n        The date the data was originally collected, given by the uploader.\n    language : str\n        Language in which the data is represented.\n        Starts with 1 upper case letter, rest lower case, e.g. 'English'.\n    licence : str\n        License of the data.\n    attributes : list, dict, or 'auto'\n        A list of tuples. Each tuple consists of the attribute name and type.\n        If passing a pandas DataFrame, the attributes can be automatically\n        inferred by passing ``'auto'``. Specific attributes can be manually\n        specified by a passing a dictionary where the key is the name of the\n        attribute and the value is the data type of the attribute.\n    data : ndarray, list, dataframe, coo_matrix, shape (n_samples, n_features)\n        An array that contains both the attributes and the targets. When\n        providing a dataframe, the attribute names and type can be inferred by\n        passing ``attributes='auto'``.\n        The target feature is indicated as meta-data of the dataset.\n    default_target_attribute : str\n        The default target attribute, if it exists.\n        Can have multiple values, comma separated.\n    ignore_attribute : str | list\n        Attributes that should be excluded in modelling,\n        such as identifiers and indexes.\n        Can have multiple values, comma separated.\n    citation : str\n        Reference(s) that should be cited when building on this data.\n    version_label : str, optional\n        Version label provided by user.\n         Can be a date, hash, or some other type of id.\n    row_id_attribute : str, optional\n        The attribute that represents the row-id column, if present in the\n        dataset. If ``data`` is a dataframe and ``row_id_attribute`` is not\n        specified, the index of the dataframe will be used as the\n        ``row_id_attribute``. If the name of the index is ``None``, it will\n        be discarded.\n\n        .. versionadded: 0.8\n            Inference of ``row_id_attribute`` from a dataframe.\n    original_data_url : str, optional\n        For derived data, the url to the original dataset.\n    paper_url : str, optional\n        Link to a paper describing the dataset.\n    update_comment : str, optional\n        An explanation for when the dataset is uploaded.\n\n    Returns\n    -------\n    class:`openml.OpenMLDataset`\n    Dataset description.\n    \"\"\"\n    if isinstance(data, pd.DataFrame):\n        # infer the row id from the index of the dataset\n        if row_id_attribute is None:\n            row_id_attribute = data.index.name\n        # When calling data.values, the index will be skipped.\n        # We need to reset the index such that it is part of the data.\n        if data.index.name is not None:\n            data = data.reset_index()\n\n    if attributes == \"auto\" or isinstance(attributes, dict):\n        if not isinstance(data, pd.DataFrame):\n            raise ValueError(\n                \"Automatically inferring attributes requires \"\n                f\"a pandas DataFrame. A {data!r} was given instead.\",\n            )\n        # infer the type of data for each column of the DataFrame\n        attributes_ = attributes_arff_from_df(data)\n        if isinstance(attributes, dict):\n            # override the attributes which was specified by the user\n            for attr_idx in range(len(attributes_)):\n                attr_name = attributes_[attr_idx][0]\n                if attr_name in attributes:\n                    attributes_[attr_idx] = (attr_name, attributes[attr_name])\n    else:\n        attributes_ = attributes\n    ignore_attributes = _expand_parameter(ignore_attribute)\n    _validated_data_attributes(ignore_attributes, attributes_, \"ignore_attribute\")\n\n    default_target_attributes = _expand_parameter(default_target_attribute)\n    _validated_data_attributes(default_target_attributes, attributes_, \"default_target_attribute\")\n\n    if row_id_attribute is not None:\n        is_row_id_an_attribute = any(attr[0] == row_id_attribute for attr in attributes_)\n        if not is_row_id_an_attribute:\n            raise ValueError(\n                \"'row_id_attribute' should be one of the data attribute. \"\n                f\" Got '{row_id_attribute}' while candidates are\"\n                f\" {[attr[0] for attr in attributes_]}.\",\n            )\n\n    if isinstance(data, pd.DataFrame):\n        if all(isinstance(dtype, pd.SparseDtype) for dtype in data.dtypes):\n            data = data.sparse.to_coo()\n            # liac-arff only support COO matrices with sorted rows\n            row_idx_sorted = np.argsort(data.row)  # type: ignore\n            data.row = data.row[row_idx_sorted]  # type: ignore\n            data.col = data.col[row_idx_sorted]  # type: ignore\n            data.data = data.data[row_idx_sorted]  # type: ignore\n        else:\n            data = data.to_numpy()\n\n    data_format: Literal[\"arff\", \"sparse_arff\"]\n    if isinstance(data, (list, np.ndarray)):\n        if isinstance(data[0], (list, np.ndarray)):\n            data_format = \"arff\"\n        elif isinstance(data[0], dict):\n            data_format = \"sparse_arff\"\n        else:\n            raise ValueError(\n                \"When giving a list or a numpy.ndarray, \"\n                \"they should contain a list/ numpy.ndarray \"\n                \"for dense data or a dictionary for sparse \"\n                f\"data. Got {data[0]!r} instead.\",\n            )\n    elif isinstance(data, coo_matrix):\n        data_format = \"sparse_arff\"\n    else:\n        raise ValueError(\n            \"When giving a list or a numpy.ndarray, \"\n            \"they should contain a list/ numpy.ndarray \"\n            \"for dense data or a dictionary for sparse \"\n            f\"data. Got {data[0]!r} instead.\",\n        )\n\n    arff_object = {\n        \"relation\": name,\n        \"description\": description,\n        \"attributes\": attributes_,\n        \"data\": data,\n    }\n\n    # serializes the ARFF dataset object and returns a string\n    arff_dataset = arff.dumps(arff_object)\n    try:\n        # check if ARFF is valid\n        decoder = arff.ArffDecoder()\n        return_type = arff.COO if data_format == \"sparse_arff\" else arff.DENSE\n        decoder.decode(arff_dataset, encode_nominal=True, return_type=return_type)\n    except arff.ArffException as e:\n        raise ValueError(\n            \"The arguments you have provided do not construct a valid ARFF file\"\n        ) from e\n\n    return OpenMLDataset(\n        name=name,\n        description=description,\n        data_format=data_format,\n        creator=creator,\n        contributor=contributor,\n        collection_date=collection_date,\n        language=language,\n        licence=licence,\n        default_target_attribute=default_target_attribute,\n        row_id_attribute=row_id_attribute,\n        ignore_attribute=ignore_attribute,\n        citation=citation,\n        version_label=version_label,\n        original_data_url=original_data_url,\n        paper_url=paper_url,\n        update_comment=update_comment,\n        dataset=arff_dataset,\n    )\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.delete_dataset","title":"delete_dataset","text":"<pre><code>delete_dataset(dataset_id: int) -&gt; bool\n</code></pre> <p>Delete dataset with id <code>dataset_id</code> from the OpenML server.</p> <p>This can only be done if you are the owner of the dataset and no tasks are attached to the dataset.</p>"},{"location":"reference/datasets/#openml.datasets.delete_dataset--parameters","title":"Parameters","text":"<p>dataset_id : int     OpenML id of the dataset</p>"},{"location":"reference/datasets/#openml.datasets.delete_dataset--returns","title":"Returns","text":"<p>bool     True if the deletion was successful. False otherwise.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def delete_dataset(dataset_id: int) -&gt; bool:\n    \"\"\"Delete dataset with id `dataset_id` from the OpenML server.\n\n    This can only be done if you are the owner of the dataset and\n    no tasks are attached to the dataset.\n\n    Parameters\n    ----------\n    dataset_id : int\n        OpenML id of the dataset\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"data\", dataset_id)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.edit_dataset","title":"edit_dataset","text":"<pre><code>edit_dataset(data_id: int, description: str | None = None, creator: str | None = None, contributor: str | None = None, collection_date: str | None = None, language: str | None = None, default_target_attribute: str | None = None, ignore_attribute: str | list[str] | None = None, citation: str | None = None, row_id_attribute: str | None = None, original_data_url: str | None = None, paper_url: str | None = None) -&gt; int\n</code></pre> <p>Edits an OpenMLDataset.</p> <p>In addition to providing the dataset id of the dataset to edit (through data_id), you must specify a value for at least one of the optional function arguments, i.e. one value for a field to edit.</p> <p>This function allows editing of both non-critical and critical fields. Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.</p> <ul> <li>Editing non-critical data fields is allowed for all authenticated users.</li> <li>Editing critical fields is allowed only for the owner, provided there are no tasks    associated with this dataset.</li> </ul> <p>If dataset has tasks or if the user is not the owner, the only way to edit critical fields is to use fork_dataset followed by edit_dataset.</p>"},{"location":"reference/datasets/#openml.datasets.edit_dataset--parameters","title":"Parameters","text":"<p>data_id : int     ID of the dataset. description : str     Description of the dataset. creator : str     The person who created the dataset. contributor : str     People who contributed to the current version of the dataset. collection_date : str     The date the data was originally collected, given by the uploader. language : str     Language in which the data is represented.     Starts with 1 upper case letter, rest lower case, e.g. 'English'. default_target_attribute : str     The default target attribute, if it exists.     Can have multiple values, comma separated. ignore_attribute : str | list     Attributes that should be excluded in modelling,     such as identifiers and indexes. citation : str     Reference(s) that should be cited when building on this data. row_id_attribute : str, optional     The attribute that represents the row-id column, if present in the     dataset. If <code>data</code> is a dataframe and <code>row_id_attribute</code> is not     specified, the index of the dataframe will be used as the     <code>row_id_attribute</code>. If the name of the index is <code>None</code>, it will     be discarded.</p> <pre><code>.. versionadded: 0.8\n    Inference of ``row_id_attribute`` from a dataframe.\n</code></pre> <p>original_data_url : str, optional     For derived data, the url to the original dataset. paper_url : str, optional     Link to a paper describing the dataset.</p>"},{"location":"reference/datasets/#openml.datasets.edit_dataset--returns","title":"Returns","text":"<p>Dataset id</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def edit_dataset(\n    data_id: int,\n    description: str | None = None,\n    creator: str | None = None,\n    contributor: str | None = None,\n    collection_date: str | None = None,\n    language: str | None = None,\n    default_target_attribute: str | None = None,\n    ignore_attribute: str | list[str] | None = None,\n    citation: str | None = None,\n    row_id_attribute: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n) -&gt; int:\n    \"\"\"Edits an OpenMLDataset.\n\n    In addition to providing the dataset id of the dataset to edit (through data_id),\n    you must specify a value for at least one of the optional function arguments,\n    i.e. one value for a field to edit.\n\n    This function allows editing of both non-critical and critical fields.\n    Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.\n\n     - Editing non-critical data fields is allowed for all authenticated users.\n     - Editing critical fields is allowed only for the owner, provided there are no tasks\n       associated with this dataset.\n\n    If dataset has tasks or if the user is not the owner, the only way\n    to edit critical fields is to use fork_dataset followed by edit_dataset.\n\n    Parameters\n    ----------\n    data_id : int\n        ID of the dataset.\n    description : str\n        Description of the dataset.\n    creator : str\n        The person who created the dataset.\n    contributor : str\n        People who contributed to the current version of the dataset.\n    collection_date : str\n        The date the data was originally collected, given by the uploader.\n    language : str\n        Language in which the data is represented.\n        Starts with 1 upper case letter, rest lower case, e.g. 'English'.\n    default_target_attribute : str\n        The default target attribute, if it exists.\n        Can have multiple values, comma separated.\n    ignore_attribute : str | list\n        Attributes that should be excluded in modelling,\n        such as identifiers and indexes.\n    citation : str\n        Reference(s) that should be cited when building on this data.\n    row_id_attribute : str, optional\n        The attribute that represents the row-id column, if present in the\n        dataset. If ``data`` is a dataframe and ``row_id_attribute`` is not\n        specified, the index of the dataframe will be used as the\n        ``row_id_attribute``. If the name of the index is ``None``, it will\n        be discarded.\n\n        .. versionadded: 0.8\n            Inference of ``row_id_attribute`` from a dataframe.\n    original_data_url : str, optional\n        For derived data, the url to the original dataset.\n    paper_url : str, optional\n        Link to a paper describing the dataset.\n\n    Returns\n    -------\n    Dataset id\n    \"\"\"\n    if not isinstance(data_id, int):\n        raise TypeError(f\"`data_id` must be of type `int`, not {type(data_id)}.\")\n\n    # compose data edit parameters as xml\n    form_data = {\"data_id\": data_id}  # type: openml._api_calls.DATA_TYPE\n    xml = OrderedDict()  # type: 'OrderedDict[str, OrderedDict]'\n    xml[\"oml:data_edit_parameters\"] = OrderedDict()\n    xml[\"oml:data_edit_parameters\"][\"@xmlns:oml\"] = \"http://openml.org/openml\"\n    xml[\"oml:data_edit_parameters\"][\"oml:description\"] = description\n    xml[\"oml:data_edit_parameters\"][\"oml:creator\"] = creator\n    xml[\"oml:data_edit_parameters\"][\"oml:contributor\"] = contributor\n    xml[\"oml:data_edit_parameters\"][\"oml:collection_date\"] = collection_date\n    xml[\"oml:data_edit_parameters\"][\"oml:language\"] = language\n    xml[\"oml:data_edit_parameters\"][\"oml:default_target_attribute\"] = default_target_attribute\n    xml[\"oml:data_edit_parameters\"][\"oml:row_id_attribute\"] = row_id_attribute\n    xml[\"oml:data_edit_parameters\"][\"oml:ignore_attribute\"] = ignore_attribute\n    xml[\"oml:data_edit_parameters\"][\"oml:citation\"] = citation\n    xml[\"oml:data_edit_parameters\"][\"oml:original_data_url\"] = original_data_url\n    xml[\"oml:data_edit_parameters\"][\"oml:paper_url\"] = paper_url\n\n    # delete None inputs\n    for k in list(xml[\"oml:data_edit_parameters\"]):\n        if not xml[\"oml:data_edit_parameters\"][k]:\n            del xml[\"oml:data_edit_parameters\"][k]\n\n    file_elements = {\n        \"edit_parameters\": (\"description.xml\", xmltodict.unparse(xml)),\n    }  # type: openml._api_calls.FILE_ELEMENTS_TYPE\n    result_xml = openml._api_calls._perform_api_call(\n        \"data/edit\",\n        \"post\",\n        data=form_data,\n        file_elements=file_elements,\n    )\n    result = xmltodict.parse(result_xml)\n    data_id = result[\"oml:data_edit\"][\"oml:id\"]\n    return int(data_id)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.fork_dataset","title":"fork_dataset","text":"<pre><code>fork_dataset(data_id: int) -&gt; int\n</code></pre> <p>Creates a new dataset version, with the authenticated user as the new owner.  The forked dataset can have distinct dataset meta-data,  but the actual data itself is shared with the original version.</p> <p>This API is intended for use when a user is unable to edit the critical fields of a dataset  through the edit_dataset API.  (Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.)</p> <p>Specifically, this happens when the user is:         1. Not the owner of the dataset.         2. User is the owner of the dataset, but the dataset has tasks.</p> <p>In these two cases the only way to edit critical fields is:         1. STEP 1: Fork the dataset using fork_dataset API         2. STEP 2: Call edit_dataset API on the forked version.</p>"},{"location":"reference/datasets/#openml.datasets.fork_dataset--parameters","title":"Parameters","text":"<p>data_id : int     id of the dataset to be forked</p>"},{"location":"reference/datasets/#openml.datasets.fork_dataset--returns","title":"Returns","text":"<p>Dataset id of the forked dataset</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def fork_dataset(data_id: int) -&gt; int:\n    \"\"\"\n     Creates a new dataset version, with the authenticated user as the new owner.\n     The forked dataset can have distinct dataset meta-data,\n     but the actual data itself is shared with the original version.\n\n     This API is intended for use when a user is unable to edit the critical fields of a dataset\n     through the edit_dataset API.\n     (Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.)\n\n     Specifically, this happens when the user is:\n            1. Not the owner of the dataset.\n            2. User is the owner of the dataset, but the dataset has tasks.\n\n     In these two cases the only way to edit critical fields is:\n            1. STEP 1: Fork the dataset using fork_dataset API\n            2. STEP 2: Call edit_dataset API on the forked version.\n\n\n    Parameters\n    ----------\n    data_id : int\n        id of the dataset to be forked\n\n    Returns\n    -------\n    Dataset id of the forked dataset\n\n    \"\"\"\n    if not isinstance(data_id, int):\n        raise TypeError(f\"`data_id` must be of type `int`, not {type(data_id)}.\")\n    # compose data fork parameters\n    form_data = {\"data_id\": data_id}  # type: openml._api_calls.DATA_TYPE\n    result_xml = openml._api_calls._perform_api_call(\"data/fork\", \"post\", data=form_data)\n    result = xmltodict.parse(result_xml)\n    data_id = result[\"oml:data_fork\"][\"oml:id\"]\n    return int(data_id)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(dataset_id: int | str, download_data: bool = False, version: int | None = None, error_if_multiple: bool = False, cache_format: Literal['pickle', 'feather'] = 'pickle', download_qualities: bool = False, download_features_meta_data: bool = False, download_all_files: bool = False, force_refresh_cache: bool = False) -&gt; OpenMLDataset\n</code></pre> <p>Download the OpenML dataset representation, optionally also download actual data file.</p> <p>This function is by default NOT thread/multiprocessing safe, as this function uses caching. A check will be performed to determine if the information has previously been downloaded to a cache, and if so be loaded from disk instead of retrieved from the server.</p> <p>To make this function thread safe, you can install the python package <code>oslo.concurrency</code>. If <code>oslo.concurrency</code> is installed <code>get_dataset</code> becomes thread safe.</p> <p>Alternatively, to make this function thread/multiprocessing safe initialize the cache first by calling <code>get_dataset(args)</code> once before calling <code>get_dataset(args)</code> many times in parallel. This will initialize the cache and later calls will use the cache in a thread/multiprocessing safe way.</p> <p>If dataset is retrieved by name, a version may be specified. If no version is specified and multiple versions of the dataset exist, the earliest version of the dataset that is still active will be returned. If no version is specified, multiple versions of the dataset exist and <code>exception_if_multiple</code> is set to <code>True</code>, this function will raise an exception.</p>"},{"location":"reference/datasets/#openml.datasets.get_dataset--parameters","title":"Parameters","text":"<p>dataset_id : int or str     Dataset ID (integer) or dataset name (string) of the dataset to download. download_data : bool (default=False)     If True, also download the data file. Beware that some datasets are large and it might     make the operation noticeably slower. Metadata is also still retrieved.     If False, create the OpenMLDataset and only populate it with the metadata.     The data may later be retrieved through the <code>OpenMLDataset.get_data</code> method. version : int, optional (default=None)     Specifies the version if <code>dataset_id</code> is specified by name.     If no version is specified, retrieve the least recent still active version. error_if_multiple : bool (default=False)     If <code>True</code> raise an error if multiple datasets are found with matching criteria. cache_format : str (default='pickle') in {'pickle', 'feather'}     Format for caching the dataset - may be feather or pickle     Note that the default 'pickle' option may load slower than feather when     no.of.rows is very high. download_qualities : bool (default=False)     Option to download 'qualities' meta-data in addition to the minimal dataset description.     If True, download and cache the qualities file.     If False, create the OpenMLDataset without qualities metadata. The data may later be added     to the OpenMLDataset through the <code>OpenMLDataset.load_metadata(qualities=True)</code> method. download_features_meta_data : bool (default=False)     Option to download 'features' meta-data in addition to the minimal dataset description.     If True, download and cache the features file.     If False, create the OpenMLDataset without features metadata. The data may later be added     to the OpenMLDataset through the <code>OpenMLDataset.load_metadata(features=True)</code> method. download_all_files: bool (default=False)     EXPERIMENTAL. Download all files related to the dataset that reside on the server.     Useful for datasets which refer to auxiliary files (e.g., meta-album). force_refresh_cache : bool (default=False)     Force the cache to refreshed by deleting the cache directory and re-downloading the data.     Note, if <code>force_refresh_cache</code> is True, <code>get_dataset</code> is NOT thread/multiprocessing safe,     because this creates a race condition to creating and deleting the cache; as in general with     the cache.</p>"},{"location":"reference/datasets/#openml.datasets.get_dataset--returns","title":"Returns","text":"<p>dataset : :class:<code>openml.OpenMLDataset</code>     The downloaded dataset.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_dataset(  # noqa: C901, PLR0912\n    dataset_id: int | str,\n    download_data: bool = False,  # noqa: FBT002, FBT001\n    version: int | None = None,\n    error_if_multiple: bool = False,  # noqa: FBT002, FBT001\n    cache_format: Literal[\"pickle\", \"feather\"] = \"pickle\",\n    download_qualities: bool = False,  # noqa: FBT002, FBT001\n    download_features_meta_data: bool = False,  # noqa: FBT002, FBT001\n    download_all_files: bool = False,  # noqa: FBT002, FBT001\n    force_refresh_cache: bool = False,  # noqa: FBT001, FBT002\n) -&gt; OpenMLDataset:\n    \"\"\"Download the OpenML dataset representation, optionally also download actual data file.\n\n    This function is by default NOT thread/multiprocessing safe, as this function uses caching.\n    A check will be performed to determine if the information has previously been downloaded to a\n    cache, and if so be loaded from disk instead of retrieved from the server.\n\n    To make this function thread safe, you can install the python package ``oslo.concurrency``.\n    If ``oslo.concurrency`` is installed `get_dataset` becomes thread safe.\n\n    Alternatively, to make this function thread/multiprocessing safe initialize the cache first by\n    calling `get_dataset(args)` once before calling `get_dataset(args)` many times in parallel.\n    This will initialize the cache and later calls will use the cache in a thread/multiprocessing\n    safe way.\n\n    If dataset is retrieved by name, a version may be specified.\n    If no version is specified and multiple versions of the dataset exist,\n    the earliest version of the dataset that is still active will be returned.\n    If no version is specified, multiple versions of the dataset exist and\n    ``exception_if_multiple`` is set to ``True``, this function will raise an exception.\n\n    Parameters\n    ----------\n    dataset_id : int or str\n        Dataset ID (integer) or dataset name (string) of the dataset to download.\n    download_data : bool (default=False)\n        If True, also download the data file. Beware that some datasets are large and it might\n        make the operation noticeably slower. Metadata is also still retrieved.\n        If False, create the OpenMLDataset and only populate it with the metadata.\n        The data may later be retrieved through the `OpenMLDataset.get_data` method.\n    version : int, optional (default=None)\n        Specifies the version if `dataset_id` is specified by name.\n        If no version is specified, retrieve the least recent still active version.\n    error_if_multiple : bool (default=False)\n        If ``True`` raise an error if multiple datasets are found with matching criteria.\n    cache_format : str (default='pickle') in {'pickle', 'feather'}\n        Format for caching the dataset - may be feather or pickle\n        Note that the default 'pickle' option may load slower than feather when\n        no.of.rows is very high.\n    download_qualities : bool (default=False)\n        Option to download 'qualities' meta-data in addition to the minimal dataset description.\n        If True, download and cache the qualities file.\n        If False, create the OpenMLDataset without qualities metadata. The data may later be added\n        to the OpenMLDataset through the `OpenMLDataset.load_metadata(qualities=True)` method.\n    download_features_meta_data : bool (default=False)\n        Option to download 'features' meta-data in addition to the minimal dataset description.\n        If True, download and cache the features file.\n        If False, create the OpenMLDataset without features metadata. The data may later be added\n        to the OpenMLDataset through the `OpenMLDataset.load_metadata(features=True)` method.\n    download_all_files: bool (default=False)\n        EXPERIMENTAL. Download all files related to the dataset that reside on the server.\n        Useful for datasets which refer to auxiliary files (e.g., meta-album).\n    force_refresh_cache : bool (default=False)\n        Force the cache to refreshed by deleting the cache directory and re-downloading the data.\n        Note, if `force_refresh_cache` is True, `get_dataset` is NOT thread/multiprocessing safe,\n        because this creates a race condition to creating and deleting the cache; as in general with\n        the cache.\n\n    Returns\n    -------\n    dataset : :class:`openml.OpenMLDataset`\n        The downloaded dataset.\n    \"\"\"\n    if download_all_files:\n        warnings.warn(\n            \"``download_all_files`` is experimental and is likely to break with new releases.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n    if cache_format not in [\"feather\", \"pickle\"]:\n        raise ValueError(\n            \"cache_format must be one of 'feather' or 'pickle. \"\n            f\"Invalid format specified: {cache_format}\",\n        )\n\n    if isinstance(dataset_id, str):\n        try:\n            dataset_id = int(dataset_id)\n        except ValueError:\n            dataset_id = _name_to_id(dataset_id, version, error_if_multiple)  # type: ignore\n    elif not isinstance(dataset_id, int):\n        raise TypeError(\n            f\"`dataset_id` must be one of `str` or `int`, not {type(dataset_id)}.\",\n        )\n\n    if force_refresh_cache:\n        did_cache_dir = _get_cache_dir_for_id(DATASETS_CACHE_DIR_NAME, dataset_id)\n        if did_cache_dir.exists():\n            _remove_cache_dir_for_id(DATASETS_CACHE_DIR_NAME, did_cache_dir)\n\n    did_cache_dir = _create_cache_directory_for_id(\n        DATASETS_CACHE_DIR_NAME,\n        dataset_id,\n    )\n\n    remove_dataset_cache = True\n    try:\n        description = _get_dataset_description(did_cache_dir, dataset_id)\n        features_file = None\n        qualities_file = None\n\n        if download_features_meta_data:\n            features_file = _get_dataset_features_file(did_cache_dir, dataset_id)\n        if download_qualities:\n            qualities_file = _get_dataset_qualities_file(did_cache_dir, dataset_id)\n\n        parquet_file = None\n        skip_parquet = os.environ.get(OPENML_SKIP_PARQUET_ENV_VAR, \"false\").casefold() == \"true\"\n        download_parquet = \"oml:parquet_url\" in description and not skip_parquet\n        if download_parquet and (download_data or download_all_files):\n            try:\n                parquet_file = _get_dataset_parquet(\n                    description,\n                    download_all_files=download_all_files,\n                )\n            except urllib3.exceptions.MaxRetryError:\n                parquet_file = None\n\n        arff_file = None\n        if parquet_file is None and download_data:\n            if download_parquet:\n                logger.warning(\"Failed to download parquet, fallback on ARFF.\")\n            arff_file = _get_dataset_arff(description)\n\n        remove_dataset_cache = False\n    except OpenMLServerException as e:\n        # if there was an exception\n        # check if the user had access to the dataset\n        if e.code == NO_ACCESS_GRANTED_ERRCODE:\n            raise OpenMLPrivateDatasetError(e.message) from None\n\n        raise e\n    finally:\n        if remove_dataset_cache:\n            _remove_cache_dir_for_id(DATASETS_CACHE_DIR_NAME, did_cache_dir)\n\n    return _create_dataset_from_description(\n        description,\n        features_file,\n        qualities_file,\n        arff_file,\n        parquet_file,\n        cache_format,\n    )\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.get_datasets","title":"get_datasets","text":"<pre><code>get_datasets(dataset_ids: list[str | int], download_data: bool = False, download_qualities: bool = False) -&gt; list[OpenMLDataset]\n</code></pre> <p>Download datasets.</p> <p>This function iterates :meth:<code>openml.datasets.get_dataset</code>.</p>"},{"location":"reference/datasets/#openml.datasets.get_datasets--parameters","title":"Parameters","text":"<p>dataset_ids : iterable     Integers or strings representing dataset ids or dataset names.     If dataset names are specified, the least recent still active dataset version is returned. download_data : bool, optional     If True, also download the data file. Beware that some datasets are large and it might     make the operation noticeably slower. Metadata is also still retrieved.     If False, create the OpenMLDataset and only populate it with the metadata.     The data may later be retrieved through the <code>OpenMLDataset.get_data</code> method. download_qualities : bool, optional (default=True)     If True, also download qualities.xml file. If False it skip the qualities.xml.</p>"},{"location":"reference/datasets/#openml.datasets.get_datasets--returns","title":"Returns","text":"<p>datasets : list of datasets     A list of dataset objects.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def get_datasets(\n    dataset_ids: list[str | int],\n    download_data: bool = False,  # noqa: FBT001, FBT002\n    download_qualities: bool = False,  # noqa: FBT001, FBT002\n) -&gt; list[OpenMLDataset]:\n    \"\"\"Download datasets.\n\n    This function iterates :meth:`openml.datasets.get_dataset`.\n\n    Parameters\n    ----------\n    dataset_ids : iterable\n        Integers or strings representing dataset ids or dataset names.\n        If dataset names are specified, the least recent still active dataset version is returned.\n    download_data : bool, optional\n        If True, also download the data file. Beware that some datasets are large and it might\n        make the operation noticeably slower. Metadata is also still retrieved.\n        If False, create the OpenMLDataset and only populate it with the metadata.\n        The data may later be retrieved through the `OpenMLDataset.get_data` method.\n    download_qualities : bool, optional (default=True)\n        If True, also download qualities.xml file. If False it skip the qualities.xml.\n\n    Returns\n    -------\n    datasets : list of datasets\n        A list of dataset objects.\n    \"\"\"\n    datasets = []\n    for dataset_id in dataset_ids:\n        datasets.append(\n            get_dataset(dataset_id, download_data, download_qualities=download_qualities),\n        )\n    return datasets\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.list_datasets","title":"list_datasets","text":"<pre><code>list_datasets(data_id: list[int] | None = None, offset: int | None = None, size: int | None = None, status: str | None = None, tag: str | None = None, data_name: str | None = None, data_version: int | None = None, number_instances: int | str | None = None, number_features: int | str | None = None, number_classes: int | str | None = None, number_missing_values: int | str | None = None) -&gt; DataFrame\n</code></pre> <p>Return a dataframe of all dataset which are on OpenML.</p> <p>Supports large amount of results.</p>"},{"location":"reference/datasets/#openml.datasets.list_datasets--parameters","title":"Parameters","text":"<p>data_id : list, optional     A list of data ids, to specify which datasets should be     listed offset : int, optional     The number of datasets to skip, starting from the first. size : int, optional     The maximum number of datasets to show. status : str, optional     Should be {active, in_preparation, deactivated}. By     default active datasets are returned, but also datasets     from another status can be requested. tag : str, optional data_name : str, optional data_version : int, optional number_instances : int | str, optional number_features : int | str, optional number_classes : int | str, optional number_missing_values : int | str, optional</p>"},{"location":"reference/datasets/#openml.datasets.list_datasets--returns","title":"Returns","text":"<p>datasets: dataframe     Each row maps to a dataset     Each column contains the following information:     - dataset id     - name     - format     - status     If qualities are calculated for the dataset, some of     these are also included as columns.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def list_datasets(\n    data_id: list[int] | None = None,\n    offset: int | None = None,\n    size: int | None = None,\n    status: str | None = None,\n    tag: str | None = None,\n    data_name: str | None = None,\n    data_version: int | None = None,\n    number_instances: int | str | None = None,\n    number_features: int | str | None = None,\n    number_classes: int | str | None = None,\n    number_missing_values: int | str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Return a dataframe of all dataset which are on OpenML.\n\n    Supports large amount of results.\n\n    Parameters\n    ----------\n    data_id : list, optional\n        A list of data ids, to specify which datasets should be\n        listed\n    offset : int, optional\n        The number of datasets to skip, starting from the first.\n    size : int, optional\n        The maximum number of datasets to show.\n    status : str, optional\n        Should be {active, in_preparation, deactivated}. By\n        default active datasets are returned, but also datasets\n        from another status can be requested.\n    tag : str, optional\n    data_name : str, optional\n    data_version : int, optional\n    number_instances : int | str, optional\n    number_features : int | str, optional\n    number_classes : int | str, optional\n    number_missing_values : int | str, optional\n\n    Returns\n    -------\n    datasets: dataframe\n        Each row maps to a dataset\n        Each column contains the following information:\n        - dataset id\n        - name\n        - format\n        - status\n        If qualities are calculated for the dataset, some of\n        these are also included as columns.\n    \"\"\"\n    listing_call = partial(\n        _list_datasets,\n        data_id=data_id,\n        status=status,\n        tag=tag,\n        data_name=data_name,\n        data_version=data_version,\n        number_instances=number_instances,\n        number_features=number_features,\n        number_classes=number_classes,\n        number_missing_values=number_missing_values,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.list_qualities","title":"list_qualities","text":"<pre><code>list_qualities() -&gt; list[str]\n</code></pre> <p>Return list of data qualities available.</p> <p>The function performs an API call to retrieve the entire list of data qualities that are computed on the datasets uploaded.</p>"},{"location":"reference/datasets/#openml.datasets.list_qualities--returns","title":"Returns","text":"<p>list</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def list_qualities() -&gt; list[str]:\n    \"\"\"Return list of data qualities available.\n\n    The function performs an API call to retrieve the entire list of\n    data qualities that are computed on the datasets uploaded.\n\n    Returns\n    -------\n    list\n    \"\"\"\n    api_call = \"data/qualities/list\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    qualities = xmltodict.parse(xml_string, force_list=(\"oml:quality\"))\n    # Minimalistic check if the XML is useful\n    if \"oml:data_qualities_list\" not in qualities:\n        raise ValueError('Error in return XML, does not contain \"oml:data_qualities_list\"')\n\n    if not isinstance(qualities[\"oml:data_qualities_list\"][\"oml:quality\"], list):\n        raise TypeError('Error in return XML, does not contain \"oml:quality\" as a list')\n\n    return qualities[\"oml:data_qualities_list\"][\"oml:quality\"]\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.status_update","title":"status_update","text":"<pre><code>status_update(data_id: int, status: Literal['active', 'deactivated']) -&gt; None\n</code></pre> <p>Updates the status of a dataset to either 'active' or 'deactivated'. Please see the OpenML API documentation for a description of the status and all legal status transitions: docs.openml.org/concepts/data/#dataset-status</p>"},{"location":"reference/datasets/#openml.datasets.status_update--parameters","title":"Parameters","text":"<p>data_id : int     The data id of the dataset status : str,     'active' or 'deactivated'</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def status_update(data_id: int, status: Literal[\"active\", \"deactivated\"]) -&gt; None:\n    \"\"\"\n    Updates the status of a dataset to either 'active' or 'deactivated'.\n    Please see the OpenML API documentation for a description of the status\n    and all legal status transitions:\n    https://docs.openml.org/concepts/data/#dataset-status\n\n    Parameters\n    ----------\n    data_id : int\n        The data id of the dataset\n    status : str,\n        'active' or 'deactivated'\n    \"\"\"\n    legal_status = {\"active\", \"deactivated\"}\n    if status not in legal_status:\n        raise ValueError(f\"Illegal status value. Legal values: {legal_status}\")\n\n    data: openml._api_calls.DATA_TYPE = {\"data_id\": data_id, \"status\": status}\n    result_xml = openml._api_calls._perform_api_call(\"data/status/update\", \"post\", data=data)\n    result = xmltodict.parse(result_xml)\n    server_data_id = result[\"oml:data_status_update\"][\"oml:id\"]\n    server_status = result[\"oml:data_status_update\"][\"oml:status\"]\n    if status != server_status or int(data_id) != int(server_data_id):\n        # This should never happen\n        raise ValueError(\"Data id/status does not collide\")\n</code></pre>"},{"location":"reference/datasets/data_feature/","title":"data_feature","text":""},{"location":"reference/datasets/data_feature/#openml.datasets.data_feature","title":"openml.datasets.data_feature","text":""},{"location":"reference/datasets/data_feature/#openml.datasets.data_feature.OpenMLDataFeature","title":"OpenMLDataFeature","text":"<pre><code>OpenMLDataFeature(index: int, name: str, data_type: str, nominal_values: list[str], number_missing_values: int, ontologies: list[str] | None = None)\n</code></pre> <p>Data Feature (a.k.a. Attribute) object.</p>"},{"location":"reference/datasets/data_feature/#openml.datasets.data_feature.OpenMLDataFeature--parameters","title":"Parameters","text":"<p>index : int     The index of this feature name : str     Name of the feature data_type : str     can be nominal, numeric, string, date (corresponds to arff) nominal_values : list(str)     list of the possible values, in case of nominal attribute number_missing_values : int     Number of rows that have a missing value for this feature. ontologies : list(str)     list of ontologies attached to this feature. An ontology describes the     concept that are described in a feature. An ontology is defined by an     URL where the information is provided.</p> Source code in <code>openml/datasets/data_feature.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    index: int,\n    name: str,\n    data_type: str,\n    nominal_values: list[str],\n    number_missing_values: int,\n    ontologies: list[str] | None = None,\n):\n    if not isinstance(index, int):\n        raise TypeError(f\"Index must be `int` but is {type(index)}\")\n\n    if data_type not in self.LEGAL_DATA_TYPES:\n        raise ValueError(\n            f\"data type should be in {self.LEGAL_DATA_TYPES!s}, found: {data_type}\",\n        )\n\n    if data_type == \"nominal\":\n        if nominal_values is None:\n            raise TypeError(\n                \"Dataset features require attribute `nominal_values` for nominal \"\n                \"feature type.\",\n            )\n\n        if not isinstance(nominal_values, list):\n            raise TypeError(\n                \"Argument `nominal_values` is of wrong datatype, should be list, \"\n                f\"but is {type(nominal_values)}\",\n            )\n    elif nominal_values is not None:\n        raise TypeError(\"Argument `nominal_values` must be None for non-nominal feature.\")\n\n    if not isinstance(number_missing_values, int):\n        msg = f\"number_missing_values must be int but is {type(number_missing_values)}\"\n        raise TypeError(msg)\n\n    self.index = index\n    self.name = str(name)\n    self.data_type = str(data_type)\n    self.nominal_values = nominal_values\n    self.number_missing_values = number_missing_values\n    self.ontologies = ontologies\n</code></pre>"},{"location":"reference/datasets/dataset/","title":"dataset","text":""},{"location":"reference/datasets/dataset/#openml.datasets.dataset","title":"openml.datasets.dataset","text":""},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset","title":"OpenMLDataset","text":"<pre><code>OpenMLDataset(name: str, description: str | None, data_format: Literal['arff', 'sparse_arff'] = 'arff', cache_format: Literal['feather', 'pickle'] = 'pickle', dataset_id: int | None = None, version: int | None = None, creator: str | None = None, contributor: str | None = None, collection_date: str | None = None, upload_date: str | None = None, language: str | None = None, licence: str | None = None, url: str | None = None, default_target_attribute: str | None = None, row_id_attribute: str | None = None, ignore_attribute: str | list[str] | None = None, version_label: str | None = None, citation: str | None = None, tag: str | None = None, visibility: str | None = None, original_data_url: str | None = None, paper_url: str | None = None, update_comment: str | None = None, md5_checksum: str | None = None, data_file: str | None = None, features_file: str | None = None, qualities_file: str | None = None, dataset: str | None = None, parquet_url: str | None = None, parquet_file: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>Dataset object.</p> <p>Allows fetching and uploading datasets to OpenML.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset--parameters","title":"Parameters","text":"<p>name : str     Name of the dataset. description : str     Description of the dataset. data_format : str     Format of the dataset which can be either 'arff' or 'sparse_arff'. cache_format : str     Format for caching the dataset which can be either 'feather' or 'pickle'. dataset_id : int, optional     Id autogenerated by the server. version : int, optional     Version of this dataset. '1' for original version.     Auto-incremented by server. creator : str, optional     The person who created the dataset. contributor : str, optional     People who contributed to the current version of the dataset. collection_date : str, optional     The date the data was originally collected, given by the uploader. upload_date : str, optional     The date-time when the dataset was uploaded, generated by server. language : str, optional     Language in which the data is represented.     Starts with 1 upper case letter, rest lower case, e.g. 'English'. licence : str, optional     License of the data. url : str, optional     Valid URL, points to actual data file.     The file can be on the OpenML server or another dataset repository. default_target_attribute : str, optional     The default target attribute, if it exists.     Can have multiple values, comma separated. row_id_attribute : str, optional     The attribute that represents the row-id column,     if present in the dataset. ignore_attribute : str | list, optional     Attributes that should be excluded in modelling,     such as identifiers and indexes. version_label : str, optional     Version label provided by user.     Can be a date, hash, or some other type of id. citation : str, optional     Reference(s) that should be cited when building on this data. tag : str, optional     Tags, describing the algorithms. visibility : str, optional     Who can see the dataset.     Typical values: 'Everyone','All my friends','Only me'.     Can also be any of the user's circles. original_data_url : str, optional     For derived data, the url to the original dataset. paper_url : str, optional     Link to a paper describing the dataset. update_comment : str, optional     An explanation for when the dataset is uploaded. md5_checksum : str, optional     MD5 checksum to check if the dataset is downloaded without corruption. data_file : str, optional     Path to where the dataset is located. features_file : dict, optional     A dictionary of dataset features,     which maps a feature index to a OpenMLDataFeature. qualities_file : dict, optional     A dictionary of dataset qualities,     which maps a quality name to a quality value. dataset: string, optional     Serialized arff dataset string. parquet_url: string, optional     This is the URL to the storage location where the dataset files are hosted.     This can be a MinIO bucket URL. If specified, the data will be accessed     from this URL when reading the files. parquet_file: string, optional     Path to the local file.</p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def __init__(  # noqa: C901, PLR0912, PLR0913, PLR0915\n    self,\n    name: str,\n    description: str | None,\n    data_format: Literal[\"arff\", \"sparse_arff\"] = \"arff\",\n    cache_format: Literal[\"feather\", \"pickle\"] = \"pickle\",\n    dataset_id: int | None = None,\n    version: int | None = None,\n    creator: str | None = None,\n    contributor: str | None = None,\n    collection_date: str | None = None,\n    upload_date: str | None = None,\n    language: str | None = None,\n    licence: str | None = None,\n    url: str | None = None,\n    default_target_attribute: str | None = None,\n    row_id_attribute: str | None = None,\n    ignore_attribute: str | list[str] | None = None,\n    version_label: str | None = None,\n    citation: str | None = None,\n    tag: str | None = None,\n    visibility: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n    update_comment: str | None = None,\n    md5_checksum: str | None = None,\n    data_file: str | None = None,\n    features_file: str | None = None,\n    qualities_file: str | None = None,\n    dataset: str | None = None,\n    parquet_url: str | None = None,\n    parquet_file: str | None = None,\n):\n    if cache_format not in [\"feather\", \"pickle\"]:\n        raise ValueError(\n            \"cache_format must be one of 'feather' or 'pickle. \"\n            f\"Invalid format specified: {cache_format}\",\n        )\n\n    def find_invalid_characters(string: str, pattern: str) -&gt; str:\n        invalid_chars = set()\n        regex = re.compile(pattern)\n        for char in string:\n            if not regex.match(char):\n                invalid_chars.add(char)\n        return \",\".join(\n            [f\"'{char}'\" if char != \"'\" else f'\"{char}\"' for char in invalid_chars],\n        )\n\n    if dataset_id is None:\n        pattern = \"^[\\x00-\\x7f]*$\"\n        if description and not re.match(pattern, description):\n            # not basiclatin (XSD complains)\n            invalid_characters = find_invalid_characters(description, pattern)\n            raise ValueError(\n                f\"Invalid symbols {invalid_characters} in description: {description}\",\n            )\n        pattern = \"^[\\x00-\\x7f]*$\"\n        if citation and not re.match(pattern, citation):\n            # not basiclatin (XSD complains)\n            invalid_characters = find_invalid_characters(citation, pattern)\n            raise ValueError(\n                f\"Invalid symbols {invalid_characters} in citation: {citation}\",\n            )\n        pattern = \"^[a-zA-Z0-9_\\\\-\\\\.\\\\(\\\\),]+$\"\n        if not re.match(pattern, name):\n            # regex given by server in error message\n            invalid_characters = find_invalid_characters(name, pattern)\n            raise ValueError(f\"Invalid symbols {invalid_characters} in name: {name}\")\n\n    self.ignore_attribute: list[str] | None = None\n    if isinstance(ignore_attribute, str):\n        self.ignore_attribute = [ignore_attribute]\n    elif isinstance(ignore_attribute, list) or ignore_attribute is None:\n        self.ignore_attribute = ignore_attribute\n    else:\n        raise ValueError(\"Wrong data type for ignore_attribute. Should be list.\")\n\n    # TODO add function to check if the name is casual_string128\n    # Attributes received by querying the RESTful API\n    self.dataset_id = int(dataset_id) if dataset_id is not None else None\n    self.name = name\n    self.version = int(version) if version is not None else None\n    self.description = description\n    self.cache_format = cache_format\n    # Has to be called format, otherwise there will be an XML upload error\n    self.format = data_format\n    self.creator = creator\n    self.contributor = contributor\n    self.collection_date = collection_date\n    self.upload_date = upload_date\n    self.language = language\n    self.licence = licence\n    self.url = url\n    self.default_target_attribute = default_target_attribute\n    self.row_id_attribute = row_id_attribute\n\n    self.version_label = version_label\n    self.citation = citation\n    self.tag = tag\n    self.visibility = visibility\n    self.original_data_url = original_data_url\n    self.paper_url = paper_url\n    self.update_comment = update_comment\n    self.md5_checksum = md5_checksum\n    self.data_file = data_file\n    self.parquet_file = parquet_file\n    self._dataset = dataset\n    self._parquet_url = parquet_url\n\n    self._features: dict[int, OpenMLDataFeature] | None = None\n    self._qualities: dict[str, float] | None = None\n    self._no_qualities_found = False\n\n    if features_file is not None:\n        self._features = _read_features(Path(features_file))\n\n    # \"\" was the old default value by `get_dataset` and maybe still used by some\n    if qualities_file == \"\":\n        # TODO(0.15): to switch to \"qualities_file is not None\" below and remove warning\n        warnings.warn(\n            \"Starting from Version 0.15 `qualities_file` must be None and not an empty string \"\n            \"to avoid reading the qualities from file. Set `qualities_file` to None to avoid \"\n            \"this warning.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        qualities_file = None\n\n    if qualities_file is not None:\n        self._qualities = _read_qualities(Path(qualities_file))\n\n    if data_file is not None:\n        data_pickle, data_feather, feather_attribute = self._compressed_cache_file_paths(\n            Path(data_file)\n        )\n        self.data_pickle_file = data_pickle if Path(data_pickle).exists() else None\n        self.data_feather_file = data_feather if Path(data_feather).exists() else None\n        self.feather_attribute_file = feather_attribute if Path(feather_attribute) else None\n    else:\n        self.data_pickle_file = None\n        self.data_feather_file = None\n        self.feather_attribute_file = None\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.features","title":"features  <code>property</code>","text":"<pre><code>features: dict[int, OpenMLDataFeature]\n</code></pre> <p>Get the features of this dataset.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Get the dataset numeric id.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.qualities","title":"qualities  <code>property</code>","text":"<pre><code>qualities: dict[str, float] | None\n</code></pre> <p>Get the qualities of this dataset.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.get_data","title":"get_data","text":"<pre><code>get_data(target: list[str] | str | None = None, include_row_id: bool = False, include_ignore_attribute: bool = False) -&gt; tuple[DataFrame, Series | None, list[bool], list[str]]\n</code></pre> <p>Returns dataset content as dataframes.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.get_data--parameters","title":"Parameters","text":"<p>target : string, List[str] or None (default=None)     Name of target column to separate from the data.     Splitting multiple columns is currently not supported. include_row_id : boolean (default=False)     Whether to include row ids in the returned dataset. include_ignore_attribute : boolean (default=False)     Whether to include columns that are marked as \"ignore\"     on the server in the dataset.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.get_data--returns","title":"Returns","text":"<p>X : dataframe, shape (n_samples, n_columns)     Dataset, may have sparse dtypes in the columns if required. y : pd.Series, shape (n_samples, ) or None     Target column categorical_indicator : list[bool]     Mask that indicate categorical features. attribute_names : list[str]     List of attribute names.</p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def get_data(  # noqa: C901\n    self,\n    target: list[str] | str | None = None,\n    include_row_id: bool = False,  # noqa: FBT001, FBT002\n    include_ignore_attribute: bool = False,  # noqa: FBT001, FBT002\n) -&gt; tuple[pd.DataFrame, pd.Series | None, list[bool], list[str]]:\n    \"\"\"Returns dataset content as dataframes.\n\n    Parameters\n    ----------\n    target : string, List[str] or None (default=None)\n        Name of target column to separate from the data.\n        Splitting multiple columns is currently not supported.\n    include_row_id : boolean (default=False)\n        Whether to include row ids in the returned dataset.\n    include_ignore_attribute : boolean (default=False)\n        Whether to include columns that are marked as \"ignore\"\n        on the server in the dataset.\n\n\n    Returns\n    -------\n    X : dataframe, shape (n_samples, n_columns)\n        Dataset, may have sparse dtypes in the columns if required.\n    y : pd.Series, shape (n_samples, ) or None\n        Target column\n    categorical_indicator : list[bool]\n        Mask that indicate categorical features.\n    attribute_names : list[str]\n        List of attribute names.\n    \"\"\"\n    data, categorical_mask, attribute_names = self._load_data()\n\n    to_exclude = []\n    if not include_row_id and self.row_id_attribute is not None:\n        if isinstance(self.row_id_attribute, str):\n            to_exclude.append(self.row_id_attribute)\n        elif isinstance(self.row_id_attribute, Iterable):\n            to_exclude.extend(self.row_id_attribute)\n\n    if not include_ignore_attribute and self.ignore_attribute is not None:\n        if isinstance(self.ignore_attribute, str):\n            to_exclude.append(self.ignore_attribute)\n        elif isinstance(self.ignore_attribute, Iterable):\n            to_exclude.extend(self.ignore_attribute)\n\n    if len(to_exclude) &gt; 0:\n        logger.info(f\"Going to remove the following attributes: {to_exclude}\")\n        keep = np.array([column not in to_exclude for column in attribute_names])\n        data = data.drop(columns=to_exclude)\n        categorical_mask = [cat for cat, k in zip(categorical_mask, keep) if k]\n        attribute_names = [att for att, k in zip(attribute_names, keep) if k]\n\n    if target is None:\n        return data, None, categorical_mask, attribute_names\n\n    if isinstance(target, str):\n        target_names = target.split(\",\") if \",\" in target else [target]\n    else:\n        target_names = target\n\n    # All the assumptions below for the target are dependant on the number of targets being 1\n    n_targets = len(target_names)\n    if n_targets &gt; 1:\n        raise NotImplementedError(f\"Number of targets {n_targets} not implemented.\")\n\n    target_name = target_names[0]\n    x = data.drop(columns=[target_name])\n    y = data[target_name].squeeze()\n\n    # Finally, remove the target from the list of attributes and categorical mask\n    target_index = attribute_names.index(target_name)\n    categorical_mask.pop(target_index)\n    attribute_names.remove(target_name)\n\n    assert isinstance(y, pd.Series)\n    return x, y, categorical_mask, attribute_names\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.get_features_by_type","title":"get_features_by_type","text":"<pre><code>get_features_by_type(data_type: str, exclude: list[str] | None = None, exclude_ignore_attribute: bool = True, exclude_row_id_attribute: bool = True) -&gt; list[int]\n</code></pre> <p>Return indices of features of a given type, e.g. all nominal features. Optional parameters to exclude various features by index or ontology.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.get_features_by_type--parameters","title":"Parameters","text":"<p>data_type : str     The data type to return (e.g., nominal, numeric, date, string) exclude : list(int)     List of columns to exclude from the return value exclude_ignore_attribute : bool     Whether to exclude the defined ignore attributes (and adapt the     return values as if these indices are not present) exclude_row_id_attribute : bool     Whether to exclude the defined row id attributes (and adapt the     return values as if these indices are not present)</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.get_features_by_type--returns","title":"Returns","text":"<p>result : list     a list of indices that have the specified data type</p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def get_features_by_type(  # noqa: C901\n    self,\n    data_type: str,\n    exclude: list[str] | None = None,\n    exclude_ignore_attribute: bool = True,  # noqa: FBT002, FBT001\n    exclude_row_id_attribute: bool = True,  # noqa: FBT002, FBT001\n) -&gt; list[int]:\n    \"\"\"\n    Return indices of features of a given type, e.g. all nominal features.\n    Optional parameters to exclude various features by index or ontology.\n\n    Parameters\n    ----------\n    data_type : str\n        The data type to return (e.g., nominal, numeric, date, string)\n    exclude : list(int)\n        List of columns to exclude from the return value\n    exclude_ignore_attribute : bool\n        Whether to exclude the defined ignore attributes (and adapt the\n        return values as if these indices are not present)\n    exclude_row_id_attribute : bool\n        Whether to exclude the defined row id attributes (and adapt the\n        return values as if these indices are not present)\n\n    Returns\n    -------\n    result : list\n        a list of indices that have the specified data type\n    \"\"\"\n    if data_type not in OpenMLDataFeature.LEGAL_DATA_TYPES:\n        raise TypeError(\"Illegal feature type requested\")\n    if self.ignore_attribute is not None and not isinstance(self.ignore_attribute, list):\n        raise TypeError(\"ignore_attribute should be a list\")\n    if self.row_id_attribute is not None and not isinstance(self.row_id_attribute, str):\n        raise TypeError(\"row id attribute should be a str\")\n    if exclude is not None and not isinstance(exclude, list):\n        raise TypeError(\"Exclude should be a list\")\n        # assert all(isinstance(elem, str) for elem in exclude),\n        #            \"Exclude should be a list of strings\"\n    to_exclude = []\n    if exclude is not None:\n        to_exclude.extend(exclude)\n    if exclude_ignore_attribute and self.ignore_attribute is not None:\n        to_exclude.extend(self.ignore_attribute)\n    if exclude_row_id_attribute and self.row_id_attribute is not None:\n        to_exclude.append(self.row_id_attribute)\n\n    result = []\n    offset = 0\n    # this function assumes that everything in to_exclude will\n    # be 'excluded' from the dataset (hence the offset)\n    for idx in self.features:\n        name = self.features[idx].name\n        if name in to_exclude:\n            offset += 1\n        elif self.features[idx].data_type == data_type:\n            result.append(idx - offset)\n    return result\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.retrieve_class_labels","title":"retrieve_class_labels","text":"<pre><code>retrieve_class_labels(target_name: str = 'class') -&gt; None | list[str]\n</code></pre> <p>Reads the datasets arff to determine the class-labels.</p> <p>If the task has no class labels (for example a regression problem) it returns None. Necessary because the data returned by get_data only contains the indices of the classes, while OpenML needs the real classname when uploading the results of a run.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.retrieve_class_labels--parameters","title":"Parameters","text":"<p>target_name : str     Name of the target attribute</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.retrieve_class_labels--returns","title":"Returns","text":"<p>list</p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def retrieve_class_labels(self, target_name: str = \"class\") -&gt; None | list[str]:\n    \"\"\"Reads the datasets arff to determine the class-labels.\n\n    If the task has no class labels (for example a regression problem)\n    it returns None. Necessary because the data returned by get_data\n    only contains the indices of the classes, while OpenML needs the real\n    classname when uploading the results of a run.\n\n    Parameters\n    ----------\n    target_name : str\n        Name of the target attribute\n\n    Returns\n    -------\n    list\n    \"\"\"\n    for feature in self.features.values():\n        if feature.name == target_name:\n            if feature.data_type == \"nominal\":\n                return feature.nominal_values\n\n            if feature.data_type == \"string\":\n                # Rel.: #1311\n                # The target is invalid for a classification task if the feature type is string\n                # and not nominal. For such miss-configured tasks, we silently fix it here as\n                # we can safely interpreter string as nominal.\n                df, *_ = self.get_data()\n                return list(df[feature.name].unique())\n\n    return None\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/datasets/functions/","title":"functions","text":""},{"location":"reference/datasets/functions/#openml.datasets.functions","title":"openml.datasets.functions","text":""},{"location":"reference/datasets/functions/#openml.datasets.functions.attributes_arff_from_df","title":"attributes_arff_from_df","text":"<pre><code>attributes_arff_from_df(df: DataFrame) -&gt; list[tuple[str, list[str] | str]]\n</code></pre> <p>Describe attributes of the dataframe according to ARFF specification.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.attributes_arff_from_df--parameters","title":"Parameters","text":"<p>df : DataFrame, shape (n_samples, n_features)     The dataframe containing the data set.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.attributes_arff_from_df--returns","title":"Returns","text":"<p>attributes_arff : list[str]     The data set attributes as required by the ARFF format.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def attributes_arff_from_df(df: pd.DataFrame) -&gt; list[tuple[str, list[str] | str]]:\n    \"\"\"Describe attributes of the dataframe according to ARFF specification.\n\n    Parameters\n    ----------\n    df : DataFrame, shape (n_samples, n_features)\n        The dataframe containing the data set.\n\n    Returns\n    -------\n    attributes_arff : list[str]\n        The data set attributes as required by the ARFF format.\n    \"\"\"\n    PD_DTYPES_TO_ARFF_DTYPE = {\"integer\": \"INTEGER\", \"floating\": \"REAL\", \"string\": \"STRING\"}\n    attributes_arff: list[tuple[str, list[str] | str]] = []\n\n    if not all(isinstance(column_name, str) for column_name in df.columns):\n        logger.warning(\"Converting non-str column names to str.\")\n        df.columns = [str(column_name) for column_name in df.columns]\n\n    for column_name in df:\n        # skipna=True does not infer properly the dtype. The NA values are\n        # dropped before the inference instead.\n        column_dtype = pd.api.types.infer_dtype(df[column_name].dropna(), skipna=False)\n\n        if column_dtype == \"categorical\":\n            # for categorical feature, arff expects a list string. However, a\n            # categorical column can contain mixed type and should therefore\n            # raise an error asking to convert all entries to string.\n            categories = df[column_name].cat.categories\n            categories_dtype = pd.api.types.infer_dtype(categories)\n            if categories_dtype not in (\"string\", \"unicode\"):\n                raise ValueError(\n                    f\"The column '{column_name}' of the dataframe is of \"\n                    \"'category' dtype. Therefore, all values in \"\n                    \"this columns should be string. Please \"\n                    \"convert the entries which are not string. \"\n                    f\"Got {categories_dtype} dtype in this column.\",\n                )\n            attributes_arff.append((column_name, categories.tolist()))\n        elif column_dtype == \"boolean\":\n            # boolean are encoded as categorical.\n            attributes_arff.append((column_name, [\"True\", \"False\"]))\n        elif column_dtype in PD_DTYPES_TO_ARFF_DTYPE:\n            attributes_arff.append((column_name, PD_DTYPES_TO_ARFF_DTYPE[column_dtype]))\n        else:\n            raise ValueError(\n                f\"The dtype '{column_dtype}' of the column '{column_name}' is not \"\n                \"currently supported by liac-arff. Supported \"\n                \"dtypes are categorical, string, integer, \"\n                \"floating, and boolean.\",\n            )\n    return attributes_arff\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.check_datasets_active","title":"check_datasets_active","text":"<pre><code>check_datasets_active(dataset_ids: list[int], raise_error_if_not_exist: bool = True) -&gt; dict[int, bool]\n</code></pre> <p>Check if the dataset ids provided are active.</p> <p>Raises an error if a dataset_id in the given list of dataset_ids does not exist on the server and <code>raise_error_if_not_exist</code> is set to True (default).</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.check_datasets_active--parameters","title":"Parameters","text":"<p>dataset_ids : List[int]     A list of integers representing dataset ids. raise_error_if_not_exist : bool (default=True)     Flag that if activated can raise an error, if one or more of the     given dataset ids do not exist on the server.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.check_datasets_active--returns","title":"Returns","text":"<p>dict     A dictionary with items {did: bool}</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def check_datasets_active(\n    dataset_ids: list[int],\n    raise_error_if_not_exist: bool = True,  # noqa: FBT001, FBT002\n) -&gt; dict[int, bool]:\n    \"\"\"\n    Check if the dataset ids provided are active.\n\n    Raises an error if a dataset_id in the given list\n    of dataset_ids does not exist on the server and\n    `raise_error_if_not_exist` is set to True (default).\n\n    Parameters\n    ----------\n    dataset_ids : List[int]\n        A list of integers representing dataset ids.\n    raise_error_if_not_exist : bool (default=True)\n        Flag that if activated can raise an error, if one or more of the\n        given dataset ids do not exist on the server.\n\n    Returns\n    -------\n    dict\n        A dictionary with items {did: bool}\n    \"\"\"\n    datasets = list_datasets(status=\"all\", data_id=dataset_ids)\n    missing = set(dataset_ids) - set(datasets.index)\n    if raise_error_if_not_exist and missing:\n        missing_str = \", \".join(str(did) for did in missing)\n        raise ValueError(f\"Could not find dataset(s) {missing_str} in OpenML dataset list.\")\n    mask = datasets[\"status\"] == \"active\"\n    return dict(mask)\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(name: str, description: str | None, creator: str | None, contributor: str | None, collection_date: str | None, language: str | None, licence: str | None, attributes: list[tuple[str, str | list[str]]] | dict[str, str | list[str]] | Literal['auto'], data: DataFrame | ndarray | coo_matrix, default_target_attribute: str, ignore_attribute: str | list[str] | None, citation: str, row_id_attribute: str | None = None, original_data_url: str | None = None, paper_url: str | None = None, update_comment: str | None = None, version_label: str | None = None) -&gt; OpenMLDataset\n</code></pre> <p>Create a dataset.</p> <p>This function creates an OpenMLDataset object. The OpenMLDataset object contains information related to the dataset and the actual data file.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.create_dataset--parameters","title":"Parameters","text":"<p>name : str     Name of the dataset. description : str     Description of the dataset. creator : str     The person who created the dataset. contributor : str     People who contributed to the current version of the dataset. collection_date : str     The date the data was originally collected, given by the uploader. language : str     Language in which the data is represented.     Starts with 1 upper case letter, rest lower case, e.g. 'English'. licence : str     License of the data. attributes : list, dict, or 'auto'     A list of tuples. Each tuple consists of the attribute name and type.     If passing a pandas DataFrame, the attributes can be automatically     inferred by passing <code>'auto'</code>. Specific attributes can be manually     specified by a passing a dictionary where the key is the name of the     attribute and the value is the data type of the attribute. data : ndarray, list, dataframe, coo_matrix, shape (n_samples, n_features)     An array that contains both the attributes and the targets. When     providing a dataframe, the attribute names and type can be inferred by     passing <code>attributes='auto'</code>.     The target feature is indicated as meta-data of the dataset. default_target_attribute : str     The default target attribute, if it exists.     Can have multiple values, comma separated. ignore_attribute : str | list     Attributes that should be excluded in modelling,     such as identifiers and indexes.     Can have multiple values, comma separated. citation : str     Reference(s) that should be cited when building on this data. version_label : str, optional     Version label provided by user.      Can be a date, hash, or some other type of id. row_id_attribute : str, optional     The attribute that represents the row-id column, if present in the     dataset. If <code>data</code> is a dataframe and <code>row_id_attribute</code> is not     specified, the index of the dataframe will be used as the     <code>row_id_attribute</code>. If the name of the index is <code>None</code>, it will     be discarded.</p> <pre><code>.. versionadded: 0.8\n    Inference of ``row_id_attribute`` from a dataframe.\n</code></pre> <p>original_data_url : str, optional     For derived data, the url to the original dataset. paper_url : str, optional     Link to a paper describing the dataset. update_comment : str, optional     An explanation for when the dataset is uploaded.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.create_dataset--returns","title":"Returns","text":"<p>class:<code>openml.OpenMLDataset</code> Dataset description.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def create_dataset(  # noqa: C901, PLR0912, PLR0915\n    name: str,\n    description: str | None,\n    creator: str | None,\n    contributor: str | None,\n    collection_date: str | None,\n    language: str | None,\n    licence: str | None,\n    # TODO(eddiebergman): Docstring says `type` but I don't know what this is other than strings\n    # Edit: Found it could also be like [\"True\", \"False\"]\n    attributes: list[tuple[str, str | list[str]]] | dict[str, str | list[str]] | Literal[\"auto\"],\n    data: pd.DataFrame | np.ndarray | scipy.sparse.coo_matrix,\n    # TODO(eddiebergman): Function requires `default_target_attribute` exist but API allows None\n    default_target_attribute: str,\n    ignore_attribute: str | list[str] | None,\n    citation: str,\n    row_id_attribute: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n    update_comment: str | None = None,\n    version_label: str | None = None,\n) -&gt; OpenMLDataset:\n    \"\"\"Create a dataset.\n\n    This function creates an OpenMLDataset object.\n    The OpenMLDataset object contains information related to the dataset\n    and the actual data file.\n\n    Parameters\n    ----------\n    name : str\n        Name of the dataset.\n    description : str\n        Description of the dataset.\n    creator : str\n        The person who created the dataset.\n    contributor : str\n        People who contributed to the current version of the dataset.\n    collection_date : str\n        The date the data was originally collected, given by the uploader.\n    language : str\n        Language in which the data is represented.\n        Starts with 1 upper case letter, rest lower case, e.g. 'English'.\n    licence : str\n        License of the data.\n    attributes : list, dict, or 'auto'\n        A list of tuples. Each tuple consists of the attribute name and type.\n        If passing a pandas DataFrame, the attributes can be automatically\n        inferred by passing ``'auto'``. Specific attributes can be manually\n        specified by a passing a dictionary where the key is the name of the\n        attribute and the value is the data type of the attribute.\n    data : ndarray, list, dataframe, coo_matrix, shape (n_samples, n_features)\n        An array that contains both the attributes and the targets. When\n        providing a dataframe, the attribute names and type can be inferred by\n        passing ``attributes='auto'``.\n        The target feature is indicated as meta-data of the dataset.\n    default_target_attribute : str\n        The default target attribute, if it exists.\n        Can have multiple values, comma separated.\n    ignore_attribute : str | list\n        Attributes that should be excluded in modelling,\n        such as identifiers and indexes.\n        Can have multiple values, comma separated.\n    citation : str\n        Reference(s) that should be cited when building on this data.\n    version_label : str, optional\n        Version label provided by user.\n         Can be a date, hash, or some other type of id.\n    row_id_attribute : str, optional\n        The attribute that represents the row-id column, if present in the\n        dataset. If ``data`` is a dataframe and ``row_id_attribute`` is not\n        specified, the index of the dataframe will be used as the\n        ``row_id_attribute``. If the name of the index is ``None``, it will\n        be discarded.\n\n        .. versionadded: 0.8\n            Inference of ``row_id_attribute`` from a dataframe.\n    original_data_url : str, optional\n        For derived data, the url to the original dataset.\n    paper_url : str, optional\n        Link to a paper describing the dataset.\n    update_comment : str, optional\n        An explanation for when the dataset is uploaded.\n\n    Returns\n    -------\n    class:`openml.OpenMLDataset`\n    Dataset description.\n    \"\"\"\n    if isinstance(data, pd.DataFrame):\n        # infer the row id from the index of the dataset\n        if row_id_attribute is None:\n            row_id_attribute = data.index.name\n        # When calling data.values, the index will be skipped.\n        # We need to reset the index such that it is part of the data.\n        if data.index.name is not None:\n            data = data.reset_index()\n\n    if attributes == \"auto\" or isinstance(attributes, dict):\n        if not isinstance(data, pd.DataFrame):\n            raise ValueError(\n                \"Automatically inferring attributes requires \"\n                f\"a pandas DataFrame. A {data!r} was given instead.\",\n            )\n        # infer the type of data for each column of the DataFrame\n        attributes_ = attributes_arff_from_df(data)\n        if isinstance(attributes, dict):\n            # override the attributes which was specified by the user\n            for attr_idx in range(len(attributes_)):\n                attr_name = attributes_[attr_idx][0]\n                if attr_name in attributes:\n                    attributes_[attr_idx] = (attr_name, attributes[attr_name])\n    else:\n        attributes_ = attributes\n    ignore_attributes = _expand_parameter(ignore_attribute)\n    _validated_data_attributes(ignore_attributes, attributes_, \"ignore_attribute\")\n\n    default_target_attributes = _expand_parameter(default_target_attribute)\n    _validated_data_attributes(default_target_attributes, attributes_, \"default_target_attribute\")\n\n    if row_id_attribute is not None:\n        is_row_id_an_attribute = any(attr[0] == row_id_attribute for attr in attributes_)\n        if not is_row_id_an_attribute:\n            raise ValueError(\n                \"'row_id_attribute' should be one of the data attribute. \"\n                f\" Got '{row_id_attribute}' while candidates are\"\n                f\" {[attr[0] for attr in attributes_]}.\",\n            )\n\n    if isinstance(data, pd.DataFrame):\n        if all(isinstance(dtype, pd.SparseDtype) for dtype in data.dtypes):\n            data = data.sparse.to_coo()\n            # liac-arff only support COO matrices with sorted rows\n            row_idx_sorted = np.argsort(data.row)  # type: ignore\n            data.row = data.row[row_idx_sorted]  # type: ignore\n            data.col = data.col[row_idx_sorted]  # type: ignore\n            data.data = data.data[row_idx_sorted]  # type: ignore\n        else:\n            data = data.to_numpy()\n\n    data_format: Literal[\"arff\", \"sparse_arff\"]\n    if isinstance(data, (list, np.ndarray)):\n        if isinstance(data[0], (list, np.ndarray)):\n            data_format = \"arff\"\n        elif isinstance(data[0], dict):\n            data_format = \"sparse_arff\"\n        else:\n            raise ValueError(\n                \"When giving a list or a numpy.ndarray, \"\n                \"they should contain a list/ numpy.ndarray \"\n                \"for dense data or a dictionary for sparse \"\n                f\"data. Got {data[0]!r} instead.\",\n            )\n    elif isinstance(data, coo_matrix):\n        data_format = \"sparse_arff\"\n    else:\n        raise ValueError(\n            \"When giving a list or a numpy.ndarray, \"\n            \"they should contain a list/ numpy.ndarray \"\n            \"for dense data or a dictionary for sparse \"\n            f\"data. Got {data[0]!r} instead.\",\n        )\n\n    arff_object = {\n        \"relation\": name,\n        \"description\": description,\n        \"attributes\": attributes_,\n        \"data\": data,\n    }\n\n    # serializes the ARFF dataset object and returns a string\n    arff_dataset = arff.dumps(arff_object)\n    try:\n        # check if ARFF is valid\n        decoder = arff.ArffDecoder()\n        return_type = arff.COO if data_format == \"sparse_arff\" else arff.DENSE\n        decoder.decode(arff_dataset, encode_nominal=True, return_type=return_type)\n    except arff.ArffException as e:\n        raise ValueError(\n            \"The arguments you have provided do not construct a valid ARFF file\"\n        ) from e\n\n    return OpenMLDataset(\n        name=name,\n        description=description,\n        data_format=data_format,\n        creator=creator,\n        contributor=contributor,\n        collection_date=collection_date,\n        language=language,\n        licence=licence,\n        default_target_attribute=default_target_attribute,\n        row_id_attribute=row_id_attribute,\n        ignore_attribute=ignore_attribute,\n        citation=citation,\n        version_label=version_label,\n        original_data_url=original_data_url,\n        paper_url=paper_url,\n        update_comment=update_comment,\n        dataset=arff_dataset,\n    )\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.data_feature_add_ontology","title":"data_feature_add_ontology","text":"<pre><code>data_feature_add_ontology(data_id: int, index: int, ontology: str) -&gt; bool\n</code></pre> <p>An ontology describes the concept that are described in a feature. An ontology is defined by an URL where the information is provided. Adds an ontology (URL) to a given dataset feature (defined by a dataset id and index). The dataset has to exists on OpenML and needs to have been processed by the evaluation engine.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.data_feature_add_ontology--parameters","title":"Parameters","text":"<p>data_id : int     id of the dataset to which the feature belongs index : int     index of the feature in dataset (0-based) ontology : str     URL to ontology (max. 256 characters)</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.data_feature_add_ontology--returns","title":"Returns","text":"<p>True or throws an OpenML server exception</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def data_feature_add_ontology(data_id: int, index: int, ontology: str) -&gt; bool:\n    \"\"\"\n    An ontology describes the concept that are described in a feature. An\n    ontology is defined by an URL where the information is provided. Adds\n    an ontology (URL) to a given dataset feature (defined by a dataset id\n    and index). The dataset has to exists on OpenML and needs to have been\n    processed by the evaluation engine.\n\n    Parameters\n    ----------\n    data_id : int\n        id of the dataset to which the feature belongs\n    index : int\n        index of the feature in dataset (0-based)\n    ontology : str\n        URL to ontology (max. 256 characters)\n\n    Returns\n    -------\n    True or throws an OpenML server exception\n    \"\"\"\n    upload_data: dict[str, int | str] = {\"data_id\": data_id, \"index\": index, \"ontology\": ontology}\n    openml._api_calls._perform_api_call(\"data/feature/ontology/add\", \"post\", data=upload_data)\n    # an error will be thrown in case the request was unsuccessful\n    return True\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.data_feature_remove_ontology","title":"data_feature_remove_ontology","text":"<pre><code>data_feature_remove_ontology(data_id: int, index: int, ontology: str) -&gt; bool\n</code></pre> <p>Removes an existing ontology (URL) from a given dataset feature (defined by a dataset id and index). The dataset has to exists on OpenML and needs to have been processed by the evaluation engine. Ontology needs to be attached to the specific fearure.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.data_feature_remove_ontology--parameters","title":"Parameters","text":"<p>data_id : int     id of the dataset to which the feature belongs index : int     index of the feature in dataset (0-based) ontology : str     URL to ontology (max. 256 characters)</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.data_feature_remove_ontology--returns","title":"Returns","text":"<p>True or throws an OpenML server exception</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def data_feature_remove_ontology(data_id: int, index: int, ontology: str) -&gt; bool:\n    \"\"\"\n    Removes an existing ontology (URL) from a given dataset feature (defined\n    by a dataset id and index). The dataset has to exists on OpenML and needs\n    to have been processed by the evaluation engine. Ontology needs to be\n    attached to the specific fearure.\n\n    Parameters\n    ----------\n    data_id : int\n        id of the dataset to which the feature belongs\n    index : int\n        index of the feature in dataset (0-based)\n    ontology : str\n        URL to ontology (max. 256 characters)\n\n    Returns\n    -------\n    True or throws an OpenML server exception\n    \"\"\"\n    upload_data: dict[str, int | str] = {\"data_id\": data_id, \"index\": index, \"ontology\": ontology}\n    openml._api_calls._perform_api_call(\"data/feature/ontology/remove\", \"post\", data=upload_data)\n    # an error will be thrown in case the request was unsuccessful\n    return True\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.delete_dataset","title":"delete_dataset","text":"<pre><code>delete_dataset(dataset_id: int) -&gt; bool\n</code></pre> <p>Delete dataset with id <code>dataset_id</code> from the OpenML server.</p> <p>This can only be done if you are the owner of the dataset and no tasks are attached to the dataset.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.delete_dataset--parameters","title":"Parameters","text":"<p>dataset_id : int     OpenML id of the dataset</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.delete_dataset--returns","title":"Returns","text":"<p>bool     True if the deletion was successful. False otherwise.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def delete_dataset(dataset_id: int) -&gt; bool:\n    \"\"\"Delete dataset with id `dataset_id` from the OpenML server.\n\n    This can only be done if you are the owner of the dataset and\n    no tasks are attached to the dataset.\n\n    Parameters\n    ----------\n    dataset_id : int\n        OpenML id of the dataset\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"data\", dataset_id)\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.edit_dataset","title":"edit_dataset","text":"<pre><code>edit_dataset(data_id: int, description: str | None = None, creator: str | None = None, contributor: str | None = None, collection_date: str | None = None, language: str | None = None, default_target_attribute: str | None = None, ignore_attribute: str | list[str] | None = None, citation: str | None = None, row_id_attribute: str | None = None, original_data_url: str | None = None, paper_url: str | None = None) -&gt; int\n</code></pre> <p>Edits an OpenMLDataset.</p> <p>In addition to providing the dataset id of the dataset to edit (through data_id), you must specify a value for at least one of the optional function arguments, i.e. one value for a field to edit.</p> <p>This function allows editing of both non-critical and critical fields. Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.</p> <ul> <li>Editing non-critical data fields is allowed for all authenticated users.</li> <li>Editing critical fields is allowed only for the owner, provided there are no tasks    associated with this dataset.</li> </ul> <p>If dataset has tasks or if the user is not the owner, the only way to edit critical fields is to use fork_dataset followed by edit_dataset.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.edit_dataset--parameters","title":"Parameters","text":"<p>data_id : int     ID of the dataset. description : str     Description of the dataset. creator : str     The person who created the dataset. contributor : str     People who contributed to the current version of the dataset. collection_date : str     The date the data was originally collected, given by the uploader. language : str     Language in which the data is represented.     Starts with 1 upper case letter, rest lower case, e.g. 'English'. default_target_attribute : str     The default target attribute, if it exists.     Can have multiple values, comma separated. ignore_attribute : str | list     Attributes that should be excluded in modelling,     such as identifiers and indexes. citation : str     Reference(s) that should be cited when building on this data. row_id_attribute : str, optional     The attribute that represents the row-id column, if present in the     dataset. If <code>data</code> is a dataframe and <code>row_id_attribute</code> is not     specified, the index of the dataframe will be used as the     <code>row_id_attribute</code>. If the name of the index is <code>None</code>, it will     be discarded.</p> <pre><code>.. versionadded: 0.8\n    Inference of ``row_id_attribute`` from a dataframe.\n</code></pre> <p>original_data_url : str, optional     For derived data, the url to the original dataset. paper_url : str, optional     Link to a paper describing the dataset.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.edit_dataset--returns","title":"Returns","text":"<p>Dataset id</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def edit_dataset(\n    data_id: int,\n    description: str | None = None,\n    creator: str | None = None,\n    contributor: str | None = None,\n    collection_date: str | None = None,\n    language: str | None = None,\n    default_target_attribute: str | None = None,\n    ignore_attribute: str | list[str] | None = None,\n    citation: str | None = None,\n    row_id_attribute: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n) -&gt; int:\n    \"\"\"Edits an OpenMLDataset.\n\n    In addition to providing the dataset id of the dataset to edit (through data_id),\n    you must specify a value for at least one of the optional function arguments,\n    i.e. one value for a field to edit.\n\n    This function allows editing of both non-critical and critical fields.\n    Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.\n\n     - Editing non-critical data fields is allowed for all authenticated users.\n     - Editing critical fields is allowed only for the owner, provided there are no tasks\n       associated with this dataset.\n\n    If dataset has tasks or if the user is not the owner, the only way\n    to edit critical fields is to use fork_dataset followed by edit_dataset.\n\n    Parameters\n    ----------\n    data_id : int\n        ID of the dataset.\n    description : str\n        Description of the dataset.\n    creator : str\n        The person who created the dataset.\n    contributor : str\n        People who contributed to the current version of the dataset.\n    collection_date : str\n        The date the data was originally collected, given by the uploader.\n    language : str\n        Language in which the data is represented.\n        Starts with 1 upper case letter, rest lower case, e.g. 'English'.\n    default_target_attribute : str\n        The default target attribute, if it exists.\n        Can have multiple values, comma separated.\n    ignore_attribute : str | list\n        Attributes that should be excluded in modelling,\n        such as identifiers and indexes.\n    citation : str\n        Reference(s) that should be cited when building on this data.\n    row_id_attribute : str, optional\n        The attribute that represents the row-id column, if present in the\n        dataset. If ``data`` is a dataframe and ``row_id_attribute`` is not\n        specified, the index of the dataframe will be used as the\n        ``row_id_attribute``. If the name of the index is ``None``, it will\n        be discarded.\n\n        .. versionadded: 0.8\n            Inference of ``row_id_attribute`` from a dataframe.\n    original_data_url : str, optional\n        For derived data, the url to the original dataset.\n    paper_url : str, optional\n        Link to a paper describing the dataset.\n\n    Returns\n    -------\n    Dataset id\n    \"\"\"\n    if not isinstance(data_id, int):\n        raise TypeError(f\"`data_id` must be of type `int`, not {type(data_id)}.\")\n\n    # compose data edit parameters as xml\n    form_data = {\"data_id\": data_id}  # type: openml._api_calls.DATA_TYPE\n    xml = OrderedDict()  # type: 'OrderedDict[str, OrderedDict]'\n    xml[\"oml:data_edit_parameters\"] = OrderedDict()\n    xml[\"oml:data_edit_parameters\"][\"@xmlns:oml\"] = \"http://openml.org/openml\"\n    xml[\"oml:data_edit_parameters\"][\"oml:description\"] = description\n    xml[\"oml:data_edit_parameters\"][\"oml:creator\"] = creator\n    xml[\"oml:data_edit_parameters\"][\"oml:contributor\"] = contributor\n    xml[\"oml:data_edit_parameters\"][\"oml:collection_date\"] = collection_date\n    xml[\"oml:data_edit_parameters\"][\"oml:language\"] = language\n    xml[\"oml:data_edit_parameters\"][\"oml:default_target_attribute\"] = default_target_attribute\n    xml[\"oml:data_edit_parameters\"][\"oml:row_id_attribute\"] = row_id_attribute\n    xml[\"oml:data_edit_parameters\"][\"oml:ignore_attribute\"] = ignore_attribute\n    xml[\"oml:data_edit_parameters\"][\"oml:citation\"] = citation\n    xml[\"oml:data_edit_parameters\"][\"oml:original_data_url\"] = original_data_url\n    xml[\"oml:data_edit_parameters\"][\"oml:paper_url\"] = paper_url\n\n    # delete None inputs\n    for k in list(xml[\"oml:data_edit_parameters\"]):\n        if not xml[\"oml:data_edit_parameters\"][k]:\n            del xml[\"oml:data_edit_parameters\"][k]\n\n    file_elements = {\n        \"edit_parameters\": (\"description.xml\", xmltodict.unparse(xml)),\n    }  # type: openml._api_calls.FILE_ELEMENTS_TYPE\n    result_xml = openml._api_calls._perform_api_call(\n        \"data/edit\",\n        \"post\",\n        data=form_data,\n        file_elements=file_elements,\n    )\n    result = xmltodict.parse(result_xml)\n    data_id = result[\"oml:data_edit\"][\"oml:id\"]\n    return int(data_id)\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.fork_dataset","title":"fork_dataset","text":"<pre><code>fork_dataset(data_id: int) -&gt; int\n</code></pre> <p>Creates a new dataset version, with the authenticated user as the new owner.  The forked dataset can have distinct dataset meta-data,  but the actual data itself is shared with the original version.</p> <p>This API is intended for use when a user is unable to edit the critical fields of a dataset  through the edit_dataset API.  (Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.)</p> <p>Specifically, this happens when the user is:         1. Not the owner of the dataset.         2. User is the owner of the dataset, but the dataset has tasks.</p> <p>In these two cases the only way to edit critical fields is:         1. STEP 1: Fork the dataset using fork_dataset API         2. STEP 2: Call edit_dataset API on the forked version.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.fork_dataset--parameters","title":"Parameters","text":"<p>data_id : int     id of the dataset to be forked</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.fork_dataset--returns","title":"Returns","text":"<p>Dataset id of the forked dataset</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def fork_dataset(data_id: int) -&gt; int:\n    \"\"\"\n     Creates a new dataset version, with the authenticated user as the new owner.\n     The forked dataset can have distinct dataset meta-data,\n     but the actual data itself is shared with the original version.\n\n     This API is intended for use when a user is unable to edit the critical fields of a dataset\n     through the edit_dataset API.\n     (Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.)\n\n     Specifically, this happens when the user is:\n            1. Not the owner of the dataset.\n            2. User is the owner of the dataset, but the dataset has tasks.\n\n     In these two cases the only way to edit critical fields is:\n            1. STEP 1: Fork the dataset using fork_dataset API\n            2. STEP 2: Call edit_dataset API on the forked version.\n\n\n    Parameters\n    ----------\n    data_id : int\n        id of the dataset to be forked\n\n    Returns\n    -------\n    Dataset id of the forked dataset\n\n    \"\"\"\n    if not isinstance(data_id, int):\n        raise TypeError(f\"`data_id` must be of type `int`, not {type(data_id)}.\")\n    # compose data fork parameters\n    form_data = {\"data_id\": data_id}  # type: openml._api_calls.DATA_TYPE\n    result_xml = openml._api_calls._perform_api_call(\"data/fork\", \"post\", data=form_data)\n    result = xmltodict.parse(result_xml)\n    data_id = result[\"oml:data_fork\"][\"oml:id\"]\n    return int(data_id)\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(dataset_id: int | str, download_data: bool = False, version: int | None = None, error_if_multiple: bool = False, cache_format: Literal['pickle', 'feather'] = 'pickle', download_qualities: bool = False, download_features_meta_data: bool = False, download_all_files: bool = False, force_refresh_cache: bool = False) -&gt; OpenMLDataset\n</code></pre> <p>Download the OpenML dataset representation, optionally also download actual data file.</p> <p>This function is by default NOT thread/multiprocessing safe, as this function uses caching. A check will be performed to determine if the information has previously been downloaded to a cache, and if so be loaded from disk instead of retrieved from the server.</p> <p>To make this function thread safe, you can install the python package <code>oslo.concurrency</code>. If <code>oslo.concurrency</code> is installed <code>get_dataset</code> becomes thread safe.</p> <p>Alternatively, to make this function thread/multiprocessing safe initialize the cache first by calling <code>get_dataset(args)</code> once before calling <code>get_dataset(args)</code> many times in parallel. This will initialize the cache and later calls will use the cache in a thread/multiprocessing safe way.</p> <p>If dataset is retrieved by name, a version may be specified. If no version is specified and multiple versions of the dataset exist, the earliest version of the dataset that is still active will be returned. If no version is specified, multiple versions of the dataset exist and <code>exception_if_multiple</code> is set to <code>True</code>, this function will raise an exception.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.get_dataset--parameters","title":"Parameters","text":"<p>dataset_id : int or str     Dataset ID (integer) or dataset name (string) of the dataset to download. download_data : bool (default=False)     If True, also download the data file. Beware that some datasets are large and it might     make the operation noticeably slower. Metadata is also still retrieved.     If False, create the OpenMLDataset and only populate it with the metadata.     The data may later be retrieved through the <code>OpenMLDataset.get_data</code> method. version : int, optional (default=None)     Specifies the version if <code>dataset_id</code> is specified by name.     If no version is specified, retrieve the least recent still active version. error_if_multiple : bool (default=False)     If <code>True</code> raise an error if multiple datasets are found with matching criteria. cache_format : str (default='pickle') in {'pickle', 'feather'}     Format for caching the dataset - may be feather or pickle     Note that the default 'pickle' option may load slower than feather when     no.of.rows is very high. download_qualities : bool (default=False)     Option to download 'qualities' meta-data in addition to the minimal dataset description.     If True, download and cache the qualities file.     If False, create the OpenMLDataset without qualities metadata. The data may later be added     to the OpenMLDataset through the <code>OpenMLDataset.load_metadata(qualities=True)</code> method. download_features_meta_data : bool (default=False)     Option to download 'features' meta-data in addition to the minimal dataset description.     If True, download and cache the features file.     If False, create the OpenMLDataset without features metadata. The data may later be added     to the OpenMLDataset through the <code>OpenMLDataset.load_metadata(features=True)</code> method. download_all_files: bool (default=False)     EXPERIMENTAL. Download all files related to the dataset that reside on the server.     Useful for datasets which refer to auxiliary files (e.g., meta-album). force_refresh_cache : bool (default=False)     Force the cache to refreshed by deleting the cache directory and re-downloading the data.     Note, if <code>force_refresh_cache</code> is True, <code>get_dataset</code> is NOT thread/multiprocessing safe,     because this creates a race condition to creating and deleting the cache; as in general with     the cache.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.get_dataset--returns","title":"Returns","text":"<p>dataset : :class:<code>openml.OpenMLDataset</code>     The downloaded dataset.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_dataset(  # noqa: C901, PLR0912\n    dataset_id: int | str,\n    download_data: bool = False,  # noqa: FBT002, FBT001\n    version: int | None = None,\n    error_if_multiple: bool = False,  # noqa: FBT002, FBT001\n    cache_format: Literal[\"pickle\", \"feather\"] = \"pickle\",\n    download_qualities: bool = False,  # noqa: FBT002, FBT001\n    download_features_meta_data: bool = False,  # noqa: FBT002, FBT001\n    download_all_files: bool = False,  # noqa: FBT002, FBT001\n    force_refresh_cache: bool = False,  # noqa: FBT001, FBT002\n) -&gt; OpenMLDataset:\n    \"\"\"Download the OpenML dataset representation, optionally also download actual data file.\n\n    This function is by default NOT thread/multiprocessing safe, as this function uses caching.\n    A check will be performed to determine if the information has previously been downloaded to a\n    cache, and if so be loaded from disk instead of retrieved from the server.\n\n    To make this function thread safe, you can install the python package ``oslo.concurrency``.\n    If ``oslo.concurrency`` is installed `get_dataset` becomes thread safe.\n\n    Alternatively, to make this function thread/multiprocessing safe initialize the cache first by\n    calling `get_dataset(args)` once before calling `get_dataset(args)` many times in parallel.\n    This will initialize the cache and later calls will use the cache in a thread/multiprocessing\n    safe way.\n\n    If dataset is retrieved by name, a version may be specified.\n    If no version is specified and multiple versions of the dataset exist,\n    the earliest version of the dataset that is still active will be returned.\n    If no version is specified, multiple versions of the dataset exist and\n    ``exception_if_multiple`` is set to ``True``, this function will raise an exception.\n\n    Parameters\n    ----------\n    dataset_id : int or str\n        Dataset ID (integer) or dataset name (string) of the dataset to download.\n    download_data : bool (default=False)\n        If True, also download the data file. Beware that some datasets are large and it might\n        make the operation noticeably slower. Metadata is also still retrieved.\n        If False, create the OpenMLDataset and only populate it with the metadata.\n        The data may later be retrieved through the `OpenMLDataset.get_data` method.\n    version : int, optional (default=None)\n        Specifies the version if `dataset_id` is specified by name.\n        If no version is specified, retrieve the least recent still active version.\n    error_if_multiple : bool (default=False)\n        If ``True`` raise an error if multiple datasets are found with matching criteria.\n    cache_format : str (default='pickle') in {'pickle', 'feather'}\n        Format for caching the dataset - may be feather or pickle\n        Note that the default 'pickle' option may load slower than feather when\n        no.of.rows is very high.\n    download_qualities : bool (default=False)\n        Option to download 'qualities' meta-data in addition to the minimal dataset description.\n        If True, download and cache the qualities file.\n        If False, create the OpenMLDataset without qualities metadata. The data may later be added\n        to the OpenMLDataset through the `OpenMLDataset.load_metadata(qualities=True)` method.\n    download_features_meta_data : bool (default=False)\n        Option to download 'features' meta-data in addition to the minimal dataset description.\n        If True, download and cache the features file.\n        If False, create the OpenMLDataset without features metadata. The data may later be added\n        to the OpenMLDataset through the `OpenMLDataset.load_metadata(features=True)` method.\n    download_all_files: bool (default=False)\n        EXPERIMENTAL. Download all files related to the dataset that reside on the server.\n        Useful for datasets which refer to auxiliary files (e.g., meta-album).\n    force_refresh_cache : bool (default=False)\n        Force the cache to refreshed by deleting the cache directory and re-downloading the data.\n        Note, if `force_refresh_cache` is True, `get_dataset` is NOT thread/multiprocessing safe,\n        because this creates a race condition to creating and deleting the cache; as in general with\n        the cache.\n\n    Returns\n    -------\n    dataset : :class:`openml.OpenMLDataset`\n        The downloaded dataset.\n    \"\"\"\n    if download_all_files:\n        warnings.warn(\n            \"``download_all_files`` is experimental and is likely to break with new releases.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n    if cache_format not in [\"feather\", \"pickle\"]:\n        raise ValueError(\n            \"cache_format must be one of 'feather' or 'pickle. \"\n            f\"Invalid format specified: {cache_format}\",\n        )\n\n    if isinstance(dataset_id, str):\n        try:\n            dataset_id = int(dataset_id)\n        except ValueError:\n            dataset_id = _name_to_id(dataset_id, version, error_if_multiple)  # type: ignore\n    elif not isinstance(dataset_id, int):\n        raise TypeError(\n            f\"`dataset_id` must be one of `str` or `int`, not {type(dataset_id)}.\",\n        )\n\n    if force_refresh_cache:\n        did_cache_dir = _get_cache_dir_for_id(DATASETS_CACHE_DIR_NAME, dataset_id)\n        if did_cache_dir.exists():\n            _remove_cache_dir_for_id(DATASETS_CACHE_DIR_NAME, did_cache_dir)\n\n    did_cache_dir = _create_cache_directory_for_id(\n        DATASETS_CACHE_DIR_NAME,\n        dataset_id,\n    )\n\n    remove_dataset_cache = True\n    try:\n        description = _get_dataset_description(did_cache_dir, dataset_id)\n        features_file = None\n        qualities_file = None\n\n        if download_features_meta_data:\n            features_file = _get_dataset_features_file(did_cache_dir, dataset_id)\n        if download_qualities:\n            qualities_file = _get_dataset_qualities_file(did_cache_dir, dataset_id)\n\n        parquet_file = None\n        skip_parquet = os.environ.get(OPENML_SKIP_PARQUET_ENV_VAR, \"false\").casefold() == \"true\"\n        download_parquet = \"oml:parquet_url\" in description and not skip_parquet\n        if download_parquet and (download_data or download_all_files):\n            try:\n                parquet_file = _get_dataset_parquet(\n                    description,\n                    download_all_files=download_all_files,\n                )\n            except urllib3.exceptions.MaxRetryError:\n                parquet_file = None\n\n        arff_file = None\n        if parquet_file is None and download_data:\n            if download_parquet:\n                logger.warning(\"Failed to download parquet, fallback on ARFF.\")\n            arff_file = _get_dataset_arff(description)\n\n        remove_dataset_cache = False\n    except OpenMLServerException as e:\n        # if there was an exception\n        # check if the user had access to the dataset\n        if e.code == NO_ACCESS_GRANTED_ERRCODE:\n            raise OpenMLPrivateDatasetError(e.message) from None\n\n        raise e\n    finally:\n        if remove_dataset_cache:\n            _remove_cache_dir_for_id(DATASETS_CACHE_DIR_NAME, did_cache_dir)\n\n    return _create_dataset_from_description(\n        description,\n        features_file,\n        qualities_file,\n        arff_file,\n        parquet_file,\n        cache_format,\n    )\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.get_datasets","title":"get_datasets","text":"<pre><code>get_datasets(dataset_ids: list[str | int], download_data: bool = False, download_qualities: bool = False) -&gt; list[OpenMLDataset]\n</code></pre> <p>Download datasets.</p> <p>This function iterates :meth:<code>openml.datasets.get_dataset</code>.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.get_datasets--parameters","title":"Parameters","text":"<p>dataset_ids : iterable     Integers or strings representing dataset ids or dataset names.     If dataset names are specified, the least recent still active dataset version is returned. download_data : bool, optional     If True, also download the data file. Beware that some datasets are large and it might     make the operation noticeably slower. Metadata is also still retrieved.     If False, create the OpenMLDataset and only populate it with the metadata.     The data may later be retrieved through the <code>OpenMLDataset.get_data</code> method. download_qualities : bool, optional (default=True)     If True, also download qualities.xml file. If False it skip the qualities.xml.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.get_datasets--returns","title":"Returns","text":"<p>datasets : list of datasets     A list of dataset objects.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def get_datasets(\n    dataset_ids: list[str | int],\n    download_data: bool = False,  # noqa: FBT001, FBT002\n    download_qualities: bool = False,  # noqa: FBT001, FBT002\n) -&gt; list[OpenMLDataset]:\n    \"\"\"Download datasets.\n\n    This function iterates :meth:`openml.datasets.get_dataset`.\n\n    Parameters\n    ----------\n    dataset_ids : iterable\n        Integers or strings representing dataset ids or dataset names.\n        If dataset names are specified, the least recent still active dataset version is returned.\n    download_data : bool, optional\n        If True, also download the data file. Beware that some datasets are large and it might\n        make the operation noticeably slower. Metadata is also still retrieved.\n        If False, create the OpenMLDataset and only populate it with the metadata.\n        The data may later be retrieved through the `OpenMLDataset.get_data` method.\n    download_qualities : bool, optional (default=True)\n        If True, also download qualities.xml file. If False it skip the qualities.xml.\n\n    Returns\n    -------\n    datasets : list of datasets\n        A list of dataset objects.\n    \"\"\"\n    datasets = []\n    for dataset_id in dataset_ids:\n        datasets.append(\n            get_dataset(dataset_id, download_data, download_qualities=download_qualities),\n        )\n    return datasets\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.list_datasets","title":"list_datasets","text":"<pre><code>list_datasets(data_id: list[int] | None = None, offset: int | None = None, size: int | None = None, status: str | None = None, tag: str | None = None, data_name: str | None = None, data_version: int | None = None, number_instances: int | str | None = None, number_features: int | str | None = None, number_classes: int | str | None = None, number_missing_values: int | str | None = None) -&gt; DataFrame\n</code></pre> <p>Return a dataframe of all dataset which are on OpenML.</p> <p>Supports large amount of results.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.list_datasets--parameters","title":"Parameters","text":"<p>data_id : list, optional     A list of data ids, to specify which datasets should be     listed offset : int, optional     The number of datasets to skip, starting from the first. size : int, optional     The maximum number of datasets to show. status : str, optional     Should be {active, in_preparation, deactivated}. By     default active datasets are returned, but also datasets     from another status can be requested. tag : str, optional data_name : str, optional data_version : int, optional number_instances : int | str, optional number_features : int | str, optional number_classes : int | str, optional number_missing_values : int | str, optional</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.list_datasets--returns","title":"Returns","text":"<p>datasets: dataframe     Each row maps to a dataset     Each column contains the following information:     - dataset id     - name     - format     - status     If qualities are calculated for the dataset, some of     these are also included as columns.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def list_datasets(\n    data_id: list[int] | None = None,\n    offset: int | None = None,\n    size: int | None = None,\n    status: str | None = None,\n    tag: str | None = None,\n    data_name: str | None = None,\n    data_version: int | None = None,\n    number_instances: int | str | None = None,\n    number_features: int | str | None = None,\n    number_classes: int | str | None = None,\n    number_missing_values: int | str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Return a dataframe of all dataset which are on OpenML.\n\n    Supports large amount of results.\n\n    Parameters\n    ----------\n    data_id : list, optional\n        A list of data ids, to specify which datasets should be\n        listed\n    offset : int, optional\n        The number of datasets to skip, starting from the first.\n    size : int, optional\n        The maximum number of datasets to show.\n    status : str, optional\n        Should be {active, in_preparation, deactivated}. By\n        default active datasets are returned, but also datasets\n        from another status can be requested.\n    tag : str, optional\n    data_name : str, optional\n    data_version : int, optional\n    number_instances : int | str, optional\n    number_features : int | str, optional\n    number_classes : int | str, optional\n    number_missing_values : int | str, optional\n\n    Returns\n    -------\n    datasets: dataframe\n        Each row maps to a dataset\n        Each column contains the following information:\n        - dataset id\n        - name\n        - format\n        - status\n        If qualities are calculated for the dataset, some of\n        these are also included as columns.\n    \"\"\"\n    listing_call = partial(\n        _list_datasets,\n        data_id=data_id,\n        status=status,\n        tag=tag,\n        data_name=data_name,\n        data_version=data_version,\n        number_instances=number_instances,\n        number_features=number_features,\n        number_classes=number_classes,\n        number_missing_values=number_missing_values,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.list_qualities","title":"list_qualities","text":"<pre><code>list_qualities() -&gt; list[str]\n</code></pre> <p>Return list of data qualities available.</p> <p>The function performs an API call to retrieve the entire list of data qualities that are computed on the datasets uploaded.</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.list_qualities--returns","title":"Returns","text":"<p>list</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def list_qualities() -&gt; list[str]:\n    \"\"\"Return list of data qualities available.\n\n    The function performs an API call to retrieve the entire list of\n    data qualities that are computed on the datasets uploaded.\n\n    Returns\n    -------\n    list\n    \"\"\"\n    api_call = \"data/qualities/list\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    qualities = xmltodict.parse(xml_string, force_list=(\"oml:quality\"))\n    # Minimalistic check if the XML is useful\n    if \"oml:data_qualities_list\" not in qualities:\n        raise ValueError('Error in return XML, does not contain \"oml:data_qualities_list\"')\n\n    if not isinstance(qualities[\"oml:data_qualities_list\"][\"oml:quality\"], list):\n        raise TypeError('Error in return XML, does not contain \"oml:quality\" as a list')\n\n    return qualities[\"oml:data_qualities_list\"][\"oml:quality\"]\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.status_update","title":"status_update","text":"<pre><code>status_update(data_id: int, status: Literal['active', 'deactivated']) -&gt; None\n</code></pre> <p>Updates the status of a dataset to either 'active' or 'deactivated'. Please see the OpenML API documentation for a description of the status and all legal status transitions: docs.openml.org/concepts/data/#dataset-status</p>"},{"location":"reference/datasets/functions/#openml.datasets.functions.status_update--parameters","title":"Parameters","text":"<p>data_id : int     The data id of the dataset status : str,     'active' or 'deactivated'</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def status_update(data_id: int, status: Literal[\"active\", \"deactivated\"]) -&gt; None:\n    \"\"\"\n    Updates the status of a dataset to either 'active' or 'deactivated'.\n    Please see the OpenML API documentation for a description of the status\n    and all legal status transitions:\n    https://docs.openml.org/concepts/data/#dataset-status\n\n    Parameters\n    ----------\n    data_id : int\n        The data id of the dataset\n    status : str,\n        'active' or 'deactivated'\n    \"\"\"\n    legal_status = {\"active\", \"deactivated\"}\n    if status not in legal_status:\n        raise ValueError(f\"Illegal status value. Legal values: {legal_status}\")\n\n    data: openml._api_calls.DATA_TYPE = {\"data_id\": data_id, \"status\": status}\n    result_xml = openml._api_calls._perform_api_call(\"data/status/update\", \"post\", data=data)\n    result = xmltodict.parse(result_xml)\n    server_data_id = result[\"oml:data_status_update\"][\"oml:id\"]\n    server_status = result[\"oml:data_status_update\"][\"oml:status\"]\n    if status != server_status or int(data_id) != int(server_data_id):\n        # This should never happen\n        raise ValueError(\"Data id/status does not collide\")\n</code></pre>"},{"location":"reference/evaluations/","title":"evaluations","text":""},{"location":"reference/evaluations/#openml.evaluations","title":"openml.evaluations","text":""},{"location":"reference/evaluations/#openml.evaluations.OpenMLEvaluation","title":"OpenMLEvaluation","text":"<pre><code>OpenMLEvaluation(run_id: int, task_id: int, setup_id: int, flow_id: int, flow_name: str, data_id: int, data_name: str, function: str, upload_time: str, uploader: int, uploader_name: str, value: float | None, values: list[float] | None, array_data: str | None = None)\n</code></pre> <p>Contains all meta-information about a run / evaluation combination, according to the evaluation/list function</p>"},{"location":"reference/evaluations/#openml.evaluations.OpenMLEvaluation--parameters","title":"Parameters","text":"<p>run_id : int     Refers to the run. task_id : int     Refers to the task. setup_id : int     Refers to the setup. flow_id : int     Refers to the flow. flow_name : str     Name of the referred flow. data_id : int     Refers to the dataset. data_name : str     The name of the dataset. function : str     The evaluation metric of this item (e.g., accuracy). upload_time : str     The time of evaluation. uploader: int     Uploader ID (user ID) upload_name : str     Name of the uploader of this evaluation value : float     The value (score) of this evaluation. values : List[float]     The values (scores) per repeat and fold (if requested) array_data : str     list of information per class.     (e.g., in case of precision, auroc, recall)</p> Source code in <code>openml/evaluations/evaluation.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    run_id: int,\n    task_id: int,\n    setup_id: int,\n    flow_id: int,\n    flow_name: str,\n    data_id: int,\n    data_name: str,\n    function: str,\n    upload_time: str,\n    uploader: int,\n    uploader_name: str,\n    value: float | None,\n    values: list[float] | None,\n    array_data: str | None = None,\n):\n    self.run_id = run_id\n    self.task_id = task_id\n    self.setup_id = setup_id\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.data_id = data_id\n    self.data_name = data_name\n    self.function = function\n    self.upload_time = upload_time\n    self.uploader = uploader\n    self.uploader_name = uploader_name\n    self.value = value\n    self.values = values\n    self.array_data = array_data\n</code></pre>"},{"location":"reference/evaluations/#openml.evaluations.list_evaluation_measures","title":"list_evaluation_measures","text":"<pre><code>list_evaluation_measures() -&gt; list[str]\n</code></pre> <p>Return list of evaluation measures available.</p> <p>The function performs an API call to retrieve the entire list of evaluation measures that are available.</p>"},{"location":"reference/evaluations/#openml.evaluations.list_evaluation_measures--returns","title":"Returns","text":"<p>list</p> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_evaluation_measures() -&gt; list[str]:\n    \"\"\"Return list of evaluation measures available.\n\n    The function performs an API call to retrieve the entire list of\n    evaluation measures that are available.\n\n    Returns\n    -------\n    list\n\n    \"\"\"\n    api_call = \"evaluationmeasure/list\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    qualities = xmltodict.parse(xml_string, force_list=(\"oml:measures\"))\n    # Minimalistic check if the XML is useful\n    if \"oml:evaluation_measures\" not in qualities:\n        raise ValueError('Error in return XML, does not contain \"oml:evaluation_measures\"')\n\n    if not isinstance(qualities[\"oml:evaluation_measures\"][\"oml:measures\"][0][\"oml:measure\"], list):\n        raise TypeError('Error in return XML, does not contain \"oml:measure\" as a list')\n\n    return qualities[\"oml:evaluation_measures\"][\"oml:measures\"][0][\"oml:measure\"]\n</code></pre>"},{"location":"reference/evaluations/#openml.evaluations.list_evaluations","title":"list_evaluations","text":"<pre><code>list_evaluations(function: str, offset: int | None = None, size: int | None = None, tasks: list[str | int] | None = None, setups: list[str | int] | None = None, flows: list[str | int] | None = None, runs: list[str | int] | None = None, uploaders: list[str | int] | None = None, tag: str | None = None, study: int | None = None, per_fold: bool | None = None, sort_order: str | None = None, output_format: Literal['object', 'dataframe'] = 'object') -&gt; dict[int, OpenMLEvaluation] | DataFrame\n</code></pre> <p>List all run-evaluation pairs matching all of the given filters.</p> <p>(Supports large amount of results)</p>"},{"location":"reference/evaluations/#openml.evaluations.list_evaluations--parameters","title":"Parameters","text":"<p>function : str     the evaluation function. e.g., predictive_accuracy offset : int, optional     the number of runs to skip, starting from the first size : int, default 10000     The maximum number of runs to show.     If set to <code>None</code>, it returns all the results.</p> list[int,str], optional <p>the list of task IDs</p> <p>setups: list[int,str], optional     the list of setup IDs flows : list[int,str], optional     the list of flow IDs runs :list[int,str], optional     the list of run IDs uploaders : list[int,str], optional     the list of uploader IDs tag : str, optional     filter evaluation based on given tag</p> <p>study : int, optional</p> <p>per_fold : bool, optional</p> str, optional <p>order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")</p> str, optional (default='object') <p>The parameter decides the format of the output. - If 'object' the output is a dict of OpenMLEvaluation objects - If 'dataframe' the output is a pandas DataFrame</p>"},{"location":"reference/evaluations/#openml.evaluations.list_evaluations--returns","title":"Returns","text":"<p>dict or dataframe</p> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_evaluations(\n    function: str,\n    offset: int | None = None,\n    size: int | None = None,\n    tasks: list[str | int] | None = None,\n    setups: list[str | int] | None = None,\n    flows: list[str | int] | None = None,\n    runs: list[str | int] | None = None,\n    uploaders: list[str | int] | None = None,\n    tag: str | None = None,\n    study: int | None = None,\n    per_fold: bool | None = None,\n    sort_order: str | None = None,\n    output_format: Literal[\"object\", \"dataframe\"] = \"object\",\n) -&gt; dict[int, OpenMLEvaluation] | pd.DataFrame:\n    \"\"\"List all run-evaluation pairs matching all of the given filters.\n\n    (Supports large amount of results)\n\n    Parameters\n    ----------\n    function : str\n        the evaluation function. e.g., predictive_accuracy\n    offset : int, optional\n        the number of runs to skip, starting from the first\n    size : int, default 10000\n        The maximum number of runs to show.\n        If set to ``None``, it returns all the results.\n\n    tasks : list[int,str], optional\n        the list of task IDs\n    setups: list[int,str], optional\n        the list of setup IDs\n    flows : list[int,str], optional\n        the list of flow IDs\n    runs :list[int,str], optional\n        the list of run IDs\n    uploaders : list[int,str], optional\n        the list of uploader IDs\n    tag : str, optional\n        filter evaluation based on given tag\n\n    study : int, optional\n\n    per_fold : bool, optional\n\n    sort_order : str, optional\n       order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")\n\n    output_format: str, optional (default='object')\n        The parameter decides the format of the output.\n        - If 'object' the output is a dict of OpenMLEvaluation objects\n        - If 'dataframe' the output is a pandas DataFrame\n\n    Returns\n    -------\n    dict or dataframe\n    \"\"\"\n    if output_format not in (\"dataframe\", \"object\"):\n        raise ValueError(\"Invalid output format. Only 'object', 'dataframe'.\")\n\n    per_fold_str = None\n    if per_fold is not None:\n        per_fold_str = str(per_fold).lower()\n\n    listing_call = partial(\n        _list_evaluations,\n        function=function,\n        tasks=tasks,\n        setups=setups,\n        flows=flows,\n        runs=runs,\n        uploaders=uploaders,\n        tag=tag,\n        study=study,\n        sort_order=sort_order,\n        per_fold=per_fold_str,\n    )\n    eval_collection = openml.utils._list_all(listing_call, offset=offset, limit=size)\n\n    flattened = list(chain.from_iterable(eval_collection))\n    if output_format == \"dataframe\":\n        records = [item._to_dict() for item in flattened]\n        return pd.DataFrame.from_records(records)  # No index...\n\n    return {e.run_id: e for e in flattened}\n</code></pre>"},{"location":"reference/evaluations/#openml.evaluations.list_evaluations_setups","title":"list_evaluations_setups","text":"<pre><code>list_evaluations_setups(function: str, offset: int | None = None, size: int | None = None, tasks: list | None = None, setups: list | None = None, flows: list | None = None, runs: list | None = None, uploaders: list | None = None, tag: str | None = None, per_fold: bool | None = None, sort_order: str | None = None, parameters_in_separate_columns: bool = False) -&gt; DataFrame\n</code></pre> <p>List all run-evaluation pairs matching all of the given filters and their hyperparameter settings.</p>"},{"location":"reference/evaluations/#openml.evaluations.list_evaluations_setups--parameters","title":"Parameters","text":"<p>function : str     the evaluation function. e.g., predictive_accuracy offset : int, optional     the number of runs to skip, starting from the first size : int, optional     the maximum number of runs to show tasks : list[int], optional     the list of task IDs setups: list[int], optional     the list of setup IDs flows : list[int], optional     the list of flow IDs runs : list[int], optional     the list of run IDs uploaders : list[int], optional     the list of uploader IDs tag : str, optional     filter evaluation based on given tag per_fold : bool, optional sort_order : str, optional    order of sorting evaluations, ascending (\"asc\") or descending (\"desc\") parameters_in_separate_columns: bool, optional (default= False)     Returns hyperparameters in separate columns if set to True.     Valid only for a single flow</p>"},{"location":"reference/evaluations/#openml.evaluations.list_evaluations_setups--returns","title":"Returns","text":"<p>dataframe with hyperparameter settings as a list of tuples.</p> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_evaluations_setups(\n    function: str,\n    offset: int | None = None,\n    size: int | None = None,\n    tasks: list | None = None,\n    setups: list | None = None,\n    flows: list | None = None,\n    runs: list | None = None,\n    uploaders: list | None = None,\n    tag: str | None = None,\n    per_fold: bool | None = None,\n    sort_order: str | None = None,\n    parameters_in_separate_columns: bool = False,  # noqa: FBT001, FBT002\n) -&gt; pd.DataFrame:\n    \"\"\"List all run-evaluation pairs matching all of the given filters\n    and their hyperparameter settings.\n\n    Parameters\n    ----------\n    function : str\n        the evaluation function. e.g., predictive_accuracy\n    offset : int, optional\n        the number of runs to skip, starting from the first\n    size : int, optional\n        the maximum number of runs to show\n    tasks : list[int], optional\n        the list of task IDs\n    setups: list[int], optional\n        the list of setup IDs\n    flows : list[int], optional\n        the list of flow IDs\n    runs : list[int], optional\n        the list of run IDs\n    uploaders : list[int], optional\n        the list of uploader IDs\n    tag : str, optional\n        filter evaluation based on given tag\n    per_fold : bool, optional\n    sort_order : str, optional\n       order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")\n    parameters_in_separate_columns: bool, optional (default= False)\n        Returns hyperparameters in separate columns if set to True.\n        Valid only for a single flow\n\n    Returns\n    -------\n    dataframe with hyperparameter settings as a list of tuples.\n    \"\"\"\n    if parameters_in_separate_columns and (flows is None or len(flows) != 1):\n        raise ValueError(\"Can set parameters_in_separate_columns to true only for single flow_id\")\n\n    # List evaluations\n    evals = list_evaluations(\n        function=function,\n        offset=offset,\n        size=size,\n        runs=runs,\n        tasks=tasks,\n        setups=setups,\n        flows=flows,\n        uploaders=uploaders,\n        tag=tag,\n        per_fold=per_fold,\n        sort_order=sort_order,\n        output_format=\"dataframe\",\n    )\n    # List setups\n    # list_setups by setup id does not support large sizes (exceeds URL length limit)\n    # Hence we split the list of unique setup ids returned by list_evaluations into chunks of size N\n    _df = pd.DataFrame()\n    if len(evals) != 0:\n        N = 100  # size of section\n        uniq = np.asarray(evals[\"setup_id\"].unique())\n        length = len(uniq)\n\n        # array_split - allows indices_or_sections to not equally divide the array\n        # array_split -length % N sub-arrays of size length//N + 1 and the rest of size length//N.\n        split_size = ((length - 1) // N) + 1\n        setup_chunks = np.array_split(uniq, split_size)\n\n        setup_data = pd.DataFrame()\n        for _setups in setup_chunks:\n            result = openml.setups.list_setups(setup=_setups, output_format=\"dataframe\")\n            assert isinstance(result, pd.DataFrame)\n            result = result.drop(\"flow_id\", axis=1)\n            # concat resulting setup chunks into single datframe\n            setup_data = pd.concat([setup_data, result])\n\n        parameters = []\n        # Convert parameters of setup into dict of (hyperparameter, value)\n        for parameter_dict in setup_data[\"parameters\"]:\n            if parameter_dict is not None:\n                parameters.append(\n                    {param[\"full_name\"]: param[\"value\"] for param in parameter_dict.values()},\n                )\n            else:\n                parameters.append({})\n        setup_data[\"parameters\"] = parameters\n        # Merge setups with evaluations\n        _df = evals.merge(setup_data, on=\"setup_id\", how=\"left\")\n\n    if parameters_in_separate_columns:\n        _df = pd.concat(\n            [_df.drop(\"parameters\", axis=1), _df[\"parameters\"].apply(pd.Series)],\n            axis=1,\n        )\n\n    return _df\n</code></pre>"},{"location":"reference/evaluations/evaluation/","title":"evaluation","text":""},{"location":"reference/evaluations/evaluation/#openml.evaluations.evaluation","title":"openml.evaluations.evaluation","text":""},{"location":"reference/evaluations/evaluation/#openml.evaluations.evaluation.OpenMLEvaluation","title":"OpenMLEvaluation","text":"<pre><code>OpenMLEvaluation(run_id: int, task_id: int, setup_id: int, flow_id: int, flow_name: str, data_id: int, data_name: str, function: str, upload_time: str, uploader: int, uploader_name: str, value: float | None, values: list[float] | None, array_data: str | None = None)\n</code></pre> <p>Contains all meta-information about a run / evaluation combination, according to the evaluation/list function</p>"},{"location":"reference/evaluations/evaluation/#openml.evaluations.evaluation.OpenMLEvaluation--parameters","title":"Parameters","text":"<p>run_id : int     Refers to the run. task_id : int     Refers to the task. setup_id : int     Refers to the setup. flow_id : int     Refers to the flow. flow_name : str     Name of the referred flow. data_id : int     Refers to the dataset. data_name : str     The name of the dataset. function : str     The evaluation metric of this item (e.g., accuracy). upload_time : str     The time of evaluation. uploader: int     Uploader ID (user ID) upload_name : str     Name of the uploader of this evaluation value : float     The value (score) of this evaluation. values : List[float]     The values (scores) per repeat and fold (if requested) array_data : str     list of information per class.     (e.g., in case of precision, auroc, recall)</p> Source code in <code>openml/evaluations/evaluation.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    run_id: int,\n    task_id: int,\n    setup_id: int,\n    flow_id: int,\n    flow_name: str,\n    data_id: int,\n    data_name: str,\n    function: str,\n    upload_time: str,\n    uploader: int,\n    uploader_name: str,\n    value: float | None,\n    values: list[float] | None,\n    array_data: str | None = None,\n):\n    self.run_id = run_id\n    self.task_id = task_id\n    self.setup_id = setup_id\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.data_id = data_id\n    self.data_name = data_name\n    self.function = function\n    self.upload_time = upload_time\n    self.uploader = uploader\n    self.uploader_name = uploader_name\n    self.value = value\n    self.values = values\n    self.array_data = array_data\n</code></pre>"},{"location":"reference/evaluations/functions/","title":"functions","text":""},{"location":"reference/evaluations/functions/#openml.evaluations.functions","title":"openml.evaluations.functions","text":""},{"location":"reference/evaluations/functions/#openml.evaluations.functions.__list_evaluations","title":"__list_evaluations","text":"<pre><code>__list_evaluations(api_call: str) -&gt; list[OpenMLEvaluation]\n</code></pre> <p>Helper function to parse API calls which are lists of runs</p> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def __list_evaluations(api_call: str) -&gt; list[OpenMLEvaluation]:\n    \"\"\"Helper function to parse API calls which are lists of runs\"\"\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    evals_dict = xmltodict.parse(xml_string, force_list=(\"oml:evaluation\",))\n    # Minimalistic check if the XML is useful\n    if \"oml:evaluations\" not in evals_dict:\n        raise ValueError(\n            \"Error in return XML, does not contain \" f'\"oml:evaluations\": {evals_dict!s}',\n        )\n\n    assert isinstance(evals_dict[\"oml:evaluations\"][\"oml:evaluation\"], list), type(\n        evals_dict[\"oml:evaluations\"],\n    )\n\n    uploader_ids = list(\n        {eval_[\"oml:uploader\"] for eval_ in evals_dict[\"oml:evaluations\"][\"oml:evaluation\"]},\n    )\n    api_users = \"user/list/user_id/\" + \",\".join(uploader_ids)\n    xml_string_user = openml._api_calls._perform_api_call(api_users, \"get\")\n\n    users = xmltodict.parse(xml_string_user, force_list=(\"oml:user\",))\n    user_dict = {user[\"oml:id\"]: user[\"oml:username\"] for user in users[\"oml:users\"][\"oml:user\"]}\n\n    evals = []\n    for eval_ in evals_dict[\"oml:evaluations\"][\"oml:evaluation\"]:\n        run_id = int(eval_[\"oml:run_id\"])\n        value = float(eval_[\"oml:value\"]) if \"oml:value\" in eval_ else None\n        values = json.loads(eval_[\"oml:values\"]) if eval_.get(\"oml:values\", None) else None\n        array_data = eval_.get(\"oml:array_data\")\n\n        evals.append(\n            OpenMLEvaluation(\n                run_id=run_id,\n                task_id=int(eval_[\"oml:task_id\"]),\n                setup_id=int(eval_[\"oml:setup_id\"]),\n                flow_id=int(eval_[\"oml:flow_id\"]),\n                flow_name=eval_[\"oml:flow_name\"],\n                data_id=int(eval_[\"oml:data_id\"]),\n                data_name=eval_[\"oml:data_name\"],\n                function=eval_[\"oml:function\"],\n                upload_time=eval_[\"oml:upload_time\"],\n                uploader=int(eval_[\"oml:uploader\"]),\n                uploader_name=user_dict[eval_[\"oml:uploader\"]],\n                value=value,\n                values=values,\n                array_data=array_data,\n            )\n        )\n\n    return evals\n</code></pre>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_estimation_procedures","title":"list_estimation_procedures","text":"<pre><code>list_estimation_procedures() -&gt; list[str]\n</code></pre> <p>Return list of evaluation procedures available.</p> <p>The function performs an API call to retrieve the entire list of evaluation procedures' names that are available.</p>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_estimation_procedures--returns","title":"Returns","text":"<p>list</p> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_estimation_procedures() -&gt; list[str]:\n    \"\"\"Return list of evaluation procedures available.\n\n    The function performs an API call to retrieve the entire list of\n    evaluation procedures' names that are available.\n\n    Returns\n    -------\n    list\n    \"\"\"\n    api_call = \"estimationprocedure/list\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    api_results = xmltodict.parse(xml_string)\n\n    # Minimalistic check if the XML is useful\n    if \"oml:estimationprocedures\" not in api_results:\n        raise ValueError('Error in return XML, does not contain \"oml:estimationprocedures\"')\n\n    if \"oml:estimationprocedure\" not in api_results[\"oml:estimationprocedures\"]:\n        raise ValueError('Error in return XML, does not contain \"oml:estimationprocedure\"')\n\n    if not isinstance(api_results[\"oml:estimationprocedures\"][\"oml:estimationprocedure\"], list):\n        raise TypeError('Error in return XML, does not contain \"oml:estimationprocedure\" as a list')\n\n    return [\n        prod[\"oml:name\"]\n        for prod in api_results[\"oml:estimationprocedures\"][\"oml:estimationprocedure\"]\n    ]\n</code></pre>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_evaluation_measures","title":"list_evaluation_measures","text":"<pre><code>list_evaluation_measures() -&gt; list[str]\n</code></pre> <p>Return list of evaluation measures available.</p> <p>The function performs an API call to retrieve the entire list of evaluation measures that are available.</p>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_evaluation_measures--returns","title":"Returns","text":"<p>list</p> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_evaluation_measures() -&gt; list[str]:\n    \"\"\"Return list of evaluation measures available.\n\n    The function performs an API call to retrieve the entire list of\n    evaluation measures that are available.\n\n    Returns\n    -------\n    list\n\n    \"\"\"\n    api_call = \"evaluationmeasure/list\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    qualities = xmltodict.parse(xml_string, force_list=(\"oml:measures\"))\n    # Minimalistic check if the XML is useful\n    if \"oml:evaluation_measures\" not in qualities:\n        raise ValueError('Error in return XML, does not contain \"oml:evaluation_measures\"')\n\n    if not isinstance(qualities[\"oml:evaluation_measures\"][\"oml:measures\"][0][\"oml:measure\"], list):\n        raise TypeError('Error in return XML, does not contain \"oml:measure\" as a list')\n\n    return qualities[\"oml:evaluation_measures\"][\"oml:measures\"][0][\"oml:measure\"]\n</code></pre>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_evaluations","title":"list_evaluations","text":"<pre><code>list_evaluations(function: str, offset: int | None = None, size: int | None = None, tasks: list[str | int] | None = None, setups: list[str | int] | None = None, flows: list[str | int] | None = None, runs: list[str | int] | None = None, uploaders: list[str | int] | None = None, tag: str | None = None, study: int | None = None, per_fold: bool | None = None, sort_order: str | None = None, output_format: Literal['object', 'dataframe'] = 'object') -&gt; dict[int, OpenMLEvaluation] | DataFrame\n</code></pre> <p>List all run-evaluation pairs matching all of the given filters.</p> <p>(Supports large amount of results)</p>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_evaluations--parameters","title":"Parameters","text":"<p>function : str     the evaluation function. e.g., predictive_accuracy offset : int, optional     the number of runs to skip, starting from the first size : int, default 10000     The maximum number of runs to show.     If set to <code>None</code>, it returns all the results.</p> list[int,str], optional <p>the list of task IDs</p> <p>setups: list[int,str], optional     the list of setup IDs flows : list[int,str], optional     the list of flow IDs runs :list[int,str], optional     the list of run IDs uploaders : list[int,str], optional     the list of uploader IDs tag : str, optional     filter evaluation based on given tag</p> <p>study : int, optional</p> <p>per_fold : bool, optional</p> str, optional <p>order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")</p> str, optional (default='object') <p>The parameter decides the format of the output. - If 'object' the output is a dict of OpenMLEvaluation objects - If 'dataframe' the output is a pandas DataFrame</p>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_evaluations--returns","title":"Returns","text":"<p>dict or dataframe</p> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_evaluations(\n    function: str,\n    offset: int | None = None,\n    size: int | None = None,\n    tasks: list[str | int] | None = None,\n    setups: list[str | int] | None = None,\n    flows: list[str | int] | None = None,\n    runs: list[str | int] | None = None,\n    uploaders: list[str | int] | None = None,\n    tag: str | None = None,\n    study: int | None = None,\n    per_fold: bool | None = None,\n    sort_order: str | None = None,\n    output_format: Literal[\"object\", \"dataframe\"] = \"object\",\n) -&gt; dict[int, OpenMLEvaluation] | pd.DataFrame:\n    \"\"\"List all run-evaluation pairs matching all of the given filters.\n\n    (Supports large amount of results)\n\n    Parameters\n    ----------\n    function : str\n        the evaluation function. e.g., predictive_accuracy\n    offset : int, optional\n        the number of runs to skip, starting from the first\n    size : int, default 10000\n        The maximum number of runs to show.\n        If set to ``None``, it returns all the results.\n\n    tasks : list[int,str], optional\n        the list of task IDs\n    setups: list[int,str], optional\n        the list of setup IDs\n    flows : list[int,str], optional\n        the list of flow IDs\n    runs :list[int,str], optional\n        the list of run IDs\n    uploaders : list[int,str], optional\n        the list of uploader IDs\n    tag : str, optional\n        filter evaluation based on given tag\n\n    study : int, optional\n\n    per_fold : bool, optional\n\n    sort_order : str, optional\n       order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")\n\n    output_format: str, optional (default='object')\n        The parameter decides the format of the output.\n        - If 'object' the output is a dict of OpenMLEvaluation objects\n        - If 'dataframe' the output is a pandas DataFrame\n\n    Returns\n    -------\n    dict or dataframe\n    \"\"\"\n    if output_format not in (\"dataframe\", \"object\"):\n        raise ValueError(\"Invalid output format. Only 'object', 'dataframe'.\")\n\n    per_fold_str = None\n    if per_fold is not None:\n        per_fold_str = str(per_fold).lower()\n\n    listing_call = partial(\n        _list_evaluations,\n        function=function,\n        tasks=tasks,\n        setups=setups,\n        flows=flows,\n        runs=runs,\n        uploaders=uploaders,\n        tag=tag,\n        study=study,\n        sort_order=sort_order,\n        per_fold=per_fold_str,\n    )\n    eval_collection = openml.utils._list_all(listing_call, offset=offset, limit=size)\n\n    flattened = list(chain.from_iterable(eval_collection))\n    if output_format == \"dataframe\":\n        records = [item._to_dict() for item in flattened]\n        return pd.DataFrame.from_records(records)  # No index...\n\n    return {e.run_id: e for e in flattened}\n</code></pre>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_evaluations_setups","title":"list_evaluations_setups","text":"<pre><code>list_evaluations_setups(function: str, offset: int | None = None, size: int | None = None, tasks: list | None = None, setups: list | None = None, flows: list | None = None, runs: list | None = None, uploaders: list | None = None, tag: str | None = None, per_fold: bool | None = None, sort_order: str | None = None, parameters_in_separate_columns: bool = False) -&gt; DataFrame\n</code></pre> <p>List all run-evaluation pairs matching all of the given filters and their hyperparameter settings.</p>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_evaluations_setups--parameters","title":"Parameters","text":"<p>function : str     the evaluation function. e.g., predictive_accuracy offset : int, optional     the number of runs to skip, starting from the first size : int, optional     the maximum number of runs to show tasks : list[int], optional     the list of task IDs setups: list[int], optional     the list of setup IDs flows : list[int], optional     the list of flow IDs runs : list[int], optional     the list of run IDs uploaders : list[int], optional     the list of uploader IDs tag : str, optional     filter evaluation based on given tag per_fold : bool, optional sort_order : str, optional    order of sorting evaluations, ascending (\"asc\") or descending (\"desc\") parameters_in_separate_columns: bool, optional (default= False)     Returns hyperparameters in separate columns if set to True.     Valid only for a single flow</p>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_evaluations_setups--returns","title":"Returns","text":"<p>dataframe with hyperparameter settings as a list of tuples.</p> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_evaluations_setups(\n    function: str,\n    offset: int | None = None,\n    size: int | None = None,\n    tasks: list | None = None,\n    setups: list | None = None,\n    flows: list | None = None,\n    runs: list | None = None,\n    uploaders: list | None = None,\n    tag: str | None = None,\n    per_fold: bool | None = None,\n    sort_order: str | None = None,\n    parameters_in_separate_columns: bool = False,  # noqa: FBT001, FBT002\n) -&gt; pd.DataFrame:\n    \"\"\"List all run-evaluation pairs matching all of the given filters\n    and their hyperparameter settings.\n\n    Parameters\n    ----------\n    function : str\n        the evaluation function. e.g., predictive_accuracy\n    offset : int, optional\n        the number of runs to skip, starting from the first\n    size : int, optional\n        the maximum number of runs to show\n    tasks : list[int], optional\n        the list of task IDs\n    setups: list[int], optional\n        the list of setup IDs\n    flows : list[int], optional\n        the list of flow IDs\n    runs : list[int], optional\n        the list of run IDs\n    uploaders : list[int], optional\n        the list of uploader IDs\n    tag : str, optional\n        filter evaluation based on given tag\n    per_fold : bool, optional\n    sort_order : str, optional\n       order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")\n    parameters_in_separate_columns: bool, optional (default= False)\n        Returns hyperparameters in separate columns if set to True.\n        Valid only for a single flow\n\n    Returns\n    -------\n    dataframe with hyperparameter settings as a list of tuples.\n    \"\"\"\n    if parameters_in_separate_columns and (flows is None or len(flows) != 1):\n        raise ValueError(\"Can set parameters_in_separate_columns to true only for single flow_id\")\n\n    # List evaluations\n    evals = list_evaluations(\n        function=function,\n        offset=offset,\n        size=size,\n        runs=runs,\n        tasks=tasks,\n        setups=setups,\n        flows=flows,\n        uploaders=uploaders,\n        tag=tag,\n        per_fold=per_fold,\n        sort_order=sort_order,\n        output_format=\"dataframe\",\n    )\n    # List setups\n    # list_setups by setup id does not support large sizes (exceeds URL length limit)\n    # Hence we split the list of unique setup ids returned by list_evaluations into chunks of size N\n    _df = pd.DataFrame()\n    if len(evals) != 0:\n        N = 100  # size of section\n        uniq = np.asarray(evals[\"setup_id\"].unique())\n        length = len(uniq)\n\n        # array_split - allows indices_or_sections to not equally divide the array\n        # array_split -length % N sub-arrays of size length//N + 1 and the rest of size length//N.\n        split_size = ((length - 1) // N) + 1\n        setup_chunks = np.array_split(uniq, split_size)\n\n        setup_data = pd.DataFrame()\n        for _setups in setup_chunks:\n            result = openml.setups.list_setups(setup=_setups, output_format=\"dataframe\")\n            assert isinstance(result, pd.DataFrame)\n            result = result.drop(\"flow_id\", axis=1)\n            # concat resulting setup chunks into single datframe\n            setup_data = pd.concat([setup_data, result])\n\n        parameters = []\n        # Convert parameters of setup into dict of (hyperparameter, value)\n        for parameter_dict in setup_data[\"parameters\"]:\n            if parameter_dict is not None:\n                parameters.append(\n                    {param[\"full_name\"]: param[\"value\"] for param in parameter_dict.values()},\n                )\n            else:\n                parameters.append({})\n        setup_data[\"parameters\"] = parameters\n        # Merge setups with evaluations\n        _df = evals.merge(setup_data, on=\"setup_id\", how=\"left\")\n\n    if parameters_in_separate_columns:\n        _df = pd.concat(\n            [_df.drop(\"parameters\", axis=1), _df[\"parameters\"].apply(pd.Series)],\n            axis=1,\n        )\n\n    return _df\n</code></pre>"},{"location":"reference/extensions/","title":"extensions","text":""},{"location":"reference/extensions/#openml.extensions","title":"openml.extensions","text":""},{"location":"reference/extensions/#openml.extensions.Extension","title":"Extension","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the interface to connect machine learning libraries to OpenML-Python.</p> <p>See <code>openml.extension.sklearn.extension</code> for an implementation to bootstrap from.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.can_handle_flow","title":"can_handle_flow  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_handle_flow(flow: OpenMLFlow) -&gt; bool\n</code></pre> <p>Check whether a given flow can be handled by this extension.</p> <p>This is typically done by parsing the <code>external_version</code> field.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.can_handle_flow--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow</p>"},{"location":"reference/extensions/#openml.extensions.Extension.can_handle_flow--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@classmethod\n@abstractmethod\ndef can_handle_flow(cls, flow: OpenMLFlow) -&gt; bool:\n    \"\"\"Check whether a given flow can be handled by this extension.\n\n    This is typically done by parsing the ``external_version`` field.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.can_handle_model","title":"can_handle_model  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_handle_model(model: Any) -&gt; bool\n</code></pre> <p>Check whether a model flow can be handled by this extension.</p> <p>This is typically done by checking the type of the model, or the package it belongs to.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.can_handle_model--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/#openml.extensions.Extension.can_handle_model--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@classmethod\n@abstractmethod\ndef can_handle_model(cls, model: Any) -&gt; bool:\n    \"\"\"Check whether a model flow can be handled by this extension.\n\n    This is typically done by checking the type of the model, or the package it belongs to.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.check_if_model_fitted","title":"check_if_model_fitted  <code>abstractmethod</code>","text":"<pre><code>check_if_model_fitted(model: Any) -&gt; bool\n</code></pre> <p>Returns True/False denoting if the model has already been fitted/trained.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.check_if_model_fitted--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/#openml.extensions.Extension.check_if_model_fitted--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef check_if_model_fitted(self, model: Any) -&gt; bool:\n    \"\"\"Returns True/False denoting if the model has already been fitted/trained.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.create_setup_string","title":"create_setup_string  <code>abstractmethod</code>","text":"<pre><code>create_setup_string(model: Any) -&gt; str\n</code></pre> <p>Create a string which can be used to reinstantiate the given model.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.create_setup_string--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/#openml.extensions.Extension.create_setup_string--returns","title":"Returns","text":"<p>str</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef create_setup_string(self, model: Any) -&gt; str:\n    \"\"\"Create a string which can be used to reinstantiate the given model.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    str\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.flow_to_model","title":"flow_to_model  <code>abstractmethod</code>","text":"<pre><code>flow_to_model(flow: OpenMLFlow, initialize_with_defaults: bool = False, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Instantiate a model from the flow representation.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.flow_to_model--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow</p> bool, optional (default=False) <p>If this flag is set, the hyperparameter values of flows will be ignored and a flow with its defaults is returned.</p> bool, default=True <p>Whether to fail if version requirements are not fulfilled.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.flow_to_model--returns","title":"Returns","text":"<p>Any</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef flow_to_model(\n    self,\n    flow: OpenMLFlow,\n    initialize_with_defaults: bool = False,  # noqa: FBT001, FBT002\n    strict_version: bool = True,  # noqa: FBT002, FBT001\n) -&gt; Any:\n    \"\"\"Instantiate a model from the flow representation.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    initialize_with_defaults : bool, optional (default=False)\n        If this flag is set, the hyperparameter values of flows will be\n        ignored and a flow with its defaults is returned.\n\n    strict_version : bool, default=True\n        Whether to fail if version requirements are not fulfilled.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.get_version_information","title":"get_version_information  <code>abstractmethod</code>","text":"<pre><code>get_version_information() -&gt; list[str]\n</code></pre> <p>List versions of libraries required by the flow.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.get_version_information--returns","title":"Returns","text":"<p>List</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef get_version_information(self) -&gt; list[str]:\n    \"\"\"List versions of libraries required by the flow.\n\n    Returns\n    -------\n    List\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.instantiate_model_from_hpo_class","title":"instantiate_model_from_hpo_class  <code>abstractmethod</code>","text":"<pre><code>instantiate_model_from_hpo_class(model: Any, trace_iteration: OpenMLTraceIteration) -&gt; Any\n</code></pre> <p>Instantiate a base model which can be searched over by the hyperparameter optimization model.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.instantiate_model_from_hpo_class--parameters","title":"Parameters","text":"<p>model : Any     A hyperparameter optimization model which defines the model to be instantiated. trace_iteration : OpenMLTraceIteration     Describing the hyperparameter settings to instantiate.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.instantiate_model_from_hpo_class--returns","title":"Returns","text":"<p>Any</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef instantiate_model_from_hpo_class(\n    self,\n    model: Any,\n    trace_iteration: OpenMLTraceIteration,\n) -&gt; Any:\n    \"\"\"Instantiate a base model which can be searched over by the hyperparameter optimization\n    model.\n\n    Parameters\n    ----------\n    model : Any\n        A hyperparameter optimization model which defines the model to be instantiated.\n    trace_iteration : OpenMLTraceIteration\n        Describing the hyperparameter settings to instantiate.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.is_estimator","title":"is_estimator  <code>abstractmethod</code>","text":"<pre><code>is_estimator(model: Any) -&gt; bool\n</code></pre> <p>Check whether the given model is an estimator for the given extension.</p> <p>This function is only required for backwards compatibility and will be removed in the near future.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.is_estimator--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/#openml.extensions.Extension.is_estimator--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef is_estimator(self, model: Any) -&gt; bool:\n    \"\"\"Check whether the given model is an estimator for the given extension.\n\n    This function is only required for backwards compatibility and will be removed in the\n    near future.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.model_to_flow","title":"model_to_flow  <code>abstractmethod</code>","text":"<pre><code>model_to_flow(model: Any) -&gt; OpenMLFlow\n</code></pre> <p>Transform a model to a flow for uploading it to OpenML.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.model_to_flow--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/#openml.extensions.Extension.model_to_flow--returns","title":"Returns","text":"<p>OpenMLFlow</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef model_to_flow(self, model: Any) -&gt; OpenMLFlow:\n    \"\"\"Transform a model to a flow for uploading it to OpenML.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    OpenMLFlow\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.obtain_parameter_values","title":"obtain_parameter_values  <code>abstractmethod</code>","text":"<pre><code>obtain_parameter_values(flow: OpenMLFlow, model: Any = None) -&gt; list[dict[str, Any]]\n</code></pre> <p>Extracts all parameter settings required for the flow from the model.</p> <p>If no explicit model is provided, the parameters will be extracted from <code>flow.model</code> instead.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.obtain_parameter_values--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow     OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)</p> Any, optional (default=None) <p>The model from which to obtain the parameter values. Must match the flow signature. If None, use the model specified in <code>OpenMLFlow.model</code>.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.obtain_parameter_values--returns","title":"Returns","text":"<p>list     A list of dicts, where each dict has the following entries:     - <code>oml:name</code> : str: The OpenML parameter name     - <code>oml:value</code> : mixed: A representation of the parameter value     - <code>oml:component</code> : int: flow id to which the parameter belongs</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef obtain_parameter_values(\n    self,\n    flow: OpenMLFlow,\n    model: Any = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Extracts all parameter settings required for the flow from the model.\n\n    If no explicit model is provided, the parameters will be extracted from `flow.model`\n    instead.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)\n\n    model: Any, optional (default=None)\n        The model from which to obtain the parameter values. Must match the flow signature.\n        If None, use the model specified in ``OpenMLFlow.model``.\n\n    Returns\n    -------\n    list\n        A list of dicts, where each dict has the following entries:\n        - ``oml:name`` : str: The OpenML parameter name\n        - ``oml:value`` : mixed: A representation of the parameter value\n        - ``oml:component`` : int: flow id to which the parameter belongs\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.seed_model","title":"seed_model  <code>abstractmethod</code>","text":"<pre><code>seed_model(model: Any, seed: int | None) -&gt; Any\n</code></pre> <p>Set the seed of all the unseeded components of a model and return the seeded model.</p> <p>Required so that all seed information can be uploaded to OpenML for reproducible results.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.seed_model--parameters","title":"Parameters","text":"<p>model : Any     The model to be seeded seed : int</p>"},{"location":"reference/extensions/#openml.extensions.Extension.seed_model--returns","title":"Returns","text":"<p>model</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef seed_model(self, model: Any, seed: int | None) -&gt; Any:\n    \"\"\"Set the seed of all the unseeded components of a model and return the seeded model.\n\n    Required so that all seed information can be uploaded to OpenML for reproducible results.\n\n    Parameters\n    ----------\n    model : Any\n        The model to be seeded\n    seed : int\n\n    Returns\n    -------\n    model\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.get_extension_by_flow","title":"get_extension_by_flow","text":"<pre><code>get_extension_by_flow(flow: OpenMLFlow, raise_if_no_extension: bool = False) -&gt; Extension | None\n</code></pre> <p>Get an extension which can handle the given flow.</p> <p>Iterates all registered extensions and checks whether they can handle the presented flow. Raises an exception if two extensions can handle a flow.</p>"},{"location":"reference/extensions/#openml.extensions.get_extension_by_flow--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow</p> bool (optional, default=False) <p>Raise an exception if no registered extension can handle the presented flow.</p>"},{"location":"reference/extensions/#openml.extensions.get_extension_by_flow--returns","title":"Returns","text":"<p>Extension or None</p> Source code in <code>openml/extensions/functions.py</code> <pre><code>def get_extension_by_flow(\n    flow: OpenMLFlow,\n    raise_if_no_extension: bool = False,  # noqa: FBT001, FBT002\n) -&gt; Extension | None:\n    \"\"\"Get an extension which can handle the given flow.\n\n    Iterates all registered extensions and checks whether they can handle the presented flow.\n    Raises an exception if two extensions can handle a flow.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    raise_if_no_extension : bool (optional, default=False)\n        Raise an exception if no registered extension can handle the presented flow.\n\n    Returns\n    -------\n    Extension or None\n    \"\"\"\n    candidates = []\n    for extension_class in openml.extensions.extensions:\n        if extension_class.can_handle_flow(flow):\n            candidates.append(extension_class())\n    if len(candidates) == 0:\n        if raise_if_no_extension:\n            raise ValueError(f\"No extension registered which can handle flow: {flow}\")\n\n        return None\n\n    if len(candidates) == 1:\n        return candidates[0]\n\n    raise ValueError(\n        f\"Multiple extensions registered which can handle flow: {flow}, but only one \"\n        f\"is allowed ({candidates}).\",\n    )\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.get_extension_by_model","title":"get_extension_by_model","text":"<pre><code>get_extension_by_model(model: Any, raise_if_no_extension: bool = False) -&gt; Extension | None\n</code></pre> <p>Get an extension which can handle the given flow.</p> <p>Iterates all registered extensions and checks whether they can handle the presented model. Raises an exception if two extensions can handle a model.</p>"},{"location":"reference/extensions/#openml.extensions.get_extension_by_model--parameters","title":"Parameters","text":"<p>model : Any</p> bool (optional, default=False) <p>Raise an exception if no registered extension can handle the presented model.</p>"},{"location":"reference/extensions/#openml.extensions.get_extension_by_model--returns","title":"Returns","text":"<p>Extension or None</p> Source code in <code>openml/extensions/functions.py</code> <pre><code>def get_extension_by_model(\n    model: Any,\n    raise_if_no_extension: bool = False,  # noqa: FBT001, FBT002\n) -&gt; Extension | None:\n    \"\"\"Get an extension which can handle the given flow.\n\n    Iterates all registered extensions and checks whether they can handle the presented model.\n    Raises an exception if two extensions can handle a model.\n\n    Parameters\n    ----------\n    model : Any\n\n    raise_if_no_extension : bool (optional, default=False)\n        Raise an exception if no registered extension can handle the presented model.\n\n    Returns\n    -------\n    Extension or None\n    \"\"\"\n    candidates = []\n    for extension_class in openml.extensions.extensions:\n        if extension_class.can_handle_model(model):\n            candidates.append(extension_class())\n    if len(candidates) == 0:\n        if raise_if_no_extension:\n            raise ValueError(f\"No extension registered which can handle model: {model}\")\n\n        return None\n\n    if len(candidates) == 1:\n        return candidates[0]\n\n    raise ValueError(\n        f\"Multiple extensions registered which can handle model: {model}, but only one \"\n        f\"is allowed ({candidates}).\",\n    )\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.register_extension","title":"register_extension","text":"<pre><code>register_extension(extension: type[Extension]) -&gt; None\n</code></pre> <p>Register an extension.</p> <p>Registered extensions are considered by <code>get_extension_by_flow</code> and <code>get_extension_by_model</code>, which are used by <code>openml.flow</code> and <code>openml.runs</code>.</p>"},{"location":"reference/extensions/#openml.extensions.register_extension--parameters","title":"Parameters","text":"<p>extension : Type[Extension]</p>"},{"location":"reference/extensions/#openml.extensions.register_extension--returns","title":"Returns","text":"<p>None</p> Source code in <code>openml/extensions/functions.py</code> <pre><code>def register_extension(extension: type[Extension]) -&gt; None:\n    \"\"\"Register an extension.\n\n    Registered extensions are considered by ``get_extension_by_flow`` and\n    ``get_extension_by_model``, which are used by ``openml.flow`` and ``openml.runs``.\n\n    Parameters\n    ----------\n    extension : Type[Extension]\n\n    Returns\n    -------\n    None\n    \"\"\"\n    openml.extensions.extensions.append(extension)\n</code></pre>"},{"location":"reference/extensions/extension_interface/","title":"extension_interface","text":""},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface","title":"openml.extensions.extension_interface","text":""},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension","title":"Extension","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the interface to connect machine learning libraries to OpenML-Python.</p> <p>See <code>openml.extension.sklearn.extension</code> for an implementation to bootstrap from.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.can_handle_flow","title":"can_handle_flow  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_handle_flow(flow: OpenMLFlow) -&gt; bool\n</code></pre> <p>Check whether a given flow can be handled by this extension.</p> <p>This is typically done by parsing the <code>external_version</code> field.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.can_handle_flow--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.can_handle_flow--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@classmethod\n@abstractmethod\ndef can_handle_flow(cls, flow: OpenMLFlow) -&gt; bool:\n    \"\"\"Check whether a given flow can be handled by this extension.\n\n    This is typically done by parsing the ``external_version`` field.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.can_handle_model","title":"can_handle_model  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_handle_model(model: Any) -&gt; bool\n</code></pre> <p>Check whether a model flow can be handled by this extension.</p> <p>This is typically done by checking the type of the model, or the package it belongs to.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.can_handle_model--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.can_handle_model--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@classmethod\n@abstractmethod\ndef can_handle_model(cls, model: Any) -&gt; bool:\n    \"\"\"Check whether a model flow can be handled by this extension.\n\n    This is typically done by checking the type of the model, or the package it belongs to.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.check_if_model_fitted","title":"check_if_model_fitted  <code>abstractmethod</code>","text":"<pre><code>check_if_model_fitted(model: Any) -&gt; bool\n</code></pre> <p>Returns True/False denoting if the model has already been fitted/trained.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.check_if_model_fitted--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.check_if_model_fitted--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef check_if_model_fitted(self, model: Any) -&gt; bool:\n    \"\"\"Returns True/False denoting if the model has already been fitted/trained.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.create_setup_string","title":"create_setup_string  <code>abstractmethod</code>","text":"<pre><code>create_setup_string(model: Any) -&gt; str\n</code></pre> <p>Create a string which can be used to reinstantiate the given model.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.create_setup_string--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.create_setup_string--returns","title":"Returns","text":"<p>str</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef create_setup_string(self, model: Any) -&gt; str:\n    \"\"\"Create a string which can be used to reinstantiate the given model.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    str\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.flow_to_model","title":"flow_to_model  <code>abstractmethod</code>","text":"<pre><code>flow_to_model(flow: OpenMLFlow, initialize_with_defaults: bool = False, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Instantiate a model from the flow representation.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.flow_to_model--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow</p> bool, optional (default=False) <p>If this flag is set, the hyperparameter values of flows will be ignored and a flow with its defaults is returned.</p> bool, default=True <p>Whether to fail if version requirements are not fulfilled.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.flow_to_model--returns","title":"Returns","text":"<p>Any</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef flow_to_model(\n    self,\n    flow: OpenMLFlow,\n    initialize_with_defaults: bool = False,  # noqa: FBT001, FBT002\n    strict_version: bool = True,  # noqa: FBT002, FBT001\n) -&gt; Any:\n    \"\"\"Instantiate a model from the flow representation.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    initialize_with_defaults : bool, optional (default=False)\n        If this flag is set, the hyperparameter values of flows will be\n        ignored and a flow with its defaults is returned.\n\n    strict_version : bool, default=True\n        Whether to fail if version requirements are not fulfilled.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.get_version_information","title":"get_version_information  <code>abstractmethod</code>","text":"<pre><code>get_version_information() -&gt; list[str]\n</code></pre> <p>List versions of libraries required by the flow.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.get_version_information--returns","title":"Returns","text":"<p>List</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef get_version_information(self) -&gt; list[str]:\n    \"\"\"List versions of libraries required by the flow.\n\n    Returns\n    -------\n    List\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.instantiate_model_from_hpo_class","title":"instantiate_model_from_hpo_class  <code>abstractmethod</code>","text":"<pre><code>instantiate_model_from_hpo_class(model: Any, trace_iteration: OpenMLTraceIteration) -&gt; Any\n</code></pre> <p>Instantiate a base model which can be searched over by the hyperparameter optimization model.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.instantiate_model_from_hpo_class--parameters","title":"Parameters","text":"<p>model : Any     A hyperparameter optimization model which defines the model to be instantiated. trace_iteration : OpenMLTraceIteration     Describing the hyperparameter settings to instantiate.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.instantiate_model_from_hpo_class--returns","title":"Returns","text":"<p>Any</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef instantiate_model_from_hpo_class(\n    self,\n    model: Any,\n    trace_iteration: OpenMLTraceIteration,\n) -&gt; Any:\n    \"\"\"Instantiate a base model which can be searched over by the hyperparameter optimization\n    model.\n\n    Parameters\n    ----------\n    model : Any\n        A hyperparameter optimization model which defines the model to be instantiated.\n    trace_iteration : OpenMLTraceIteration\n        Describing the hyperparameter settings to instantiate.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.is_estimator","title":"is_estimator  <code>abstractmethod</code>","text":"<pre><code>is_estimator(model: Any) -&gt; bool\n</code></pre> <p>Check whether the given model is an estimator for the given extension.</p> <p>This function is only required for backwards compatibility and will be removed in the near future.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.is_estimator--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.is_estimator--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef is_estimator(self, model: Any) -&gt; bool:\n    \"\"\"Check whether the given model is an estimator for the given extension.\n\n    This function is only required for backwards compatibility and will be removed in the\n    near future.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.model_to_flow","title":"model_to_flow  <code>abstractmethod</code>","text":"<pre><code>model_to_flow(model: Any) -&gt; OpenMLFlow\n</code></pre> <p>Transform a model to a flow for uploading it to OpenML.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.model_to_flow--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.model_to_flow--returns","title":"Returns","text":"<p>OpenMLFlow</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef model_to_flow(self, model: Any) -&gt; OpenMLFlow:\n    \"\"\"Transform a model to a flow for uploading it to OpenML.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    OpenMLFlow\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.obtain_parameter_values","title":"obtain_parameter_values  <code>abstractmethod</code>","text":"<pre><code>obtain_parameter_values(flow: OpenMLFlow, model: Any = None) -&gt; list[dict[str, Any]]\n</code></pre> <p>Extracts all parameter settings required for the flow from the model.</p> <p>If no explicit model is provided, the parameters will be extracted from <code>flow.model</code> instead.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.obtain_parameter_values--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow     OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)</p> Any, optional (default=None) <p>The model from which to obtain the parameter values. Must match the flow signature. If None, use the model specified in <code>OpenMLFlow.model</code>.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.obtain_parameter_values--returns","title":"Returns","text":"<p>list     A list of dicts, where each dict has the following entries:     - <code>oml:name</code> : str: The OpenML parameter name     - <code>oml:value</code> : mixed: A representation of the parameter value     - <code>oml:component</code> : int: flow id to which the parameter belongs</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef obtain_parameter_values(\n    self,\n    flow: OpenMLFlow,\n    model: Any = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Extracts all parameter settings required for the flow from the model.\n\n    If no explicit model is provided, the parameters will be extracted from `flow.model`\n    instead.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)\n\n    model: Any, optional (default=None)\n        The model from which to obtain the parameter values. Must match the flow signature.\n        If None, use the model specified in ``OpenMLFlow.model``.\n\n    Returns\n    -------\n    list\n        A list of dicts, where each dict has the following entries:\n        - ``oml:name`` : str: The OpenML parameter name\n        - ``oml:value`` : mixed: A representation of the parameter value\n        - ``oml:component`` : int: flow id to which the parameter belongs\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.seed_model","title":"seed_model  <code>abstractmethod</code>","text":"<pre><code>seed_model(model: Any, seed: int | None) -&gt; Any\n</code></pre> <p>Set the seed of all the unseeded components of a model and return the seeded model.</p> <p>Required so that all seed information can be uploaded to OpenML for reproducible results.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.seed_model--parameters","title":"Parameters","text":"<p>model : Any     The model to be seeded seed : int</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.seed_model--returns","title":"Returns","text":"<p>model</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef seed_model(self, model: Any, seed: int | None) -&gt; Any:\n    \"\"\"Set the seed of all the unseeded components of a model and return the seeded model.\n\n    Required so that all seed information can be uploaded to OpenML for reproducible results.\n\n    Parameters\n    ----------\n    model : Any\n        The model to be seeded\n    seed : int\n\n    Returns\n    -------\n    model\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/functions/","title":"functions","text":""},{"location":"reference/extensions/functions/#openml.extensions.functions","title":"openml.extensions.functions","text":""},{"location":"reference/extensions/functions/#openml.extensions.functions.get_extension_by_flow","title":"get_extension_by_flow","text":"<pre><code>get_extension_by_flow(flow: OpenMLFlow, raise_if_no_extension: bool = False) -&gt; Extension | None\n</code></pre> <p>Get an extension which can handle the given flow.</p> <p>Iterates all registered extensions and checks whether they can handle the presented flow. Raises an exception if two extensions can handle a flow.</p>"},{"location":"reference/extensions/functions/#openml.extensions.functions.get_extension_by_flow--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow</p> bool (optional, default=False) <p>Raise an exception if no registered extension can handle the presented flow.</p>"},{"location":"reference/extensions/functions/#openml.extensions.functions.get_extension_by_flow--returns","title":"Returns","text":"<p>Extension or None</p> Source code in <code>openml/extensions/functions.py</code> <pre><code>def get_extension_by_flow(\n    flow: OpenMLFlow,\n    raise_if_no_extension: bool = False,  # noqa: FBT001, FBT002\n) -&gt; Extension | None:\n    \"\"\"Get an extension which can handle the given flow.\n\n    Iterates all registered extensions and checks whether they can handle the presented flow.\n    Raises an exception if two extensions can handle a flow.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    raise_if_no_extension : bool (optional, default=False)\n        Raise an exception if no registered extension can handle the presented flow.\n\n    Returns\n    -------\n    Extension or None\n    \"\"\"\n    candidates = []\n    for extension_class in openml.extensions.extensions:\n        if extension_class.can_handle_flow(flow):\n            candidates.append(extension_class())\n    if len(candidates) == 0:\n        if raise_if_no_extension:\n            raise ValueError(f\"No extension registered which can handle flow: {flow}\")\n\n        return None\n\n    if len(candidates) == 1:\n        return candidates[0]\n\n    raise ValueError(\n        f\"Multiple extensions registered which can handle flow: {flow}, but only one \"\n        f\"is allowed ({candidates}).\",\n    )\n</code></pre>"},{"location":"reference/extensions/functions/#openml.extensions.functions.get_extension_by_model","title":"get_extension_by_model","text":"<pre><code>get_extension_by_model(model: Any, raise_if_no_extension: bool = False) -&gt; Extension | None\n</code></pre> <p>Get an extension which can handle the given flow.</p> <p>Iterates all registered extensions and checks whether they can handle the presented model. Raises an exception if two extensions can handle a model.</p>"},{"location":"reference/extensions/functions/#openml.extensions.functions.get_extension_by_model--parameters","title":"Parameters","text":"<p>model : Any</p> bool (optional, default=False) <p>Raise an exception if no registered extension can handle the presented model.</p>"},{"location":"reference/extensions/functions/#openml.extensions.functions.get_extension_by_model--returns","title":"Returns","text":"<p>Extension or None</p> Source code in <code>openml/extensions/functions.py</code> <pre><code>def get_extension_by_model(\n    model: Any,\n    raise_if_no_extension: bool = False,  # noqa: FBT001, FBT002\n) -&gt; Extension | None:\n    \"\"\"Get an extension which can handle the given flow.\n\n    Iterates all registered extensions and checks whether they can handle the presented model.\n    Raises an exception if two extensions can handle a model.\n\n    Parameters\n    ----------\n    model : Any\n\n    raise_if_no_extension : bool (optional, default=False)\n        Raise an exception if no registered extension can handle the presented model.\n\n    Returns\n    -------\n    Extension or None\n    \"\"\"\n    candidates = []\n    for extension_class in openml.extensions.extensions:\n        if extension_class.can_handle_model(model):\n            candidates.append(extension_class())\n    if len(candidates) == 0:\n        if raise_if_no_extension:\n            raise ValueError(f\"No extension registered which can handle model: {model}\")\n\n        return None\n\n    if len(candidates) == 1:\n        return candidates[0]\n\n    raise ValueError(\n        f\"Multiple extensions registered which can handle model: {model}, but only one \"\n        f\"is allowed ({candidates}).\",\n    )\n</code></pre>"},{"location":"reference/extensions/functions/#openml.extensions.functions.register_extension","title":"register_extension","text":"<pre><code>register_extension(extension: type[Extension]) -&gt; None\n</code></pre> <p>Register an extension.</p> <p>Registered extensions are considered by <code>get_extension_by_flow</code> and <code>get_extension_by_model</code>, which are used by <code>openml.flow</code> and <code>openml.runs</code>.</p>"},{"location":"reference/extensions/functions/#openml.extensions.functions.register_extension--parameters","title":"Parameters","text":"<p>extension : Type[Extension]</p>"},{"location":"reference/extensions/functions/#openml.extensions.functions.register_extension--returns","title":"Returns","text":"<p>None</p> Source code in <code>openml/extensions/functions.py</code> <pre><code>def register_extension(extension: type[Extension]) -&gt; None:\n    \"\"\"Register an extension.\n\n    Registered extensions are considered by ``get_extension_by_flow`` and\n    ``get_extension_by_model``, which are used by ``openml.flow`` and ``openml.runs``.\n\n    Parameters\n    ----------\n    extension : Type[Extension]\n\n    Returns\n    -------\n    None\n    \"\"\"\n    openml.extensions.extensions.append(extension)\n</code></pre>"},{"location":"reference/extensions/sklearn/","title":"sklearn","text":""},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn","title":"openml.extensions.sklearn","text":""},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension","title":"SklearnExtension","text":"<p>               Bases: <code>Extension</code></p> <p>Connect scikit-learn to OpenML-Python. The estimators which use this extension must be scikit-learn compatible, i.e needs to be a subclass of sklearn.base.BaseEstimator\".</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.can_handle_flow","title":"can_handle_flow  <code>classmethod</code>","text":"<pre><code>can_handle_flow(flow: OpenMLFlow) -&gt; bool\n</code></pre> <p>Check whether a given describes a scikit-learn estimator.</p> <p>This is done by parsing the <code>external_version</code> field.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.can_handle_flow--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.can_handle_flow--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>@classmethod\ndef can_handle_flow(cls, flow: OpenMLFlow) -&gt; bool:\n    \"\"\"Check whether a given describes a scikit-learn estimator.\n\n    This is done by parsing the ``external_version`` field.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    return cls._is_sklearn_flow(flow)\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.can_handle_model","title":"can_handle_model  <code>classmethod</code>","text":"<pre><code>can_handle_model(model: Any) -&gt; bool\n</code></pre> <p>Check whether a model is an instance of <code>sklearn.base.BaseEstimator</code>.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.can_handle_model--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.can_handle_model--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>@classmethod\ndef can_handle_model(cls, model: Any) -&gt; bool:\n    \"\"\"Check whether a model is an instance of ``sklearn.base.BaseEstimator``.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    return isinstance(model, sklearn.base.BaseEstimator)\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.check_if_model_fitted","title":"check_if_model_fitted","text":"<pre><code>check_if_model_fitted(model: Any) -&gt; bool\n</code></pre> <p>Returns True/False denoting if the model has already been fitted/trained</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.check_if_model_fitted--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.check_if_model_fitted--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def check_if_model_fitted(self, model: Any) -&gt; bool:\n    \"\"\"Returns True/False denoting if the model has already been fitted/trained\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    from sklearn.exceptions import NotFittedError\n    from sklearn.utils.validation import check_is_fitted\n\n    try:\n        # check if model is fitted\n        check_is_fitted(model)\n\n        # Creating random dummy data of arbitrary size\n        dummy_data = np.random.uniform(size=(10, 3))  # noqa: NPY002\n        # Using 'predict' instead of 'sklearn.utils.validation.check_is_fitted' for a more\n        # robust check that works across sklearn versions and models. Internally, 'predict'\n        # should call 'check_is_fitted' for every concerned attribute, thus offering a more\n        # assured check than explicit calls to 'check_is_fitted'\n        model.predict(dummy_data)\n        # Will reach here if the model was fit on a dataset with 3 features\n        return True\n    except NotFittedError:  # needs to be the first exception to be caught\n        # Model is not fitted, as is required\n        return False\n    except ValueError:\n        # Will reach here if the model was fit on a dataset with more or less than 3 features\n        return True\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.create_setup_string","title":"create_setup_string","text":"<pre><code>create_setup_string(model: Any) -&gt; str\n</code></pre> <p>Create a string which can be used to reinstantiate the given model.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.create_setup_string--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.create_setup_string--returns","title":"Returns","text":"<p>str</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def create_setup_string(self, model: Any) -&gt; str:  # noqa: ARG002\n    \"\"\"Create a string which can be used to reinstantiate the given model.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    str\n    \"\"\"\n    return \" \".join(self.get_version_information())\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.flow_to_model","title":"flow_to_model","text":"<pre><code>flow_to_model(flow: OpenMLFlow, initialize_with_defaults: bool = False, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Initializes a sklearn model based on a flow.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.flow_to_model--parameters","title":"Parameters","text":"<p>flow : mixed     the object to deserialize (can be flow object, or any serialized     parameter value that is accepted by)</p> bool, optional (default=False) <p>If this flag is set, the hyperparameter values of flows will be ignored and a flow with its defaults is returned.</p> bool, default=True <p>Whether to fail if version requirements are not fulfilled.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.flow_to_model--returns","title":"Returns","text":"<p>mixed</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def flow_to_model(\n    self,\n    flow: OpenMLFlow,\n    initialize_with_defaults: bool = False,  # noqa: FBT001, FBT002\n    strict_version: bool = True,  # noqa: FBT001, FBT002\n) -&gt; Any:\n    \"\"\"Initializes a sklearn model based on a flow.\n\n    Parameters\n    ----------\n    flow : mixed\n        the object to deserialize (can be flow object, or any serialized\n        parameter value that is accepted by)\n\n    initialize_with_defaults : bool, optional (default=False)\n        If this flag is set, the hyperparameter values of flows will be\n        ignored and a flow with its defaults is returned.\n\n    strict_version : bool, default=True\n        Whether to fail if version requirements are not fulfilled.\n\n    Returns\n    -------\n    mixed\n    \"\"\"\n    return self._deserialize_sklearn(\n        flow,\n        initialize_with_defaults=initialize_with_defaults,\n        strict_version=strict_version,\n    )\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.get_version_information","title":"get_version_information","text":"<pre><code>get_version_information() -&gt; list[str]\n</code></pre> <p>List versions of libraries required by the flow.</p> <p>Libraries listed are <code>Python</code>, <code>scikit-learn</code>, <code>numpy</code> and <code>scipy</code>.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.get_version_information--returns","title":"Returns","text":"<p>List</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def get_version_information(self) -&gt; list[str]:\n    \"\"\"List versions of libraries required by the flow.\n\n    Libraries listed are ``Python``, ``scikit-learn``, ``numpy`` and ``scipy``.\n\n    Returns\n    -------\n    List\n    \"\"\"\n    # This can possibly be done by a package such as pyxb, but I could not get\n    # it to work properly.\n    import numpy\n    import scipy\n    import sklearn\n\n    major, minor, micro, _, _ = sys.version_info\n    python_version = f\"Python_{'.'.join([str(major), str(minor), str(micro)])}.\"\n    sklearn_version = f\"Sklearn_{sklearn.__version__}.\"\n    numpy_version = f\"NumPy_{numpy.__version__}.\"  # type: ignore\n    scipy_version = f\"SciPy_{scipy.__version__}.\"\n\n    return [python_version, sklearn_version, numpy_version, scipy_version]\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.instantiate_model_from_hpo_class","title":"instantiate_model_from_hpo_class","text":"<pre><code>instantiate_model_from_hpo_class(model: Any, trace_iteration: OpenMLTraceIteration) -&gt; Any\n</code></pre> <p>Instantiate a <code>base_estimator</code> which can be searched over by the hyperparameter optimization model.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.instantiate_model_from_hpo_class--parameters","title":"Parameters","text":"<p>model : Any     A hyperparameter optimization model which defines the model to be instantiated. trace_iteration : OpenMLTraceIteration     Describing the hyperparameter settings to instantiate.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.instantiate_model_from_hpo_class--returns","title":"Returns","text":"<p>Any</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def instantiate_model_from_hpo_class(\n    self,\n    model: Any,\n    trace_iteration: OpenMLTraceIteration,\n) -&gt; Any:\n    \"\"\"Instantiate a ``base_estimator`` which can be searched over by the hyperparameter\n    optimization model.\n\n    Parameters\n    ----------\n    model : Any\n        A hyperparameter optimization model which defines the model to be instantiated.\n    trace_iteration : OpenMLTraceIteration\n        Describing the hyperparameter settings to instantiate.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n    if not self._is_hpo_class(model):\n        raise AssertionError(\n            f\"Flow model {model} is not an instance of\"\n            \" sklearn.model_selection._search.BaseSearchCV\",\n        )\n    base_estimator = model.estimator\n    base_estimator.set_params(**trace_iteration.get_parameters())\n    return base_estimator\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.is_estimator","title":"is_estimator","text":"<pre><code>is_estimator(model: Any) -&gt; bool\n</code></pre> <p>Check whether the given model is a scikit-learn estimator.</p> <p>This function is only required for backwards compatibility and will be removed in the near future.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.is_estimator--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.is_estimator--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def is_estimator(self, model: Any) -&gt; bool:\n    \"\"\"Check whether the given model is a scikit-learn estimator.\n\n    This function is only required for backwards compatibility and will be removed in the\n    near future.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    o = model\n    return hasattr(o, \"fit\") and hasattr(o, \"get_params\") and hasattr(o, \"set_params\")\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.model_to_flow","title":"model_to_flow","text":"<pre><code>model_to_flow(model: Any) -&gt; OpenMLFlow\n</code></pre> <p>Transform a scikit-learn model to a flow for uploading it to OpenML.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.model_to_flow--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.model_to_flow--returns","title":"Returns","text":"<p>OpenMLFlow</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def model_to_flow(self, model: Any) -&gt; OpenMLFlow:\n    \"\"\"Transform a scikit-learn model to a flow for uploading it to OpenML.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    OpenMLFlow\n    \"\"\"\n    # Necessary to make pypy not complain about all the different possible return types\n    return self._serialize_sklearn(model)\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.obtain_parameter_values","title":"obtain_parameter_values","text":"<pre><code>obtain_parameter_values(flow: OpenMLFlow, model: Any = None) -&gt; list[dict[str, Any]]\n</code></pre> <p>Extracts all parameter settings required for the flow from the model.</p> <p>If no explicit model is provided, the parameters will be extracted from <code>flow.model</code> instead.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.obtain_parameter_values--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow     OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)</p> Any, optional (default=None) <p>The model from which to obtain the parameter values. Must match the flow signature. If None, use the model specified in <code>OpenMLFlow.model</code>.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.obtain_parameter_values--returns","title":"Returns","text":"<p>list     A list of dicts, where each dict has the following entries:     - <code>oml:name</code> : str: The OpenML parameter name     - <code>oml:value</code> : mixed: A representation of the parameter value     - <code>oml:component</code> : int: flow id to which the parameter belongs</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def obtain_parameter_values(  # noqa: C901, PLR0915\n    self,\n    flow: OpenMLFlow,\n    model: Any = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Extracts all parameter settings required for the flow from the model.\n\n    If no explicit model is provided, the parameters will be extracted from `flow.model`\n    instead.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)\n\n    model: Any, optional (default=None)\n        The model from which to obtain the parameter values. Must match the flow signature.\n        If None, use the model specified in ``OpenMLFlow.model``.\n\n    Returns\n    -------\n    list\n        A list of dicts, where each dict has the following entries:\n        - ``oml:name`` : str: The OpenML parameter name\n        - ``oml:value`` : mixed: A representation of the parameter value\n        - ``oml:component`` : int: flow id to which the parameter belongs\n    \"\"\"\n    openml.flows.functions._check_flow_for_server_id(flow)\n\n    def get_flow_dict(_flow):\n        flow_map = {_flow.name: _flow.flow_id}\n        for subflow in _flow.components:\n            flow_map.update(get_flow_dict(_flow.components[subflow]))\n        return flow_map\n\n    def extract_parameters(  # noqa: PLR0915, PLR0912, C901\n        _flow,\n        _flow_dict,\n        component_model,\n        _main_call=False,  # noqa: FBT002\n        main_id=None,\n    ):\n        def is_subcomponent_specification(values):\n            # checks whether the current value can be a specification of\n            # subcomponents, as for example the value for steps parameter\n            # (in Pipeline) or transformers parameter (in\n            # ColumnTransformer).\n            return (\n                # Specification requires list/tuple of list/tuple with\n                # at least length 2.\n                isinstance(values, (tuple, list))\n                and all(isinstance(item, (tuple, list)) and len(item) &gt; 1 for item in values)\n                # And each component needs to be a flow or interpretable string\n                and all(\n                    isinstance(item[1], openml.flows.OpenMLFlow)\n                    or (\n                        isinstance(item[1], str)\n                        and item[1] in SKLEARN_PIPELINE_STRING_COMPONENTS\n                    )\n                    for item in values\n                )\n            )\n\n        # _flow is openml flow object, _param dict maps from flow name to flow\n        # id for the main call, the param dict can be overridden (useful for\n        # unit tests / sentinels) this way, for flows without subflows we do\n        # not have to rely on _flow_dict\n        exp_parameters = set(_flow.parameters)\n        if (\n            isinstance(component_model, str)\n            and component_model in SKLEARN_PIPELINE_STRING_COMPONENTS\n        ):\n            model_parameters = set()\n        else:\n            model_parameters = set(component_model.get_params(deep=False))\n        if len(exp_parameters.symmetric_difference(model_parameters)) != 0:\n            flow_params = sorted(exp_parameters)\n            model_params = sorted(model_parameters)\n            raise ValueError(\n                \"Parameters of the model do not match the \"\n                \"parameters expected by the \"\n                \"flow:\\nexpected flow parameters: \"\n                f\"{flow_params}\\nmodel parameters: {model_params}\",\n            )\n        exp_components = set(_flow.components)\n        if (\n            isinstance(component_model, str)\n            and component_model in SKLEARN_PIPELINE_STRING_COMPONENTS\n        ):\n            model_components = set()\n        else:\n            _ = set(component_model.get_params(deep=False))\n            model_components = {\n                mp\n                for mp in component_model.get_params(deep=True)\n                if \"__\" not in mp and mp not in _\n            }\n        if len(exp_components.symmetric_difference(model_components)) != 0:\n            is_problem = True\n            if len(exp_components - model_components) &gt; 0:\n                # If an expected component is not returned as a component by get_params(),\n                # this means that it is also a parameter -&gt; we need to check that this is\n                # actually the case\n                difference = exp_components - model_components\n                component_in_model_parameters = []\n                for component in difference:\n                    if component in model_parameters:\n                        component_in_model_parameters.append(True)\n                    else:\n                        component_in_model_parameters.append(False)\n                is_problem = not all(component_in_model_parameters)\n            if is_problem:\n                flow_components = sorted(exp_components)\n                model_components = sorted(model_components)\n                raise ValueError(\n                    \"Subcomponents of the model do not match the \"\n                    \"parameters expected by the \"\n                    \"flow:\\nexpected flow subcomponents: \"\n                    f\"{flow_components}\\nmodel subcomponents: {model_components}\",\n                )\n\n        _params = []\n        for _param_name in _flow.parameters:\n            _current = OrderedDict()\n            _current[\"oml:name\"] = _param_name\n\n            current_param_values = self.model_to_flow(component_model.get_params()[_param_name])\n\n            # Try to filter out components (a.k.a. subflows) which are\n            # handled further down in the code (by recursively calling\n            # this function)!\n            if isinstance(current_param_values, openml.flows.OpenMLFlow):\n                continue\n\n            if is_subcomponent_specification(current_param_values):\n                # complex parameter value, with subcomponents\n                parsed_values = []\n                for subcomponent in current_param_values:\n                    # scikit-learn stores usually tuples in the form\n                    # (name (str), subcomponent (mixed), argument\n                    # (mixed)). OpenML replaces the subcomponent by an\n                    # OpenMLFlow object.\n                    if len(subcomponent) &lt; 2 or len(subcomponent) &gt; 3:\n                        raise ValueError(\"Component reference should be size {2,3}. \")\n\n                    subcomponent_identifier = subcomponent[0]\n                    subcomponent_flow = subcomponent[1]\n                    if not isinstance(subcomponent_identifier, str):\n                        raise TypeError(\n                            \"Subcomponent identifier should be of type string, \"\n                            f\"but is {type(subcomponent_identifier)}\",\n                        )\n                    if not isinstance(subcomponent_flow, (openml.flows.OpenMLFlow, str)):\n                        if (\n                            isinstance(subcomponent_flow, str)\n                            and subcomponent_flow in SKLEARN_PIPELINE_STRING_COMPONENTS\n                        ):\n                            pass\n                        else:\n                            raise TypeError(\n                                \"Subcomponent flow should be of type flow, but is\"\n                                f\" {type(subcomponent_flow)}\",\n                            )\n\n                    current = {\n                        \"oml-python:serialized_object\": COMPONENT_REFERENCE,\n                        \"value\": {\n                            \"key\": subcomponent_identifier,\n                            \"step_name\": subcomponent_identifier,\n                        },\n                    }\n                    if len(subcomponent) == 3:\n                        if not isinstance(subcomponent[2], list) and not isinstance(\n                            subcomponent[2],\n                            OrderedDict,\n                        ):\n                            raise TypeError(\n                                \"Subcomponent argument should be list or OrderedDict\",\n                            )\n                        current[\"value\"][\"argument_1\"] = subcomponent[2]\n                    parsed_values.append(current)\n                parsed_values = json.dumps(parsed_values)\n            else:\n                # vanilla parameter value\n                parsed_values = json.dumps(current_param_values)\n\n            _current[\"oml:value\"] = parsed_values\n            if _main_call:\n                _current[\"oml:component\"] = main_id\n            else:\n                _current[\"oml:component\"] = _flow_dict[_flow.name]\n            _params.append(_current)\n\n        for _identifier in _flow.components:\n            subcomponent_model = component_model.get_params()[_identifier]\n            _params.extend(\n                extract_parameters(\n                    _flow.components[_identifier],\n                    _flow_dict,\n                    subcomponent_model,\n                ),\n            )\n        return _params\n\n    flow_dict = get_flow_dict(flow)\n    model = model if model is not None else flow.model\n    return extract_parameters(flow, flow_dict, model, _main_call=True, main_id=flow.flow_id)\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.seed_model","title":"seed_model","text":"<pre><code>seed_model(model: Any, seed: int | None = None) -&gt; Any\n</code></pre> <p>Set the random state of all the unseeded components of a model and return the seeded model.</p> <p>Required so that all seed information can be uploaded to OpenML for reproducible results.</p> <p>Models that are already seeded will maintain the seed. In this case, only integer seeds are allowed (An exception is raised when a RandomState was used as seed).</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.seed_model--parameters","title":"Parameters","text":"<p>model : sklearn model     The model to be seeded seed : int     The seed to initialize the RandomState with. Unseeded subcomponents     will be seeded with a random number from the RandomState.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.seed_model--returns","title":"Returns","text":"<p>Any</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def seed_model(self, model: Any, seed: int | None = None) -&gt; Any:  # noqa: C901\n    \"\"\"Set the random state of all the unseeded components of a model and return the seeded\n    model.\n\n    Required so that all seed information can be uploaded to OpenML for reproducible results.\n\n    Models that are already seeded will maintain the seed. In this case,\n    only integer seeds are allowed (An exception is raised when a RandomState was used as\n    seed).\n\n    Parameters\n    ----------\n    model : sklearn model\n        The model to be seeded\n    seed : int\n        The seed to initialize the RandomState with. Unseeded subcomponents\n        will be seeded with a random number from the RandomState.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    def _seed_current_object(current_value):\n        if isinstance(current_value, int):  # acceptable behaviour\n            return False\n\n        if isinstance(current_value, np.random.RandomState):\n            raise ValueError(\n                \"Models initialized with a RandomState object are not \"\n                \"supported. Please seed with an integer. \",\n            )\n\n        if current_value is not None:\n            raise ValueError(\n                \"Models should be seeded with int or None (this should never happen). \",\n            )\n\n        return True\n\n    rs = np.random.RandomState(seed)\n    model_params = model.get_params()\n    random_states = {}\n    for param_name in sorted(model_params):\n        if \"random_state\" in param_name:\n            current_value = model_params[param_name]\n            # important to draw the value at this point (and not in the if\n            # statement) this way we guarantee that if a different set of\n            # subflows is seeded, the same number of the random generator is\n            # used\n            new_value = rs.randint(0, 2**16)\n            if _seed_current_object(current_value):\n                random_states[param_name] = new_value\n\n        # Also seed CV objects!\n        elif isinstance(model_params[param_name], sklearn.model_selection.BaseCrossValidator):\n            if not hasattr(model_params[param_name], \"random_state\"):\n                continue\n\n            current_value = model_params[param_name].random_state\n            new_value = rs.randint(0, 2**16)\n            if _seed_current_object(current_value):\n                model_params[param_name].random_state = new_value\n\n    model.set_params(**random_states)\n    return model\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.trim_flow_name","title":"trim_flow_name  <code>classmethod</code>","text":"<pre><code>trim_flow_name(long_name: str, extra_trim_length: int = 100, _outer: bool = True) -&gt; str\n</code></pre> <p>Shorten generated sklearn flow name to at most <code>max_length</code> characters.</p> <p>Flows are assumed to have the following naming structure: <code>(model_selection)? (pipeline)? (steps)+</code> and will be shortened to: <code>sklearn.(selection.)?(pipeline.)?(steps)+</code> e.g. (white spaces and newlines added for readability)</p> <p>.. code ::</p> <pre><code>sklearn.pipeline.Pipeline(\n    columntransformer=sklearn.compose._column_transformer.ColumnTransformer(\n        numeric=sklearn.pipeline.Pipeline(\n            imputer=sklearn.preprocessing.imputation.Imputer,\n            standardscaler=sklearn.preprocessing.data.StandardScaler),\n        nominal=sklearn.pipeline.Pipeline(\n            simpleimputer=sklearn.impute.SimpleImputer,\n            onehotencoder=sklearn.preprocessing._encoders.OneHotEncoder)),\n    variancethreshold=sklearn.feature_selection.variance_threshold.VarianceThreshold,\n    svc=sklearn.svm.classes.SVC)\n</code></pre> <p>-&gt; <code>sklearn.Pipeline(ColumnTransformer,VarianceThreshold,SVC)</code></p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.trim_flow_name--parameters","title":"Parameters","text":"<p>long_name : str     The full flow name generated by the scikit-learn extension. extra_trim_length: int (default=100)     If the trimmed name would exceed <code>extra_trim_length</code> characters, additional trimming     of the short name is performed. This reduces the produced short name length.     There is no guarantee the end result will not exceed <code>extra_trim_length</code>. _outer : bool (default=True)     For internal use only. Specifies if the function is called recursively.</p>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.SklearnExtension.trim_flow_name--returns","title":"Returns","text":"<p>str</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>@classmethod\ndef trim_flow_name(  # noqa: C901\n    cls,\n    long_name: str,\n    extra_trim_length: int = 100,\n    _outer: bool = True,  # noqa: FBT001, FBT002\n) -&gt; str:\n    \"\"\"Shorten generated sklearn flow name to at most ``max_length`` characters.\n\n    Flows are assumed to have the following naming structure:\n    ``(model_selection)? (pipeline)? (steps)+``\n    and will be shortened to:\n    ``sklearn.(selection.)?(pipeline.)?(steps)+``\n    e.g. (white spaces and newlines added for readability)\n\n    .. code ::\n\n        sklearn.pipeline.Pipeline(\n            columntransformer=sklearn.compose._column_transformer.ColumnTransformer(\n                numeric=sklearn.pipeline.Pipeline(\n                    imputer=sklearn.preprocessing.imputation.Imputer,\n                    standardscaler=sklearn.preprocessing.data.StandardScaler),\n                nominal=sklearn.pipeline.Pipeline(\n                    simpleimputer=sklearn.impute.SimpleImputer,\n                    onehotencoder=sklearn.preprocessing._encoders.OneHotEncoder)),\n            variancethreshold=sklearn.feature_selection.variance_threshold.VarianceThreshold,\n            svc=sklearn.svm.classes.SVC)\n\n    -&gt;\n    ``sklearn.Pipeline(ColumnTransformer,VarianceThreshold,SVC)``\n\n    Parameters\n    ----------\n    long_name : str\n        The full flow name generated by the scikit-learn extension.\n    extra_trim_length: int (default=100)\n        If the trimmed name would exceed `extra_trim_length` characters, additional trimming\n        of the short name is performed. This reduces the produced short name length.\n        There is no guarantee the end result will not exceed `extra_trim_length`.\n    _outer : bool (default=True)\n        For internal use only. Specifies if the function is called recursively.\n\n    Returns\n    -------\n    str\n\n    \"\"\"\n\n    def remove_all_in_parentheses(string: str) -&gt; str:\n        string, removals = re.subn(r\"\\([^()]*\\)\", \"\", string)\n        while removals &gt; 0:\n            string, removals = re.subn(r\"\\([^()]*\\)\", \"\", string)\n        return string\n\n    # Generally, we want to trim all hyperparameters, the exception to that is for model\n    # selection, as the `estimator` hyperparameter is very indicative of what is in the flow.\n    # So we first trim name of the `estimator` specified in mode selection. For reference, in\n    # the example below, we want to trim `sklearn.tree.tree.DecisionTreeClassifier`, and\n    # keep it in the final trimmed flow name:\n    # sklearn.pipeline.Pipeline(Imputer=sklearn.preprocessing.imputation.Imputer,\n    # VarianceThreshold=sklearn.feature_selection.variance_threshold.VarianceThreshold,  # noqa: ERA001, E501\n    # Estimator=sklearn.model_selection._search.RandomizedSearchCV(estimator=\n    # sklearn.tree.tree.DecisionTreeClassifier))\n    if \"sklearn.model_selection\" in long_name:\n        start_index = long_name.index(\"sklearn.model_selection\")\n        estimator_start = (\n            start_index + long_name[start_index:].index(\"estimator=\") + len(\"estimator=\")\n        )\n\n        model_select_boilerplate = long_name[start_index:estimator_start]\n        # above is .g. \"sklearn.model_selection._search.RandomizedSearchCV(estimator=\"\n        model_selection_class = model_select_boilerplate.split(\"(\")[0].split(\".\")[-1]\n\n        # Now we want to also find and parse the `estimator`, for this we find the closing\n        # parenthesis to the model selection technique:\n        closing_parenthesis_expected = 1\n        for char in long_name[estimator_start:]:\n            if char == \"(\":\n                closing_parenthesis_expected += 1\n            if char == \")\":\n                closing_parenthesis_expected -= 1\n            if closing_parenthesis_expected == 0:\n                break\n\n        _end: int = estimator_start + len(long_name[estimator_start:]) - 1\n        model_select_pipeline = long_name[estimator_start:_end]\n\n        trimmed_pipeline = cls.trim_flow_name(model_select_pipeline, _outer=False)\n        _, trimmed_pipeline = trimmed_pipeline.split(\".\", maxsplit=1)  # trim module prefix\n        model_select_short = f\"sklearn.{model_selection_class}[{trimmed_pipeline}]\"\n        name = long_name[:start_index] + model_select_short + long_name[_end + 1 :]\n    else:\n        name = long_name\n\n    module_name = long_name.split(\".\")[0]\n    short_name = module_name + \".{}\"\n\n    if name.startswith(\"sklearn.pipeline\"):\n        full_pipeline_class, pipeline = name[:-1].split(\"(\", maxsplit=1)\n        pipeline_class = full_pipeline_class.split(\".\")[-1]\n        # We don't want nested pipelines in the short name, so we trim all complicated\n        # subcomponents, i.e. those with parentheses:\n        pipeline = remove_all_in_parentheses(pipeline)\n\n        # then the pipeline steps are formatted e.g.:\n        # step1name=sklearn.submodule.ClassName,step2name...\n        components = [component.split(\".\")[-1] for component in pipeline.split(\",\")]\n        pipeline = f\"{pipeline_class}({','.join(components)})\"\n        if len(short_name.format(pipeline)) &gt; extra_trim_length:\n            pipeline = f\"{pipeline_class}(...,{components[-1]})\"\n    else:\n        # Just a simple component: e.g. sklearn.tree.DecisionTreeClassifier\n        pipeline = remove_all_in_parentheses(name).split(\".\")[-1]\n\n    if not _outer:\n        # Anything from parenthesis in inner calls should not be culled, so we use brackets\n        pipeline = pipeline.replace(\"(\", \"[\").replace(\")\", \"]\")\n    else:\n        # Square brackets may be introduced with nested model_selection\n        pipeline = pipeline.replace(\"[\", \"(\").replace(\"]\", \")\")\n\n    return short_name.format(pipeline)\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.cat","title":"cat","text":"<pre><code>cat(X: DataFrame) -&gt; Series\n</code></pre> <p>Returns True for all categorical columns, False for the rest.</p> <p>This is a helper function for OpenML datasets encoded as DataFrames simplifying the handling of mixed data types. To build sklearn models on mixed data types, a ColumnTransformer is required to process each type of columns separately. This function allows transformations meant for categorical columns to access the categorical columns given the dataset as DataFrame.</p> Source code in <code>openml/extensions/sklearn/__init__.py</code> <pre><code>def cat(X: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Returns True for all categorical columns, False for the rest.\n\n    This is a helper function for OpenML datasets encoded as DataFrames simplifying the handling\n    of mixed data types. To build sklearn models on mixed data types, a ColumnTransformer is\n    required to process each type of columns separately.\n    This function allows transformations meant for categorical columns to access the\n    categorical columns given the dataset as DataFrame.\n    \"\"\"\n    if not hasattr(X, \"dtypes\"):\n        raise AttributeError(\"Not a Pandas DataFrame with 'dtypes' as attribute!\")\n    return X.dtypes == \"category\"\n</code></pre>"},{"location":"reference/extensions/sklearn/#openml.extensions.sklearn.cont","title":"cont","text":"<pre><code>cont(X: DataFrame) -&gt; Series\n</code></pre> <p>Returns True for all non-categorical columns, False for the rest.</p> <p>This is a helper function for OpenML datasets encoded as DataFrames simplifying the handling of mixed data types. To build sklearn models on mixed data types, a ColumnTransformer is required to process each type of columns separately. This function allows transformations meant for continuous/numeric columns to access the continuous/numeric columns given the dataset as DataFrame.</p> Source code in <code>openml/extensions/sklearn/__init__.py</code> <pre><code>def cont(X: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Returns True for all non-categorical columns, False for the rest.\n\n    This is a helper function for OpenML datasets encoded as DataFrames simplifying the handling\n    of mixed data types. To build sklearn models on mixed data types, a ColumnTransformer is\n    required to process each type of columns separately.\n    This function allows transformations meant for continuous/numeric columns to access the\n    continuous/numeric columns given the dataset as DataFrame.\n    \"\"\"\n    if not hasattr(X, \"dtypes\"):\n        raise AttributeError(\"Not a Pandas DataFrame with 'dtypes' as attribute!\")\n    return X.dtypes != \"category\"\n</code></pre>"},{"location":"reference/extensions/sklearn/extension/","title":"extension","text":""},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension","title":"openml.extensions.sklearn.extension","text":""},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension","title":"SklearnExtension","text":"<p>               Bases: <code>Extension</code></p> <p>Connect scikit-learn to OpenML-Python. The estimators which use this extension must be scikit-learn compatible, i.e needs to be a subclass of sklearn.base.BaseEstimator\".</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.can_handle_flow","title":"can_handle_flow  <code>classmethod</code>","text":"<pre><code>can_handle_flow(flow: OpenMLFlow) -&gt; bool\n</code></pre> <p>Check whether a given describes a scikit-learn estimator.</p> <p>This is done by parsing the <code>external_version</code> field.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.can_handle_flow--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.can_handle_flow--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>@classmethod\ndef can_handle_flow(cls, flow: OpenMLFlow) -&gt; bool:\n    \"\"\"Check whether a given describes a scikit-learn estimator.\n\n    This is done by parsing the ``external_version`` field.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    return cls._is_sklearn_flow(flow)\n</code></pre>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.can_handle_model","title":"can_handle_model  <code>classmethod</code>","text":"<pre><code>can_handle_model(model: Any) -&gt; bool\n</code></pre> <p>Check whether a model is an instance of <code>sklearn.base.BaseEstimator</code>.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.can_handle_model--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.can_handle_model--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>@classmethod\ndef can_handle_model(cls, model: Any) -&gt; bool:\n    \"\"\"Check whether a model is an instance of ``sklearn.base.BaseEstimator``.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    return isinstance(model, sklearn.base.BaseEstimator)\n</code></pre>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.check_if_model_fitted","title":"check_if_model_fitted","text":"<pre><code>check_if_model_fitted(model: Any) -&gt; bool\n</code></pre> <p>Returns True/False denoting if the model has already been fitted/trained</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.check_if_model_fitted--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.check_if_model_fitted--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def check_if_model_fitted(self, model: Any) -&gt; bool:\n    \"\"\"Returns True/False denoting if the model has already been fitted/trained\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    from sklearn.exceptions import NotFittedError\n    from sklearn.utils.validation import check_is_fitted\n\n    try:\n        # check if model is fitted\n        check_is_fitted(model)\n\n        # Creating random dummy data of arbitrary size\n        dummy_data = np.random.uniform(size=(10, 3))  # noqa: NPY002\n        # Using 'predict' instead of 'sklearn.utils.validation.check_is_fitted' for a more\n        # robust check that works across sklearn versions and models. Internally, 'predict'\n        # should call 'check_is_fitted' for every concerned attribute, thus offering a more\n        # assured check than explicit calls to 'check_is_fitted'\n        model.predict(dummy_data)\n        # Will reach here if the model was fit on a dataset with 3 features\n        return True\n    except NotFittedError:  # needs to be the first exception to be caught\n        # Model is not fitted, as is required\n        return False\n    except ValueError:\n        # Will reach here if the model was fit on a dataset with more or less than 3 features\n        return True\n</code></pre>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.create_setup_string","title":"create_setup_string","text":"<pre><code>create_setup_string(model: Any) -&gt; str\n</code></pre> <p>Create a string which can be used to reinstantiate the given model.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.create_setup_string--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.create_setup_string--returns","title":"Returns","text":"<p>str</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def create_setup_string(self, model: Any) -&gt; str:  # noqa: ARG002\n    \"\"\"Create a string which can be used to reinstantiate the given model.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    str\n    \"\"\"\n    return \" \".join(self.get_version_information())\n</code></pre>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.flow_to_model","title":"flow_to_model","text":"<pre><code>flow_to_model(flow: OpenMLFlow, initialize_with_defaults: bool = False, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Initializes a sklearn model based on a flow.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.flow_to_model--parameters","title":"Parameters","text":"<p>flow : mixed     the object to deserialize (can be flow object, or any serialized     parameter value that is accepted by)</p> bool, optional (default=False) <p>If this flag is set, the hyperparameter values of flows will be ignored and a flow with its defaults is returned.</p> bool, default=True <p>Whether to fail if version requirements are not fulfilled.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.flow_to_model--returns","title":"Returns","text":"<p>mixed</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def flow_to_model(\n    self,\n    flow: OpenMLFlow,\n    initialize_with_defaults: bool = False,  # noqa: FBT001, FBT002\n    strict_version: bool = True,  # noqa: FBT001, FBT002\n) -&gt; Any:\n    \"\"\"Initializes a sklearn model based on a flow.\n\n    Parameters\n    ----------\n    flow : mixed\n        the object to deserialize (can be flow object, or any serialized\n        parameter value that is accepted by)\n\n    initialize_with_defaults : bool, optional (default=False)\n        If this flag is set, the hyperparameter values of flows will be\n        ignored and a flow with its defaults is returned.\n\n    strict_version : bool, default=True\n        Whether to fail if version requirements are not fulfilled.\n\n    Returns\n    -------\n    mixed\n    \"\"\"\n    return self._deserialize_sklearn(\n        flow,\n        initialize_with_defaults=initialize_with_defaults,\n        strict_version=strict_version,\n    )\n</code></pre>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.get_version_information","title":"get_version_information","text":"<pre><code>get_version_information() -&gt; list[str]\n</code></pre> <p>List versions of libraries required by the flow.</p> <p>Libraries listed are <code>Python</code>, <code>scikit-learn</code>, <code>numpy</code> and <code>scipy</code>.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.get_version_information--returns","title":"Returns","text":"<p>List</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def get_version_information(self) -&gt; list[str]:\n    \"\"\"List versions of libraries required by the flow.\n\n    Libraries listed are ``Python``, ``scikit-learn``, ``numpy`` and ``scipy``.\n\n    Returns\n    -------\n    List\n    \"\"\"\n    # This can possibly be done by a package such as pyxb, but I could not get\n    # it to work properly.\n    import numpy\n    import scipy\n    import sklearn\n\n    major, minor, micro, _, _ = sys.version_info\n    python_version = f\"Python_{'.'.join([str(major), str(minor), str(micro)])}.\"\n    sklearn_version = f\"Sklearn_{sklearn.__version__}.\"\n    numpy_version = f\"NumPy_{numpy.__version__}.\"  # type: ignore\n    scipy_version = f\"SciPy_{scipy.__version__}.\"\n\n    return [python_version, sklearn_version, numpy_version, scipy_version]\n</code></pre>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.instantiate_model_from_hpo_class","title":"instantiate_model_from_hpo_class","text":"<pre><code>instantiate_model_from_hpo_class(model: Any, trace_iteration: OpenMLTraceIteration) -&gt; Any\n</code></pre> <p>Instantiate a <code>base_estimator</code> which can be searched over by the hyperparameter optimization model.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.instantiate_model_from_hpo_class--parameters","title":"Parameters","text":"<p>model : Any     A hyperparameter optimization model which defines the model to be instantiated. trace_iteration : OpenMLTraceIteration     Describing the hyperparameter settings to instantiate.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.instantiate_model_from_hpo_class--returns","title":"Returns","text":"<p>Any</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def instantiate_model_from_hpo_class(\n    self,\n    model: Any,\n    trace_iteration: OpenMLTraceIteration,\n) -&gt; Any:\n    \"\"\"Instantiate a ``base_estimator`` which can be searched over by the hyperparameter\n    optimization model.\n\n    Parameters\n    ----------\n    model : Any\n        A hyperparameter optimization model which defines the model to be instantiated.\n    trace_iteration : OpenMLTraceIteration\n        Describing the hyperparameter settings to instantiate.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n    if not self._is_hpo_class(model):\n        raise AssertionError(\n            f\"Flow model {model} is not an instance of\"\n            \" sklearn.model_selection._search.BaseSearchCV\",\n        )\n    base_estimator = model.estimator\n    base_estimator.set_params(**trace_iteration.get_parameters())\n    return base_estimator\n</code></pre>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.is_estimator","title":"is_estimator","text":"<pre><code>is_estimator(model: Any) -&gt; bool\n</code></pre> <p>Check whether the given model is a scikit-learn estimator.</p> <p>This function is only required for backwards compatibility and will be removed in the near future.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.is_estimator--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.is_estimator--returns","title":"Returns","text":"<p>bool</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def is_estimator(self, model: Any) -&gt; bool:\n    \"\"\"Check whether the given model is a scikit-learn estimator.\n\n    This function is only required for backwards compatibility and will be removed in the\n    near future.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    o = model\n    return hasattr(o, \"fit\") and hasattr(o, \"get_params\") and hasattr(o, \"set_params\")\n</code></pre>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.model_to_flow","title":"model_to_flow","text":"<pre><code>model_to_flow(model: Any) -&gt; OpenMLFlow\n</code></pre> <p>Transform a scikit-learn model to a flow for uploading it to OpenML.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.model_to_flow--parameters","title":"Parameters","text":"<p>model : Any</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.model_to_flow--returns","title":"Returns","text":"<p>OpenMLFlow</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def model_to_flow(self, model: Any) -&gt; OpenMLFlow:\n    \"\"\"Transform a scikit-learn model to a flow for uploading it to OpenML.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    OpenMLFlow\n    \"\"\"\n    # Necessary to make pypy not complain about all the different possible return types\n    return self._serialize_sklearn(model)\n</code></pre>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.obtain_parameter_values","title":"obtain_parameter_values","text":"<pre><code>obtain_parameter_values(flow: OpenMLFlow, model: Any = None) -&gt; list[dict[str, Any]]\n</code></pre> <p>Extracts all parameter settings required for the flow from the model.</p> <p>If no explicit model is provided, the parameters will be extracted from <code>flow.model</code> instead.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.obtain_parameter_values--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow     OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)</p> Any, optional (default=None) <p>The model from which to obtain the parameter values. Must match the flow signature. If None, use the model specified in <code>OpenMLFlow.model</code>.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.obtain_parameter_values--returns","title":"Returns","text":"<p>list     A list of dicts, where each dict has the following entries:     - <code>oml:name</code> : str: The OpenML parameter name     - <code>oml:value</code> : mixed: A representation of the parameter value     - <code>oml:component</code> : int: flow id to which the parameter belongs</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def obtain_parameter_values(  # noqa: C901, PLR0915\n    self,\n    flow: OpenMLFlow,\n    model: Any = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Extracts all parameter settings required for the flow from the model.\n\n    If no explicit model is provided, the parameters will be extracted from `flow.model`\n    instead.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)\n\n    model: Any, optional (default=None)\n        The model from which to obtain the parameter values. Must match the flow signature.\n        If None, use the model specified in ``OpenMLFlow.model``.\n\n    Returns\n    -------\n    list\n        A list of dicts, where each dict has the following entries:\n        - ``oml:name`` : str: The OpenML parameter name\n        - ``oml:value`` : mixed: A representation of the parameter value\n        - ``oml:component`` : int: flow id to which the parameter belongs\n    \"\"\"\n    openml.flows.functions._check_flow_for_server_id(flow)\n\n    def get_flow_dict(_flow):\n        flow_map = {_flow.name: _flow.flow_id}\n        for subflow in _flow.components:\n            flow_map.update(get_flow_dict(_flow.components[subflow]))\n        return flow_map\n\n    def extract_parameters(  # noqa: PLR0915, PLR0912, C901\n        _flow,\n        _flow_dict,\n        component_model,\n        _main_call=False,  # noqa: FBT002\n        main_id=None,\n    ):\n        def is_subcomponent_specification(values):\n            # checks whether the current value can be a specification of\n            # subcomponents, as for example the value for steps parameter\n            # (in Pipeline) or transformers parameter (in\n            # ColumnTransformer).\n            return (\n                # Specification requires list/tuple of list/tuple with\n                # at least length 2.\n                isinstance(values, (tuple, list))\n                and all(isinstance(item, (tuple, list)) and len(item) &gt; 1 for item in values)\n                # And each component needs to be a flow or interpretable string\n                and all(\n                    isinstance(item[1], openml.flows.OpenMLFlow)\n                    or (\n                        isinstance(item[1], str)\n                        and item[1] in SKLEARN_PIPELINE_STRING_COMPONENTS\n                    )\n                    for item in values\n                )\n            )\n\n        # _flow is openml flow object, _param dict maps from flow name to flow\n        # id for the main call, the param dict can be overridden (useful for\n        # unit tests / sentinels) this way, for flows without subflows we do\n        # not have to rely on _flow_dict\n        exp_parameters = set(_flow.parameters)\n        if (\n            isinstance(component_model, str)\n            and component_model in SKLEARN_PIPELINE_STRING_COMPONENTS\n        ):\n            model_parameters = set()\n        else:\n            model_parameters = set(component_model.get_params(deep=False))\n        if len(exp_parameters.symmetric_difference(model_parameters)) != 0:\n            flow_params = sorted(exp_parameters)\n            model_params = sorted(model_parameters)\n            raise ValueError(\n                \"Parameters of the model do not match the \"\n                \"parameters expected by the \"\n                \"flow:\\nexpected flow parameters: \"\n                f\"{flow_params}\\nmodel parameters: {model_params}\",\n            )\n        exp_components = set(_flow.components)\n        if (\n            isinstance(component_model, str)\n            and component_model in SKLEARN_PIPELINE_STRING_COMPONENTS\n        ):\n            model_components = set()\n        else:\n            _ = set(component_model.get_params(deep=False))\n            model_components = {\n                mp\n                for mp in component_model.get_params(deep=True)\n                if \"__\" not in mp and mp not in _\n            }\n        if len(exp_components.symmetric_difference(model_components)) != 0:\n            is_problem = True\n            if len(exp_components - model_components) &gt; 0:\n                # If an expected component is not returned as a component by get_params(),\n                # this means that it is also a parameter -&gt; we need to check that this is\n                # actually the case\n                difference = exp_components - model_components\n                component_in_model_parameters = []\n                for component in difference:\n                    if component in model_parameters:\n                        component_in_model_parameters.append(True)\n                    else:\n                        component_in_model_parameters.append(False)\n                is_problem = not all(component_in_model_parameters)\n            if is_problem:\n                flow_components = sorted(exp_components)\n                model_components = sorted(model_components)\n                raise ValueError(\n                    \"Subcomponents of the model do not match the \"\n                    \"parameters expected by the \"\n                    \"flow:\\nexpected flow subcomponents: \"\n                    f\"{flow_components}\\nmodel subcomponents: {model_components}\",\n                )\n\n        _params = []\n        for _param_name in _flow.parameters:\n            _current = OrderedDict()\n            _current[\"oml:name\"] = _param_name\n\n            current_param_values = self.model_to_flow(component_model.get_params()[_param_name])\n\n            # Try to filter out components (a.k.a. subflows) which are\n            # handled further down in the code (by recursively calling\n            # this function)!\n            if isinstance(current_param_values, openml.flows.OpenMLFlow):\n                continue\n\n            if is_subcomponent_specification(current_param_values):\n                # complex parameter value, with subcomponents\n                parsed_values = []\n                for subcomponent in current_param_values:\n                    # scikit-learn stores usually tuples in the form\n                    # (name (str), subcomponent (mixed), argument\n                    # (mixed)). OpenML replaces the subcomponent by an\n                    # OpenMLFlow object.\n                    if len(subcomponent) &lt; 2 or len(subcomponent) &gt; 3:\n                        raise ValueError(\"Component reference should be size {2,3}. \")\n\n                    subcomponent_identifier = subcomponent[0]\n                    subcomponent_flow = subcomponent[1]\n                    if not isinstance(subcomponent_identifier, str):\n                        raise TypeError(\n                            \"Subcomponent identifier should be of type string, \"\n                            f\"but is {type(subcomponent_identifier)}\",\n                        )\n                    if not isinstance(subcomponent_flow, (openml.flows.OpenMLFlow, str)):\n                        if (\n                            isinstance(subcomponent_flow, str)\n                            and subcomponent_flow in SKLEARN_PIPELINE_STRING_COMPONENTS\n                        ):\n                            pass\n                        else:\n                            raise TypeError(\n                                \"Subcomponent flow should be of type flow, but is\"\n                                f\" {type(subcomponent_flow)}\",\n                            )\n\n                    current = {\n                        \"oml-python:serialized_object\": COMPONENT_REFERENCE,\n                        \"value\": {\n                            \"key\": subcomponent_identifier,\n                            \"step_name\": subcomponent_identifier,\n                        },\n                    }\n                    if len(subcomponent) == 3:\n                        if not isinstance(subcomponent[2], list) and not isinstance(\n                            subcomponent[2],\n                            OrderedDict,\n                        ):\n                            raise TypeError(\n                                \"Subcomponent argument should be list or OrderedDict\",\n                            )\n                        current[\"value\"][\"argument_1\"] = subcomponent[2]\n                    parsed_values.append(current)\n                parsed_values = json.dumps(parsed_values)\n            else:\n                # vanilla parameter value\n                parsed_values = json.dumps(current_param_values)\n\n            _current[\"oml:value\"] = parsed_values\n            if _main_call:\n                _current[\"oml:component\"] = main_id\n            else:\n                _current[\"oml:component\"] = _flow_dict[_flow.name]\n            _params.append(_current)\n\n        for _identifier in _flow.components:\n            subcomponent_model = component_model.get_params()[_identifier]\n            _params.extend(\n                extract_parameters(\n                    _flow.components[_identifier],\n                    _flow_dict,\n                    subcomponent_model,\n                ),\n            )\n        return _params\n\n    flow_dict = get_flow_dict(flow)\n    model = model if model is not None else flow.model\n    return extract_parameters(flow, flow_dict, model, _main_call=True, main_id=flow.flow_id)\n</code></pre>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.seed_model","title":"seed_model","text":"<pre><code>seed_model(model: Any, seed: int | None = None) -&gt; Any\n</code></pre> <p>Set the random state of all the unseeded components of a model and return the seeded model.</p> <p>Required so that all seed information can be uploaded to OpenML for reproducible results.</p> <p>Models that are already seeded will maintain the seed. In this case, only integer seeds are allowed (An exception is raised when a RandomState was used as seed).</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.seed_model--parameters","title":"Parameters","text":"<p>model : sklearn model     The model to be seeded seed : int     The seed to initialize the RandomState with. Unseeded subcomponents     will be seeded with a random number from the RandomState.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.seed_model--returns","title":"Returns","text":"<p>Any</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>def seed_model(self, model: Any, seed: int | None = None) -&gt; Any:  # noqa: C901\n    \"\"\"Set the random state of all the unseeded components of a model and return the seeded\n    model.\n\n    Required so that all seed information can be uploaded to OpenML for reproducible results.\n\n    Models that are already seeded will maintain the seed. In this case,\n    only integer seeds are allowed (An exception is raised when a RandomState was used as\n    seed).\n\n    Parameters\n    ----------\n    model : sklearn model\n        The model to be seeded\n    seed : int\n        The seed to initialize the RandomState with. Unseeded subcomponents\n        will be seeded with a random number from the RandomState.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    def _seed_current_object(current_value):\n        if isinstance(current_value, int):  # acceptable behaviour\n            return False\n\n        if isinstance(current_value, np.random.RandomState):\n            raise ValueError(\n                \"Models initialized with a RandomState object are not \"\n                \"supported. Please seed with an integer. \",\n            )\n\n        if current_value is not None:\n            raise ValueError(\n                \"Models should be seeded with int or None (this should never happen). \",\n            )\n\n        return True\n\n    rs = np.random.RandomState(seed)\n    model_params = model.get_params()\n    random_states = {}\n    for param_name in sorted(model_params):\n        if \"random_state\" in param_name:\n            current_value = model_params[param_name]\n            # important to draw the value at this point (and not in the if\n            # statement) this way we guarantee that if a different set of\n            # subflows is seeded, the same number of the random generator is\n            # used\n            new_value = rs.randint(0, 2**16)\n            if _seed_current_object(current_value):\n                random_states[param_name] = new_value\n\n        # Also seed CV objects!\n        elif isinstance(model_params[param_name], sklearn.model_selection.BaseCrossValidator):\n            if not hasattr(model_params[param_name], \"random_state\"):\n                continue\n\n            current_value = model_params[param_name].random_state\n            new_value = rs.randint(0, 2**16)\n            if _seed_current_object(current_value):\n                model_params[param_name].random_state = new_value\n\n    model.set_params(**random_states)\n    return model\n</code></pre>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.trim_flow_name","title":"trim_flow_name  <code>classmethod</code>","text":"<pre><code>trim_flow_name(long_name: str, extra_trim_length: int = 100, _outer: bool = True) -&gt; str\n</code></pre> <p>Shorten generated sklearn flow name to at most <code>max_length</code> characters.</p> <p>Flows are assumed to have the following naming structure: <code>(model_selection)? (pipeline)? (steps)+</code> and will be shortened to: <code>sklearn.(selection.)?(pipeline.)?(steps)+</code> e.g. (white spaces and newlines added for readability)</p> <p>.. code ::</p> <pre><code>sklearn.pipeline.Pipeline(\n    columntransformer=sklearn.compose._column_transformer.ColumnTransformer(\n        numeric=sklearn.pipeline.Pipeline(\n            imputer=sklearn.preprocessing.imputation.Imputer,\n            standardscaler=sklearn.preprocessing.data.StandardScaler),\n        nominal=sklearn.pipeline.Pipeline(\n            simpleimputer=sklearn.impute.SimpleImputer,\n            onehotencoder=sklearn.preprocessing._encoders.OneHotEncoder)),\n    variancethreshold=sklearn.feature_selection.variance_threshold.VarianceThreshold,\n    svc=sklearn.svm.classes.SVC)\n</code></pre> <p>-&gt; <code>sklearn.Pipeline(ColumnTransformer,VarianceThreshold,SVC)</code></p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.trim_flow_name--parameters","title":"Parameters","text":"<p>long_name : str     The full flow name generated by the scikit-learn extension. extra_trim_length: int (default=100)     If the trimmed name would exceed <code>extra_trim_length</code> characters, additional trimming     of the short name is performed. This reduces the produced short name length.     There is no guarantee the end result will not exceed <code>extra_trim_length</code>. _outer : bool (default=True)     For internal use only. Specifies if the function is called recursively.</p>"},{"location":"reference/extensions/sklearn/extension/#openml.extensions.sklearn.extension.SklearnExtension.trim_flow_name--returns","title":"Returns","text":"<p>str</p> Source code in <code>openml/extensions/sklearn/extension.py</code> <pre><code>@classmethod\ndef trim_flow_name(  # noqa: C901\n    cls,\n    long_name: str,\n    extra_trim_length: int = 100,\n    _outer: bool = True,  # noqa: FBT001, FBT002\n) -&gt; str:\n    \"\"\"Shorten generated sklearn flow name to at most ``max_length`` characters.\n\n    Flows are assumed to have the following naming structure:\n    ``(model_selection)? (pipeline)? (steps)+``\n    and will be shortened to:\n    ``sklearn.(selection.)?(pipeline.)?(steps)+``\n    e.g. (white spaces and newlines added for readability)\n\n    .. code ::\n\n        sklearn.pipeline.Pipeline(\n            columntransformer=sklearn.compose._column_transformer.ColumnTransformer(\n                numeric=sklearn.pipeline.Pipeline(\n                    imputer=sklearn.preprocessing.imputation.Imputer,\n                    standardscaler=sklearn.preprocessing.data.StandardScaler),\n                nominal=sklearn.pipeline.Pipeline(\n                    simpleimputer=sklearn.impute.SimpleImputer,\n                    onehotencoder=sklearn.preprocessing._encoders.OneHotEncoder)),\n            variancethreshold=sklearn.feature_selection.variance_threshold.VarianceThreshold,\n            svc=sklearn.svm.classes.SVC)\n\n    -&gt;\n    ``sklearn.Pipeline(ColumnTransformer,VarianceThreshold,SVC)``\n\n    Parameters\n    ----------\n    long_name : str\n        The full flow name generated by the scikit-learn extension.\n    extra_trim_length: int (default=100)\n        If the trimmed name would exceed `extra_trim_length` characters, additional trimming\n        of the short name is performed. This reduces the produced short name length.\n        There is no guarantee the end result will not exceed `extra_trim_length`.\n    _outer : bool (default=True)\n        For internal use only. Specifies if the function is called recursively.\n\n    Returns\n    -------\n    str\n\n    \"\"\"\n\n    def remove_all_in_parentheses(string: str) -&gt; str:\n        string, removals = re.subn(r\"\\([^()]*\\)\", \"\", string)\n        while removals &gt; 0:\n            string, removals = re.subn(r\"\\([^()]*\\)\", \"\", string)\n        return string\n\n    # Generally, we want to trim all hyperparameters, the exception to that is for model\n    # selection, as the `estimator` hyperparameter is very indicative of what is in the flow.\n    # So we first trim name of the `estimator` specified in mode selection. For reference, in\n    # the example below, we want to trim `sklearn.tree.tree.DecisionTreeClassifier`, and\n    # keep it in the final trimmed flow name:\n    # sklearn.pipeline.Pipeline(Imputer=sklearn.preprocessing.imputation.Imputer,\n    # VarianceThreshold=sklearn.feature_selection.variance_threshold.VarianceThreshold,  # noqa: ERA001, E501\n    # Estimator=sklearn.model_selection._search.RandomizedSearchCV(estimator=\n    # sklearn.tree.tree.DecisionTreeClassifier))\n    if \"sklearn.model_selection\" in long_name:\n        start_index = long_name.index(\"sklearn.model_selection\")\n        estimator_start = (\n            start_index + long_name[start_index:].index(\"estimator=\") + len(\"estimator=\")\n        )\n\n        model_select_boilerplate = long_name[start_index:estimator_start]\n        # above is .g. \"sklearn.model_selection._search.RandomizedSearchCV(estimator=\"\n        model_selection_class = model_select_boilerplate.split(\"(\")[0].split(\".\")[-1]\n\n        # Now we want to also find and parse the `estimator`, for this we find the closing\n        # parenthesis to the model selection technique:\n        closing_parenthesis_expected = 1\n        for char in long_name[estimator_start:]:\n            if char == \"(\":\n                closing_parenthesis_expected += 1\n            if char == \")\":\n                closing_parenthesis_expected -= 1\n            if closing_parenthesis_expected == 0:\n                break\n\n        _end: int = estimator_start + len(long_name[estimator_start:]) - 1\n        model_select_pipeline = long_name[estimator_start:_end]\n\n        trimmed_pipeline = cls.trim_flow_name(model_select_pipeline, _outer=False)\n        _, trimmed_pipeline = trimmed_pipeline.split(\".\", maxsplit=1)  # trim module prefix\n        model_select_short = f\"sklearn.{model_selection_class}[{trimmed_pipeline}]\"\n        name = long_name[:start_index] + model_select_short + long_name[_end + 1 :]\n    else:\n        name = long_name\n\n    module_name = long_name.split(\".\")[0]\n    short_name = module_name + \".{}\"\n\n    if name.startswith(\"sklearn.pipeline\"):\n        full_pipeline_class, pipeline = name[:-1].split(\"(\", maxsplit=1)\n        pipeline_class = full_pipeline_class.split(\".\")[-1]\n        # We don't want nested pipelines in the short name, so we trim all complicated\n        # subcomponents, i.e. those with parentheses:\n        pipeline = remove_all_in_parentheses(pipeline)\n\n        # then the pipeline steps are formatted e.g.:\n        # step1name=sklearn.submodule.ClassName,step2name...\n        components = [component.split(\".\")[-1] for component in pipeline.split(\",\")]\n        pipeline = f\"{pipeline_class}({','.join(components)})\"\n        if len(short_name.format(pipeline)) &gt; extra_trim_length:\n            pipeline = f\"{pipeline_class}(...,{components[-1]})\"\n    else:\n        # Just a simple component: e.g. sklearn.tree.DecisionTreeClassifier\n        pipeline = remove_all_in_parentheses(name).split(\".\")[-1]\n\n    if not _outer:\n        # Anything from parenthesis in inner calls should not be culled, so we use brackets\n        pipeline = pipeline.replace(\"(\", \"[\").replace(\")\", \"]\")\n    else:\n        # Square brackets may be introduced with nested model_selection\n        pipeline = pipeline.replace(\"[\", \"(\").replace(\"]\", \")\")\n\n    return short_name.format(pipeline)\n</code></pre>"},{"location":"reference/flows/","title":"flows","text":""},{"location":"reference/flows/#openml.flows","title":"openml.flows","text":""},{"location":"reference/flows/#openml.flows.OpenMLFlow","title":"OpenMLFlow","text":"<pre><code>OpenMLFlow(name: str, description: str, model: object, components: dict, parameters: dict, parameters_meta_info: dict, external_version: str, tags: list, language: str, dependencies: str, class_name: str | None = None, custom_name: str | None = None, binary_url: str | None = None, binary_format: str | None = None, binary_md5: str | None = None, uploader: str | None = None, upload_date: str | None = None, flow_id: int | None = None, extension: Extension | None = None, version: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Flow. Stores machine learning models.</p> <p>Flows should not be generated manually, but by the function :meth:<code>openml.flows.create_flow_from_model</code>. Using this helper function ensures that all relevant fields are filled in.</p> <p>Implements <code>openml.implementation.upload.xsd &lt;https://github.com/openml/openml/blob/master/openml_OS/views/pages/api_new/v1/xsd/ openml.implementation.upload.xsd&gt;</code>_.</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow--parameters","title":"Parameters","text":"<p>name : str     Name of the flow. Is used together with the attribute     <code>external_version</code> as a unique identifier of the flow. description : str     Human-readable description of the flow (free text). model : object     ML model which is described by this flow. components : OrderedDict     Mapping from component identifier to an OpenMLFlow object. Components     are usually subfunctions of an algorithm (e.g. kernels), base learners     in ensemble algorithms (decision tree in adaboost) or building blocks     of a machine learning pipeline. Components are modeled as independent     flows and can be shared between flows (different pipelines can use     the same components). parameters : OrderedDict     Mapping from parameter name to the parameter default value. The     parameter default value must be of type <code>str</code>, so that the respective     toolbox plugin can take care of casting the parameter default value to     the correct type. parameters_meta_info : OrderedDict     Mapping from parameter name to <code>dict</code>. Stores additional information     for each parameter. Required keys are <code>data_type</code> and <code>description</code>. external_version : str     Version number of the software the flow is implemented in. Is used     together with the attribute <code>name</code> as a uniquer identifier of the flow. tags : list     List of tags. Created on the server by other API calls. language : str     Natural language the flow is described in (not the programming     language). dependencies : str     A list of dependencies necessary to run the flow. This field should     contain all libraries the flow depends on. To allow reproducibility     it should also specify the exact version numbers. class_name : str, optional     The development language name of the class which is described by this     flow. custom_name : str, optional     Custom name of the flow given by the owner. binary_url : str, optional     Url from which the binary can be downloaded. Added by the server.     Ignored when uploaded manually. Will not be used by the python API     because binaries aren't compatible across machines. binary_format : str, optional     Format in which the binary code was uploaded. Will not be used by the     python API because binaries aren't compatible across machines. binary_md5 : str, optional     MD5 checksum to check if the binary code was correctly downloaded. Will     not be used by the python API because binaries aren't compatible across     machines. uploader : str, optional     OpenML user ID of the uploader. Filled in by the server. upload_date : str, optional     Date the flow was uploaded. Filled in by the server. flow_id : int, optional     Flow ID. Assigned by the server. extension : Extension, optional     The extension for a flow (e.g., sklearn). version : str, optional     OpenML version of the flow. Assigned by the server.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    name: str,\n    description: str,\n    model: object,\n    components: dict,\n    parameters: dict,\n    parameters_meta_info: dict,\n    external_version: str,\n    tags: list,\n    language: str,\n    dependencies: str,\n    class_name: str | None = None,\n    custom_name: str | None = None,\n    binary_url: str | None = None,\n    binary_format: str | None = None,\n    binary_md5: str | None = None,\n    uploader: str | None = None,\n    upload_date: str | None = None,\n    flow_id: int | None = None,\n    extension: Extension | None = None,\n    version: str | None = None,\n):\n    self.name = name\n    self.description = description\n    self.model = model\n\n    for variable, variable_name in [\n        [components, \"components\"],\n        [parameters, \"parameters\"],\n        [parameters_meta_info, \"parameters_meta_info\"],\n    ]:\n        if not isinstance(variable, (OrderedDict, dict)):\n            raise TypeError(\n                f\"{variable_name} must be of type OrderedDict or dict, \"\n                f\"but is {type(variable)}.\",\n            )\n\n    self.components = components\n    self.parameters = parameters\n    self.parameters_meta_info = parameters_meta_info\n    self.class_name = class_name\n\n    keys_parameters = set(parameters.keys())\n    keys_parameters_meta_info = set(parameters_meta_info.keys())\n    if len(keys_parameters.difference(keys_parameters_meta_info)) &gt; 0:\n        raise ValueError(\n            f\"Parameter {keys_parameters.difference(keys_parameters_meta_info)!s} only in \"\n            \"parameters, but not in parameters_meta_info.\",\n        )\n    if len(keys_parameters_meta_info.difference(keys_parameters)) &gt; 0:\n        raise ValueError(\n            f\"Parameter {keys_parameters_meta_info.difference(keys_parameters)!s} only in \"\n            \" parameters_meta_info, but not in parameters.\",\n        )\n\n    self.external_version = external_version\n    self.uploader = uploader\n\n    self.custom_name = custom_name\n    self.tags = tags if tags is not None else []\n    self.binary_url = binary_url\n    self.binary_format = binary_format\n    self.binary_md5 = binary_md5\n    self.version = version\n    self.upload_date = upload_date\n    self.language = language\n    self.dependencies = dependencies\n    self.flow_id = flow_id\n    if extension is None:\n        self._extension = get_extension_by_flow(self)\n    else:\n        self._extension = extension\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.extension","title":"extension  <code>property</code>","text":"<pre><code>extension: Extension\n</code></pre> <p>The extension of the flow (e.g., sklearn).</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The ID of the flow.</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.from_filesystem","title":"from_filesystem  <code>classmethod</code>","text":"<pre><code>from_filesystem(input_directory: str | Path) -&gt; OpenMLFlow\n</code></pre> <p>Read a flow from an XML in input_directory on the filesystem.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>@classmethod\ndef from_filesystem(cls, input_directory: str | Path) -&gt; OpenMLFlow:\n    \"\"\"Read a flow from an XML in input_directory on the filesystem.\"\"\"\n    input_directory = Path(input_directory) / \"flow.xml\"\n    with input_directory.open() as f:\n        xml_string = f.read()\n    return OpenMLFlow._from_dict(xmltodict.parse(xml_string))\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.get_structure","title":"get_structure","text":"<pre><code>get_structure(key_item: str) -&gt; dict[str, list[str]]\n</code></pre> <p>Returns for each sub-component of the flow the path of identifiers that should be traversed to reach this component. The resulting dict maps a key (identifying a flow by either its id, name or fullname) to the parameter prefix.</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.get_structure--parameters","title":"Parameters","text":"<p>key_item: str     The flow attribute that will be used to identify flows in the     structure. Allowed values {flow_id, name}</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.get_structure--returns","title":"Returns","text":"<p>dict[str, List[str]]     The flow structure</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def get_structure(self, key_item: str) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Returns for each sub-component of the flow the path of identifiers\n    that should be traversed to reach this component. The resulting dict\n    maps a key (identifying a flow by either its id, name or fullname) to\n    the parameter prefix.\n\n    Parameters\n    ----------\n    key_item: str\n        The flow attribute that will be used to identify flows in the\n        structure. Allowed values {flow_id, name}\n\n    Returns\n    -------\n    dict[str, List[str]]\n        The flow structure\n    \"\"\"\n    if key_item not in [\"flow_id\", \"name\"]:\n        raise ValueError(\"key_item should be in {flow_id, name}\")\n    structure = {}\n    for key, sub_flow in self.components.items():\n        sub_structure = sub_flow.get_structure(key_item)\n        for flow_name, flow_sub_structure in sub_structure.items():\n            structure[flow_name] = [key, *flow_sub_structure]\n    structure[getattr(self, key_item)] = []\n    return structure\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.get_subflow","title":"get_subflow","text":"<pre><code>get_subflow(structure: list[str]) -&gt; OpenMLFlow\n</code></pre> <p>Returns a subflow from the tree of dependencies.</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.get_subflow--parameters","title":"Parameters","text":"<p>structure: list[str]     A list of strings, indicating the location of the subflow</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.get_subflow--returns","title":"Returns","text":"<p>OpenMLFlow     The OpenMLFlow that corresponds to the structure</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def get_subflow(self, structure: list[str]) -&gt; OpenMLFlow:\n    \"\"\"\n    Returns a subflow from the tree of dependencies.\n\n    Parameters\n    ----------\n    structure: list[str]\n        A list of strings, indicating the location of the subflow\n\n    Returns\n    -------\n    OpenMLFlow\n        The OpenMLFlow that corresponds to the structure\n    \"\"\"\n    # make a copy of structure, as we don't want to change it in the\n    # outer scope\n    structure = list(structure)\n    if len(structure) &lt; 1:\n        raise ValueError(\"Please provide a structure list of size &gt;= 1\")\n    sub_identifier = structure[0]\n    if sub_identifier not in self.components:\n        raise ValueError(\n            f\"Flow {self.name} does not contain component with \" f\"identifier {sub_identifier}\",\n        )\n    if len(structure) == 1:\n        return self.components[sub_identifier]  # type: ignore\n\n    structure.pop(0)\n    return self.components[sub_identifier].get_subflow(structure)  # type: ignore\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.publish","title":"publish","text":"<pre><code>publish(raise_error_if_exists: bool = False) -&gt; OpenMLFlow\n</code></pre> <p>Publish this flow to OpenML server.</p> <p>Raises a PyOpenMLError if the flow exists on the server, but <code>self.flow_id</code> does not match the server known flow id.</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.publish--parameters","title":"Parameters","text":"<p>raise_error_if_exists : bool, optional (default=False)     If True, raise PyOpenMLError if the flow exists on the server.     If False, update the local flow to match the server flow.</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.publish--returns","title":"Returns","text":"<p>self : OpenMLFlow</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def publish(self, raise_error_if_exists: bool = False) -&gt; OpenMLFlow:  # noqa: FBT001, FBT002\n    \"\"\"Publish this flow to OpenML server.\n\n    Raises a PyOpenMLError if the flow exists on the server, but\n    `self.flow_id` does not match the server known flow id.\n\n    Parameters\n    ----------\n    raise_error_if_exists : bool, optional (default=False)\n        If True, raise PyOpenMLError if the flow exists on the server.\n        If False, update the local flow to match the server flow.\n\n    Returns\n    -------\n    self : OpenMLFlow\n\n    \"\"\"\n    # Import at top not possible because of cyclic dependencies. In\n    # particular, flow.py tries to import functions.py in order to call\n    # get_flow(), while functions.py tries to import flow.py in order to\n    # instantiate an OpenMLFlow.\n    import openml.flows.functions\n\n    flow_id = openml.flows.functions.flow_exists(self.name, self.external_version)\n    if not flow_id:\n        if self.flow_id:\n            raise openml.exceptions.PyOpenMLError(\n                \"Flow does not exist on the server, \" \"but 'flow.flow_id' is not None.\",\n            )\n        super().publish()\n        assert self.flow_id is not None  # for mypy\n        flow_id = self.flow_id\n    elif raise_error_if_exists:\n        error_message = f\"This OpenMLFlow already exists with id: {flow_id}.\"\n        raise openml.exceptions.PyOpenMLError(error_message)\n    elif self.flow_id is not None and self.flow_id != flow_id:\n        raise openml.exceptions.PyOpenMLError(\n            \"Local flow_id does not match server flow_id: \" f\"'{self.flow_id}' vs '{flow_id}'\",\n        )\n\n    flow = openml.flows.functions.get_flow(flow_id)\n    _copy_server_fields(flow, self)\n    try:\n        openml.flows.functions.assert_flows_equal(\n            self,\n            flow,\n            flow.upload_date,\n            ignore_parameter_values=True,\n            ignore_custom_name_if_none=True,\n        )\n    except ValueError as e:\n        message = e.args[0]\n        raise ValueError(\n            \"The flow on the server is inconsistent with the local flow. \"\n            f\"The server flow ID is {flow_id}. Please check manually and remove \"\n            f\"the flow if necessary! Error is:\\n'{message}'\",\n        ) from e\n    return self\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.to_filesystem","title":"to_filesystem","text":"<pre><code>to_filesystem(output_directory: str | Path) -&gt; None\n</code></pre> <p>Write a flow to the filesystem as XML to output_directory.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def to_filesystem(self, output_directory: str | Path) -&gt; None:\n    \"\"\"Write a flow to the filesystem as XML to output_directory.\"\"\"\n    output_directory = Path(output_directory)\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    output_path = output_directory / \"flow.xml\"\n    if output_path.exists():\n        raise ValueError(\"Output directory already contains a flow.xml file.\")\n\n    run_xml = self._to_xml()\n    with output_path.open(\"w\") as f:\n        f.write(run_xml)\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/flows/#openml.flows.assert_flows_equal","title":"assert_flows_equal","text":"<pre><code>assert_flows_equal(flow1: OpenMLFlow, flow2: OpenMLFlow, ignore_parameter_values_on_older_children: str | None = None, ignore_parameter_values: bool = False, ignore_custom_name_if_none: bool = False, check_description: bool = True) -&gt; None\n</code></pre> <p>Check equality of two flows.</p> <p>Two flows are equal if their all keys which are not set by the server are equal, as well as all their parameters and components.</p>"},{"location":"reference/flows/#openml.flows.assert_flows_equal--parameters","title":"Parameters","text":"<p>flow1 : OpenMLFlow</p> <p>flow2 : OpenMLFlow</p> str (optional) <p>If set to <code>OpenMLFlow.upload_date</code>, ignores parameters in a child flow if it's upload date predates the upload date of the parent flow.</p> bool <p>Whether to ignore parameter values when comparing flows.</p> bool <p>Whether to ignore the custom name field if either flow has <code>custom_name</code> equal to <code>None</code>.</p> bool <p>Whether to ignore matching of flow descriptions.</p> Source code in <code>openml/flows/functions.py</code> <pre><code>def assert_flows_equal(  # noqa: C901, PLR0912, PLR0913, PLR0915\n    flow1: OpenMLFlow,\n    flow2: OpenMLFlow,\n    ignore_parameter_values_on_older_children: str | None = None,\n    ignore_parameter_values: bool = False,  # noqa: FBT001, FBT002\n    ignore_custom_name_if_none: bool = False,  # noqa:  FBT001, FBT002\n    check_description: bool = True,  # noqa:  FBT001, FBT002\n) -&gt; None:\n    \"\"\"Check equality of two flows.\n\n    Two flows are equal if their all keys which are not set by the server\n    are equal, as well as all their parameters and components.\n\n    Parameters\n    ----------\n    flow1 : OpenMLFlow\n\n    flow2 : OpenMLFlow\n\n    ignore_parameter_values_on_older_children : str (optional)\n        If set to ``OpenMLFlow.upload_date``, ignores parameters in a child\n        flow if it's upload date predates the upload date of the parent flow.\n\n    ignore_parameter_values : bool\n        Whether to ignore parameter values when comparing flows.\n\n    ignore_custom_name_if_none : bool\n        Whether to ignore the custom name field if either flow has `custom_name` equal to `None`.\n\n    check_description : bool\n        Whether to ignore matching of flow descriptions.\n    \"\"\"\n    if not isinstance(flow1, OpenMLFlow):\n        raise TypeError(f\"Argument 1 must be of type OpenMLFlow, but is {type(flow1)}\")\n\n    if not isinstance(flow2, OpenMLFlow):\n        raise TypeError(f\"Argument 2 must be of type OpenMLFlow, but is {type(flow2)}\")\n\n    # TODO as they are actually now saved during publish, it might be good to\n    # check for the equality of these as well.\n    generated_by_the_server = [\n        \"flow_id\",\n        \"uploader\",\n        \"version\",\n        \"upload_date\",\n        # Tags aren't directly created by the server,\n        # but the uploader has no control over them!\n        \"tags\",\n    ]\n    ignored_by_python_api = [\"binary_url\", \"binary_format\", \"binary_md5\", \"model\", \"_entity_id\"]\n\n    for key in set(flow1.__dict__.keys()).union(flow2.__dict__.keys()):\n        if key in generated_by_the_server + ignored_by_python_api:\n            continue\n        attr1 = getattr(flow1, key, None)\n        attr2 = getattr(flow2, key, None)\n        if key == \"components\":\n            if not (isinstance(attr1, Dict) and isinstance(attr2, Dict)):\n                raise TypeError(\"Cannot compare components because they are not dictionary.\")\n\n            for name in set(attr1.keys()).union(attr2.keys()):\n                if name not in attr1:\n                    raise ValueError(\n                        f\"Component {name} only available in argument2, but not in argument1.\",\n                    )\n                if name not in attr2:\n                    raise ValueError(\n                        f\"Component {name} only available in argument2, but not in argument1.\",\n                    )\n                assert_flows_equal(\n                    attr1[name],\n                    attr2[name],\n                    ignore_parameter_values_on_older_children,\n                    ignore_parameter_values,\n                    ignore_custom_name_if_none,\n                )\n        elif key == \"_extension\":\n            continue\n        elif check_description and key == \"description\":\n            # to ignore matching of descriptions since sklearn based flows may have\n            # altering docstrings and is not guaranteed to be consistent\n            continue\n        else:\n            if key == \"parameters\":\n                if ignore_parameter_values or ignore_parameter_values_on_older_children:\n                    params_flow_1 = set(flow1.parameters.keys())\n                    params_flow_2 = set(flow2.parameters.keys())\n                    symmetric_difference = params_flow_1 ^ params_flow_2\n                    if len(symmetric_difference) &gt; 0:\n                        raise ValueError(\n                            f\"Flow {flow1.name}: parameter set of flow \"\n                            \"differs from the parameters stored \"\n                            \"on the server.\",\n                        )\n\n                if ignore_parameter_values_on_older_children:\n                    assert (\n                        flow1.upload_date is not None\n                    ), \"Flow1 has no upload date that allows us to compare age of children.\"\n                    upload_date_current_flow = dateutil.parser.parse(flow1.upload_date)\n                    upload_date_parent_flow = dateutil.parser.parse(\n                        ignore_parameter_values_on_older_children,\n                    )\n                    if upload_date_current_flow &lt; upload_date_parent_flow:\n                        continue\n\n                if ignore_parameter_values:\n                    # Continue needs to be done here as the first if\n                    # statement triggers in both special cases\n                    continue\n            elif (\n                key == \"custom_name\"\n                and ignore_custom_name_if_none\n                and (attr1 is None or attr2 is None)\n            ):\n                # If specified, we allow `custom_name` inequality if one flow's name is None.\n                # Helps with backwards compatibility as `custom_name` is now auto-generated, but\n                # before it used to be `None`.\n                continue\n            elif key == \"parameters_meta_info\":\n                # this value is a dictionary where each key is a parameter name, containing another\n                # dictionary with keys specifying the parameter's 'description' and 'data_type'\n                # checking parameter descriptions can be ignored since that might change\n                # data type check can also be ignored if one of them is not defined, i.e., None\n                params1 = set(flow1.parameters_meta_info)\n                params2 = set(flow2.parameters_meta_info)\n                if params1 != params2:\n                    raise ValueError(\n                        \"Parameter list in meta info for parameters differ in the two flows.\",\n                    )\n                # iterating over the parameter's meta info list\n                for param in params1:\n                    if (\n                        isinstance(flow1.parameters_meta_info[param], Dict)\n                        and isinstance(flow2.parameters_meta_info[param], Dict)\n                        and \"data_type\" in flow1.parameters_meta_info[param]\n                        and \"data_type\" in flow2.parameters_meta_info[param]\n                    ):\n                        value1 = flow1.parameters_meta_info[param][\"data_type\"]\n                        value2 = flow2.parameters_meta_info[param][\"data_type\"]\n                    else:\n                        value1 = flow1.parameters_meta_info[param]\n                        value2 = flow2.parameters_meta_info[param]\n                    if value1 is None or value2 is None:\n                        continue\n\n                    if value1 != value2:\n                        raise ValueError(\n                            f\"Flow {flow1.name}: data type for parameter {param} in {key} differ \"\n                            f\"as {value1}\\nvs\\n{value2}\",\n                        )\n                # the continue is to avoid the 'attr != attr2' check at end of function\n                continue\n\n            if attr1 != attr2:\n                raise ValueError(\n                    f\"Flow {flow1.name!s}: values for attribute '{key!s}' differ: \"\n                    f\"'{attr1!s}'\\nvs\\n'{attr2!s}'.\",\n                )\n</code></pre>"},{"location":"reference/flows/#openml.flows.delete_flow","title":"delete_flow","text":"<pre><code>delete_flow(flow_id: int) -&gt; bool\n</code></pre> <p>Delete flow with id <code>flow_id</code> from the OpenML server.</p> <p>You can only delete flows which you uploaded and which which are not linked to runs.</p>"},{"location":"reference/flows/#openml.flows.delete_flow--parameters","title":"Parameters","text":"<p>flow_id : int     OpenML id of the flow</p>"},{"location":"reference/flows/#openml.flows.delete_flow--returns","title":"Returns","text":"<p>bool     True if the deletion was successful. False otherwise.</p> Source code in <code>openml/flows/functions.py</code> <pre><code>def delete_flow(flow_id: int) -&gt; bool:\n    \"\"\"Delete flow with id `flow_id` from the OpenML server.\n\n    You can only delete flows which you uploaded and which\n    which are not linked to runs.\n\n    Parameters\n    ----------\n    flow_id : int\n        OpenML id of the flow\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"flow\", flow_id)\n</code></pre>"},{"location":"reference/flows/#openml.flows.flow_exists","title":"flow_exists","text":"<pre><code>flow_exists(name: str, external_version: str) -&gt; int | bool\n</code></pre> <p>Retrieves the flow id.</p> <p>A flow is uniquely identified by name + external_version.</p>"},{"location":"reference/flows/#openml.flows.flow_exists--parameters","title":"Parameters","text":"<p>name : string     Name of the flow external_version : string     Version information associated with flow.</p>"},{"location":"reference/flows/#openml.flows.flow_exists--returns","title":"Returns","text":"<p>flow_exist : int or bool     flow id iff exists, False otherwise</p>"},{"location":"reference/flows/#openml.flows.flow_exists--notes","title":"Notes","text":"<p>see www.openml.org/api_docs/#!/flow/get_flow_exists_name_version</p> Source code in <code>openml/flows/functions.py</code> <pre><code>def flow_exists(name: str, external_version: str) -&gt; int | bool:\n    \"\"\"Retrieves the flow id.\n\n    A flow is uniquely identified by name + external_version.\n\n    Parameters\n    ----------\n    name : string\n        Name of the flow\n    external_version : string\n        Version information associated with flow.\n\n    Returns\n    -------\n    flow_exist : int or bool\n        flow id iff exists, False otherwise\n\n    Notes\n    -----\n    see https://www.openml.org/api_docs/#!/flow/get_flow_exists_name_version\n    \"\"\"\n    if not (isinstance(name, str) and len(name) &gt; 0):\n        raise ValueError(\"Argument 'name' should be a non-empty string\")\n    if not (isinstance(name, str) and len(external_version) &gt; 0):\n        raise ValueError(\"Argument 'version' should be a non-empty string\")\n\n    xml_response = openml._api_calls._perform_api_call(\n        \"flow/exists\",\n        \"post\",\n        data={\"name\": name, \"external_version\": external_version},\n    )\n\n    result_dict = xmltodict.parse(xml_response)\n    flow_id = int(result_dict[\"oml:flow_exists\"][\"oml:id\"])\n    return flow_id if flow_id &gt; 0 else False\n</code></pre>"},{"location":"reference/flows/#openml.flows.get_flow","title":"get_flow","text":"<pre><code>get_flow(flow_id: int, reinstantiate: bool = False, strict_version: bool = True) -&gt; OpenMLFlow\n</code></pre> <p>Download the OpenML flow for a given flow ID.</p>"},{"location":"reference/flows/#openml.flows.get_flow--parameters","title":"Parameters","text":"<p>flow_id : int     The OpenML flow id.</p> bool <p>Whether to reinstantiate the flow to a model instance.</p> bool, default=True <p>Whether to fail if version requirements are not fulfilled.</p>"},{"location":"reference/flows/#openml.flows.get_flow--returns","title":"Returns","text":"<p>flow : OpenMLFlow     the flow</p> Source code in <code>openml/flows/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_flow(flow_id: int, reinstantiate: bool = False, strict_version: bool = True) -&gt; OpenMLFlow:  # noqa: FBT001, FBT002\n    \"\"\"Download the OpenML flow for a given flow ID.\n\n    Parameters\n    ----------\n    flow_id : int\n        The OpenML flow id.\n\n    reinstantiate: bool\n        Whether to reinstantiate the flow to a model instance.\n\n    strict_version : bool, default=True\n        Whether to fail if version requirements are not fulfilled.\n\n    Returns\n    -------\n    flow : OpenMLFlow\n        the flow\n    \"\"\"\n    flow_id = int(flow_id)\n    flow = _get_flow_description(flow_id)\n\n    if reinstantiate:\n        flow.model = flow.extension.flow_to_model(flow, strict_version=strict_version)\n        if not strict_version:\n            # check if we need to return a new flow b/c of version mismatch\n            new_flow = flow.extension.model_to_flow(flow.model)\n            if new_flow.dependencies != flow.dependencies:\n                return new_flow\n    return flow\n</code></pre>"},{"location":"reference/flows/#openml.flows.get_flow_id","title":"get_flow_id","text":"<pre><code>get_flow_id(model: Any | None = None, name: str | None = None, exact_version: bool = True) -&gt; int | bool | list[int]\n</code></pre> <p>Retrieves the flow id for a model or a flow name.</p> <p>Provide either a model or a name to this function. Depending on the input, it does</p> <ul> <li><code>model</code> and <code>exact_version == True</code>: This helper function first queries for the necessary   extension. Second, it uses that extension to convert the model into a flow. Third, it   executes <code>flow_exists</code> to potentially obtain the flow id the flow is published to the   server.</li> <li><code>model</code> and <code>exact_version == False</code>: This helper function first queries for the   necessary extension. Second, it uses that extension to convert the model into a flow. Third   it calls <code>list_flows</code> and filters the returned values based on the flow name.</li> <li><code>name</code>: Ignores <code>exact_version</code> and calls <code>list_flows</code>, then filters the returned   values based on the flow name.</li> </ul>"},{"location":"reference/flows/#openml.flows.get_flow_id--parameters","title":"Parameters","text":"<p>model : object     Any model. Must provide either <code>model</code> or <code>name</code>. name : str     Name of the flow. Must provide either <code>model</code> or <code>name</code>. exact_version : bool     Whether to return the flow id of the exact version or all flow ids where the name     of the flow matches. This is only taken into account for a model where a version number     is available (requires <code>model</code> to be set).</p>"},{"location":"reference/flows/#openml.flows.get_flow_id--returns","title":"Returns","text":"<p>int or bool, List     flow id iff exists, <code>False</code> otherwise, List if <code>exact_version is False</code></p> Source code in <code>openml/flows/functions.py</code> <pre><code>def get_flow_id(\n    model: Any | None = None,\n    name: str | None = None,\n    exact_version: bool = True,  # noqa: FBT001, FBT002\n) -&gt; int | bool | list[int]:\n    \"\"\"Retrieves the flow id for a model or a flow name.\n\n    Provide either a model or a name to this function. Depending on the input, it does\n\n    * ``model`` and ``exact_version == True``: This helper function first queries for the necessary\n      extension. Second, it uses that extension to convert the model into a flow. Third, it\n      executes ``flow_exists`` to potentially obtain the flow id the flow is published to the\n      server.\n    * ``model`` and ``exact_version == False``: This helper function first queries for the\n      necessary extension. Second, it uses that extension to convert the model into a flow. Third\n      it calls ``list_flows`` and filters the returned values based on the flow name.\n    * ``name``: Ignores ``exact_version`` and calls ``list_flows``, then filters the returned\n      values based on the flow name.\n\n    Parameters\n    ----------\n    model : object\n        Any model. Must provide either ``model`` or ``name``.\n    name : str\n        Name of the flow. Must provide either ``model`` or ``name``.\n    exact_version : bool\n        Whether to return the flow id of the exact version or all flow ids where the name\n        of the flow matches. This is only taken into account for a model where a version number\n        is available (requires ``model`` to be set).\n\n    Returns\n    -------\n    int or bool, List\n        flow id iff exists, ``False`` otherwise, List if ``exact_version is False``\n    \"\"\"\n    if model is not None and name is not None:\n        raise ValueError(\"Must provide either argument `model` or argument `name`, but not both.\")\n\n    if model is not None:\n        extension = openml.extensions.get_extension_by_model(model, raise_if_no_extension=True)\n        if extension is None:\n            # This should never happen and is only here to please mypy will be gone soon once the\n            # whole function is removed\n            raise TypeError(extension)\n        flow = extension.model_to_flow(model)\n        flow_name = flow.name\n        external_version = flow.external_version\n    elif name is not None:\n        flow_name = name\n        exact_version = False\n        external_version = None\n    else:\n        raise ValueError(\n            \"Need to provide either argument `model` or argument `name`, but both are `None`.\"\n        )\n\n    if exact_version:\n        if external_version is None:\n            raise ValueError(\"exact_version should be False if model is None!\")\n        return flow_exists(name=flow_name, external_version=external_version)\n\n    flows = list_flows()\n    flows = flows.query(f'name == \"{flow_name}\"')\n    return flows[\"id\"].to_list()  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/flows/#openml.flows.list_flows","title":"list_flows","text":"<pre><code>list_flows(offset: int | None = None, size: int | None = None, tag: str | None = None, uploader: str | None = None) -&gt; DataFrame\n</code></pre> <p>Return a list of all flows which are on OpenML. (Supports large amount of results)</p>"},{"location":"reference/flows/#openml.flows.list_flows--parameters","title":"Parameters","text":"<p>offset : int, optional     the number of flows to skip, starting from the first size : int, optional     the maximum number of flows to return tag : str, optional     the tag to include kwargs: dict, optional     Legal filter operators: uploader.</p>"},{"location":"reference/flows/#openml.flows.list_flows--returns","title":"Returns","text":"<p>flows : dataframe         Each row maps to a dataset         Each column contains the following information:         - flow id         - full name         - name         - version         - external version         - uploader</p> Source code in <code>openml/flows/functions.py</code> <pre><code>def list_flows(\n    offset: int | None = None,\n    size: int | None = None,\n    tag: str | None = None,\n    uploader: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a list of all flows which are on OpenML.\n    (Supports large amount of results)\n\n    Parameters\n    ----------\n    offset : int, optional\n        the number of flows to skip, starting from the first\n    size : int, optional\n        the maximum number of flows to return\n    tag : str, optional\n        the tag to include\n    kwargs: dict, optional\n        Legal filter operators: uploader.\n\n    Returns\n    -------\n    flows : dataframe\n            Each row maps to a dataset\n            Each column contains the following information:\n            - flow id\n            - full name\n            - name\n            - version\n            - external version\n            - uploader\n    \"\"\"\n    listing_call = partial(_list_flows, tag=tag, uploader=uploader)\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/flows/flow/","title":"flow","text":""},{"location":"reference/flows/flow/#openml.flows.flow","title":"openml.flows.flow","text":""},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow","title":"OpenMLFlow","text":"<pre><code>OpenMLFlow(name: str, description: str, model: object, components: dict, parameters: dict, parameters_meta_info: dict, external_version: str, tags: list, language: str, dependencies: str, class_name: str | None = None, custom_name: str | None = None, binary_url: str | None = None, binary_format: str | None = None, binary_md5: str | None = None, uploader: str | None = None, upload_date: str | None = None, flow_id: int | None = None, extension: Extension | None = None, version: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Flow. Stores machine learning models.</p> <p>Flows should not be generated manually, but by the function :meth:<code>openml.flows.create_flow_from_model</code>. Using this helper function ensures that all relevant fields are filled in.</p> <p>Implements <code>openml.implementation.upload.xsd &lt;https://github.com/openml/openml/blob/master/openml_OS/views/pages/api_new/v1/xsd/ openml.implementation.upload.xsd&gt;</code>_.</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow--parameters","title":"Parameters","text":"<p>name : str     Name of the flow. Is used together with the attribute     <code>external_version</code> as a unique identifier of the flow. description : str     Human-readable description of the flow (free text). model : object     ML model which is described by this flow. components : OrderedDict     Mapping from component identifier to an OpenMLFlow object. Components     are usually subfunctions of an algorithm (e.g. kernels), base learners     in ensemble algorithms (decision tree in adaboost) or building blocks     of a machine learning pipeline. Components are modeled as independent     flows and can be shared between flows (different pipelines can use     the same components). parameters : OrderedDict     Mapping from parameter name to the parameter default value. The     parameter default value must be of type <code>str</code>, so that the respective     toolbox plugin can take care of casting the parameter default value to     the correct type. parameters_meta_info : OrderedDict     Mapping from parameter name to <code>dict</code>. Stores additional information     for each parameter. Required keys are <code>data_type</code> and <code>description</code>. external_version : str     Version number of the software the flow is implemented in. Is used     together with the attribute <code>name</code> as a uniquer identifier of the flow. tags : list     List of tags. Created on the server by other API calls. language : str     Natural language the flow is described in (not the programming     language). dependencies : str     A list of dependencies necessary to run the flow. This field should     contain all libraries the flow depends on. To allow reproducibility     it should also specify the exact version numbers. class_name : str, optional     The development language name of the class which is described by this     flow. custom_name : str, optional     Custom name of the flow given by the owner. binary_url : str, optional     Url from which the binary can be downloaded. Added by the server.     Ignored when uploaded manually. Will not be used by the python API     because binaries aren't compatible across machines. binary_format : str, optional     Format in which the binary code was uploaded. Will not be used by the     python API because binaries aren't compatible across machines. binary_md5 : str, optional     MD5 checksum to check if the binary code was correctly downloaded. Will     not be used by the python API because binaries aren't compatible across     machines. uploader : str, optional     OpenML user ID of the uploader. Filled in by the server. upload_date : str, optional     Date the flow was uploaded. Filled in by the server. flow_id : int, optional     Flow ID. Assigned by the server. extension : Extension, optional     The extension for a flow (e.g., sklearn). version : str, optional     OpenML version of the flow. Assigned by the server.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    name: str,\n    description: str,\n    model: object,\n    components: dict,\n    parameters: dict,\n    parameters_meta_info: dict,\n    external_version: str,\n    tags: list,\n    language: str,\n    dependencies: str,\n    class_name: str | None = None,\n    custom_name: str | None = None,\n    binary_url: str | None = None,\n    binary_format: str | None = None,\n    binary_md5: str | None = None,\n    uploader: str | None = None,\n    upload_date: str | None = None,\n    flow_id: int | None = None,\n    extension: Extension | None = None,\n    version: str | None = None,\n):\n    self.name = name\n    self.description = description\n    self.model = model\n\n    for variable, variable_name in [\n        [components, \"components\"],\n        [parameters, \"parameters\"],\n        [parameters_meta_info, \"parameters_meta_info\"],\n    ]:\n        if not isinstance(variable, (OrderedDict, dict)):\n            raise TypeError(\n                f\"{variable_name} must be of type OrderedDict or dict, \"\n                f\"but is {type(variable)}.\",\n            )\n\n    self.components = components\n    self.parameters = parameters\n    self.parameters_meta_info = parameters_meta_info\n    self.class_name = class_name\n\n    keys_parameters = set(parameters.keys())\n    keys_parameters_meta_info = set(parameters_meta_info.keys())\n    if len(keys_parameters.difference(keys_parameters_meta_info)) &gt; 0:\n        raise ValueError(\n            f\"Parameter {keys_parameters.difference(keys_parameters_meta_info)!s} only in \"\n            \"parameters, but not in parameters_meta_info.\",\n        )\n    if len(keys_parameters_meta_info.difference(keys_parameters)) &gt; 0:\n        raise ValueError(\n            f\"Parameter {keys_parameters_meta_info.difference(keys_parameters)!s} only in \"\n            \" parameters_meta_info, but not in parameters.\",\n        )\n\n    self.external_version = external_version\n    self.uploader = uploader\n\n    self.custom_name = custom_name\n    self.tags = tags if tags is not None else []\n    self.binary_url = binary_url\n    self.binary_format = binary_format\n    self.binary_md5 = binary_md5\n    self.version = version\n    self.upload_date = upload_date\n    self.language = language\n    self.dependencies = dependencies\n    self.flow_id = flow_id\n    if extension is None:\n        self._extension = get_extension_by_flow(self)\n    else:\n        self._extension = extension\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.extension","title":"extension  <code>property</code>","text":"<pre><code>extension: Extension\n</code></pre> <p>The extension of the flow (e.g., sklearn).</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The ID of the flow.</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.from_filesystem","title":"from_filesystem  <code>classmethod</code>","text":"<pre><code>from_filesystem(input_directory: str | Path) -&gt; OpenMLFlow\n</code></pre> <p>Read a flow from an XML in input_directory on the filesystem.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>@classmethod\ndef from_filesystem(cls, input_directory: str | Path) -&gt; OpenMLFlow:\n    \"\"\"Read a flow from an XML in input_directory on the filesystem.\"\"\"\n    input_directory = Path(input_directory) / \"flow.xml\"\n    with input_directory.open() as f:\n        xml_string = f.read()\n    return OpenMLFlow._from_dict(xmltodict.parse(xml_string))\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.get_structure","title":"get_structure","text":"<pre><code>get_structure(key_item: str) -&gt; dict[str, list[str]]\n</code></pre> <p>Returns for each sub-component of the flow the path of identifiers that should be traversed to reach this component. The resulting dict maps a key (identifying a flow by either its id, name or fullname) to the parameter prefix.</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.get_structure--parameters","title":"Parameters","text":"<p>key_item: str     The flow attribute that will be used to identify flows in the     structure. Allowed values {flow_id, name}</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.get_structure--returns","title":"Returns","text":"<p>dict[str, List[str]]     The flow structure</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def get_structure(self, key_item: str) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Returns for each sub-component of the flow the path of identifiers\n    that should be traversed to reach this component. The resulting dict\n    maps a key (identifying a flow by either its id, name or fullname) to\n    the parameter prefix.\n\n    Parameters\n    ----------\n    key_item: str\n        The flow attribute that will be used to identify flows in the\n        structure. Allowed values {flow_id, name}\n\n    Returns\n    -------\n    dict[str, List[str]]\n        The flow structure\n    \"\"\"\n    if key_item not in [\"flow_id\", \"name\"]:\n        raise ValueError(\"key_item should be in {flow_id, name}\")\n    structure = {}\n    for key, sub_flow in self.components.items():\n        sub_structure = sub_flow.get_structure(key_item)\n        for flow_name, flow_sub_structure in sub_structure.items():\n            structure[flow_name] = [key, *flow_sub_structure]\n    structure[getattr(self, key_item)] = []\n    return structure\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.get_subflow","title":"get_subflow","text":"<pre><code>get_subflow(structure: list[str]) -&gt; OpenMLFlow\n</code></pre> <p>Returns a subflow from the tree of dependencies.</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.get_subflow--parameters","title":"Parameters","text":"<p>structure: list[str]     A list of strings, indicating the location of the subflow</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.get_subflow--returns","title":"Returns","text":"<p>OpenMLFlow     The OpenMLFlow that corresponds to the structure</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def get_subflow(self, structure: list[str]) -&gt; OpenMLFlow:\n    \"\"\"\n    Returns a subflow from the tree of dependencies.\n\n    Parameters\n    ----------\n    structure: list[str]\n        A list of strings, indicating the location of the subflow\n\n    Returns\n    -------\n    OpenMLFlow\n        The OpenMLFlow that corresponds to the structure\n    \"\"\"\n    # make a copy of structure, as we don't want to change it in the\n    # outer scope\n    structure = list(structure)\n    if len(structure) &lt; 1:\n        raise ValueError(\"Please provide a structure list of size &gt;= 1\")\n    sub_identifier = structure[0]\n    if sub_identifier not in self.components:\n        raise ValueError(\n            f\"Flow {self.name} does not contain component with \" f\"identifier {sub_identifier}\",\n        )\n    if len(structure) == 1:\n        return self.components[sub_identifier]  # type: ignore\n\n    structure.pop(0)\n    return self.components[sub_identifier].get_subflow(structure)  # type: ignore\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.publish","title":"publish","text":"<pre><code>publish(raise_error_if_exists: bool = False) -&gt; OpenMLFlow\n</code></pre> <p>Publish this flow to OpenML server.</p> <p>Raises a PyOpenMLError if the flow exists on the server, but <code>self.flow_id</code> does not match the server known flow id.</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.publish--parameters","title":"Parameters","text":"<p>raise_error_if_exists : bool, optional (default=False)     If True, raise PyOpenMLError if the flow exists on the server.     If False, update the local flow to match the server flow.</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.publish--returns","title":"Returns","text":"<p>self : OpenMLFlow</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def publish(self, raise_error_if_exists: bool = False) -&gt; OpenMLFlow:  # noqa: FBT001, FBT002\n    \"\"\"Publish this flow to OpenML server.\n\n    Raises a PyOpenMLError if the flow exists on the server, but\n    `self.flow_id` does not match the server known flow id.\n\n    Parameters\n    ----------\n    raise_error_if_exists : bool, optional (default=False)\n        If True, raise PyOpenMLError if the flow exists on the server.\n        If False, update the local flow to match the server flow.\n\n    Returns\n    -------\n    self : OpenMLFlow\n\n    \"\"\"\n    # Import at top not possible because of cyclic dependencies. In\n    # particular, flow.py tries to import functions.py in order to call\n    # get_flow(), while functions.py tries to import flow.py in order to\n    # instantiate an OpenMLFlow.\n    import openml.flows.functions\n\n    flow_id = openml.flows.functions.flow_exists(self.name, self.external_version)\n    if not flow_id:\n        if self.flow_id:\n            raise openml.exceptions.PyOpenMLError(\n                \"Flow does not exist on the server, \" \"but 'flow.flow_id' is not None.\",\n            )\n        super().publish()\n        assert self.flow_id is not None  # for mypy\n        flow_id = self.flow_id\n    elif raise_error_if_exists:\n        error_message = f\"This OpenMLFlow already exists with id: {flow_id}.\"\n        raise openml.exceptions.PyOpenMLError(error_message)\n    elif self.flow_id is not None and self.flow_id != flow_id:\n        raise openml.exceptions.PyOpenMLError(\n            \"Local flow_id does not match server flow_id: \" f\"'{self.flow_id}' vs '{flow_id}'\",\n        )\n\n    flow = openml.flows.functions.get_flow(flow_id)\n    _copy_server_fields(flow, self)\n    try:\n        openml.flows.functions.assert_flows_equal(\n            self,\n            flow,\n            flow.upload_date,\n            ignore_parameter_values=True,\n            ignore_custom_name_if_none=True,\n        )\n    except ValueError as e:\n        message = e.args[0]\n        raise ValueError(\n            \"The flow on the server is inconsistent with the local flow. \"\n            f\"The server flow ID is {flow_id}. Please check manually and remove \"\n            f\"the flow if necessary! Error is:\\n'{message}'\",\n        ) from e\n    return self\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.to_filesystem","title":"to_filesystem","text":"<pre><code>to_filesystem(output_directory: str | Path) -&gt; None\n</code></pre> <p>Write a flow to the filesystem as XML to output_directory.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def to_filesystem(self, output_directory: str | Path) -&gt; None:\n    \"\"\"Write a flow to the filesystem as XML to output_directory.\"\"\"\n    output_directory = Path(output_directory)\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    output_path = output_directory / \"flow.xml\"\n    if output_path.exists():\n        raise ValueError(\"Output directory already contains a flow.xml file.\")\n\n    run_xml = self._to_xml()\n    with output_path.open(\"w\") as f:\n        f.write(run_xml)\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/flows/functions/","title":"functions","text":""},{"location":"reference/flows/functions/#openml.flows.functions","title":"openml.flows.functions","text":""},{"location":"reference/flows/functions/#openml.flows.functions.__list_flows","title":"__list_flows","text":"<pre><code>__list_flows(api_call: str) -&gt; DataFrame\n</code></pre> <p>Retrieve information about flows from OpenML API and parse it to a dictionary or a Pandas DataFrame.</p>"},{"location":"reference/flows/functions/#openml.flows.functions.__list_flows--parameters","title":"Parameters","text":"<p>api_call: str     Retrieves the information about flows.</p>"},{"location":"reference/flows/functions/#openml.flows.functions.__list_flows--returns","title":"Returns","text":"<pre><code>The flows information in the specified output format.\n</code></pre> Source code in <code>openml/flows/functions.py</code> <pre><code>def __list_flows(api_call: str) -&gt; pd.DataFrame:\n    \"\"\"Retrieve information about flows from OpenML API\n    and parse it to a dictionary or a Pandas DataFrame.\n\n    Parameters\n    ----------\n    api_call: str\n        Retrieves the information about flows.\n\n    Returns\n    -------\n        The flows information in the specified output format.\n    \"\"\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    flows_dict = xmltodict.parse(xml_string, force_list=(\"oml:flow\",))\n\n    # Minimalistic check if the XML is useful\n    assert isinstance(flows_dict[\"oml:flows\"][\"oml:flow\"], list), type(flows_dict[\"oml:flows\"])\n    assert flows_dict[\"oml:flows\"][\"@xmlns:oml\"] == \"http://openml.org/openml\", flows_dict[\n        \"oml:flows\"\n    ][\"@xmlns:oml\"]\n\n    flows = {}\n    for flow_ in flows_dict[\"oml:flows\"][\"oml:flow\"]:\n        fid = int(flow_[\"oml:id\"])\n        flow = {\n            \"id\": fid,\n            \"full_name\": flow_[\"oml:full_name\"],\n            \"name\": flow_[\"oml:name\"],\n            \"version\": flow_[\"oml:version\"],\n            \"external_version\": flow_[\"oml:external_version\"],\n            \"uploader\": flow_[\"oml:uploader\"],\n        }\n        flows[fid] = flow\n\n    return pd.DataFrame.from_dict(flows, orient=\"index\")\n</code></pre>"},{"location":"reference/flows/functions/#openml.flows.functions.assert_flows_equal","title":"assert_flows_equal","text":"<pre><code>assert_flows_equal(flow1: OpenMLFlow, flow2: OpenMLFlow, ignore_parameter_values_on_older_children: str | None = None, ignore_parameter_values: bool = False, ignore_custom_name_if_none: bool = False, check_description: bool = True) -&gt; None\n</code></pre> <p>Check equality of two flows.</p> <p>Two flows are equal if their all keys which are not set by the server are equal, as well as all their parameters and components.</p>"},{"location":"reference/flows/functions/#openml.flows.functions.assert_flows_equal--parameters","title":"Parameters","text":"<p>flow1 : OpenMLFlow</p> <p>flow2 : OpenMLFlow</p> str (optional) <p>If set to <code>OpenMLFlow.upload_date</code>, ignores parameters in a child flow if it's upload date predates the upload date of the parent flow.</p> bool <p>Whether to ignore parameter values when comparing flows.</p> bool <p>Whether to ignore the custom name field if either flow has <code>custom_name</code> equal to <code>None</code>.</p> bool <p>Whether to ignore matching of flow descriptions.</p> Source code in <code>openml/flows/functions.py</code> <pre><code>def assert_flows_equal(  # noqa: C901, PLR0912, PLR0913, PLR0915\n    flow1: OpenMLFlow,\n    flow2: OpenMLFlow,\n    ignore_parameter_values_on_older_children: str | None = None,\n    ignore_parameter_values: bool = False,  # noqa: FBT001, FBT002\n    ignore_custom_name_if_none: bool = False,  # noqa:  FBT001, FBT002\n    check_description: bool = True,  # noqa:  FBT001, FBT002\n) -&gt; None:\n    \"\"\"Check equality of two flows.\n\n    Two flows are equal if their all keys which are not set by the server\n    are equal, as well as all their parameters and components.\n\n    Parameters\n    ----------\n    flow1 : OpenMLFlow\n\n    flow2 : OpenMLFlow\n\n    ignore_parameter_values_on_older_children : str (optional)\n        If set to ``OpenMLFlow.upload_date``, ignores parameters in a child\n        flow if it's upload date predates the upload date of the parent flow.\n\n    ignore_parameter_values : bool\n        Whether to ignore parameter values when comparing flows.\n\n    ignore_custom_name_if_none : bool\n        Whether to ignore the custom name field if either flow has `custom_name` equal to `None`.\n\n    check_description : bool\n        Whether to ignore matching of flow descriptions.\n    \"\"\"\n    if not isinstance(flow1, OpenMLFlow):\n        raise TypeError(f\"Argument 1 must be of type OpenMLFlow, but is {type(flow1)}\")\n\n    if not isinstance(flow2, OpenMLFlow):\n        raise TypeError(f\"Argument 2 must be of type OpenMLFlow, but is {type(flow2)}\")\n\n    # TODO as they are actually now saved during publish, it might be good to\n    # check for the equality of these as well.\n    generated_by_the_server = [\n        \"flow_id\",\n        \"uploader\",\n        \"version\",\n        \"upload_date\",\n        # Tags aren't directly created by the server,\n        # but the uploader has no control over them!\n        \"tags\",\n    ]\n    ignored_by_python_api = [\"binary_url\", \"binary_format\", \"binary_md5\", \"model\", \"_entity_id\"]\n\n    for key in set(flow1.__dict__.keys()).union(flow2.__dict__.keys()):\n        if key in generated_by_the_server + ignored_by_python_api:\n            continue\n        attr1 = getattr(flow1, key, None)\n        attr2 = getattr(flow2, key, None)\n        if key == \"components\":\n            if not (isinstance(attr1, Dict) and isinstance(attr2, Dict)):\n                raise TypeError(\"Cannot compare components because they are not dictionary.\")\n\n            for name in set(attr1.keys()).union(attr2.keys()):\n                if name not in attr1:\n                    raise ValueError(\n                        f\"Component {name} only available in argument2, but not in argument1.\",\n                    )\n                if name not in attr2:\n                    raise ValueError(\n                        f\"Component {name} only available in argument2, but not in argument1.\",\n                    )\n                assert_flows_equal(\n                    attr1[name],\n                    attr2[name],\n                    ignore_parameter_values_on_older_children,\n                    ignore_parameter_values,\n                    ignore_custom_name_if_none,\n                )\n        elif key == \"_extension\":\n            continue\n        elif check_description and key == \"description\":\n            # to ignore matching of descriptions since sklearn based flows may have\n            # altering docstrings and is not guaranteed to be consistent\n            continue\n        else:\n            if key == \"parameters\":\n                if ignore_parameter_values or ignore_parameter_values_on_older_children:\n                    params_flow_1 = set(flow1.parameters.keys())\n                    params_flow_2 = set(flow2.parameters.keys())\n                    symmetric_difference = params_flow_1 ^ params_flow_2\n                    if len(symmetric_difference) &gt; 0:\n                        raise ValueError(\n                            f\"Flow {flow1.name}: parameter set of flow \"\n                            \"differs from the parameters stored \"\n                            \"on the server.\",\n                        )\n\n                if ignore_parameter_values_on_older_children:\n                    assert (\n                        flow1.upload_date is not None\n                    ), \"Flow1 has no upload date that allows us to compare age of children.\"\n                    upload_date_current_flow = dateutil.parser.parse(flow1.upload_date)\n                    upload_date_parent_flow = dateutil.parser.parse(\n                        ignore_parameter_values_on_older_children,\n                    )\n                    if upload_date_current_flow &lt; upload_date_parent_flow:\n                        continue\n\n                if ignore_parameter_values:\n                    # Continue needs to be done here as the first if\n                    # statement triggers in both special cases\n                    continue\n            elif (\n                key == \"custom_name\"\n                and ignore_custom_name_if_none\n                and (attr1 is None or attr2 is None)\n            ):\n                # If specified, we allow `custom_name` inequality if one flow's name is None.\n                # Helps with backwards compatibility as `custom_name` is now auto-generated, but\n                # before it used to be `None`.\n                continue\n            elif key == \"parameters_meta_info\":\n                # this value is a dictionary where each key is a parameter name, containing another\n                # dictionary with keys specifying the parameter's 'description' and 'data_type'\n                # checking parameter descriptions can be ignored since that might change\n                # data type check can also be ignored if one of them is not defined, i.e., None\n                params1 = set(flow1.parameters_meta_info)\n                params2 = set(flow2.parameters_meta_info)\n                if params1 != params2:\n                    raise ValueError(\n                        \"Parameter list in meta info for parameters differ in the two flows.\",\n                    )\n                # iterating over the parameter's meta info list\n                for param in params1:\n                    if (\n                        isinstance(flow1.parameters_meta_info[param], Dict)\n                        and isinstance(flow2.parameters_meta_info[param], Dict)\n                        and \"data_type\" in flow1.parameters_meta_info[param]\n                        and \"data_type\" in flow2.parameters_meta_info[param]\n                    ):\n                        value1 = flow1.parameters_meta_info[param][\"data_type\"]\n                        value2 = flow2.parameters_meta_info[param][\"data_type\"]\n                    else:\n                        value1 = flow1.parameters_meta_info[param]\n                        value2 = flow2.parameters_meta_info[param]\n                    if value1 is None or value2 is None:\n                        continue\n\n                    if value1 != value2:\n                        raise ValueError(\n                            f\"Flow {flow1.name}: data type for parameter {param} in {key} differ \"\n                            f\"as {value1}\\nvs\\n{value2}\",\n                        )\n                # the continue is to avoid the 'attr != attr2' check at end of function\n                continue\n\n            if attr1 != attr2:\n                raise ValueError(\n                    f\"Flow {flow1.name!s}: values for attribute '{key!s}' differ: \"\n                    f\"'{attr1!s}'\\nvs\\n'{attr2!s}'.\",\n                )\n</code></pre>"},{"location":"reference/flows/functions/#openml.flows.functions.delete_flow","title":"delete_flow","text":"<pre><code>delete_flow(flow_id: int) -&gt; bool\n</code></pre> <p>Delete flow with id <code>flow_id</code> from the OpenML server.</p> <p>You can only delete flows which you uploaded and which which are not linked to runs.</p>"},{"location":"reference/flows/functions/#openml.flows.functions.delete_flow--parameters","title":"Parameters","text":"<p>flow_id : int     OpenML id of the flow</p>"},{"location":"reference/flows/functions/#openml.flows.functions.delete_flow--returns","title":"Returns","text":"<p>bool     True if the deletion was successful. False otherwise.</p> Source code in <code>openml/flows/functions.py</code> <pre><code>def delete_flow(flow_id: int) -&gt; bool:\n    \"\"\"Delete flow with id `flow_id` from the OpenML server.\n\n    You can only delete flows which you uploaded and which\n    which are not linked to runs.\n\n    Parameters\n    ----------\n    flow_id : int\n        OpenML id of the flow\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"flow\", flow_id)\n</code></pre>"},{"location":"reference/flows/functions/#openml.flows.functions.flow_exists","title":"flow_exists","text":"<pre><code>flow_exists(name: str, external_version: str) -&gt; int | bool\n</code></pre> <p>Retrieves the flow id.</p> <p>A flow is uniquely identified by name + external_version.</p>"},{"location":"reference/flows/functions/#openml.flows.functions.flow_exists--parameters","title":"Parameters","text":"<p>name : string     Name of the flow external_version : string     Version information associated with flow.</p>"},{"location":"reference/flows/functions/#openml.flows.functions.flow_exists--returns","title":"Returns","text":"<p>flow_exist : int or bool     flow id iff exists, False otherwise</p>"},{"location":"reference/flows/functions/#openml.flows.functions.flow_exists--notes","title":"Notes","text":"<p>see www.openml.org/api_docs/#!/flow/get_flow_exists_name_version</p> Source code in <code>openml/flows/functions.py</code> <pre><code>def flow_exists(name: str, external_version: str) -&gt; int | bool:\n    \"\"\"Retrieves the flow id.\n\n    A flow is uniquely identified by name + external_version.\n\n    Parameters\n    ----------\n    name : string\n        Name of the flow\n    external_version : string\n        Version information associated with flow.\n\n    Returns\n    -------\n    flow_exist : int or bool\n        flow id iff exists, False otherwise\n\n    Notes\n    -----\n    see https://www.openml.org/api_docs/#!/flow/get_flow_exists_name_version\n    \"\"\"\n    if not (isinstance(name, str) and len(name) &gt; 0):\n        raise ValueError(\"Argument 'name' should be a non-empty string\")\n    if not (isinstance(name, str) and len(external_version) &gt; 0):\n        raise ValueError(\"Argument 'version' should be a non-empty string\")\n\n    xml_response = openml._api_calls._perform_api_call(\n        \"flow/exists\",\n        \"post\",\n        data={\"name\": name, \"external_version\": external_version},\n    )\n\n    result_dict = xmltodict.parse(xml_response)\n    flow_id = int(result_dict[\"oml:flow_exists\"][\"oml:id\"])\n    return flow_id if flow_id &gt; 0 else False\n</code></pre>"},{"location":"reference/flows/functions/#openml.flows.functions.get_flow","title":"get_flow","text":"<pre><code>get_flow(flow_id: int, reinstantiate: bool = False, strict_version: bool = True) -&gt; OpenMLFlow\n</code></pre> <p>Download the OpenML flow for a given flow ID.</p>"},{"location":"reference/flows/functions/#openml.flows.functions.get_flow--parameters","title":"Parameters","text":"<p>flow_id : int     The OpenML flow id.</p> bool <p>Whether to reinstantiate the flow to a model instance.</p> bool, default=True <p>Whether to fail if version requirements are not fulfilled.</p>"},{"location":"reference/flows/functions/#openml.flows.functions.get_flow--returns","title":"Returns","text":"<p>flow : OpenMLFlow     the flow</p> Source code in <code>openml/flows/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_flow(flow_id: int, reinstantiate: bool = False, strict_version: bool = True) -&gt; OpenMLFlow:  # noqa: FBT001, FBT002\n    \"\"\"Download the OpenML flow for a given flow ID.\n\n    Parameters\n    ----------\n    flow_id : int\n        The OpenML flow id.\n\n    reinstantiate: bool\n        Whether to reinstantiate the flow to a model instance.\n\n    strict_version : bool, default=True\n        Whether to fail if version requirements are not fulfilled.\n\n    Returns\n    -------\n    flow : OpenMLFlow\n        the flow\n    \"\"\"\n    flow_id = int(flow_id)\n    flow = _get_flow_description(flow_id)\n\n    if reinstantiate:\n        flow.model = flow.extension.flow_to_model(flow, strict_version=strict_version)\n        if not strict_version:\n            # check if we need to return a new flow b/c of version mismatch\n            new_flow = flow.extension.model_to_flow(flow.model)\n            if new_flow.dependencies != flow.dependencies:\n                return new_flow\n    return flow\n</code></pre>"},{"location":"reference/flows/functions/#openml.flows.functions.get_flow_id","title":"get_flow_id","text":"<pre><code>get_flow_id(model: Any | None = None, name: str | None = None, exact_version: bool = True) -&gt; int | bool | list[int]\n</code></pre> <p>Retrieves the flow id for a model or a flow name.</p> <p>Provide either a model or a name to this function. Depending on the input, it does</p> <ul> <li><code>model</code> and <code>exact_version == True</code>: This helper function first queries for the necessary   extension. Second, it uses that extension to convert the model into a flow. Third, it   executes <code>flow_exists</code> to potentially obtain the flow id the flow is published to the   server.</li> <li><code>model</code> and <code>exact_version == False</code>: This helper function first queries for the   necessary extension. Second, it uses that extension to convert the model into a flow. Third   it calls <code>list_flows</code> and filters the returned values based on the flow name.</li> <li><code>name</code>: Ignores <code>exact_version</code> and calls <code>list_flows</code>, then filters the returned   values based on the flow name.</li> </ul>"},{"location":"reference/flows/functions/#openml.flows.functions.get_flow_id--parameters","title":"Parameters","text":"<p>model : object     Any model. Must provide either <code>model</code> or <code>name</code>. name : str     Name of the flow. Must provide either <code>model</code> or <code>name</code>. exact_version : bool     Whether to return the flow id of the exact version or all flow ids where the name     of the flow matches. This is only taken into account for a model where a version number     is available (requires <code>model</code> to be set).</p>"},{"location":"reference/flows/functions/#openml.flows.functions.get_flow_id--returns","title":"Returns","text":"<p>int or bool, List     flow id iff exists, <code>False</code> otherwise, List if <code>exact_version is False</code></p> Source code in <code>openml/flows/functions.py</code> <pre><code>def get_flow_id(\n    model: Any | None = None,\n    name: str | None = None,\n    exact_version: bool = True,  # noqa: FBT001, FBT002\n) -&gt; int | bool | list[int]:\n    \"\"\"Retrieves the flow id for a model or a flow name.\n\n    Provide either a model or a name to this function. Depending on the input, it does\n\n    * ``model`` and ``exact_version == True``: This helper function first queries for the necessary\n      extension. Second, it uses that extension to convert the model into a flow. Third, it\n      executes ``flow_exists`` to potentially obtain the flow id the flow is published to the\n      server.\n    * ``model`` and ``exact_version == False``: This helper function first queries for the\n      necessary extension. Second, it uses that extension to convert the model into a flow. Third\n      it calls ``list_flows`` and filters the returned values based on the flow name.\n    * ``name``: Ignores ``exact_version`` and calls ``list_flows``, then filters the returned\n      values based on the flow name.\n\n    Parameters\n    ----------\n    model : object\n        Any model. Must provide either ``model`` or ``name``.\n    name : str\n        Name of the flow. Must provide either ``model`` or ``name``.\n    exact_version : bool\n        Whether to return the flow id of the exact version or all flow ids where the name\n        of the flow matches. This is only taken into account for a model where a version number\n        is available (requires ``model`` to be set).\n\n    Returns\n    -------\n    int or bool, List\n        flow id iff exists, ``False`` otherwise, List if ``exact_version is False``\n    \"\"\"\n    if model is not None and name is not None:\n        raise ValueError(\"Must provide either argument `model` or argument `name`, but not both.\")\n\n    if model is not None:\n        extension = openml.extensions.get_extension_by_model(model, raise_if_no_extension=True)\n        if extension is None:\n            # This should never happen and is only here to please mypy will be gone soon once the\n            # whole function is removed\n            raise TypeError(extension)\n        flow = extension.model_to_flow(model)\n        flow_name = flow.name\n        external_version = flow.external_version\n    elif name is not None:\n        flow_name = name\n        exact_version = False\n        external_version = None\n    else:\n        raise ValueError(\n            \"Need to provide either argument `model` or argument `name`, but both are `None`.\"\n        )\n\n    if exact_version:\n        if external_version is None:\n            raise ValueError(\"exact_version should be False if model is None!\")\n        return flow_exists(name=flow_name, external_version=external_version)\n\n    flows = list_flows()\n    flows = flows.query(f'name == \"{flow_name}\"')\n    return flows[\"id\"].to_list()  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/flows/functions/#openml.flows.functions.list_flows","title":"list_flows","text":"<pre><code>list_flows(offset: int | None = None, size: int | None = None, tag: str | None = None, uploader: str | None = None) -&gt; DataFrame\n</code></pre> <p>Return a list of all flows which are on OpenML. (Supports large amount of results)</p>"},{"location":"reference/flows/functions/#openml.flows.functions.list_flows--parameters","title":"Parameters","text":"<p>offset : int, optional     the number of flows to skip, starting from the first size : int, optional     the maximum number of flows to return tag : str, optional     the tag to include kwargs: dict, optional     Legal filter operators: uploader.</p>"},{"location":"reference/flows/functions/#openml.flows.functions.list_flows--returns","title":"Returns","text":"<p>flows : dataframe         Each row maps to a dataset         Each column contains the following information:         - flow id         - full name         - name         - version         - external version         - uploader</p> Source code in <code>openml/flows/functions.py</code> <pre><code>def list_flows(\n    offset: int | None = None,\n    size: int | None = None,\n    tag: str | None = None,\n    uploader: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a list of all flows which are on OpenML.\n    (Supports large amount of results)\n\n    Parameters\n    ----------\n    offset : int, optional\n        the number of flows to skip, starting from the first\n    size : int, optional\n        the maximum number of flows to return\n    tag : str, optional\n        the tag to include\n    kwargs: dict, optional\n        Legal filter operators: uploader.\n\n    Returns\n    -------\n    flows : dataframe\n            Each row maps to a dataset\n            Each column contains the following information:\n            - flow id\n            - full name\n            - name\n            - version\n            - external version\n            - uploader\n    \"\"\"\n    listing_call = partial(_list_flows, tag=tag, uploader=uploader)\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/runs/","title":"runs","text":""},{"location":"reference/runs/#openml.runs","title":"openml.runs","text":""},{"location":"reference/runs/#openml.runs.OpenMLRun","title":"OpenMLRun","text":"<pre><code>OpenMLRun(task_id: int, flow_id: int | None, dataset_id: int | None, setup_string: str | None = None, output_files: dict[str, int] | None = None, setup_id: int | None = None, tags: list[str] | None = None, uploader: int | None = None, uploader_name: str | None = None, evaluations: dict | None = None, fold_evaluations: dict | None = None, sample_evaluations: dict | None = None, data_content: list[list] | None = None, trace: OpenMLRunTrace | None = None, model: object | None = None, task_type: str | None = None, task_evaluation_measure: str | None = None, flow_name: str | None = None, parameter_settings: list[dict[str, Any]] | None = None, predictions_url: str | None = None, task: OpenMLTask | None = None, flow: OpenMLFlow | None = None, run_id: int | None = None, description_text: str | None = None, run_details: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Run: result of running a model on an OpenML dataset.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun--parameters","title":"Parameters","text":"<p>task_id: int     The ID of the OpenML task associated with the run. flow_id: int     The ID of the OpenML flow associated with the run. dataset_id: int     The ID of the OpenML dataset used for the run. setup_string: str     The setup string of the run. output_files: Dict[str, int]     Specifies where each related file can be found. setup_id: int     An integer representing the ID of the setup used for the run. tags: List[str]     Representing the tags associated with the run. uploader: int     User ID of the uploader. uploader_name: str     The name of the person who uploaded the run. evaluations: Dict     Representing the evaluations of the run. fold_evaluations: Dict     The evaluations of the run for each fold. sample_evaluations: Dict     The evaluations of the run for each sample. data_content: List[List]     The predictions generated from executing this run. trace: OpenMLRunTrace     The trace containing information on internal model evaluations of this run. model: object     The untrained model that was evaluated in the run. task_type: str     The type of the OpenML task associated with the run. task_evaluation_measure: str     The evaluation measure used for the task. flow_name: str     The name of the OpenML flow associated with the run. parameter_settings: list[OrderedDict]     Representing the parameter settings used for the run. predictions_url: str     The URL of the predictions file. task: OpenMLTask     An instance of the OpenMLTask class, representing the OpenML task associated     with the run. flow: OpenMLFlow     An instance of the OpenMLFlow class, representing the OpenML flow associated     with the run. run_id: int     The ID of the run. description_text: str, optional     Description text to add to the predictions file. If left None, is set to the     time the arff file is generated. run_details: str, optional (default=None)     Description of the run stored in the run meta-data.</p> Source code in <code>openml/runs/run.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_id: int,\n    flow_id: int | None,\n    dataset_id: int | None,\n    setup_string: str | None = None,\n    output_files: dict[str, int] | None = None,\n    setup_id: int | None = None,\n    tags: list[str] | None = None,\n    uploader: int | None = None,\n    uploader_name: str | None = None,\n    evaluations: dict | None = None,\n    fold_evaluations: dict | None = None,\n    sample_evaluations: dict | None = None,\n    data_content: list[list] | None = None,\n    trace: OpenMLRunTrace | None = None,\n    model: object | None = None,\n    task_type: str | None = None,\n    task_evaluation_measure: str | None = None,\n    flow_name: str | None = None,\n    parameter_settings: list[dict[str, Any]] | None = None,\n    predictions_url: str | None = None,\n    task: OpenMLTask | None = None,\n    flow: OpenMLFlow | None = None,\n    run_id: int | None = None,\n    description_text: str | None = None,\n    run_details: str | None = None,\n):\n    self.uploader = uploader\n    self.uploader_name = uploader_name\n    self.task_id = task_id\n    self.task_type = task_type\n    self.task_evaluation_measure = task_evaluation_measure\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.setup_id = setup_id\n    self.setup_string = setup_string\n    self.parameter_settings = parameter_settings\n    self.dataset_id = dataset_id\n    self.evaluations = evaluations\n    self.fold_evaluations = fold_evaluations\n    self.sample_evaluations = sample_evaluations\n    self.data_content = data_content\n    self.output_files = output_files\n    self.trace = trace\n    self.error_message = None\n    self.task = task\n    self.flow = flow\n    self.run_id = run_id\n    self.model = model\n    self.tags = tags\n    self.predictions_url = predictions_url\n    self.description_text = description_text\n    self.run_details = run_details\n    self._predictions = None\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The ID of the run, None if not uploaded to the server yet.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.predictions","title":"predictions  <code>property</code>","text":"<pre><code>predictions: DataFrame\n</code></pre> <p>Return a DataFrame with predictions for this run</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.from_filesystem","title":"from_filesystem  <code>classmethod</code>","text":"<pre><code>from_filesystem(directory: str | Path, expect_model: bool = True) -&gt; OpenMLRun\n</code></pre> <p>The inverse of the to_filesystem method. Instantiates an OpenMLRun object based on files stored on the file system.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.from_filesystem--parameters","title":"Parameters","text":"<p>directory : str     a path leading to the folder where the results     are stored</p> bool <p>if True, it requires the model pickle to be present, and an error will be thrown if not. Otherwise, the model might or might not be present.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.from_filesystem--returns","title":"Returns","text":"<p>run : OpenMLRun     the re-instantiated run object</p> Source code in <code>openml/runs/run.py</code> <pre><code>@classmethod\ndef from_filesystem(cls, directory: str | Path, expect_model: bool = True) -&gt; OpenMLRun:  # noqa: FBT001, FBT002\n    \"\"\"\n    The inverse of the to_filesystem method. Instantiates an OpenMLRun\n    object based on files stored on the file system.\n\n    Parameters\n    ----------\n    directory : str\n        a path leading to the folder where the results\n        are stored\n\n    expect_model : bool\n        if True, it requires the model pickle to be present, and an error\n        will be thrown if not. Otherwise, the model might or might not\n        be present.\n\n    Returns\n    -------\n    run : OpenMLRun\n        the re-instantiated run object\n    \"\"\"\n    # Avoiding cyclic imports\n    import openml.runs.functions\n\n    directory = Path(directory)\n    if not directory.is_dir():\n        raise ValueError(\"Could not find folder\")\n\n    description_path = directory / \"description.xml\"\n    predictions_path = directory / \"predictions.arff\"\n    trace_path = directory / \"trace.arff\"\n    model_path = directory / \"model.pkl\"\n\n    if not description_path.is_file():\n        raise ValueError(\"Could not find description.xml\")\n    if not predictions_path.is_file():\n        raise ValueError(\"Could not find predictions.arff\")\n    if (not model_path.is_file()) and expect_model:\n        raise ValueError(\"Could not find model.pkl\")\n\n    with description_path.open() as fht:\n        xml_string = fht.read()\n    run = openml.runs.functions._create_run_from_xml(xml_string, from_server=False)\n\n    if run.flow_id is None:\n        flow = openml.flows.OpenMLFlow.from_filesystem(directory)\n        run.flow = flow\n        run.flow_name = flow.name\n\n    with predictions_path.open() as fht:\n        predictions = arff.load(fht)\n        run.data_content = predictions[\"data\"]\n\n    if model_path.is_file():\n        # note that it will load the model if the file exists, even if\n        # expect_model is False\n        with model_path.open(\"rb\") as fhb:\n            run.model = pickle.load(fhb)  # noqa: S301\n\n    if trace_path.is_file():\n        run.trace = openml.runs.OpenMLRunTrace._from_filesystem(trace_path)\n\n    return run\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.get_metric_fn","title":"get_metric_fn","text":"<pre><code>get_metric_fn(sklearn_fn: Callable, kwargs: dict | None = None) -&gt; ndarray\n</code></pre> <p>Calculates metric scores based on predicted values. Assumes the run has been executed locally (and contains run_data). Furthermore, it assumes that the 'correct' or 'truth' attribute is specified in the arff (which is an optional field, but always the case for openml-python runs)</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.get_metric_fn--parameters","title":"Parameters","text":"<p>sklearn_fn : function     a function pointer to a sklearn function that     accepts <code>y_true</code>, <code>y_pred</code> and <code>**kwargs</code> kwargs : dict     kwargs for the function</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.get_metric_fn--returns","title":"Returns","text":"<p>scores : ndarray of scores of length num_folds * num_repeats     metric results</p> Source code in <code>openml/runs/run.py</code> <pre><code>def get_metric_fn(self, sklearn_fn: Callable, kwargs: dict | None = None) -&gt; np.ndarray:  # noqa: PLR0915, PLR0912, C901\n    \"\"\"Calculates metric scores based on predicted values. Assumes the\n    run has been executed locally (and contains run_data). Furthermore,\n    it assumes that the 'correct' or 'truth' attribute is specified in\n    the arff (which is an optional field, but always the case for\n    openml-python runs)\n\n    Parameters\n    ----------\n    sklearn_fn : function\n        a function pointer to a sklearn function that\n        accepts ``y_true``, ``y_pred`` and ``**kwargs``\n    kwargs : dict\n        kwargs for the function\n\n    Returns\n    -------\n    scores : ndarray of scores of length num_folds * num_repeats\n        metric results\n    \"\"\"\n    kwargs = kwargs if kwargs else {}\n    if self.data_content is not None and self.task_id is not None:\n        predictions_arff = self._generate_arff_dict()\n    elif (self.output_files is not None) and (\"predictions\" in self.output_files):\n        predictions_file_url = openml._api_calls._file_id_to_url(\n            self.output_files[\"predictions\"],\n            \"predictions.arff\",\n        )\n        response = openml._api_calls._download_text_file(predictions_file_url)\n        predictions_arff = arff.loads(response)\n        # TODO: make this a stream reader\n    else:\n        raise ValueError(\n            \"Run should have been locally executed or \" \"contain outputfile reference.\",\n        )\n\n    # Need to know more about the task to compute scores correctly\n    task = get_task(self.task_id)\n\n    attribute_names = [att[0] for att in predictions_arff[\"attributes\"]]\n    if (\n        task.task_type_id in [TaskType.SUPERVISED_CLASSIFICATION, TaskType.LEARNING_CURVE]\n        and \"correct\" not in attribute_names\n    ):\n        raise ValueError('Attribute \"correct\" should be set for ' \"classification task runs\")\n    if task.task_type_id == TaskType.SUPERVISED_REGRESSION and \"truth\" not in attribute_names:\n        raise ValueError('Attribute \"truth\" should be set for ' \"regression task runs\")\n    if task.task_type_id != TaskType.CLUSTERING and \"prediction\" not in attribute_names:\n        raise ValueError('Attribute \"predict\" should be set for ' \"supervised task runs\")\n\n    def _attribute_list_to_dict(attribute_list):  # type: ignore\n        # convenience function: Creates a mapping to map from the name of\n        # attributes present in the arff prediction file to their index.\n        # This is necessary because the number of classes can be different\n        # for different tasks.\n        res = OrderedDict()\n        for idx in range(len(attribute_list)):\n            res[attribute_list[idx][0]] = idx\n        return res\n\n    attribute_dict = _attribute_list_to_dict(predictions_arff[\"attributes\"])\n\n    repeat_idx = attribute_dict[\"repeat\"]\n    fold_idx = attribute_dict[\"fold\"]\n    predicted_idx = attribute_dict[\"prediction\"]  # Assume supervised task\n\n    if task.task_type_id in (TaskType.SUPERVISED_CLASSIFICATION, TaskType.LEARNING_CURVE):\n        correct_idx = attribute_dict[\"correct\"]\n    elif task.task_type_id == TaskType.SUPERVISED_REGRESSION:\n        correct_idx = attribute_dict[\"truth\"]\n    has_samples = False\n    if \"sample\" in attribute_dict:\n        sample_idx = attribute_dict[\"sample\"]\n        has_samples = True\n\n    if (\n        predictions_arff[\"attributes\"][predicted_idx][1]\n        != predictions_arff[\"attributes\"][correct_idx][1]\n    ):\n        pred = predictions_arff[\"attributes\"][predicted_idx][1]\n        corr = predictions_arff[\"attributes\"][correct_idx][1]\n        raise ValueError(\n            \"Predicted and Correct do not have equal values:\" f\" {pred!s} Vs. {corr!s}\",\n        )\n\n    # TODO: these could be cached\n    values_predict: dict[int, dict[int, dict[int, list[float]]]] = {}\n    values_correct: dict[int, dict[int, dict[int, list[float]]]] = {}\n    for _line_idx, line in enumerate(predictions_arff[\"data\"]):\n        rep = line[repeat_idx]\n        fold = line[fold_idx]\n        samp = line[sample_idx] if has_samples else 0\n\n        if task.task_type_id in [\n            TaskType.SUPERVISED_CLASSIFICATION,\n            TaskType.LEARNING_CURVE,\n        ]:\n            prediction = predictions_arff[\"attributes\"][predicted_idx][1].index(\n                line[predicted_idx],\n            )\n            correct = predictions_arff[\"attributes\"][predicted_idx][1].index(line[correct_idx])\n        elif task.task_type_id == TaskType.SUPERVISED_REGRESSION:\n            prediction = line[predicted_idx]\n            correct = line[correct_idx]\n        if rep not in values_predict:\n            values_predict[rep] = OrderedDict()\n            values_correct[rep] = OrderedDict()\n        if fold not in values_predict[rep]:\n            values_predict[rep][fold] = OrderedDict()\n            values_correct[rep][fold] = OrderedDict()\n        if samp not in values_predict[rep][fold]:\n            values_predict[rep][fold][samp] = []\n            values_correct[rep][fold][samp] = []\n\n        values_predict[rep][fold][samp].append(prediction)\n        values_correct[rep][fold][samp].append(correct)\n\n    scores = []\n    for rep in values_predict:\n        for fold in values_predict[rep]:\n            last_sample = len(values_predict[rep][fold]) - 1\n            y_pred = values_predict[rep][fold][last_sample]\n            y_true = values_correct[rep][fold][last_sample]\n            scores.append(sklearn_fn(y_true, y_pred, **kwargs))\n    return np.array(scores)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.to_filesystem","title":"to_filesystem","text":"<pre><code>to_filesystem(directory: str | Path, store_model: bool = True) -&gt; None\n</code></pre> <p>The inverse of the from_filesystem method. Serializes a run on the filesystem, to be uploaded later.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.to_filesystem--parameters","title":"Parameters","text":"<p>directory : str     a path leading to the folder where the results     will be stored. Should be empty</p> bool, optional (default=True) <p>if True, a model will be pickled as well. As this is the most storage expensive part, it is often desirable to not store the model.</p> Source code in <code>openml/runs/run.py</code> <pre><code>def to_filesystem(\n    self,\n    directory: str | Path,\n    store_model: bool = True,  # noqa: FBT001, FBT002\n) -&gt; None:\n    \"\"\"\n    The inverse of the from_filesystem method. Serializes a run\n    on the filesystem, to be uploaded later.\n\n    Parameters\n    ----------\n    directory : str\n        a path leading to the folder where the results\n        will be stored. Should be empty\n\n    store_model : bool, optional (default=True)\n        if True, a model will be pickled as well. As this is the most\n        storage expensive part, it is often desirable to not store the\n        model.\n    \"\"\"\n    if self.data_content is None or self.model is None:\n        raise ValueError(\"Run should have been executed (and contain \" \"model / predictions)\")\n    directory = Path(directory)\n    directory.mkdir(exist_ok=True, parents=True)\n\n    if any(directory.iterdir()):\n        raise ValueError(f\"Output directory {directory.expanduser().resolve()} should be empty\")\n\n    run_xml = self._to_xml()\n    predictions_arff = arff.dumps(self._generate_arff_dict())\n\n    # It seems like typing does not allow to define the same variable multiple times\n    with (directory / \"description.xml\").open(\"w\") as fh:\n        fh.write(run_xml)\n    with (directory / \"predictions.arff\").open(\"w\") as fh:\n        fh.write(predictions_arff)\n    if store_model:\n        with (directory / \"model.pkl\").open(\"wb\") as fh_b:\n            pickle.dump(self.model, fh_b)\n\n    if self.flow_id is None and self.flow is not None:\n        self.flow.to_filesystem(directory)\n\n    if self.trace is not None:\n        self.trace._to_filesystem(directory)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace","title":"OpenMLRunTrace","text":"<pre><code>OpenMLRunTrace(run_id: int | None, trace_iterations: dict[tuple[int, int, int], OpenMLTraceIteration])\n</code></pre> <p>OpenML Run Trace: parsed output from Run Trace call</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace--parameters","title":"Parameters","text":"<p>run_id : int     OpenML run id.</p> dict <p>Mapping from key <code>(repeat, fold, iteration)</code> to an object of OpenMLTraceIteration.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace--parameters","title":"Parameters","text":"<p>run_id : int     Id for which the trace content is to be stored. trace_iterations : List[List]     The trace content obtained by running a flow on a task.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>def __init__(\n    self,\n    run_id: int | None,\n    trace_iterations: dict[tuple[int, int, int], OpenMLTraceIteration],\n):\n    \"\"\"Object to hold the trace content of a run.\n\n    Parameters\n    ----------\n    run_id : int\n        Id for which the trace content is to be stored.\n    trace_iterations : List[List]\n        The trace content obtained by running a flow on a task.\n    \"\"\"\n    self.run_id = run_id\n    self.trace_iterations = trace_iterations\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.generate","title":"generate  <code>classmethod</code>","text":"<pre><code>generate(attributes: list[tuple[str, str]], content: list[list[int | float | str]]) -&gt; OpenMLRunTrace\n</code></pre> <p>Generates an OpenMLRunTrace.</p> <p>Generates the trace object from the attributes and content extracted while running the underlying flow.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.generate--parameters","title":"Parameters","text":"<p>attributes : list     List of tuples describing the arff attributes.</p> list <p>List of lists containing information about the individual tuning runs.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.generate--returns","title":"Returns","text":"<p>OpenMLRunTrace</p> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef generate(\n    cls,\n    attributes: list[tuple[str, str]],\n    content: list[list[int | float | str]],\n) -&gt; OpenMLRunTrace:\n    \"\"\"Generates an OpenMLRunTrace.\n\n    Generates the trace object from the attributes and content extracted\n    while running the underlying flow.\n\n    Parameters\n    ----------\n    attributes : list\n        List of tuples describing the arff attributes.\n\n    content : list\n        List of lists containing information about the individual tuning\n        runs.\n\n    Returns\n    -------\n    OpenMLRunTrace\n    \"\"\"\n    if content is None:\n        raise ValueError(\"Trace content not available.\")\n    if attributes is None:\n        raise ValueError(\"Trace attributes not available.\")\n    if len(content) == 0:\n        raise ValueError(\"Trace content is empty.\")\n    if len(attributes) != len(content[0]):\n        raise ValueError(\n            \"Trace_attributes and trace_content not compatible:\"\n            f\" {attributes} vs {content[0]}\",\n        )\n\n    return cls._trace_from_arff_struct(\n        attributes=attributes,\n        content=content,\n        error_message=\"setup_string not allowed when constructing a \"\n        \"trace object from run results.\",\n    )\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.get_selected_iteration","title":"get_selected_iteration","text":"<pre><code>get_selected_iteration(fold: int, repeat: int) -&gt; int\n</code></pre> <p>Returns the trace iteration that was marked as selected. In case multiple are marked as selected (should not happen) the first of these is returned</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.get_selected_iteration--parameters","title":"Parameters","text":"<p>fold: int</p> <p>repeat: int</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.get_selected_iteration--returns","title":"Returns","text":"<p>int     The trace iteration from the given fold and repeat that was     selected as the best iteration by the search procedure</p> Source code in <code>openml/runs/trace.py</code> <pre><code>def get_selected_iteration(self, fold: int, repeat: int) -&gt; int:\n    \"\"\"\n    Returns the trace iteration that was marked as selected. In\n    case multiple are marked as selected (should not happen) the\n    first of these is returned\n\n    Parameters\n    ----------\n    fold: int\n\n    repeat: int\n\n    Returns\n    -------\n    int\n        The trace iteration from the given fold and repeat that was\n        selected as the best iteration by the search procedure\n    \"\"\"\n    for r, f, i in self.trace_iterations:\n        if r == repeat and f == fold and self.trace_iterations[(r, f, i)].selected is True:\n            return i\n    raise ValueError(\n        \"Could not find the selected iteration for rep/fold %d/%d\" % (repeat, fold),\n    )\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.merge_traces","title":"merge_traces  <code>classmethod</code>","text":"<pre><code>merge_traces(traces: list[OpenMLRunTrace]) -&gt; OpenMLRunTrace\n</code></pre> <p>Merge multiple traces into a single trace.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.merge_traces--parameters","title":"Parameters","text":"<p>cls : type     Type of the trace object to be created. traces : List[OpenMLRunTrace]     List of traces to merge.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.merge_traces--returns","title":"Returns","text":"<p>OpenMLRunTrace     A trace object representing the merged traces.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.merge_traces--raises","title":"Raises","text":"<p>ValueError     If the parameters in the iterations of the traces being merged are not equal.     If a key (repeat, fold, iteration) is encountered twice while merging the traces.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef merge_traces(cls, traces: list[OpenMLRunTrace]) -&gt; OpenMLRunTrace:\n    \"\"\"Merge multiple traces into a single trace.\n\n    Parameters\n    ----------\n    cls : type\n        Type of the trace object to be created.\n    traces : List[OpenMLRunTrace]\n        List of traces to merge.\n\n    Returns\n    -------\n    OpenMLRunTrace\n        A trace object representing the merged traces.\n\n    Raises\n    ------\n    ValueError\n        If the parameters in the iterations of the traces being merged are not equal.\n        If a key (repeat, fold, iteration) is encountered twice while merging the traces.\n    \"\"\"\n    merged_trace: dict[tuple[int, int, int], OpenMLTraceIteration] = {}\n\n    previous_iteration = None\n    for trace in traces:\n        for iteration in trace:\n            key = (iteration.repeat, iteration.fold, iteration.iteration)\n\n            assert iteration.parameters is not None\n            param_keys = iteration.parameters.keys()\n\n            if previous_iteration is not None:\n                trace_itr = merged_trace[previous_iteration]\n\n                assert trace_itr.parameters is not None\n                trace_itr_keys = trace_itr.parameters.keys()\n\n                if list(param_keys) != list(trace_itr_keys):\n                    raise ValueError(\n                        \"Cannot merge traces because the parameters are not equal: \"\n                        f\"{list(trace_itr.parameters.keys())} vs \"\n                        f\"{list(iteration.parameters.keys())}\",\n                    )\n\n            if key in merged_trace:\n                raise ValueError(\n                    f\"Cannot merge traces because key '{key}' was encountered twice\",\n                )\n\n            merged_trace[key] = iteration\n            previous_iteration = key\n\n    return cls(None, merged_trace)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.trace_from_arff","title":"trace_from_arff  <code>classmethod</code>","text":"<pre><code>trace_from_arff(arff_obj: dict[str, Any]) -&gt; OpenMLRunTrace\n</code></pre> <p>Generate trace from arff trace.</p> <p>Creates a trace file from arff object (for example, generated by a local run).</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.trace_from_arff--parameters","title":"Parameters","text":"<p>arff_obj : dict     LIAC arff obj, dict containing attributes, relation, data.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.trace_from_arff--returns","title":"Returns","text":"<p>OpenMLRunTrace</p> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef trace_from_arff(cls, arff_obj: dict[str, Any]) -&gt; OpenMLRunTrace:\n    \"\"\"Generate trace from arff trace.\n\n    Creates a trace file from arff object (for example, generated by a\n    local run).\n\n    Parameters\n    ----------\n    arff_obj : dict\n        LIAC arff obj, dict containing attributes, relation, data.\n\n    Returns\n    -------\n    OpenMLRunTrace\n    \"\"\"\n    attributes = arff_obj[\"attributes\"]\n    content = arff_obj[\"data\"]\n    return cls._trace_from_arff_struct(\n        attributes=attributes,\n        content=content,\n        error_message=\"setup_string not supported for arff serialization\",\n    )\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.trace_from_xml","title":"trace_from_xml  <code>classmethod</code>","text":"<pre><code>trace_from_xml(xml: str | Path | IO) -&gt; OpenMLRunTrace\n</code></pre> <p>Generate trace from xml.</p> <p>Creates a trace file from the xml description.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.trace_from_xml--parameters","title":"Parameters","text":"<p>xml : string | file-like object     An xml description that can be either a <code>string</code> or a file-like     object.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.trace_from_xml--returns","title":"Returns","text":"<p>run : OpenMLRunTrace     Object containing the run id and a dict containing the trace     iterations.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef trace_from_xml(cls, xml: str | Path | IO) -&gt; OpenMLRunTrace:\n    \"\"\"Generate trace from xml.\n\n    Creates a trace file from the xml description.\n\n    Parameters\n    ----------\n    xml : string | file-like object\n        An xml description that can be either a `string` or a file-like\n        object.\n\n    Returns\n    -------\n    run : OpenMLRunTrace\n        Object containing the run id and a dict containing the trace\n        iterations.\n    \"\"\"\n    if isinstance(xml, Path):\n        xml = str(xml.absolute())\n\n    result_dict = xmltodict.parse(xml, force_list=(\"oml:trace_iteration\",))[\"oml:trace\"]\n\n    run_id = result_dict[\"oml:run_id\"]\n    trace = OrderedDict()\n\n    if \"oml:trace_iteration\" not in result_dict:\n        raise ValueError(\"Run does not contain valid trace. \")\n    if not isinstance(result_dict[\"oml:trace_iteration\"], list):\n        raise TypeError(type(result_dict[\"oml:trace_iteration\"]))\n\n    for itt in result_dict[\"oml:trace_iteration\"]:\n        repeat = int(itt[\"oml:repeat\"])\n        fold = int(itt[\"oml:fold\"])\n        iteration = int(itt[\"oml:iteration\"])\n        setup_string = json.loads(itt[\"oml:setup_string\"])\n        evaluation = float(itt[\"oml:evaluation\"])\n        selected_value = itt[\"oml:selected\"]\n        if selected_value == \"true\":\n            selected = True\n        elif selected_value == \"false\":\n            selected = False\n        else:\n            raise ValueError(\n                'expected {\"true\", \"false\"} value for '\n                f\"selected field, received: {selected_value}\",\n            )\n\n        current = OpenMLTraceIteration(\n            repeat=repeat,\n            fold=fold,\n            iteration=iteration,\n            setup_string=setup_string,\n            evaluation=evaluation,\n            selected=selected,\n        )\n        trace[(repeat, fold, iteration)] = current\n\n    return cls(run_id, trace)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.trace_to_arff","title":"trace_to_arff","text":"<pre><code>trace_to_arff() -&gt; dict[str, Any]\n</code></pre> <p>Generate the arff dictionary for uploading predictions to the server.</p> <p>Uses the trace object to generate an arff dictionary representation.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.trace_to_arff--returns","title":"Returns","text":"<p>arff_dict : dict     Dictionary representation of the ARFF file that will be uploaded.     Contains information about the optimization trace.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>def trace_to_arff(self) -&gt; dict[str, Any]:\n    \"\"\"Generate the arff dictionary for uploading predictions to the server.\n\n    Uses the trace object to generate an arff dictionary representation.\n\n    Returns\n    -------\n    arff_dict : dict\n        Dictionary representation of the ARFF file that will be uploaded.\n        Contains information about the optimization trace.\n    \"\"\"\n    if self.trace_iterations is None:\n        raise ValueError(\"trace_iterations missing from the trace object\")\n\n    # attributes that will be in trace arff\n    trace_attributes = [\n        (\"repeat\", \"NUMERIC\"),\n        (\"fold\", \"NUMERIC\"),\n        (\"iteration\", \"NUMERIC\"),\n        (\"evaluation\", \"NUMERIC\"),\n        (\"selected\", [\"true\", \"false\"]),\n    ]\n    trace_attributes.extend(\n        [\n            (PREFIX + parameter, \"STRING\")\n            for parameter in next(iter(self.trace_iterations.values())).get_parameters()\n        ],\n    )\n\n    arff_dict: dict[str, Any] = {}\n    data = []\n    for trace_iteration in self.trace_iterations.values():\n        tmp_list = []\n        for _attr, _ in trace_attributes:\n            if _attr.startswith(PREFIX):\n                attr = _attr[len(PREFIX) :]\n                value = trace_iteration.get_parameters()[attr]\n            else:\n                attr = _attr\n                value = getattr(trace_iteration, attr)\n\n            if attr == \"selected\":\n                tmp_list.append(\"true\" if value else \"false\")\n            else:\n                tmp_list.append(value)\n        data.append(tmp_list)\n\n    arff_dict[\"attributes\"] = trace_attributes\n    arff_dict[\"data\"] = data\n    # TODO allow to pass a trace description when running a flow\n    arff_dict[\"relation\"] = \"Trace\"\n    return arff_dict\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLTraceIteration","title":"OpenMLTraceIteration  <code>dataclass</code>","text":"<pre><code>OpenMLTraceIteration(repeat: int, fold: int, iteration: int, evaluation: float, selected: bool, setup_string: dict[str, str] | None = None, parameters: dict[str, str | int | float] | None = None)\n</code></pre> <p>OpenML Trace Iteration: parsed output from Run Trace call Exactly one of <code>setup_string</code> or <code>parameters</code> must be provided.</p>"},{"location":"reference/runs/#openml.runs.OpenMLTraceIteration--parameters","title":"Parameters","text":"<p>repeat : int     repeat number (in case of no repeats: 0)</p> int <p>fold number (in case of no folds: 0)</p> int <p>iteration number of optimization procedure</p> str, optional <p>json string representing the parameters If not provided, <code>parameters</code> should be set.</p> double <p>The evaluation that was awarded to this trace iteration. Measure is defined by the task</p> bool <p>Whether this was the best of all iterations, and hence selected for making predictions. Per fold/repeat there should be only one iteration selected</p> OrderedDict, optional <p>Dictionary specifying parameter names and their values. If not provided, <code>setup_string</code> should be set.</p>"},{"location":"reference/runs/#openml.runs.OpenMLTraceIteration.get_parameters","title":"get_parameters","text":"<pre><code>get_parameters() -&gt; dict[str, Any]\n</code></pre> <p>Get the parameters of this trace iteration.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>def get_parameters(self) -&gt; dict[str, Any]:\n    \"\"\"Get the parameters of this trace iteration.\"\"\"\n    # parameters have prefix 'parameter_'\n    if self.setup_string:\n        return {\n            param[len(PREFIX) :]: json.loads(value)\n            for param, value in self.setup_string.items()\n        }\n\n    assert self.parameters is not None\n    return {param[len(PREFIX) :]: value for param, value in self.parameters.items()}\n</code></pre>"},{"location":"reference/runs/#openml.runs.delete_run","title":"delete_run","text":"<pre><code>delete_run(run_id: int) -&gt; bool\n</code></pre> <p>Delete run with id <code>run_id</code> from the OpenML server.</p> <p>You can only delete runs which you uploaded.</p>"},{"location":"reference/runs/#openml.runs.delete_run--parameters","title":"Parameters","text":"<p>run_id : int     OpenML id of the run</p>"},{"location":"reference/runs/#openml.runs.delete_run--returns","title":"Returns","text":"<p>bool     True if the deletion was successful. False otherwise.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def delete_run(run_id: int) -&gt; bool:\n    \"\"\"Delete run with id `run_id` from the OpenML server.\n\n    You can only delete runs which you uploaded.\n\n    Parameters\n    ----------\n    run_id : int\n        OpenML id of the run\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"run\", run_id)\n</code></pre>"},{"location":"reference/runs/#openml.runs.get_run","title":"get_run","text":"<pre><code>get_run(run_id: int, ignore_cache: bool = False) -&gt; OpenMLRun\n</code></pre> <p>Gets run corresponding to run_id.</p>"},{"location":"reference/runs/#openml.runs.get_run--parameters","title":"Parameters","text":"<p>run_id : int</p> bool <p>Whether to ignore the cache. If <code>true</code> this will download and overwrite the run xml even if the requested run is already cached.</p> <p>ignore_cache</p>"},{"location":"reference/runs/#openml.runs.get_run--returns","title":"Returns","text":"<p>run : OpenMLRun     Run corresponding to ID, fetched from the server.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_run(run_id: int, ignore_cache: bool = False) -&gt; OpenMLRun:  # noqa: FBT002, FBT001\n    \"\"\"Gets run corresponding to run_id.\n\n    Parameters\n    ----------\n    run_id : int\n\n    ignore_cache : bool\n        Whether to ignore the cache. If ``true`` this will download and overwrite the run xml\n        even if the requested run is already cached.\n\n    ignore_cache\n\n    Returns\n    -------\n    run : OpenMLRun\n        Run corresponding to ID, fetched from the server.\n    \"\"\"\n    run_dir = Path(openml.utils._create_cache_directory_for_id(RUNS_CACHE_DIR_NAME, run_id))\n    run_file = run_dir / \"description.xml\"\n\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    try:\n        if not ignore_cache:\n            return _get_cached_run(run_id)\n\n        raise OpenMLCacheException(message=\"dummy\")\n\n    except OpenMLCacheException:\n        run_xml = openml._api_calls._perform_api_call(\"run/%d\" % run_id, \"get\")\n        with run_file.open(\"w\", encoding=\"utf8\") as fh:\n            fh.write(run_xml)\n\n    return _create_run_from_xml(run_xml)\n</code></pre>"},{"location":"reference/runs/#openml.runs.get_run_trace","title":"get_run_trace","text":"<pre><code>get_run_trace(run_id: int) -&gt; OpenMLRunTrace\n</code></pre> <p>Get the optimization trace object for a given run id.</p>"},{"location":"reference/runs/#openml.runs.get_run_trace--parameters","title":"Parameters","text":"<p>run_id : int</p>"},{"location":"reference/runs/#openml.runs.get_run_trace--returns","title":"Returns","text":"<p>openml.runs.OpenMLTrace</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def get_run_trace(run_id: int) -&gt; OpenMLRunTrace:\n    \"\"\"\n    Get the optimization trace object for a given run id.\n\n    Parameters\n    ----------\n    run_id : int\n\n    Returns\n    -------\n    openml.runs.OpenMLTrace\n    \"\"\"\n    trace_xml = openml._api_calls._perform_api_call(\"run/trace/%d\" % run_id, \"get\")\n    return OpenMLRunTrace.trace_from_xml(trace_xml)\n</code></pre>"},{"location":"reference/runs/#openml.runs.get_runs","title":"get_runs","text":"<pre><code>get_runs(run_ids: list[int]) -&gt; list[OpenMLRun]\n</code></pre> <p>Gets all runs in run_ids list.</p>"},{"location":"reference/runs/#openml.runs.get_runs--parameters","title":"Parameters","text":"<p>run_ids : list of ints</p>"},{"location":"reference/runs/#openml.runs.get_runs--returns","title":"Returns","text":"<p>runs : list of OpenMLRun     List of runs corresponding to IDs, fetched from the server.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def get_runs(run_ids: list[int]) -&gt; list[OpenMLRun]:\n    \"\"\"Gets all runs in run_ids list.\n\n    Parameters\n    ----------\n    run_ids : list of ints\n\n    Returns\n    -------\n    runs : list of OpenMLRun\n        List of runs corresponding to IDs, fetched from the server.\n    \"\"\"\n    runs = []\n    for run_id in run_ids:\n        runs.append(get_run(run_id))\n    return runs\n</code></pre>"},{"location":"reference/runs/#openml.runs.initialize_model_from_run","title":"initialize_model_from_run","text":"<pre><code>initialize_model_from_run(run_id: int, *, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Initialized a model based on a run_id (i.e., using the exact same parameter settings)</p>"},{"location":"reference/runs/#openml.runs.initialize_model_from_run--parameters","title":"Parameters","text":"<p>run_id : int     The Openml run_id strict_version: bool (default=True)     See <code>flow_to_model</code> strict_version.</p>"},{"location":"reference/runs/#openml.runs.initialize_model_from_run--returns","title":"Returns","text":"<p>model</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def initialize_model_from_run(run_id: int, *, strict_version: bool = True) -&gt; Any:\n    \"\"\"\n    Initialized a model based on a run_id (i.e., using the exact\n    same parameter settings)\n\n    Parameters\n    ----------\n    run_id : int\n        The Openml run_id\n    strict_version: bool (default=True)\n        See `flow_to_model` strict_version.\n\n    Returns\n    -------\n    model\n    \"\"\"\n    run = get_run(run_id)\n    # TODO(eddiebergman): I imagine this is None if it's not published,\n    # might need to raise an explicit error for that\n    assert run.setup_id is not None\n    return initialize_model(setup_id=run.setup_id, strict_version=strict_version)\n</code></pre>"},{"location":"reference/runs/#openml.runs.initialize_model_from_trace","title":"initialize_model_from_trace","text":"<pre><code>initialize_model_from_trace(run_id: int, repeat: int, fold: int, iteration: int | None = None) -&gt; Any\n</code></pre> <p>Initialize a model based on the parameters that were set by an optimization procedure (i.e., using the exact same parameter settings)</p>"},{"location":"reference/runs/#openml.runs.initialize_model_from_trace--parameters","title":"Parameters","text":"<p>run_id : int     The Openml run_id. Should contain a trace file,     otherwise a OpenMLServerException is raised</p> int <p>The repeat nr (column in trace file)</p> int <p>The fold nr (column in trace file)</p> int <p>The iteration nr (column in trace file). If None, the best (selected) iteration will be searched (slow), according to the selection criteria implemented in OpenMLRunTrace.get_selected_iteration</p>"},{"location":"reference/runs/#openml.runs.initialize_model_from_trace--returns","title":"Returns","text":"<p>model</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def initialize_model_from_trace(\n    run_id: int,\n    repeat: int,\n    fold: int,\n    iteration: int | None = None,\n) -&gt; Any:\n    \"\"\"\n    Initialize a model based on the parameters that were set\n    by an optimization procedure (i.e., using the exact same\n    parameter settings)\n\n    Parameters\n    ----------\n    run_id : int\n        The Openml run_id. Should contain a trace file,\n        otherwise a OpenMLServerException is raised\n\n    repeat : int\n        The repeat nr (column in trace file)\n\n    fold : int\n        The fold nr (column in trace file)\n\n    iteration : int\n        The iteration nr (column in trace file). If None, the\n        best (selected) iteration will be searched (slow),\n        according to the selection criteria implemented in\n        OpenMLRunTrace.get_selected_iteration\n\n    Returns\n    -------\n    model\n    \"\"\"\n    run = get_run(run_id)\n    # TODO(eddiebergman): I imagine this is None if it's not published,\n    # might need to raise an explicit error for that\n    assert run.flow_id is not None\n\n    flow = get_flow(run.flow_id)\n    run_trace = get_run_trace(run_id)\n\n    if iteration is None:\n        iteration = run_trace.get_selected_iteration(repeat, fold)\n\n    request = (repeat, fold, iteration)\n    if request not in run_trace.trace_iterations:\n        raise ValueError(\"Combination repeat, fold, iteration not available\")\n    current = run_trace.trace_iterations[(repeat, fold, iteration)]\n\n    search_model = initialize_model_from_run(run_id)\n    return flow.extension.instantiate_model_from_hpo_class(search_model, current)\n</code></pre>"},{"location":"reference/runs/#openml.runs.list_runs","title":"list_runs","text":"<pre><code>list_runs(offset: int | None = None, size: int | None = None, id: list | None = None, task: list[int] | None = None, setup: list | None = None, flow: list | None = None, uploader: list | None = None, tag: str | None = None, study: int | None = None, display_errors: bool = False, task_type: TaskType | int | None = None) -&gt; DataFrame\n</code></pre> <p>List all runs matching all of the given filters. (Supports large amount of results)</p>"},{"location":"reference/runs/#openml.runs.list_runs--parameters","title":"Parameters","text":"<p>offset : int, optional     the number of runs to skip, starting from the first size : int, optional     the maximum number of runs to show</p> <p>id : list, optional</p> <p>task : list, optional</p> <p>setup: list, optional</p> <p>flow : list, optional</p> <p>uploader : list, optional</p> <p>tag : str, optional</p> <p>study : int, optional</p> bool, optional (default=None) <p>Whether to list runs which have an error (for example a missing prediction file).</p> <p>task_type : str, optional</p>"},{"location":"reference/runs/#openml.runs.list_runs--returns","title":"Returns","text":"<p>dataframe</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def list_runs(  # noqa: PLR0913\n    offset: int | None = None,\n    size: int | None = None,\n    id: list | None = None,  # noqa: A002\n    task: list[int] | None = None,\n    setup: list | None = None,\n    flow: list | None = None,\n    uploader: list | None = None,\n    tag: str | None = None,\n    study: int | None = None,\n    display_errors: bool = False,  # noqa: FBT001, FBT002\n    task_type: TaskType | int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    List all runs matching all of the given filters.\n    (Supports large amount of results)\n\n    Parameters\n    ----------\n    offset : int, optional\n        the number of runs to skip, starting from the first\n    size : int, optional\n        the maximum number of runs to show\n\n    id : list, optional\n\n    task : list, optional\n\n    setup: list, optional\n\n    flow : list, optional\n\n    uploader : list, optional\n\n    tag : str, optional\n\n    study : int, optional\n\n    display_errors : bool, optional (default=None)\n        Whether to list runs which have an error (for example a missing\n        prediction file).\n\n    task_type : str, optional\n\n    Returns\n    -------\n    dataframe\n    \"\"\"\n    if id is not None and (not isinstance(id, list)):\n        raise TypeError(\"id must be of type list.\")\n    if task is not None and (not isinstance(task, list)):\n        raise TypeError(\"task must be of type list.\")\n    if setup is not None and (not isinstance(setup, list)):\n        raise TypeError(\"setup must be of type list.\")\n    if flow is not None and (not isinstance(flow, list)):\n        raise TypeError(\"flow must be of type list.\")\n    if uploader is not None and (not isinstance(uploader, list)):\n        raise TypeError(\"uploader must be of type list.\")\n\n    listing_call = partial(\n        _list_runs,\n        id=id,\n        task=task,\n        setup=setup,\n        flow=flow,\n        uploader=uploader,\n        tag=tag,\n        study=study,\n        display_errors=display_errors,\n        task_type=task_type,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/runs/#openml.runs.run_exists","title":"run_exists","text":"<pre><code>run_exists(task_id: int, setup_id: int) -&gt; set[int]\n</code></pre> <p>Checks whether a task/setup combination is already present on the server.</p>"},{"location":"reference/runs/#openml.runs.run_exists--parameters","title":"Parameters","text":"<p>task_id : int</p> <p>setup_id : int</p>"},{"location":"reference/runs/#openml.runs.run_exists--returns","title":"Returns","text":"<pre><code>Set run ids for runs where flow setup_id was run on task_id. Empty\nset if it wasn't run yet.\n</code></pre> Source code in <code>openml/runs/functions.py</code> <pre><code>def run_exists(task_id: int, setup_id: int) -&gt; set[int]:\n    \"\"\"Checks whether a task/setup combination is already present on the\n    server.\n\n    Parameters\n    ----------\n    task_id : int\n\n    setup_id : int\n\n    Returns\n    -------\n        Set run ids for runs where flow setup_id was run on task_id. Empty\n        set if it wasn't run yet.\n    \"\"\"\n    if setup_id &lt;= 0:\n        # openml setups are in range 1-inf\n        return set()\n\n    try:\n        result = list_runs(task=[task_id], setup=[setup_id])\n        return set() if result.empty else set(result[\"run_id\"])\n    except OpenMLServerException as exception:\n        # error code implies no results. The run does not exist yet\n        if exception.code != ERROR_CODE:\n            raise exception\n        return set()\n</code></pre>"},{"location":"reference/runs/#openml.runs.run_flow_on_task","title":"run_flow_on_task","text":"<pre><code>run_flow_on_task(flow: OpenMLFlow, task: OpenMLTask, avoid_duplicate_runs: bool = True, flow_tags: list[str] | None = None, seed: int | None = None, add_local_measures: bool = True, upload_flow: bool = False, n_jobs: int | None = None) -&gt; OpenMLRun\n</code></pre> <p>Run the model provided by the flow on the dataset defined by task.</p> <p>Takes the flow and repeat information into account. The Flow may optionally be published.</p>"},{"location":"reference/runs/#openml.runs.run_flow_on_task--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow     A flow wraps a machine learning model together with relevant information.     The model has a function fit(X,Y) and predict(X),     all supervised estimators of scikit learn follow this definition of a model. task : OpenMLTask     Task to perform. This may be an OpenMLFlow instead if the first argument is an OpenMLTask. avoid_duplicate_runs : bool, optional (default=True)     If True, the run will throw an error if the setup/task combination is already present on     the server. This feature requires an internet connection. flow_tags : List[str], optional (default=None)     A list of tags that the flow should have at creation. seed: int, optional (default=None)     Models that are not seeded will get this seed. add_local_measures : bool, optional (default=True)     Determines whether to calculate a set of evaluation measures locally,     to later verify server behaviour. upload_flow : bool (default=False)     If True, upload the flow to OpenML if it does not exist yet.     If False, do not upload the flow to OpenML. n_jobs : int (default=None)     The number of processes/threads to distribute the evaluation asynchronously.     If <code>None</code> or <code>1</code>, then the evaluation is treated as synchronous and processed sequentially.     If <code>-1</code>, then the job uses as many cores available.</p>"},{"location":"reference/runs/#openml.runs.run_flow_on_task--returns","title":"Returns","text":"<p>run : OpenMLRun     Result of the run.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def run_flow_on_task(  # noqa: C901, PLR0912, PLR0915, PLR0913\n    flow: OpenMLFlow,\n    task: OpenMLTask,\n    avoid_duplicate_runs: bool = True,  # noqa: FBT002, FBT001\n    flow_tags: list[str] | None = None,\n    seed: int | None = None,\n    add_local_measures: bool = True,  # noqa: FBT001, FBT002\n    upload_flow: bool = False,  # noqa: FBT001, FBT002\n    n_jobs: int | None = None,\n) -&gt; OpenMLRun:\n    \"\"\"Run the model provided by the flow on the dataset defined by task.\n\n    Takes the flow and repeat information into account.\n    The Flow may optionally be published.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        A flow wraps a machine learning model together with relevant information.\n        The model has a function fit(X,Y) and predict(X),\n        all supervised estimators of scikit learn follow this definition of a model.\n    task : OpenMLTask\n        Task to perform. This may be an OpenMLFlow instead if the first argument is an OpenMLTask.\n    avoid_duplicate_runs : bool, optional (default=True)\n        If True, the run will throw an error if the setup/task combination is already present on\n        the server. This feature requires an internet connection.\n    flow_tags : List[str], optional (default=None)\n        A list of tags that the flow should have at creation.\n    seed: int, optional (default=None)\n        Models that are not seeded will get this seed.\n    add_local_measures : bool, optional (default=True)\n        Determines whether to calculate a set of evaluation measures locally,\n        to later verify server behaviour.\n    upload_flow : bool (default=False)\n        If True, upload the flow to OpenML if it does not exist yet.\n        If False, do not upload the flow to OpenML.\n    n_jobs : int (default=None)\n        The number of processes/threads to distribute the evaluation asynchronously.\n        If `None` or `1`, then the evaluation is treated as synchronous and processed sequentially.\n        If `-1`, then the job uses as many cores available.\n\n    Returns\n    -------\n    run : OpenMLRun\n        Result of the run.\n    \"\"\"\n    if flow_tags is not None and not isinstance(flow_tags, list):\n        raise ValueError(\"flow_tags should be a list\")\n\n    # TODO: At some point in the future do not allow for arguments in old order (changed 6-2018).\n    # Flexibility currently still allowed due to code-snippet in OpenML100 paper (3-2019).\n    if isinstance(flow, OpenMLTask) and isinstance(task, OpenMLFlow):\n        # We want to allow either order of argument (to avoid confusion).\n        warnings.warn(\n            \"The old argument order (Flow, model) is deprecated and \"\n            \"will not be supported in the future. Please use the \"\n            \"order (model, Flow).\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        task, flow = flow, task\n\n    if task.task_id is None:\n        raise ValueError(\"The task should be published at OpenML\")\n\n    if flow.model is None:\n        flow.model = flow.extension.flow_to_model(flow)\n\n    flow.model = flow.extension.seed_model(flow.model, seed=seed)\n\n    # We only need to sync with the server right now if we want to upload the flow,\n    # or ensure no duplicate runs exist. Otherwise it can be synced at upload time.\n    flow_id = None\n    if upload_flow or avoid_duplicate_runs:\n        flow_id = flow_exists(flow.name, flow.external_version)\n        if isinstance(flow.flow_id, int) and flow_id != flow.flow_id:\n            if flow_id is not False:\n                raise PyOpenMLError(\n                    f\"Local flow_id does not match server flow_id: '{flow.flow_id}' vs '{flow_id}'\",\n                )\n            raise PyOpenMLError(\n                \"Flow does not exist on the server, but 'flow.flow_id' is not None.\"\n            )\n        if upload_flow and flow_id is False:\n            flow.publish()\n            flow_id = flow.flow_id\n        elif flow_id:\n            flow_from_server = get_flow(flow_id)\n            _copy_server_fields(flow_from_server, flow)\n            if avoid_duplicate_runs:\n                flow_from_server.model = flow.model\n                setup_id = setup_exists(flow_from_server)\n                ids = run_exists(task.task_id, setup_id)\n                if ids:\n                    error_message = (\n                        \"One or more runs of this setup were already performed on the task.\"\n                    )\n                    raise OpenMLRunsExistError(ids, error_message)\n        else:\n            # Flow does not exist on server and we do not want to upload it.\n            # No sync with the server happens.\n            flow_id = None\n\n    dataset = task.get_dataset()\n\n    run_environment = flow.extension.get_version_information()\n    tags = [\"openml-python\", run_environment[1]]\n\n    if flow.extension.check_if_model_fitted(flow.model):\n        warnings.warn(\n            \"The model is already fitted! This might cause inconsistency in comparison of results.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n\n    # execute the run\n    res = _run_task_get_arffcontent(\n        model=flow.model,\n        task=task,\n        extension=flow.extension,\n        add_local_measures=add_local_measures,\n        n_jobs=n_jobs,\n    )\n\n    data_content, trace, fold_evaluations, sample_evaluations = res\n    fields = [*run_environment, time.strftime(\"%c\"), \"Created by run_flow_on_task\"]\n    generated_description = \"\\n\".join(fields)\n    run = OpenMLRun(\n        task_id=task.task_id,\n        flow_id=flow_id,\n        dataset_id=dataset.dataset_id,\n        model=flow.model,\n        flow_name=flow.name,\n        tags=tags,\n        trace=trace,\n        data_content=data_content,\n        flow=flow,\n        setup_string=flow.extension.create_setup_string(flow.model),\n        description_text=generated_description,\n    )\n\n    if (upload_flow or avoid_duplicate_runs) and flow.flow_id is not None:\n        # We only extract the parameter settings if a sync happened with the server.\n        # I.e. when the flow was uploaded or we found it in the avoid_duplicate check.\n        # Otherwise, we will do this at upload time.\n        run.parameter_settings = flow.extension.obtain_parameter_values(flow)\n\n    # now we need to attach the detailed evaluations\n    if task.task_type_id == TaskType.LEARNING_CURVE:\n        run.sample_evaluations = sample_evaluations\n    else:\n        run.fold_evaluations = fold_evaluations\n\n    if flow_id:\n        message = f\"Executed Task {task.task_id} with Flow id:{run.flow_id}\"\n    else:\n        message = f\"Executed Task {task.task_id} on local Flow with name {flow.name}.\"\n    config.logger.info(message)\n\n    return run\n</code></pre>"},{"location":"reference/runs/#openml.runs.run_model_on_task","title":"run_model_on_task","text":"<pre><code>run_model_on_task(model: Any, task: int | str | OpenMLTask, avoid_duplicate_runs: bool = True, flow_tags: list[str] | None = None, seed: int | None = None, add_local_measures: bool = True, upload_flow: bool = False, return_flow: bool = False, n_jobs: int | None = None) -&gt; OpenMLRun | tuple[OpenMLRun, OpenMLFlow]\n</code></pre> <p>Run the model on the dataset defined by the task.</p>"},{"location":"reference/runs/#openml.runs.run_model_on_task--parameters","title":"Parameters","text":"<p>model : sklearn model     A model which has a function fit(X,Y) and predict(X),     all supervised estimators of scikit learn follow this definition of a model. task : OpenMLTask or int or str     Task to perform or Task id.     This may be a model instead if the first argument is an OpenMLTask. avoid_duplicate_runs : bool, optional (default=True)     If True, the run will throw an error if the setup/task combination is already present on     the server. This feature requires an internet connection. flow_tags : List[str], optional (default=None)     A list of tags that the flow should have at creation. seed: int, optional (default=None)     Models that are not seeded will get this seed. add_local_measures : bool, optional (default=True)     Determines whether to calculate a set of evaluation measures locally,     to later verify server behaviour. upload_flow : bool (default=False)     If True, upload the flow to OpenML if it does not exist yet.     If False, do not upload the flow to OpenML. return_flow : bool (default=False)     If True, returns the OpenMLFlow generated from the model in addition to the OpenMLRun. n_jobs : int (default=None)     The number of processes/threads to distribute the evaluation asynchronously.     If <code>None</code> or <code>1</code>, then the evaluation is treated as synchronous and processed sequentially.     If <code>-1</code>, then the job uses as many cores available.</p>"},{"location":"reference/runs/#openml.runs.run_model_on_task--returns","title":"Returns","text":"<p>run : OpenMLRun     Result of the run. flow : OpenMLFlow (optional, only if <code>return_flow</code> is True).     Flow generated from the model.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def run_model_on_task(  # noqa: PLR0913\n    model: Any,\n    task: int | str | OpenMLTask,\n    avoid_duplicate_runs: bool = True,  # noqa: FBT001, FBT002\n    flow_tags: list[str] | None = None,\n    seed: int | None = None,\n    add_local_measures: bool = True,  # noqa: FBT001, FBT002\n    upload_flow: bool = False,  # noqa: FBT001, FBT002\n    return_flow: bool = False,  # noqa: FBT001, FBT002\n    n_jobs: int | None = None,\n) -&gt; OpenMLRun | tuple[OpenMLRun, OpenMLFlow]:\n    \"\"\"Run the model on the dataset defined by the task.\n\n    Parameters\n    ----------\n    model : sklearn model\n        A model which has a function fit(X,Y) and predict(X),\n        all supervised estimators of scikit learn follow this definition of a model.\n    task : OpenMLTask or int or str\n        Task to perform or Task id.\n        This may be a model instead if the first argument is an OpenMLTask.\n    avoid_duplicate_runs : bool, optional (default=True)\n        If True, the run will throw an error if the setup/task combination is already present on\n        the server. This feature requires an internet connection.\n    flow_tags : List[str], optional (default=None)\n        A list of tags that the flow should have at creation.\n    seed: int, optional (default=None)\n        Models that are not seeded will get this seed.\n    add_local_measures : bool, optional (default=True)\n        Determines whether to calculate a set of evaluation measures locally,\n        to later verify server behaviour.\n    upload_flow : bool (default=False)\n        If True, upload the flow to OpenML if it does not exist yet.\n        If False, do not upload the flow to OpenML.\n    return_flow : bool (default=False)\n        If True, returns the OpenMLFlow generated from the model in addition to the OpenMLRun.\n    n_jobs : int (default=None)\n        The number of processes/threads to distribute the evaluation asynchronously.\n        If `None` or `1`, then the evaluation is treated as synchronous and processed sequentially.\n        If `-1`, then the job uses as many cores available.\n\n    Returns\n    -------\n    run : OpenMLRun\n        Result of the run.\n    flow : OpenMLFlow (optional, only if `return_flow` is True).\n        Flow generated from the model.\n    \"\"\"\n    if avoid_duplicate_runs and not config.apikey:\n        warnings.warn(\n            \"avoid_duplicate_runs is set to True, but no API key is set. \"\n            \"Please set your API key in the OpenML configuration file, see\"\n            \"https://openml.github.io/openml-python/main/examples/20_basic/introduction_tutorial\"\n            \".html#authentication for more information on authentication.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n\n    # TODO: At some point in the future do not allow for arguments in old order (6-2018).\n    # Flexibility currently still allowed due to code-snippet in OpenML100 paper (3-2019).\n    # When removing this please also remove the method `is_estimator` from the extension\n    # interface as it is only used here (MF, 3-2019)\n    if isinstance(model, (int, str, OpenMLTask)):\n        warnings.warn(\n            \"The old argument order (task, model) is deprecated and \"\n            \"will not be supported in the future. Please use the \"\n            \"order (model, task).\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        task, model = model, task\n\n    extension = get_extension_by_model(model, raise_if_no_extension=True)\n    if extension is None:\n        # This should never happen and is only here to please mypy will be gone soon once the\n        # whole function is removed\n        raise TypeError(extension)\n\n    flow = extension.model_to_flow(model)\n\n    def get_task_and_type_conversion(_task: int | str | OpenMLTask) -&gt; OpenMLTask:\n        \"\"\"Retrieve an OpenMLTask object from either an integer or string ID,\n        or directly from an OpenMLTask object.\n\n        Parameters\n        ----------\n        _task : Union[int, str, OpenMLTask]\n            The task ID or the OpenMLTask object.\n\n        Returns\n        -------\n        OpenMLTask\n            The OpenMLTask object.\n        \"\"\"\n        if isinstance(_task, (int, str)):\n            return get_task(int(_task))  # type: ignore\n\n        return _task\n\n    task = get_task_and_type_conversion(task)\n\n    run = run_flow_on_task(\n        task=task,\n        flow=flow,\n        avoid_duplicate_runs=avoid_duplicate_runs,\n        flow_tags=flow_tags,\n        seed=seed,\n        add_local_measures=add_local_measures,\n        upload_flow=upload_flow,\n        n_jobs=n_jobs,\n    )\n    if return_flow:\n        return run, flow\n    return run\n</code></pre>"},{"location":"reference/runs/functions/","title":"functions","text":""},{"location":"reference/runs/functions/#openml.runs.functions","title":"openml.runs.functions","text":""},{"location":"reference/runs/functions/#openml.runs.functions.__list_runs","title":"__list_runs","text":"<pre><code>__list_runs(api_call: str) -&gt; DataFrame\n</code></pre> <p>Helper function to parse API calls which are lists of runs</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def __list_runs(api_call: str) -&gt; pd.DataFrame:\n    \"\"\"Helper function to parse API calls which are lists of runs\"\"\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    runs_dict = xmltodict.parse(xml_string, force_list=(\"oml:run\",))\n    # Minimalistic check if the XML is useful\n    if \"oml:runs\" not in runs_dict:\n        raise ValueError(f'Error in return XML, does not contain \"oml:runs\": {runs_dict}')\n\n    if \"@xmlns:oml\" not in runs_dict[\"oml:runs\"]:\n        raise ValueError(\n            f'Error in return XML, does not contain \"oml:runs\"/@xmlns:oml: {runs_dict}'\n        )\n\n    if runs_dict[\"oml:runs\"][\"@xmlns:oml\"] != \"http://openml.org/openml\":\n        raise ValueError(\n            \"Error in return XML, value of  \"\n            '\"oml:runs\"/@xmlns:oml is not '\n            f'\"http://openml.org/openml\": {runs_dict}',\n        )\n\n    assert isinstance(runs_dict[\"oml:runs\"][\"oml:run\"], list), type(runs_dict[\"oml:runs\"])\n\n    runs = {\n        int(r[\"oml:run_id\"]): {\n            \"run_id\": int(r[\"oml:run_id\"]),\n            \"task_id\": int(r[\"oml:task_id\"]),\n            \"setup_id\": int(r[\"oml:setup_id\"]),\n            \"flow_id\": int(r[\"oml:flow_id\"]),\n            \"uploader\": int(r[\"oml:uploader\"]),\n            \"task_type\": TaskType(int(r[\"oml:task_type_id\"])),\n            \"upload_time\": str(r[\"oml:upload_time\"]),\n            \"error_message\": str((r[\"oml:error_message\"]) or \"\"),\n        }\n        for r in runs_dict[\"oml:runs\"][\"oml:run\"]\n    }\n    return pd.DataFrame.from_dict(runs, orient=\"index\")\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.delete_run","title":"delete_run","text":"<pre><code>delete_run(run_id: int) -&gt; bool\n</code></pre> <p>Delete run with id <code>run_id</code> from the OpenML server.</p> <p>You can only delete runs which you uploaded.</p>"},{"location":"reference/runs/functions/#openml.runs.functions.delete_run--parameters","title":"Parameters","text":"<p>run_id : int     OpenML id of the run</p>"},{"location":"reference/runs/functions/#openml.runs.functions.delete_run--returns","title":"Returns","text":"<p>bool     True if the deletion was successful. False otherwise.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def delete_run(run_id: int) -&gt; bool:\n    \"\"\"Delete run with id `run_id` from the OpenML server.\n\n    You can only delete runs which you uploaded.\n\n    Parameters\n    ----------\n    run_id : int\n        OpenML id of the run\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"run\", run_id)\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.format_prediction","title":"format_prediction","text":"<pre><code>format_prediction(task: OpenMLSupervisedTask, repeat: int, fold: int, index: int, prediction: str | int | float, truth: str | int | float, sample: int | None = None, proba: dict[str, float] | None = None) -&gt; list[str | int | float]\n</code></pre> <p>Format the predictions in the specific order as required for the run results.</p>"},{"location":"reference/runs/functions/#openml.runs.functions.format_prediction--parameters","title":"Parameters","text":"<p>task: OpenMLSupervisedTask     Task for which to format the predictions. repeat: int     From which repeat this predictions is made. fold: int     From which fold this prediction is made. index: int     For which index this prediction is made. prediction: str, int or float     The predicted class label or value. truth: str, int or float     The true class label or value. sample: int, optional (default=None)     From which sample set this prediction is made.     Required only for LearningCurve tasks. proba: Dict[str, float], optional (default=None)     For classification tasks only.     A mapping from each class label to their predicted probability.     The dictionary should contain an entry for each of the <code>task.class_labels</code>.     E.g.: {\"Iris-Setosa\": 0.2, \"Iris-Versicolor\": 0.7, \"Iris-Virginica\": 0.1}</p>"},{"location":"reference/runs/functions/#openml.runs.functions.format_prediction--returns","title":"Returns","text":"<p>A list with elements for the prediction results of a run.</p> <p>The returned order of the elements is (if available):     [repeat, fold, sample, index, prediction, truth, *probabilities]</p> <p>This order follows the R Client API.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def format_prediction(  # noqa: PLR0913\n    task: OpenMLSupervisedTask,\n    repeat: int,\n    fold: int,\n    index: int,\n    prediction: str | int | float,\n    truth: str | int | float,\n    sample: int | None = None,\n    proba: dict[str, float] | None = None,\n) -&gt; list[str | int | float]:\n    \"\"\"Format the predictions in the specific order as required for the run results.\n\n    Parameters\n    ----------\n    task: OpenMLSupervisedTask\n        Task for which to format the predictions.\n    repeat: int\n        From which repeat this predictions is made.\n    fold: int\n        From which fold this prediction is made.\n    index: int\n        For which index this prediction is made.\n    prediction: str, int or float\n        The predicted class label or value.\n    truth: str, int or float\n        The true class label or value.\n    sample: int, optional (default=None)\n        From which sample set this prediction is made.\n        Required only for LearningCurve tasks.\n    proba: Dict[str, float], optional (default=None)\n        For classification tasks only.\n        A mapping from each class label to their predicted probability.\n        The dictionary should contain an entry for each of the `task.class_labels`.\n        E.g.: {\"Iris-Setosa\": 0.2, \"Iris-Versicolor\": 0.7, \"Iris-Virginica\": 0.1}\n\n    Returns\n    -------\n    A list with elements for the prediction results of a run.\n\n    The returned order of the elements is (if available):\n        [repeat, fold, sample, index, prediction, truth, *probabilities]\n\n    This order follows the R Client API.\n    \"\"\"\n    if isinstance(task, OpenMLClassificationTask):\n        if proba is None:\n            raise ValueError(\"`proba` is required for classification task\")\n        if task.class_labels is None:\n            raise ValueError(\"The classification task must have class labels set\")\n        if not set(task.class_labels) == set(proba):\n            raise ValueError(\"Each class should have a predicted probability\")\n        if sample is None:\n            if isinstance(task, OpenMLLearningCurveTask):\n                raise ValueError(\"`sample` can not be none for LearningCurveTask\")\n\n            sample = 0\n        probabilities = [proba[c] for c in task.class_labels]\n        return [repeat, fold, sample, index, prediction, truth, *probabilities]\n\n    if isinstance(task, OpenMLRegressionTask):\n        return [repeat, fold, index, prediction, truth]\n\n    raise NotImplementedError(f\"Formatting for {type(task)} is not supported.\")\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.get_run","title":"get_run","text":"<pre><code>get_run(run_id: int, ignore_cache: bool = False) -&gt; OpenMLRun\n</code></pre> <p>Gets run corresponding to run_id.</p>"},{"location":"reference/runs/functions/#openml.runs.functions.get_run--parameters","title":"Parameters","text":"<p>run_id : int</p> bool <p>Whether to ignore the cache. If <code>true</code> this will download and overwrite the run xml even if the requested run is already cached.</p> <p>ignore_cache</p>"},{"location":"reference/runs/functions/#openml.runs.functions.get_run--returns","title":"Returns","text":"<p>run : OpenMLRun     Run corresponding to ID, fetched from the server.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_run(run_id: int, ignore_cache: bool = False) -&gt; OpenMLRun:  # noqa: FBT002, FBT001\n    \"\"\"Gets run corresponding to run_id.\n\n    Parameters\n    ----------\n    run_id : int\n\n    ignore_cache : bool\n        Whether to ignore the cache. If ``true`` this will download and overwrite the run xml\n        even if the requested run is already cached.\n\n    ignore_cache\n\n    Returns\n    -------\n    run : OpenMLRun\n        Run corresponding to ID, fetched from the server.\n    \"\"\"\n    run_dir = Path(openml.utils._create_cache_directory_for_id(RUNS_CACHE_DIR_NAME, run_id))\n    run_file = run_dir / \"description.xml\"\n\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    try:\n        if not ignore_cache:\n            return _get_cached_run(run_id)\n\n        raise OpenMLCacheException(message=\"dummy\")\n\n    except OpenMLCacheException:\n        run_xml = openml._api_calls._perform_api_call(\"run/%d\" % run_id, \"get\")\n        with run_file.open(\"w\", encoding=\"utf8\") as fh:\n            fh.write(run_xml)\n\n    return _create_run_from_xml(run_xml)\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.get_run_trace","title":"get_run_trace","text":"<pre><code>get_run_trace(run_id: int) -&gt; OpenMLRunTrace\n</code></pre> <p>Get the optimization trace object for a given run id.</p>"},{"location":"reference/runs/functions/#openml.runs.functions.get_run_trace--parameters","title":"Parameters","text":"<p>run_id : int</p>"},{"location":"reference/runs/functions/#openml.runs.functions.get_run_trace--returns","title":"Returns","text":"<p>openml.runs.OpenMLTrace</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def get_run_trace(run_id: int) -&gt; OpenMLRunTrace:\n    \"\"\"\n    Get the optimization trace object for a given run id.\n\n    Parameters\n    ----------\n    run_id : int\n\n    Returns\n    -------\n    openml.runs.OpenMLTrace\n    \"\"\"\n    trace_xml = openml._api_calls._perform_api_call(\"run/trace/%d\" % run_id, \"get\")\n    return OpenMLRunTrace.trace_from_xml(trace_xml)\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.get_runs","title":"get_runs","text":"<pre><code>get_runs(run_ids: list[int]) -&gt; list[OpenMLRun]\n</code></pre> <p>Gets all runs in run_ids list.</p>"},{"location":"reference/runs/functions/#openml.runs.functions.get_runs--parameters","title":"Parameters","text":"<p>run_ids : list of ints</p>"},{"location":"reference/runs/functions/#openml.runs.functions.get_runs--returns","title":"Returns","text":"<p>runs : list of OpenMLRun     List of runs corresponding to IDs, fetched from the server.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def get_runs(run_ids: list[int]) -&gt; list[OpenMLRun]:\n    \"\"\"Gets all runs in run_ids list.\n\n    Parameters\n    ----------\n    run_ids : list of ints\n\n    Returns\n    -------\n    runs : list of OpenMLRun\n        List of runs corresponding to IDs, fetched from the server.\n    \"\"\"\n    runs = []\n    for run_id in run_ids:\n        runs.append(get_run(run_id))\n    return runs\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.initialize_model_from_run","title":"initialize_model_from_run","text":"<pre><code>initialize_model_from_run(run_id: int, *, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Initialized a model based on a run_id (i.e., using the exact same parameter settings)</p>"},{"location":"reference/runs/functions/#openml.runs.functions.initialize_model_from_run--parameters","title":"Parameters","text":"<p>run_id : int     The Openml run_id strict_version: bool (default=True)     See <code>flow_to_model</code> strict_version.</p>"},{"location":"reference/runs/functions/#openml.runs.functions.initialize_model_from_run--returns","title":"Returns","text":"<p>model</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def initialize_model_from_run(run_id: int, *, strict_version: bool = True) -&gt; Any:\n    \"\"\"\n    Initialized a model based on a run_id (i.e., using the exact\n    same parameter settings)\n\n    Parameters\n    ----------\n    run_id : int\n        The Openml run_id\n    strict_version: bool (default=True)\n        See `flow_to_model` strict_version.\n\n    Returns\n    -------\n    model\n    \"\"\"\n    run = get_run(run_id)\n    # TODO(eddiebergman): I imagine this is None if it's not published,\n    # might need to raise an explicit error for that\n    assert run.setup_id is not None\n    return initialize_model(setup_id=run.setup_id, strict_version=strict_version)\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.initialize_model_from_trace","title":"initialize_model_from_trace","text":"<pre><code>initialize_model_from_trace(run_id: int, repeat: int, fold: int, iteration: int | None = None) -&gt; Any\n</code></pre> <p>Initialize a model based on the parameters that were set by an optimization procedure (i.e., using the exact same parameter settings)</p>"},{"location":"reference/runs/functions/#openml.runs.functions.initialize_model_from_trace--parameters","title":"Parameters","text":"<p>run_id : int     The Openml run_id. Should contain a trace file,     otherwise a OpenMLServerException is raised</p> int <p>The repeat nr (column in trace file)</p> int <p>The fold nr (column in trace file)</p> int <p>The iteration nr (column in trace file). If None, the best (selected) iteration will be searched (slow), according to the selection criteria implemented in OpenMLRunTrace.get_selected_iteration</p>"},{"location":"reference/runs/functions/#openml.runs.functions.initialize_model_from_trace--returns","title":"Returns","text":"<p>model</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def initialize_model_from_trace(\n    run_id: int,\n    repeat: int,\n    fold: int,\n    iteration: int | None = None,\n) -&gt; Any:\n    \"\"\"\n    Initialize a model based on the parameters that were set\n    by an optimization procedure (i.e., using the exact same\n    parameter settings)\n\n    Parameters\n    ----------\n    run_id : int\n        The Openml run_id. Should contain a trace file,\n        otherwise a OpenMLServerException is raised\n\n    repeat : int\n        The repeat nr (column in trace file)\n\n    fold : int\n        The fold nr (column in trace file)\n\n    iteration : int\n        The iteration nr (column in trace file). If None, the\n        best (selected) iteration will be searched (slow),\n        according to the selection criteria implemented in\n        OpenMLRunTrace.get_selected_iteration\n\n    Returns\n    -------\n    model\n    \"\"\"\n    run = get_run(run_id)\n    # TODO(eddiebergman): I imagine this is None if it's not published,\n    # might need to raise an explicit error for that\n    assert run.flow_id is not None\n\n    flow = get_flow(run.flow_id)\n    run_trace = get_run_trace(run_id)\n\n    if iteration is None:\n        iteration = run_trace.get_selected_iteration(repeat, fold)\n\n    request = (repeat, fold, iteration)\n    if request not in run_trace.trace_iterations:\n        raise ValueError(\"Combination repeat, fold, iteration not available\")\n    current = run_trace.trace_iterations[(repeat, fold, iteration)]\n\n    search_model = initialize_model_from_run(run_id)\n    return flow.extension.instantiate_model_from_hpo_class(search_model, current)\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.list_runs","title":"list_runs","text":"<pre><code>list_runs(offset: int | None = None, size: int | None = None, id: list | None = None, task: list[int] | None = None, setup: list | None = None, flow: list | None = None, uploader: list | None = None, tag: str | None = None, study: int | None = None, display_errors: bool = False, task_type: TaskType | int | None = None) -&gt; DataFrame\n</code></pre> <p>List all runs matching all of the given filters. (Supports large amount of results)</p>"},{"location":"reference/runs/functions/#openml.runs.functions.list_runs--parameters","title":"Parameters","text":"<p>offset : int, optional     the number of runs to skip, starting from the first size : int, optional     the maximum number of runs to show</p> <p>id : list, optional</p> <p>task : list, optional</p> <p>setup: list, optional</p> <p>flow : list, optional</p> <p>uploader : list, optional</p> <p>tag : str, optional</p> <p>study : int, optional</p> bool, optional (default=None) <p>Whether to list runs which have an error (for example a missing prediction file).</p> <p>task_type : str, optional</p>"},{"location":"reference/runs/functions/#openml.runs.functions.list_runs--returns","title":"Returns","text":"<p>dataframe</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def list_runs(  # noqa: PLR0913\n    offset: int | None = None,\n    size: int | None = None,\n    id: list | None = None,  # noqa: A002\n    task: list[int] | None = None,\n    setup: list | None = None,\n    flow: list | None = None,\n    uploader: list | None = None,\n    tag: str | None = None,\n    study: int | None = None,\n    display_errors: bool = False,  # noqa: FBT001, FBT002\n    task_type: TaskType | int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    List all runs matching all of the given filters.\n    (Supports large amount of results)\n\n    Parameters\n    ----------\n    offset : int, optional\n        the number of runs to skip, starting from the first\n    size : int, optional\n        the maximum number of runs to show\n\n    id : list, optional\n\n    task : list, optional\n\n    setup: list, optional\n\n    flow : list, optional\n\n    uploader : list, optional\n\n    tag : str, optional\n\n    study : int, optional\n\n    display_errors : bool, optional (default=None)\n        Whether to list runs which have an error (for example a missing\n        prediction file).\n\n    task_type : str, optional\n\n    Returns\n    -------\n    dataframe\n    \"\"\"\n    if id is not None and (not isinstance(id, list)):\n        raise TypeError(\"id must be of type list.\")\n    if task is not None and (not isinstance(task, list)):\n        raise TypeError(\"task must be of type list.\")\n    if setup is not None and (not isinstance(setup, list)):\n        raise TypeError(\"setup must be of type list.\")\n    if flow is not None and (not isinstance(flow, list)):\n        raise TypeError(\"flow must be of type list.\")\n    if uploader is not None and (not isinstance(uploader, list)):\n        raise TypeError(\"uploader must be of type list.\")\n\n    listing_call = partial(\n        _list_runs,\n        id=id,\n        task=task,\n        setup=setup,\n        flow=flow,\n        uploader=uploader,\n        tag=tag,\n        study=study,\n        display_errors=display_errors,\n        task_type=task_type,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.run_exists","title":"run_exists","text":"<pre><code>run_exists(task_id: int, setup_id: int) -&gt; set[int]\n</code></pre> <p>Checks whether a task/setup combination is already present on the server.</p>"},{"location":"reference/runs/functions/#openml.runs.functions.run_exists--parameters","title":"Parameters","text":"<p>task_id : int</p> <p>setup_id : int</p>"},{"location":"reference/runs/functions/#openml.runs.functions.run_exists--returns","title":"Returns","text":"<pre><code>Set run ids for runs where flow setup_id was run on task_id. Empty\nset if it wasn't run yet.\n</code></pre> Source code in <code>openml/runs/functions.py</code> <pre><code>def run_exists(task_id: int, setup_id: int) -&gt; set[int]:\n    \"\"\"Checks whether a task/setup combination is already present on the\n    server.\n\n    Parameters\n    ----------\n    task_id : int\n\n    setup_id : int\n\n    Returns\n    -------\n        Set run ids for runs where flow setup_id was run on task_id. Empty\n        set if it wasn't run yet.\n    \"\"\"\n    if setup_id &lt;= 0:\n        # openml setups are in range 1-inf\n        return set()\n\n    try:\n        result = list_runs(task=[task_id], setup=[setup_id])\n        return set() if result.empty else set(result[\"run_id\"])\n    except OpenMLServerException as exception:\n        # error code implies no results. The run does not exist yet\n        if exception.code != ERROR_CODE:\n            raise exception\n        return set()\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.run_flow_on_task","title":"run_flow_on_task","text":"<pre><code>run_flow_on_task(flow: OpenMLFlow, task: OpenMLTask, avoid_duplicate_runs: bool = True, flow_tags: list[str] | None = None, seed: int | None = None, add_local_measures: bool = True, upload_flow: bool = False, n_jobs: int | None = None) -&gt; OpenMLRun\n</code></pre> <p>Run the model provided by the flow on the dataset defined by task.</p> <p>Takes the flow and repeat information into account. The Flow may optionally be published.</p>"},{"location":"reference/runs/functions/#openml.runs.functions.run_flow_on_task--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow     A flow wraps a machine learning model together with relevant information.     The model has a function fit(X,Y) and predict(X),     all supervised estimators of scikit learn follow this definition of a model. task : OpenMLTask     Task to perform. This may be an OpenMLFlow instead if the first argument is an OpenMLTask. avoid_duplicate_runs : bool, optional (default=True)     If True, the run will throw an error if the setup/task combination is already present on     the server. This feature requires an internet connection. flow_tags : List[str], optional (default=None)     A list of tags that the flow should have at creation. seed: int, optional (default=None)     Models that are not seeded will get this seed. add_local_measures : bool, optional (default=True)     Determines whether to calculate a set of evaluation measures locally,     to later verify server behaviour. upload_flow : bool (default=False)     If True, upload the flow to OpenML if it does not exist yet.     If False, do not upload the flow to OpenML. n_jobs : int (default=None)     The number of processes/threads to distribute the evaluation asynchronously.     If <code>None</code> or <code>1</code>, then the evaluation is treated as synchronous and processed sequentially.     If <code>-1</code>, then the job uses as many cores available.</p>"},{"location":"reference/runs/functions/#openml.runs.functions.run_flow_on_task--returns","title":"Returns","text":"<p>run : OpenMLRun     Result of the run.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def run_flow_on_task(  # noqa: C901, PLR0912, PLR0915, PLR0913\n    flow: OpenMLFlow,\n    task: OpenMLTask,\n    avoid_duplicate_runs: bool = True,  # noqa: FBT002, FBT001\n    flow_tags: list[str] | None = None,\n    seed: int | None = None,\n    add_local_measures: bool = True,  # noqa: FBT001, FBT002\n    upload_flow: bool = False,  # noqa: FBT001, FBT002\n    n_jobs: int | None = None,\n) -&gt; OpenMLRun:\n    \"\"\"Run the model provided by the flow on the dataset defined by task.\n\n    Takes the flow and repeat information into account.\n    The Flow may optionally be published.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        A flow wraps a machine learning model together with relevant information.\n        The model has a function fit(X,Y) and predict(X),\n        all supervised estimators of scikit learn follow this definition of a model.\n    task : OpenMLTask\n        Task to perform. This may be an OpenMLFlow instead if the first argument is an OpenMLTask.\n    avoid_duplicate_runs : bool, optional (default=True)\n        If True, the run will throw an error if the setup/task combination is already present on\n        the server. This feature requires an internet connection.\n    flow_tags : List[str], optional (default=None)\n        A list of tags that the flow should have at creation.\n    seed: int, optional (default=None)\n        Models that are not seeded will get this seed.\n    add_local_measures : bool, optional (default=True)\n        Determines whether to calculate a set of evaluation measures locally,\n        to later verify server behaviour.\n    upload_flow : bool (default=False)\n        If True, upload the flow to OpenML if it does not exist yet.\n        If False, do not upload the flow to OpenML.\n    n_jobs : int (default=None)\n        The number of processes/threads to distribute the evaluation asynchronously.\n        If `None` or `1`, then the evaluation is treated as synchronous and processed sequentially.\n        If `-1`, then the job uses as many cores available.\n\n    Returns\n    -------\n    run : OpenMLRun\n        Result of the run.\n    \"\"\"\n    if flow_tags is not None and not isinstance(flow_tags, list):\n        raise ValueError(\"flow_tags should be a list\")\n\n    # TODO: At some point in the future do not allow for arguments in old order (changed 6-2018).\n    # Flexibility currently still allowed due to code-snippet in OpenML100 paper (3-2019).\n    if isinstance(flow, OpenMLTask) and isinstance(task, OpenMLFlow):\n        # We want to allow either order of argument (to avoid confusion).\n        warnings.warn(\n            \"The old argument order (Flow, model) is deprecated and \"\n            \"will not be supported in the future. Please use the \"\n            \"order (model, Flow).\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        task, flow = flow, task\n\n    if task.task_id is None:\n        raise ValueError(\"The task should be published at OpenML\")\n\n    if flow.model is None:\n        flow.model = flow.extension.flow_to_model(flow)\n\n    flow.model = flow.extension.seed_model(flow.model, seed=seed)\n\n    # We only need to sync with the server right now if we want to upload the flow,\n    # or ensure no duplicate runs exist. Otherwise it can be synced at upload time.\n    flow_id = None\n    if upload_flow or avoid_duplicate_runs:\n        flow_id = flow_exists(flow.name, flow.external_version)\n        if isinstance(flow.flow_id, int) and flow_id != flow.flow_id:\n            if flow_id is not False:\n                raise PyOpenMLError(\n                    f\"Local flow_id does not match server flow_id: '{flow.flow_id}' vs '{flow_id}'\",\n                )\n            raise PyOpenMLError(\n                \"Flow does not exist on the server, but 'flow.flow_id' is not None.\"\n            )\n        if upload_flow and flow_id is False:\n            flow.publish()\n            flow_id = flow.flow_id\n        elif flow_id:\n            flow_from_server = get_flow(flow_id)\n            _copy_server_fields(flow_from_server, flow)\n            if avoid_duplicate_runs:\n                flow_from_server.model = flow.model\n                setup_id = setup_exists(flow_from_server)\n                ids = run_exists(task.task_id, setup_id)\n                if ids:\n                    error_message = (\n                        \"One or more runs of this setup were already performed on the task.\"\n                    )\n                    raise OpenMLRunsExistError(ids, error_message)\n        else:\n            # Flow does not exist on server and we do not want to upload it.\n            # No sync with the server happens.\n            flow_id = None\n\n    dataset = task.get_dataset()\n\n    run_environment = flow.extension.get_version_information()\n    tags = [\"openml-python\", run_environment[1]]\n\n    if flow.extension.check_if_model_fitted(flow.model):\n        warnings.warn(\n            \"The model is already fitted! This might cause inconsistency in comparison of results.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n\n    # execute the run\n    res = _run_task_get_arffcontent(\n        model=flow.model,\n        task=task,\n        extension=flow.extension,\n        add_local_measures=add_local_measures,\n        n_jobs=n_jobs,\n    )\n\n    data_content, trace, fold_evaluations, sample_evaluations = res\n    fields = [*run_environment, time.strftime(\"%c\"), \"Created by run_flow_on_task\"]\n    generated_description = \"\\n\".join(fields)\n    run = OpenMLRun(\n        task_id=task.task_id,\n        flow_id=flow_id,\n        dataset_id=dataset.dataset_id,\n        model=flow.model,\n        flow_name=flow.name,\n        tags=tags,\n        trace=trace,\n        data_content=data_content,\n        flow=flow,\n        setup_string=flow.extension.create_setup_string(flow.model),\n        description_text=generated_description,\n    )\n\n    if (upload_flow or avoid_duplicate_runs) and flow.flow_id is not None:\n        # We only extract the parameter settings if a sync happened with the server.\n        # I.e. when the flow was uploaded or we found it in the avoid_duplicate check.\n        # Otherwise, we will do this at upload time.\n        run.parameter_settings = flow.extension.obtain_parameter_values(flow)\n\n    # now we need to attach the detailed evaluations\n    if task.task_type_id == TaskType.LEARNING_CURVE:\n        run.sample_evaluations = sample_evaluations\n    else:\n        run.fold_evaluations = fold_evaluations\n\n    if flow_id:\n        message = f\"Executed Task {task.task_id} with Flow id:{run.flow_id}\"\n    else:\n        message = f\"Executed Task {task.task_id} on local Flow with name {flow.name}.\"\n    config.logger.info(message)\n\n    return run\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.run_model_on_task","title":"run_model_on_task","text":"<pre><code>run_model_on_task(model: Any, task: int | str | OpenMLTask, avoid_duplicate_runs: bool = True, flow_tags: list[str] | None = None, seed: int | None = None, add_local_measures: bool = True, upload_flow: bool = False, return_flow: bool = False, n_jobs: int | None = None) -&gt; OpenMLRun | tuple[OpenMLRun, OpenMLFlow]\n</code></pre> <p>Run the model on the dataset defined by the task.</p>"},{"location":"reference/runs/functions/#openml.runs.functions.run_model_on_task--parameters","title":"Parameters","text":"<p>model : sklearn model     A model which has a function fit(X,Y) and predict(X),     all supervised estimators of scikit learn follow this definition of a model. task : OpenMLTask or int or str     Task to perform or Task id.     This may be a model instead if the first argument is an OpenMLTask. avoid_duplicate_runs : bool, optional (default=True)     If True, the run will throw an error if the setup/task combination is already present on     the server. This feature requires an internet connection. flow_tags : List[str], optional (default=None)     A list of tags that the flow should have at creation. seed: int, optional (default=None)     Models that are not seeded will get this seed. add_local_measures : bool, optional (default=True)     Determines whether to calculate a set of evaluation measures locally,     to later verify server behaviour. upload_flow : bool (default=False)     If True, upload the flow to OpenML if it does not exist yet.     If False, do not upload the flow to OpenML. return_flow : bool (default=False)     If True, returns the OpenMLFlow generated from the model in addition to the OpenMLRun. n_jobs : int (default=None)     The number of processes/threads to distribute the evaluation asynchronously.     If <code>None</code> or <code>1</code>, then the evaluation is treated as synchronous and processed sequentially.     If <code>-1</code>, then the job uses as many cores available.</p>"},{"location":"reference/runs/functions/#openml.runs.functions.run_model_on_task--returns","title":"Returns","text":"<p>run : OpenMLRun     Result of the run. flow : OpenMLFlow (optional, only if <code>return_flow</code> is True).     Flow generated from the model.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def run_model_on_task(  # noqa: PLR0913\n    model: Any,\n    task: int | str | OpenMLTask,\n    avoid_duplicate_runs: bool = True,  # noqa: FBT001, FBT002\n    flow_tags: list[str] | None = None,\n    seed: int | None = None,\n    add_local_measures: bool = True,  # noqa: FBT001, FBT002\n    upload_flow: bool = False,  # noqa: FBT001, FBT002\n    return_flow: bool = False,  # noqa: FBT001, FBT002\n    n_jobs: int | None = None,\n) -&gt; OpenMLRun | tuple[OpenMLRun, OpenMLFlow]:\n    \"\"\"Run the model on the dataset defined by the task.\n\n    Parameters\n    ----------\n    model : sklearn model\n        A model which has a function fit(X,Y) and predict(X),\n        all supervised estimators of scikit learn follow this definition of a model.\n    task : OpenMLTask or int or str\n        Task to perform or Task id.\n        This may be a model instead if the first argument is an OpenMLTask.\n    avoid_duplicate_runs : bool, optional (default=True)\n        If True, the run will throw an error if the setup/task combination is already present on\n        the server. This feature requires an internet connection.\n    flow_tags : List[str], optional (default=None)\n        A list of tags that the flow should have at creation.\n    seed: int, optional (default=None)\n        Models that are not seeded will get this seed.\n    add_local_measures : bool, optional (default=True)\n        Determines whether to calculate a set of evaluation measures locally,\n        to later verify server behaviour.\n    upload_flow : bool (default=False)\n        If True, upload the flow to OpenML if it does not exist yet.\n        If False, do not upload the flow to OpenML.\n    return_flow : bool (default=False)\n        If True, returns the OpenMLFlow generated from the model in addition to the OpenMLRun.\n    n_jobs : int (default=None)\n        The number of processes/threads to distribute the evaluation asynchronously.\n        If `None` or `1`, then the evaluation is treated as synchronous and processed sequentially.\n        If `-1`, then the job uses as many cores available.\n\n    Returns\n    -------\n    run : OpenMLRun\n        Result of the run.\n    flow : OpenMLFlow (optional, only if `return_flow` is True).\n        Flow generated from the model.\n    \"\"\"\n    if avoid_duplicate_runs and not config.apikey:\n        warnings.warn(\n            \"avoid_duplicate_runs is set to True, but no API key is set. \"\n            \"Please set your API key in the OpenML configuration file, see\"\n            \"https://openml.github.io/openml-python/main/examples/20_basic/introduction_tutorial\"\n            \".html#authentication for more information on authentication.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n\n    # TODO: At some point in the future do not allow for arguments in old order (6-2018).\n    # Flexibility currently still allowed due to code-snippet in OpenML100 paper (3-2019).\n    # When removing this please also remove the method `is_estimator` from the extension\n    # interface as it is only used here (MF, 3-2019)\n    if isinstance(model, (int, str, OpenMLTask)):\n        warnings.warn(\n            \"The old argument order (task, model) is deprecated and \"\n            \"will not be supported in the future. Please use the \"\n            \"order (model, task).\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        task, model = model, task\n\n    extension = get_extension_by_model(model, raise_if_no_extension=True)\n    if extension is None:\n        # This should never happen and is only here to please mypy will be gone soon once the\n        # whole function is removed\n        raise TypeError(extension)\n\n    flow = extension.model_to_flow(model)\n\n    def get_task_and_type_conversion(_task: int | str | OpenMLTask) -&gt; OpenMLTask:\n        \"\"\"Retrieve an OpenMLTask object from either an integer or string ID,\n        or directly from an OpenMLTask object.\n\n        Parameters\n        ----------\n        _task : Union[int, str, OpenMLTask]\n            The task ID or the OpenMLTask object.\n\n        Returns\n        -------\n        OpenMLTask\n            The OpenMLTask object.\n        \"\"\"\n        if isinstance(_task, (int, str)):\n            return get_task(int(_task))  # type: ignore\n\n        return _task\n\n    task = get_task_and_type_conversion(task)\n\n    run = run_flow_on_task(\n        task=task,\n        flow=flow,\n        avoid_duplicate_runs=avoid_duplicate_runs,\n        flow_tags=flow_tags,\n        seed=seed,\n        add_local_measures=add_local_measures,\n        upload_flow=upload_flow,\n        n_jobs=n_jobs,\n    )\n    if return_flow:\n        return run, flow\n    return run\n</code></pre>"},{"location":"reference/runs/run/","title":"run","text":""},{"location":"reference/runs/run/#openml.runs.run","title":"openml.runs.run","text":""},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun","title":"OpenMLRun","text":"<pre><code>OpenMLRun(task_id: int, flow_id: int | None, dataset_id: int | None, setup_string: str | None = None, output_files: dict[str, int] | None = None, setup_id: int | None = None, tags: list[str] | None = None, uploader: int | None = None, uploader_name: str | None = None, evaluations: dict | None = None, fold_evaluations: dict | None = None, sample_evaluations: dict | None = None, data_content: list[list] | None = None, trace: OpenMLRunTrace | None = None, model: object | None = None, task_type: str | None = None, task_evaluation_measure: str | None = None, flow_name: str | None = None, parameter_settings: list[dict[str, Any]] | None = None, predictions_url: str | None = None, task: OpenMLTask | None = None, flow: OpenMLFlow | None = None, run_id: int | None = None, description_text: str | None = None, run_details: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Run: result of running a model on an OpenML dataset.</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun--parameters","title":"Parameters","text":"<p>task_id: int     The ID of the OpenML task associated with the run. flow_id: int     The ID of the OpenML flow associated with the run. dataset_id: int     The ID of the OpenML dataset used for the run. setup_string: str     The setup string of the run. output_files: Dict[str, int]     Specifies where each related file can be found. setup_id: int     An integer representing the ID of the setup used for the run. tags: List[str]     Representing the tags associated with the run. uploader: int     User ID of the uploader. uploader_name: str     The name of the person who uploaded the run. evaluations: Dict     Representing the evaluations of the run. fold_evaluations: Dict     The evaluations of the run for each fold. sample_evaluations: Dict     The evaluations of the run for each sample. data_content: List[List]     The predictions generated from executing this run. trace: OpenMLRunTrace     The trace containing information on internal model evaluations of this run. model: object     The untrained model that was evaluated in the run. task_type: str     The type of the OpenML task associated with the run. task_evaluation_measure: str     The evaluation measure used for the task. flow_name: str     The name of the OpenML flow associated with the run. parameter_settings: list[OrderedDict]     Representing the parameter settings used for the run. predictions_url: str     The URL of the predictions file. task: OpenMLTask     An instance of the OpenMLTask class, representing the OpenML task associated     with the run. flow: OpenMLFlow     An instance of the OpenMLFlow class, representing the OpenML flow associated     with the run. run_id: int     The ID of the run. description_text: str, optional     Description text to add to the predictions file. If left None, is set to the     time the arff file is generated. run_details: str, optional (default=None)     Description of the run stored in the run meta-data.</p> Source code in <code>openml/runs/run.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_id: int,\n    flow_id: int | None,\n    dataset_id: int | None,\n    setup_string: str | None = None,\n    output_files: dict[str, int] | None = None,\n    setup_id: int | None = None,\n    tags: list[str] | None = None,\n    uploader: int | None = None,\n    uploader_name: str | None = None,\n    evaluations: dict | None = None,\n    fold_evaluations: dict | None = None,\n    sample_evaluations: dict | None = None,\n    data_content: list[list] | None = None,\n    trace: OpenMLRunTrace | None = None,\n    model: object | None = None,\n    task_type: str | None = None,\n    task_evaluation_measure: str | None = None,\n    flow_name: str | None = None,\n    parameter_settings: list[dict[str, Any]] | None = None,\n    predictions_url: str | None = None,\n    task: OpenMLTask | None = None,\n    flow: OpenMLFlow | None = None,\n    run_id: int | None = None,\n    description_text: str | None = None,\n    run_details: str | None = None,\n):\n    self.uploader = uploader\n    self.uploader_name = uploader_name\n    self.task_id = task_id\n    self.task_type = task_type\n    self.task_evaluation_measure = task_evaluation_measure\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.setup_id = setup_id\n    self.setup_string = setup_string\n    self.parameter_settings = parameter_settings\n    self.dataset_id = dataset_id\n    self.evaluations = evaluations\n    self.fold_evaluations = fold_evaluations\n    self.sample_evaluations = sample_evaluations\n    self.data_content = data_content\n    self.output_files = output_files\n    self.trace = trace\n    self.error_message = None\n    self.task = task\n    self.flow = flow\n    self.run_id = run_id\n    self.model = model\n    self.tags = tags\n    self.predictions_url = predictions_url\n    self.description_text = description_text\n    self.run_details = run_details\n    self._predictions = None\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The ID of the run, None if not uploaded to the server yet.</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.predictions","title":"predictions  <code>property</code>","text":"<pre><code>predictions: DataFrame\n</code></pre> <p>Return a DataFrame with predictions for this run</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.from_filesystem","title":"from_filesystem  <code>classmethod</code>","text":"<pre><code>from_filesystem(directory: str | Path, expect_model: bool = True) -&gt; OpenMLRun\n</code></pre> <p>The inverse of the to_filesystem method. Instantiates an OpenMLRun object based on files stored on the file system.</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.from_filesystem--parameters","title":"Parameters","text":"<p>directory : str     a path leading to the folder where the results     are stored</p> bool <p>if True, it requires the model pickle to be present, and an error will be thrown if not. Otherwise, the model might or might not be present.</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.from_filesystem--returns","title":"Returns","text":"<p>run : OpenMLRun     the re-instantiated run object</p> Source code in <code>openml/runs/run.py</code> <pre><code>@classmethod\ndef from_filesystem(cls, directory: str | Path, expect_model: bool = True) -&gt; OpenMLRun:  # noqa: FBT001, FBT002\n    \"\"\"\n    The inverse of the to_filesystem method. Instantiates an OpenMLRun\n    object based on files stored on the file system.\n\n    Parameters\n    ----------\n    directory : str\n        a path leading to the folder where the results\n        are stored\n\n    expect_model : bool\n        if True, it requires the model pickle to be present, and an error\n        will be thrown if not. Otherwise, the model might or might not\n        be present.\n\n    Returns\n    -------\n    run : OpenMLRun\n        the re-instantiated run object\n    \"\"\"\n    # Avoiding cyclic imports\n    import openml.runs.functions\n\n    directory = Path(directory)\n    if not directory.is_dir():\n        raise ValueError(\"Could not find folder\")\n\n    description_path = directory / \"description.xml\"\n    predictions_path = directory / \"predictions.arff\"\n    trace_path = directory / \"trace.arff\"\n    model_path = directory / \"model.pkl\"\n\n    if not description_path.is_file():\n        raise ValueError(\"Could not find description.xml\")\n    if not predictions_path.is_file():\n        raise ValueError(\"Could not find predictions.arff\")\n    if (not model_path.is_file()) and expect_model:\n        raise ValueError(\"Could not find model.pkl\")\n\n    with description_path.open() as fht:\n        xml_string = fht.read()\n    run = openml.runs.functions._create_run_from_xml(xml_string, from_server=False)\n\n    if run.flow_id is None:\n        flow = openml.flows.OpenMLFlow.from_filesystem(directory)\n        run.flow = flow\n        run.flow_name = flow.name\n\n    with predictions_path.open() as fht:\n        predictions = arff.load(fht)\n        run.data_content = predictions[\"data\"]\n\n    if model_path.is_file():\n        # note that it will load the model if the file exists, even if\n        # expect_model is False\n        with model_path.open(\"rb\") as fhb:\n            run.model = pickle.load(fhb)  # noqa: S301\n\n    if trace_path.is_file():\n        run.trace = openml.runs.OpenMLRunTrace._from_filesystem(trace_path)\n\n    return run\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.get_metric_fn","title":"get_metric_fn","text":"<pre><code>get_metric_fn(sklearn_fn: Callable, kwargs: dict | None = None) -&gt; ndarray\n</code></pre> <p>Calculates metric scores based on predicted values. Assumes the run has been executed locally (and contains run_data). Furthermore, it assumes that the 'correct' or 'truth' attribute is specified in the arff (which is an optional field, but always the case for openml-python runs)</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.get_metric_fn--parameters","title":"Parameters","text":"<p>sklearn_fn : function     a function pointer to a sklearn function that     accepts <code>y_true</code>, <code>y_pred</code> and <code>**kwargs</code> kwargs : dict     kwargs for the function</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.get_metric_fn--returns","title":"Returns","text":"<p>scores : ndarray of scores of length num_folds * num_repeats     metric results</p> Source code in <code>openml/runs/run.py</code> <pre><code>def get_metric_fn(self, sklearn_fn: Callable, kwargs: dict | None = None) -&gt; np.ndarray:  # noqa: PLR0915, PLR0912, C901\n    \"\"\"Calculates metric scores based on predicted values. Assumes the\n    run has been executed locally (and contains run_data). Furthermore,\n    it assumes that the 'correct' or 'truth' attribute is specified in\n    the arff (which is an optional field, but always the case for\n    openml-python runs)\n\n    Parameters\n    ----------\n    sklearn_fn : function\n        a function pointer to a sklearn function that\n        accepts ``y_true``, ``y_pred`` and ``**kwargs``\n    kwargs : dict\n        kwargs for the function\n\n    Returns\n    -------\n    scores : ndarray of scores of length num_folds * num_repeats\n        metric results\n    \"\"\"\n    kwargs = kwargs if kwargs else {}\n    if self.data_content is not None and self.task_id is not None:\n        predictions_arff = self._generate_arff_dict()\n    elif (self.output_files is not None) and (\"predictions\" in self.output_files):\n        predictions_file_url = openml._api_calls._file_id_to_url(\n            self.output_files[\"predictions\"],\n            \"predictions.arff\",\n        )\n        response = openml._api_calls._download_text_file(predictions_file_url)\n        predictions_arff = arff.loads(response)\n        # TODO: make this a stream reader\n    else:\n        raise ValueError(\n            \"Run should have been locally executed or \" \"contain outputfile reference.\",\n        )\n\n    # Need to know more about the task to compute scores correctly\n    task = get_task(self.task_id)\n\n    attribute_names = [att[0] for att in predictions_arff[\"attributes\"]]\n    if (\n        task.task_type_id in [TaskType.SUPERVISED_CLASSIFICATION, TaskType.LEARNING_CURVE]\n        and \"correct\" not in attribute_names\n    ):\n        raise ValueError('Attribute \"correct\" should be set for ' \"classification task runs\")\n    if task.task_type_id == TaskType.SUPERVISED_REGRESSION and \"truth\" not in attribute_names:\n        raise ValueError('Attribute \"truth\" should be set for ' \"regression task runs\")\n    if task.task_type_id != TaskType.CLUSTERING and \"prediction\" not in attribute_names:\n        raise ValueError('Attribute \"predict\" should be set for ' \"supervised task runs\")\n\n    def _attribute_list_to_dict(attribute_list):  # type: ignore\n        # convenience function: Creates a mapping to map from the name of\n        # attributes present in the arff prediction file to their index.\n        # This is necessary because the number of classes can be different\n        # for different tasks.\n        res = OrderedDict()\n        for idx in range(len(attribute_list)):\n            res[attribute_list[idx][0]] = idx\n        return res\n\n    attribute_dict = _attribute_list_to_dict(predictions_arff[\"attributes\"])\n\n    repeat_idx = attribute_dict[\"repeat\"]\n    fold_idx = attribute_dict[\"fold\"]\n    predicted_idx = attribute_dict[\"prediction\"]  # Assume supervised task\n\n    if task.task_type_id in (TaskType.SUPERVISED_CLASSIFICATION, TaskType.LEARNING_CURVE):\n        correct_idx = attribute_dict[\"correct\"]\n    elif task.task_type_id == TaskType.SUPERVISED_REGRESSION:\n        correct_idx = attribute_dict[\"truth\"]\n    has_samples = False\n    if \"sample\" in attribute_dict:\n        sample_idx = attribute_dict[\"sample\"]\n        has_samples = True\n\n    if (\n        predictions_arff[\"attributes\"][predicted_idx][1]\n        != predictions_arff[\"attributes\"][correct_idx][1]\n    ):\n        pred = predictions_arff[\"attributes\"][predicted_idx][1]\n        corr = predictions_arff[\"attributes\"][correct_idx][1]\n        raise ValueError(\n            \"Predicted and Correct do not have equal values:\" f\" {pred!s} Vs. {corr!s}\",\n        )\n\n    # TODO: these could be cached\n    values_predict: dict[int, dict[int, dict[int, list[float]]]] = {}\n    values_correct: dict[int, dict[int, dict[int, list[float]]]] = {}\n    for _line_idx, line in enumerate(predictions_arff[\"data\"]):\n        rep = line[repeat_idx]\n        fold = line[fold_idx]\n        samp = line[sample_idx] if has_samples else 0\n\n        if task.task_type_id in [\n            TaskType.SUPERVISED_CLASSIFICATION,\n            TaskType.LEARNING_CURVE,\n        ]:\n            prediction = predictions_arff[\"attributes\"][predicted_idx][1].index(\n                line[predicted_idx],\n            )\n            correct = predictions_arff[\"attributes\"][predicted_idx][1].index(line[correct_idx])\n        elif task.task_type_id == TaskType.SUPERVISED_REGRESSION:\n            prediction = line[predicted_idx]\n            correct = line[correct_idx]\n        if rep not in values_predict:\n            values_predict[rep] = OrderedDict()\n            values_correct[rep] = OrderedDict()\n        if fold not in values_predict[rep]:\n            values_predict[rep][fold] = OrderedDict()\n            values_correct[rep][fold] = OrderedDict()\n        if samp not in values_predict[rep][fold]:\n            values_predict[rep][fold][samp] = []\n            values_correct[rep][fold][samp] = []\n\n        values_predict[rep][fold][samp].append(prediction)\n        values_correct[rep][fold][samp].append(correct)\n\n    scores = []\n    for rep in values_predict:\n        for fold in values_predict[rep]:\n            last_sample = len(values_predict[rep][fold]) - 1\n            y_pred = values_predict[rep][fold][last_sample]\n            y_true = values_correct[rep][fold][last_sample]\n            scores.append(sklearn_fn(y_true, y_pred, **kwargs))\n    return np.array(scores)\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.to_filesystem","title":"to_filesystem","text":"<pre><code>to_filesystem(directory: str | Path, store_model: bool = True) -&gt; None\n</code></pre> <p>The inverse of the from_filesystem method. Serializes a run on the filesystem, to be uploaded later.</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.to_filesystem--parameters","title":"Parameters","text":"<p>directory : str     a path leading to the folder where the results     will be stored. Should be empty</p> bool, optional (default=True) <p>if True, a model will be pickled as well. As this is the most storage expensive part, it is often desirable to not store the model.</p> Source code in <code>openml/runs/run.py</code> <pre><code>def to_filesystem(\n    self,\n    directory: str | Path,\n    store_model: bool = True,  # noqa: FBT001, FBT002\n) -&gt; None:\n    \"\"\"\n    The inverse of the from_filesystem method. Serializes a run\n    on the filesystem, to be uploaded later.\n\n    Parameters\n    ----------\n    directory : str\n        a path leading to the folder where the results\n        will be stored. Should be empty\n\n    store_model : bool, optional (default=True)\n        if True, a model will be pickled as well. As this is the most\n        storage expensive part, it is often desirable to not store the\n        model.\n    \"\"\"\n    if self.data_content is None or self.model is None:\n        raise ValueError(\"Run should have been executed (and contain \" \"model / predictions)\")\n    directory = Path(directory)\n    directory.mkdir(exist_ok=True, parents=True)\n\n    if any(directory.iterdir()):\n        raise ValueError(f\"Output directory {directory.expanduser().resolve()} should be empty\")\n\n    run_xml = self._to_xml()\n    predictions_arff = arff.dumps(self._generate_arff_dict())\n\n    # It seems like typing does not allow to define the same variable multiple times\n    with (directory / \"description.xml\").open(\"w\") as fh:\n        fh.write(run_xml)\n    with (directory / \"predictions.arff\").open(\"w\") as fh:\n        fh.write(predictions_arff)\n    if store_model:\n        with (directory / \"model.pkl\").open(\"wb\") as fh_b:\n            pickle.dump(self.model, fh_b)\n\n    if self.flow_id is None and self.flow is not None:\n        self.flow.to_filesystem(directory)\n\n    if self.trace is not None:\n        self.trace._to_filesystem(directory)\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/runs/trace/","title":"trace","text":""},{"location":"reference/runs/trace/#openml.runs.trace","title":"openml.runs.trace","text":""},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace","title":"OpenMLRunTrace","text":"<pre><code>OpenMLRunTrace(run_id: int | None, trace_iterations: dict[tuple[int, int, int], OpenMLTraceIteration])\n</code></pre> <p>OpenML Run Trace: parsed output from Run Trace call</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace--parameters","title":"Parameters","text":"<p>run_id : int     OpenML run id.</p> dict <p>Mapping from key <code>(repeat, fold, iteration)</code> to an object of OpenMLTraceIteration.</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace--parameters","title":"Parameters","text":"<p>run_id : int     Id for which the trace content is to be stored. trace_iterations : List[List]     The trace content obtained by running a flow on a task.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>def __init__(\n    self,\n    run_id: int | None,\n    trace_iterations: dict[tuple[int, int, int], OpenMLTraceIteration],\n):\n    \"\"\"Object to hold the trace content of a run.\n\n    Parameters\n    ----------\n    run_id : int\n        Id for which the trace content is to be stored.\n    trace_iterations : List[List]\n        The trace content obtained by running a flow on a task.\n    \"\"\"\n    self.run_id = run_id\n    self.trace_iterations = trace_iterations\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.generate","title":"generate  <code>classmethod</code>","text":"<pre><code>generate(attributes: list[tuple[str, str]], content: list[list[int | float | str]]) -&gt; OpenMLRunTrace\n</code></pre> <p>Generates an OpenMLRunTrace.</p> <p>Generates the trace object from the attributes and content extracted while running the underlying flow.</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.generate--parameters","title":"Parameters","text":"<p>attributes : list     List of tuples describing the arff attributes.</p> list <p>List of lists containing information about the individual tuning runs.</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.generate--returns","title":"Returns","text":"<p>OpenMLRunTrace</p> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef generate(\n    cls,\n    attributes: list[tuple[str, str]],\n    content: list[list[int | float | str]],\n) -&gt; OpenMLRunTrace:\n    \"\"\"Generates an OpenMLRunTrace.\n\n    Generates the trace object from the attributes and content extracted\n    while running the underlying flow.\n\n    Parameters\n    ----------\n    attributes : list\n        List of tuples describing the arff attributes.\n\n    content : list\n        List of lists containing information about the individual tuning\n        runs.\n\n    Returns\n    -------\n    OpenMLRunTrace\n    \"\"\"\n    if content is None:\n        raise ValueError(\"Trace content not available.\")\n    if attributes is None:\n        raise ValueError(\"Trace attributes not available.\")\n    if len(content) == 0:\n        raise ValueError(\"Trace content is empty.\")\n    if len(attributes) != len(content[0]):\n        raise ValueError(\n            \"Trace_attributes and trace_content not compatible:\"\n            f\" {attributes} vs {content[0]}\",\n        )\n\n    return cls._trace_from_arff_struct(\n        attributes=attributes,\n        content=content,\n        error_message=\"setup_string not allowed when constructing a \"\n        \"trace object from run results.\",\n    )\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.get_selected_iteration","title":"get_selected_iteration","text":"<pre><code>get_selected_iteration(fold: int, repeat: int) -&gt; int\n</code></pre> <p>Returns the trace iteration that was marked as selected. In case multiple are marked as selected (should not happen) the first of these is returned</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.get_selected_iteration--parameters","title":"Parameters","text":"<p>fold: int</p> <p>repeat: int</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.get_selected_iteration--returns","title":"Returns","text":"<p>int     The trace iteration from the given fold and repeat that was     selected as the best iteration by the search procedure</p> Source code in <code>openml/runs/trace.py</code> <pre><code>def get_selected_iteration(self, fold: int, repeat: int) -&gt; int:\n    \"\"\"\n    Returns the trace iteration that was marked as selected. In\n    case multiple are marked as selected (should not happen) the\n    first of these is returned\n\n    Parameters\n    ----------\n    fold: int\n\n    repeat: int\n\n    Returns\n    -------\n    int\n        The trace iteration from the given fold and repeat that was\n        selected as the best iteration by the search procedure\n    \"\"\"\n    for r, f, i in self.trace_iterations:\n        if r == repeat and f == fold and self.trace_iterations[(r, f, i)].selected is True:\n            return i\n    raise ValueError(\n        \"Could not find the selected iteration for rep/fold %d/%d\" % (repeat, fold),\n    )\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.merge_traces","title":"merge_traces  <code>classmethod</code>","text":"<pre><code>merge_traces(traces: list[OpenMLRunTrace]) -&gt; OpenMLRunTrace\n</code></pre> <p>Merge multiple traces into a single trace.</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.merge_traces--parameters","title":"Parameters","text":"<p>cls : type     Type of the trace object to be created. traces : List[OpenMLRunTrace]     List of traces to merge.</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.merge_traces--returns","title":"Returns","text":"<p>OpenMLRunTrace     A trace object representing the merged traces.</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.merge_traces--raises","title":"Raises","text":"<p>ValueError     If the parameters in the iterations of the traces being merged are not equal.     If a key (repeat, fold, iteration) is encountered twice while merging the traces.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef merge_traces(cls, traces: list[OpenMLRunTrace]) -&gt; OpenMLRunTrace:\n    \"\"\"Merge multiple traces into a single trace.\n\n    Parameters\n    ----------\n    cls : type\n        Type of the trace object to be created.\n    traces : List[OpenMLRunTrace]\n        List of traces to merge.\n\n    Returns\n    -------\n    OpenMLRunTrace\n        A trace object representing the merged traces.\n\n    Raises\n    ------\n    ValueError\n        If the parameters in the iterations of the traces being merged are not equal.\n        If a key (repeat, fold, iteration) is encountered twice while merging the traces.\n    \"\"\"\n    merged_trace: dict[tuple[int, int, int], OpenMLTraceIteration] = {}\n\n    previous_iteration = None\n    for trace in traces:\n        for iteration in trace:\n            key = (iteration.repeat, iteration.fold, iteration.iteration)\n\n            assert iteration.parameters is not None\n            param_keys = iteration.parameters.keys()\n\n            if previous_iteration is not None:\n                trace_itr = merged_trace[previous_iteration]\n\n                assert trace_itr.parameters is not None\n                trace_itr_keys = trace_itr.parameters.keys()\n\n                if list(param_keys) != list(trace_itr_keys):\n                    raise ValueError(\n                        \"Cannot merge traces because the parameters are not equal: \"\n                        f\"{list(trace_itr.parameters.keys())} vs \"\n                        f\"{list(iteration.parameters.keys())}\",\n                    )\n\n            if key in merged_trace:\n                raise ValueError(\n                    f\"Cannot merge traces because key '{key}' was encountered twice\",\n                )\n\n            merged_trace[key] = iteration\n            previous_iteration = key\n\n    return cls(None, merged_trace)\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.trace_from_arff","title":"trace_from_arff  <code>classmethod</code>","text":"<pre><code>trace_from_arff(arff_obj: dict[str, Any]) -&gt; OpenMLRunTrace\n</code></pre> <p>Generate trace from arff trace.</p> <p>Creates a trace file from arff object (for example, generated by a local run).</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.trace_from_arff--parameters","title":"Parameters","text":"<p>arff_obj : dict     LIAC arff obj, dict containing attributes, relation, data.</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.trace_from_arff--returns","title":"Returns","text":"<p>OpenMLRunTrace</p> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef trace_from_arff(cls, arff_obj: dict[str, Any]) -&gt; OpenMLRunTrace:\n    \"\"\"Generate trace from arff trace.\n\n    Creates a trace file from arff object (for example, generated by a\n    local run).\n\n    Parameters\n    ----------\n    arff_obj : dict\n        LIAC arff obj, dict containing attributes, relation, data.\n\n    Returns\n    -------\n    OpenMLRunTrace\n    \"\"\"\n    attributes = arff_obj[\"attributes\"]\n    content = arff_obj[\"data\"]\n    return cls._trace_from_arff_struct(\n        attributes=attributes,\n        content=content,\n        error_message=\"setup_string not supported for arff serialization\",\n    )\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.trace_from_xml","title":"trace_from_xml  <code>classmethod</code>","text":"<pre><code>trace_from_xml(xml: str | Path | IO) -&gt; OpenMLRunTrace\n</code></pre> <p>Generate trace from xml.</p> <p>Creates a trace file from the xml description.</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.trace_from_xml--parameters","title":"Parameters","text":"<p>xml : string | file-like object     An xml description that can be either a <code>string</code> or a file-like     object.</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.trace_from_xml--returns","title":"Returns","text":"<p>run : OpenMLRunTrace     Object containing the run id and a dict containing the trace     iterations.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef trace_from_xml(cls, xml: str | Path | IO) -&gt; OpenMLRunTrace:\n    \"\"\"Generate trace from xml.\n\n    Creates a trace file from the xml description.\n\n    Parameters\n    ----------\n    xml : string | file-like object\n        An xml description that can be either a `string` or a file-like\n        object.\n\n    Returns\n    -------\n    run : OpenMLRunTrace\n        Object containing the run id and a dict containing the trace\n        iterations.\n    \"\"\"\n    if isinstance(xml, Path):\n        xml = str(xml.absolute())\n\n    result_dict = xmltodict.parse(xml, force_list=(\"oml:trace_iteration\",))[\"oml:trace\"]\n\n    run_id = result_dict[\"oml:run_id\"]\n    trace = OrderedDict()\n\n    if \"oml:trace_iteration\" not in result_dict:\n        raise ValueError(\"Run does not contain valid trace. \")\n    if not isinstance(result_dict[\"oml:trace_iteration\"], list):\n        raise TypeError(type(result_dict[\"oml:trace_iteration\"]))\n\n    for itt in result_dict[\"oml:trace_iteration\"]:\n        repeat = int(itt[\"oml:repeat\"])\n        fold = int(itt[\"oml:fold\"])\n        iteration = int(itt[\"oml:iteration\"])\n        setup_string = json.loads(itt[\"oml:setup_string\"])\n        evaluation = float(itt[\"oml:evaluation\"])\n        selected_value = itt[\"oml:selected\"]\n        if selected_value == \"true\":\n            selected = True\n        elif selected_value == \"false\":\n            selected = False\n        else:\n            raise ValueError(\n                'expected {\"true\", \"false\"} value for '\n                f\"selected field, received: {selected_value}\",\n            )\n\n        current = OpenMLTraceIteration(\n            repeat=repeat,\n            fold=fold,\n            iteration=iteration,\n            setup_string=setup_string,\n            evaluation=evaluation,\n            selected=selected,\n        )\n        trace[(repeat, fold, iteration)] = current\n\n    return cls(run_id, trace)\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.trace_to_arff","title":"trace_to_arff","text":"<pre><code>trace_to_arff() -&gt; dict[str, Any]\n</code></pre> <p>Generate the arff dictionary for uploading predictions to the server.</p> <p>Uses the trace object to generate an arff dictionary representation.</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.trace_to_arff--returns","title":"Returns","text":"<p>arff_dict : dict     Dictionary representation of the ARFF file that will be uploaded.     Contains information about the optimization trace.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>def trace_to_arff(self) -&gt; dict[str, Any]:\n    \"\"\"Generate the arff dictionary for uploading predictions to the server.\n\n    Uses the trace object to generate an arff dictionary representation.\n\n    Returns\n    -------\n    arff_dict : dict\n        Dictionary representation of the ARFF file that will be uploaded.\n        Contains information about the optimization trace.\n    \"\"\"\n    if self.trace_iterations is None:\n        raise ValueError(\"trace_iterations missing from the trace object\")\n\n    # attributes that will be in trace arff\n    trace_attributes = [\n        (\"repeat\", \"NUMERIC\"),\n        (\"fold\", \"NUMERIC\"),\n        (\"iteration\", \"NUMERIC\"),\n        (\"evaluation\", \"NUMERIC\"),\n        (\"selected\", [\"true\", \"false\"]),\n    ]\n    trace_attributes.extend(\n        [\n            (PREFIX + parameter, \"STRING\")\n            for parameter in next(iter(self.trace_iterations.values())).get_parameters()\n        ],\n    )\n\n    arff_dict: dict[str, Any] = {}\n    data = []\n    for trace_iteration in self.trace_iterations.values():\n        tmp_list = []\n        for _attr, _ in trace_attributes:\n            if _attr.startswith(PREFIX):\n                attr = _attr[len(PREFIX) :]\n                value = trace_iteration.get_parameters()[attr]\n            else:\n                attr = _attr\n                value = getattr(trace_iteration, attr)\n\n            if attr == \"selected\":\n                tmp_list.append(\"true\" if value else \"false\")\n            else:\n                tmp_list.append(value)\n        data.append(tmp_list)\n\n    arff_dict[\"attributes\"] = trace_attributes\n    arff_dict[\"data\"] = data\n    # TODO allow to pass a trace description when running a flow\n    arff_dict[\"relation\"] = \"Trace\"\n    return arff_dict\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLTraceIteration","title":"OpenMLTraceIteration  <code>dataclass</code>","text":"<pre><code>OpenMLTraceIteration(repeat: int, fold: int, iteration: int, evaluation: float, selected: bool, setup_string: dict[str, str] | None = None, parameters: dict[str, str | int | float] | None = None)\n</code></pre> <p>OpenML Trace Iteration: parsed output from Run Trace call Exactly one of <code>setup_string</code> or <code>parameters</code> must be provided.</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLTraceIteration--parameters","title":"Parameters","text":"<p>repeat : int     repeat number (in case of no repeats: 0)</p> int <p>fold number (in case of no folds: 0)</p> int <p>iteration number of optimization procedure</p> str, optional <p>json string representing the parameters If not provided, <code>parameters</code> should be set.</p> double <p>The evaluation that was awarded to this trace iteration. Measure is defined by the task</p> bool <p>Whether this was the best of all iterations, and hence selected for making predictions. Per fold/repeat there should be only one iteration selected</p> OrderedDict, optional <p>Dictionary specifying parameter names and their values. If not provided, <code>setup_string</code> should be set.</p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLTraceIteration.get_parameters","title":"get_parameters","text":"<pre><code>get_parameters() -&gt; dict[str, Any]\n</code></pre> <p>Get the parameters of this trace iteration.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>def get_parameters(self) -&gt; dict[str, Any]:\n    \"\"\"Get the parameters of this trace iteration.\"\"\"\n    # parameters have prefix 'parameter_'\n    if self.setup_string:\n        return {\n            param[len(PREFIX) :]: json.loads(value)\n            for param, value in self.setup_string.items()\n        }\n\n    assert self.parameters is not None\n    return {param[len(PREFIX) :]: value for param, value in self.parameters.items()}\n</code></pre>"},{"location":"reference/setups/","title":"setups","text":""},{"location":"reference/setups/#openml.setups","title":"openml.setups","text":""},{"location":"reference/setups/#openml.setups.OpenMLParameter","title":"OpenMLParameter","text":"<pre><code>OpenMLParameter(input_id: int, flow_id: int, flow_name: str, full_name: str, parameter_name: str, data_type: str, default_value: str, value: str)\n</code></pre> <p>Parameter object (used in setup).</p>"},{"location":"reference/setups/#openml.setups.OpenMLParameter--parameters","title":"Parameters","text":"<p>input_id : int     The input id from the openml database flow id : int     The flow to which this parameter is associated flow name : str     The name of the flow (no version number) to which this parameter     is associated full_name : str     The name of the flow and parameter combined parameter_name : str     The name of the parameter data_type : str     The datatype of the parameter. generally unused for sklearn flows default_value : str     The default value. For sklearn parameters, this is unknown and a     default value is selected arbitrarily value : str     If the parameter was set, the value that it was set to.</p> Source code in <code>openml/setups/setup.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    input_id: int,\n    flow_id: int,\n    flow_name: str,\n    full_name: str,\n    parameter_name: str,\n    data_type: str,\n    default_value: str,\n    value: str,\n):\n    self.id = input_id\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.full_name = full_name\n    self.parameter_name = parameter_name\n    self.data_type = data_type\n    self.default_value = default_value\n    self.value = value\n</code></pre>"},{"location":"reference/setups/#openml.setups.OpenMLSetup","title":"OpenMLSetup","text":"<pre><code>OpenMLSetup(setup_id: int, flow_id: int, parameters: dict[int, Any] | None)\n</code></pre> <p>Setup object (a.k.a. Configuration).</p>"},{"location":"reference/setups/#openml.setups.OpenMLSetup--parameters","title":"Parameters","text":"<p>setup_id : int     The OpenML setup id flow_id : int     The flow that it is build upon parameters : dict     The setting of the parameters</p> Source code in <code>openml/setups/setup.py</code> <pre><code>def __init__(self, setup_id: int, flow_id: int, parameters: dict[int, Any] | None):\n    if not isinstance(setup_id, int):\n        raise ValueError(\"setup id should be int\")\n\n    if not isinstance(flow_id, int):\n        raise ValueError(\"flow id should be int\")\n\n    if parameters is not None and not isinstance(parameters, dict):\n        raise ValueError(\"parameters should be dict\")\n\n    self.setup_id = setup_id\n    self.flow_id = flow_id\n    self.parameters = parameters\n</code></pre>"},{"location":"reference/setups/#openml.setups.get_setup","title":"get_setup","text":"<pre><code>get_setup(setup_id: int) -&gt; OpenMLSetup\n</code></pre> <p>Downloads the setup (configuration) description from OpenML  and returns a structured object</p>"},{"location":"reference/setups/#openml.setups.get_setup--parameters","title":"Parameters","text":"<p>setup_id : int     The Openml setup_id</p>"},{"location":"reference/setups/#openml.setups.get_setup--returns","title":"Returns","text":"<p>OpenMLSetup (an initialized openml setup object)</p> Source code in <code>openml/setups/functions.py</code> <pre><code>def get_setup(setup_id: int) -&gt; OpenMLSetup:\n    \"\"\"\n     Downloads the setup (configuration) description from OpenML\n     and returns a structured object\n\n    Parameters\n    ----------\n    setup_id : int\n        The Openml setup_id\n\n    Returns\n    -------\n    OpenMLSetup (an initialized openml setup object)\n    \"\"\"\n    setup_dir = Path(config.get_cache_directory()) / \"setups\" / str(setup_id)\n    setup_dir.mkdir(exist_ok=True, parents=True)\n\n    setup_file = setup_dir / \"description.xml\"\n\n    try:\n        return _get_cached_setup(setup_id)\n    except openml.exceptions.OpenMLCacheException:\n        url_suffix = f\"/setup/{setup_id}\"\n        setup_xml = openml._api_calls._perform_api_call(url_suffix, \"get\")\n        with setup_file.open(\"w\", encoding=\"utf8\") as fh:\n            fh.write(setup_xml)\n\n    result_dict = xmltodict.parse(setup_xml)\n    return _create_setup_from_xml(result_dict)\n</code></pre>"},{"location":"reference/setups/#openml.setups.initialize_model","title":"initialize_model","text":"<pre><code>initialize_model(setup_id: int, *, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Initialized a model based on a setup_id (i.e., using the exact same parameter settings)</p>"},{"location":"reference/setups/#openml.setups.initialize_model--parameters","title":"Parameters","text":"<p>setup_id : int     The Openml setup_id strict_version: bool (default=True)     See <code>flow_to_model</code> strict_version.</p>"},{"location":"reference/setups/#openml.setups.initialize_model--returns","title":"Returns","text":"<p>model</p> Source code in <code>openml/setups/functions.py</code> <pre><code>def initialize_model(setup_id: int, *, strict_version: bool = True) -&gt; Any:\n    \"\"\"\n    Initialized a model based on a setup_id (i.e., using the exact\n    same parameter settings)\n\n    Parameters\n    ----------\n    setup_id : int\n        The Openml setup_id\n    strict_version: bool (default=True)\n        See `flow_to_model` strict_version.\n\n    Returns\n    -------\n    model\n    \"\"\"\n    setup = get_setup(setup_id)\n    flow = openml.flows.get_flow(setup.flow_id)\n\n    # instead of using scikit-learns or any other library's \"set_params\" function, we override the\n    # OpenMLFlow objects default parameter value so we can utilize the\n    # Extension.flow_to_model() function to reinitialize the flow with the set defaults.\n    if setup.parameters is not None:\n        for hyperparameter in setup.parameters.values():\n            structure = flow.get_structure(\"flow_id\")\n            if len(structure[hyperparameter.flow_id]) &gt; 0:\n                subflow = flow.get_subflow(structure[hyperparameter.flow_id])\n            else:\n                subflow = flow\n            subflow.parameters[hyperparameter.parameter_name] = hyperparameter.value\n\n    return flow.extension.flow_to_model(flow, strict_version=strict_version)\n</code></pre>"},{"location":"reference/setups/#openml.setups.list_setups","title":"list_setups","text":"<pre><code>list_setups(offset: int | None = None, size: int | None = None, flow: int | None = None, tag: str | None = None, setup: Iterable[int] | None = None, output_format: Literal['object', 'dataframe'] = 'object') -&gt; dict[int, OpenMLSetup] | DataFrame\n</code></pre> <p>List all setups matching all of the given filters.</p>"},{"location":"reference/setups/#openml.setups.list_setups--parameters","title":"Parameters","text":"<p>offset : int, optional size : int, optional flow : int, optional tag : str, optional setup : Iterable[int], optional output_format: str, optional (default='object')     The parameter decides the format of the output.     - If 'dataframe' the output is a pandas DataFrame     - If 'object' the output is a dictionary of OpenMLSetup objects</p>"},{"location":"reference/setups/#openml.setups.list_setups--returns","title":"Returns","text":"<p>dict or dataframe</p> Source code in <code>openml/setups/functions.py</code> <pre><code>def list_setups(  # noqa: PLR0913\n    offset: int | None = None,\n    size: int | None = None,\n    flow: int | None = None,\n    tag: str | None = None,\n    setup: Iterable[int] | None = None,\n    output_format: Literal[\"object\", \"dataframe\"] = \"object\",\n) -&gt; dict[int, OpenMLSetup] | pd.DataFrame:\n    \"\"\"\n    List all setups matching all of the given filters.\n\n    Parameters\n    ----------\n    offset : int, optional\n    size : int, optional\n    flow : int, optional\n    tag : str, optional\n    setup : Iterable[int], optional\n    output_format: str, optional (default='object')\n        The parameter decides the format of the output.\n        - If 'dataframe' the output is a pandas DataFrame\n        - If 'object' the output is a dictionary of OpenMLSetup objects\n\n    Returns\n    -------\n    dict or dataframe\n    \"\"\"\n    if output_format not in [\"dataframe\", \"object\"]:\n        raise ValueError(\n            \"Invalid output format selected. Only 'object', or 'dataframe' applicable.\",\n        )\n\n    listing_call = partial(_list_setups, flow=flow, tag=tag, setup=setup)\n    batches = openml.utils._list_all(\n        listing_call,\n        batch_size=1_000,  # batch size for setups is lower\n        offset=offset,\n        limit=size,\n    )\n    flattened = list(chain.from_iterable(batches))\n    if output_format == \"object\":\n        return {setup.setup_id: setup for setup in flattened}\n\n    records = [setup._to_dict() for setup in flattened]\n    return pd.DataFrame.from_records(records, index=\"setup_id\")\n</code></pre>"},{"location":"reference/setups/#openml.setups.setup_exists","title":"setup_exists","text":"<pre><code>setup_exists(flow: OpenMLFlow) -&gt; int\n</code></pre> <p>Checks whether a hyperparameter configuration already exists on the server.</p>"},{"location":"reference/setups/#openml.setups.setup_exists--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow     The openml flow object. Should have flow id present for the main flow     and all subflows (i.e., it should be downloaded from the server by     means of flow.get, and not instantiated locally)</p>"},{"location":"reference/setups/#openml.setups.setup_exists--returns","title":"Returns","text":"<p>setup_id : int     setup id iff exists, False otherwise</p> Source code in <code>openml/setups/functions.py</code> <pre><code>def setup_exists(flow: OpenMLFlow) -&gt; int:\n    \"\"\"\n    Checks whether a hyperparameter configuration already exists on the server.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        The openml flow object. Should have flow id present for the main flow\n        and all subflows (i.e., it should be downloaded from the server by\n        means of flow.get, and not instantiated locally)\n\n    Returns\n    -------\n    setup_id : int\n        setup id iff exists, False otherwise\n    \"\"\"\n    # sadly, this api call relies on a run object\n    openml.flows.functions._check_flow_for_server_id(flow)\n    if flow.model is None:\n        raise ValueError(\"Flow should have model field set with the actual model.\")\n    if flow.extension is None:\n        raise ValueError(\"Flow should have model field set with the correct extension.\")\n\n    # checks whether the flow exists on the server and flow ids align\n    exists = flow_exists(flow.name, flow.external_version)\n    if exists != flow.flow_id:\n        raise ValueError(\n            f\"Local flow id ({flow.id}) differs from server id ({exists}). \"\n            \"If this issue persists, please contact the developers.\",\n        )\n\n    openml_param_settings = flow.extension.obtain_parameter_values(flow)\n    description = xmltodict.unparse(_to_dict(flow.flow_id, openml_param_settings), pretty=True)\n    file_elements = {\n        \"description\": (\"description.arff\", description),\n    }  # type: openml._api_calls.FILE_ELEMENTS_TYPE\n    result = openml._api_calls._perform_api_call(\n        \"/setup/exists/\",\n        \"post\",\n        file_elements=file_elements,\n    )\n    result_dict = xmltodict.parse(result)\n    setup_id = int(result_dict[\"oml:setup_exists\"][\"oml:id\"])\n    return setup_id if setup_id &gt; 0 else False\n</code></pre>"},{"location":"reference/setups/functions/","title":"functions","text":""},{"location":"reference/setups/functions/#openml.setups.functions","title":"openml.setups.functions","text":""},{"location":"reference/setups/functions/#openml.setups.functions.__list_setups","title":"__list_setups","text":"<pre><code>__list_setups(api_call: str) -&gt; list[OpenMLSetup]\n</code></pre> <p>Helper function to parse API calls which are lists of setups</p> Source code in <code>openml/setups/functions.py</code> <pre><code>def __list_setups(api_call: str) -&gt; list[OpenMLSetup]:\n    \"\"\"Helper function to parse API calls which are lists of setups\"\"\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    setups_dict = xmltodict.parse(xml_string, force_list=(\"oml:setup\",))\n    openml_uri = \"http://openml.org/openml\"\n    # Minimalistic check if the XML is useful\n    if \"oml:setups\" not in setups_dict:\n        raise ValueError(\n            f'Error in return XML, does not contain \"oml:setups\": {setups_dict!s}',\n        )\n\n    if \"@xmlns:oml\" not in setups_dict[\"oml:setups\"]:\n        raise ValueError(\n            f'Error in return XML, does not contain \"oml:setups\"/@xmlns:oml: {setups_dict!s}',\n        )\n\n    if setups_dict[\"oml:setups\"][\"@xmlns:oml\"] != openml_uri:\n        raise ValueError(\n            \"Error in return XML, value of  \"\n            '\"oml:seyups\"/@xmlns:oml is not '\n            f'\"{openml_uri}\": {setups_dict!s}',\n        )\n\n    assert isinstance(setups_dict[\"oml:setups\"][\"oml:setup\"], list), type(setups_dict[\"oml:setups\"])\n\n    return [\n        _create_setup_from_xml({\"oml:setup_parameters\": setup_})\n        for setup_ in setups_dict[\"oml:setups\"][\"oml:setup\"]\n    ]\n</code></pre>"},{"location":"reference/setups/functions/#openml.setups.functions.get_setup","title":"get_setup","text":"<pre><code>get_setup(setup_id: int) -&gt; OpenMLSetup\n</code></pre> <p>Downloads the setup (configuration) description from OpenML  and returns a structured object</p>"},{"location":"reference/setups/functions/#openml.setups.functions.get_setup--parameters","title":"Parameters","text":"<p>setup_id : int     The Openml setup_id</p>"},{"location":"reference/setups/functions/#openml.setups.functions.get_setup--returns","title":"Returns","text":"<p>OpenMLSetup (an initialized openml setup object)</p> Source code in <code>openml/setups/functions.py</code> <pre><code>def get_setup(setup_id: int) -&gt; OpenMLSetup:\n    \"\"\"\n     Downloads the setup (configuration) description from OpenML\n     and returns a structured object\n\n    Parameters\n    ----------\n    setup_id : int\n        The Openml setup_id\n\n    Returns\n    -------\n    OpenMLSetup (an initialized openml setup object)\n    \"\"\"\n    setup_dir = Path(config.get_cache_directory()) / \"setups\" / str(setup_id)\n    setup_dir.mkdir(exist_ok=True, parents=True)\n\n    setup_file = setup_dir / \"description.xml\"\n\n    try:\n        return _get_cached_setup(setup_id)\n    except openml.exceptions.OpenMLCacheException:\n        url_suffix = f\"/setup/{setup_id}\"\n        setup_xml = openml._api_calls._perform_api_call(url_suffix, \"get\")\n        with setup_file.open(\"w\", encoding=\"utf8\") as fh:\n            fh.write(setup_xml)\n\n    result_dict = xmltodict.parse(setup_xml)\n    return _create_setup_from_xml(result_dict)\n</code></pre>"},{"location":"reference/setups/functions/#openml.setups.functions.initialize_model","title":"initialize_model","text":"<pre><code>initialize_model(setup_id: int, *, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Initialized a model based on a setup_id (i.e., using the exact same parameter settings)</p>"},{"location":"reference/setups/functions/#openml.setups.functions.initialize_model--parameters","title":"Parameters","text":"<p>setup_id : int     The Openml setup_id strict_version: bool (default=True)     See <code>flow_to_model</code> strict_version.</p>"},{"location":"reference/setups/functions/#openml.setups.functions.initialize_model--returns","title":"Returns","text":"<p>model</p> Source code in <code>openml/setups/functions.py</code> <pre><code>def initialize_model(setup_id: int, *, strict_version: bool = True) -&gt; Any:\n    \"\"\"\n    Initialized a model based on a setup_id (i.e., using the exact\n    same parameter settings)\n\n    Parameters\n    ----------\n    setup_id : int\n        The Openml setup_id\n    strict_version: bool (default=True)\n        See `flow_to_model` strict_version.\n\n    Returns\n    -------\n    model\n    \"\"\"\n    setup = get_setup(setup_id)\n    flow = openml.flows.get_flow(setup.flow_id)\n\n    # instead of using scikit-learns or any other library's \"set_params\" function, we override the\n    # OpenMLFlow objects default parameter value so we can utilize the\n    # Extension.flow_to_model() function to reinitialize the flow with the set defaults.\n    if setup.parameters is not None:\n        for hyperparameter in setup.parameters.values():\n            structure = flow.get_structure(\"flow_id\")\n            if len(structure[hyperparameter.flow_id]) &gt; 0:\n                subflow = flow.get_subflow(structure[hyperparameter.flow_id])\n            else:\n                subflow = flow\n            subflow.parameters[hyperparameter.parameter_name] = hyperparameter.value\n\n    return flow.extension.flow_to_model(flow, strict_version=strict_version)\n</code></pre>"},{"location":"reference/setups/functions/#openml.setups.functions.list_setups","title":"list_setups","text":"<pre><code>list_setups(offset: int | None = None, size: int | None = None, flow: int | None = None, tag: str | None = None, setup: Iterable[int] | None = None, output_format: Literal['object', 'dataframe'] = 'object') -&gt; dict[int, OpenMLSetup] | DataFrame\n</code></pre> <p>List all setups matching all of the given filters.</p>"},{"location":"reference/setups/functions/#openml.setups.functions.list_setups--parameters","title":"Parameters","text":"<p>offset : int, optional size : int, optional flow : int, optional tag : str, optional setup : Iterable[int], optional output_format: str, optional (default='object')     The parameter decides the format of the output.     - If 'dataframe' the output is a pandas DataFrame     - If 'object' the output is a dictionary of OpenMLSetup objects</p>"},{"location":"reference/setups/functions/#openml.setups.functions.list_setups--returns","title":"Returns","text":"<p>dict or dataframe</p> Source code in <code>openml/setups/functions.py</code> <pre><code>def list_setups(  # noqa: PLR0913\n    offset: int | None = None,\n    size: int | None = None,\n    flow: int | None = None,\n    tag: str | None = None,\n    setup: Iterable[int] | None = None,\n    output_format: Literal[\"object\", \"dataframe\"] = \"object\",\n) -&gt; dict[int, OpenMLSetup] | pd.DataFrame:\n    \"\"\"\n    List all setups matching all of the given filters.\n\n    Parameters\n    ----------\n    offset : int, optional\n    size : int, optional\n    flow : int, optional\n    tag : str, optional\n    setup : Iterable[int], optional\n    output_format: str, optional (default='object')\n        The parameter decides the format of the output.\n        - If 'dataframe' the output is a pandas DataFrame\n        - If 'object' the output is a dictionary of OpenMLSetup objects\n\n    Returns\n    -------\n    dict or dataframe\n    \"\"\"\n    if output_format not in [\"dataframe\", \"object\"]:\n        raise ValueError(\n            \"Invalid output format selected. Only 'object', or 'dataframe' applicable.\",\n        )\n\n    listing_call = partial(_list_setups, flow=flow, tag=tag, setup=setup)\n    batches = openml.utils._list_all(\n        listing_call,\n        batch_size=1_000,  # batch size for setups is lower\n        offset=offset,\n        limit=size,\n    )\n    flattened = list(chain.from_iterable(batches))\n    if output_format == \"object\":\n        return {setup.setup_id: setup for setup in flattened}\n\n    records = [setup._to_dict() for setup in flattened]\n    return pd.DataFrame.from_records(records, index=\"setup_id\")\n</code></pre>"},{"location":"reference/setups/functions/#openml.setups.functions.setup_exists","title":"setup_exists","text":"<pre><code>setup_exists(flow: OpenMLFlow) -&gt; int\n</code></pre> <p>Checks whether a hyperparameter configuration already exists on the server.</p>"},{"location":"reference/setups/functions/#openml.setups.functions.setup_exists--parameters","title":"Parameters","text":"<p>flow : OpenMLFlow     The openml flow object. Should have flow id present for the main flow     and all subflows (i.e., it should be downloaded from the server by     means of flow.get, and not instantiated locally)</p>"},{"location":"reference/setups/functions/#openml.setups.functions.setup_exists--returns","title":"Returns","text":"<p>setup_id : int     setup id iff exists, False otherwise</p> Source code in <code>openml/setups/functions.py</code> <pre><code>def setup_exists(flow: OpenMLFlow) -&gt; int:\n    \"\"\"\n    Checks whether a hyperparameter configuration already exists on the server.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        The openml flow object. Should have flow id present for the main flow\n        and all subflows (i.e., it should be downloaded from the server by\n        means of flow.get, and not instantiated locally)\n\n    Returns\n    -------\n    setup_id : int\n        setup id iff exists, False otherwise\n    \"\"\"\n    # sadly, this api call relies on a run object\n    openml.flows.functions._check_flow_for_server_id(flow)\n    if flow.model is None:\n        raise ValueError(\"Flow should have model field set with the actual model.\")\n    if flow.extension is None:\n        raise ValueError(\"Flow should have model field set with the correct extension.\")\n\n    # checks whether the flow exists on the server and flow ids align\n    exists = flow_exists(flow.name, flow.external_version)\n    if exists != flow.flow_id:\n        raise ValueError(\n            f\"Local flow id ({flow.id}) differs from server id ({exists}). \"\n            \"If this issue persists, please contact the developers.\",\n        )\n\n    openml_param_settings = flow.extension.obtain_parameter_values(flow)\n    description = xmltodict.unparse(_to_dict(flow.flow_id, openml_param_settings), pretty=True)\n    file_elements = {\n        \"description\": (\"description.arff\", description),\n    }  # type: openml._api_calls.FILE_ELEMENTS_TYPE\n    result = openml._api_calls._perform_api_call(\n        \"/setup/exists/\",\n        \"post\",\n        file_elements=file_elements,\n    )\n    result_dict = xmltodict.parse(result)\n    setup_id = int(result_dict[\"oml:setup_exists\"][\"oml:id\"])\n    return setup_id if setup_id &gt; 0 else False\n</code></pre>"},{"location":"reference/setups/setup/","title":"setup","text":""},{"location":"reference/setups/setup/#openml.setups.setup","title":"openml.setups.setup","text":""},{"location":"reference/setups/setup/#openml.setups.setup.OpenMLParameter","title":"OpenMLParameter","text":"<pre><code>OpenMLParameter(input_id: int, flow_id: int, flow_name: str, full_name: str, parameter_name: str, data_type: str, default_value: str, value: str)\n</code></pre> <p>Parameter object (used in setup).</p>"},{"location":"reference/setups/setup/#openml.setups.setup.OpenMLParameter--parameters","title":"Parameters","text":"<p>input_id : int     The input id from the openml database flow id : int     The flow to which this parameter is associated flow name : str     The name of the flow (no version number) to which this parameter     is associated full_name : str     The name of the flow and parameter combined parameter_name : str     The name of the parameter data_type : str     The datatype of the parameter. generally unused for sklearn flows default_value : str     The default value. For sklearn parameters, this is unknown and a     default value is selected arbitrarily value : str     If the parameter was set, the value that it was set to.</p> Source code in <code>openml/setups/setup.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    input_id: int,\n    flow_id: int,\n    flow_name: str,\n    full_name: str,\n    parameter_name: str,\n    data_type: str,\n    default_value: str,\n    value: str,\n):\n    self.id = input_id\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.full_name = full_name\n    self.parameter_name = parameter_name\n    self.data_type = data_type\n    self.default_value = default_value\n    self.value = value\n</code></pre>"},{"location":"reference/setups/setup/#openml.setups.setup.OpenMLSetup","title":"OpenMLSetup","text":"<pre><code>OpenMLSetup(setup_id: int, flow_id: int, parameters: dict[int, Any] | None)\n</code></pre> <p>Setup object (a.k.a. Configuration).</p>"},{"location":"reference/setups/setup/#openml.setups.setup.OpenMLSetup--parameters","title":"Parameters","text":"<p>setup_id : int     The OpenML setup id flow_id : int     The flow that it is build upon parameters : dict     The setting of the parameters</p> Source code in <code>openml/setups/setup.py</code> <pre><code>def __init__(self, setup_id: int, flow_id: int, parameters: dict[int, Any] | None):\n    if not isinstance(setup_id, int):\n        raise ValueError(\"setup id should be int\")\n\n    if not isinstance(flow_id, int):\n        raise ValueError(\"flow id should be int\")\n\n    if parameters is not None and not isinstance(parameters, dict):\n        raise ValueError(\"parameters should be dict\")\n\n    self.setup_id = setup_id\n    self.flow_id = flow_id\n    self.parameters = parameters\n</code></pre>"},{"location":"reference/study/","title":"study","text":""},{"location":"reference/study/#openml.study","title":"openml.study","text":""},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite","title":"OpenMLBenchmarkSuite","text":"<pre><code>OpenMLBenchmarkSuite(suite_id: int | None, alias: str | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None)\n</code></pre> <p>               Bases: <code>BaseStudy</code></p> <p>An OpenMLBenchmarkSuite represents the OpenML concept of a suite (a collection of tasks).</p> <p>It contains the following information: name, id, description, creation date, creator id and the task ids.</p> <p>According to this list of task ids, the suite object receives a list of OpenML object ids (datasets).</p>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite--parameters","title":"Parameters","text":"<p>suite_id : int     the study id alias : str (optional)     a string ID, unique on server (url-friendly) main_entity_type : str     the entity type (e.g., task, run) that is core in this study.     only entities of this type can be added explicitly name : str     the name of the study (meta-info) description : str     brief description (meta-info) status : str     Whether the study is in preparation, active or deactivated creation_date : str     date of creation (meta-info) creator : int     openml user id of the owner / creator tags : list(dict)     The list of tags shows which tags are associated with the study.     Each tag is a dict of (tag) name, window_start and write_access. data : list     a list of data ids associated with this study tasks : list     a list of task ids associated with this study</p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    suite_id: int | None,\n    alias: str | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n):\n    super().__init__(\n        study_id=suite_id,\n        alias=alias,\n        main_entity_type=\"task\",\n        benchmark_suite=None,\n        name=name,\n        description=description,\n        status=status,\n        creation_date=creation_date,\n        creator=creator,\n        tags=tags,\n        data=data,\n        tasks=tasks,\n        flows=None,\n        runs=None,\n        setups=None,\n    )\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLStudy","title":"OpenMLStudy","text":"<pre><code>OpenMLStudy(study_id: int | None, alias: str | None, benchmark_suite: int | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None, flows: list[int] | None, runs: list[int] | None, setups: list[int] | None)\n</code></pre> <p>               Bases: <code>BaseStudy</code></p> <p>An OpenMLStudy represents the OpenML concept of a study (a collection of runs).</p> <p>It contains the following information: name, id, description, creation date, creator id and a list of run ids.</p> <p>According to this list of run ids, the study object receives a list of OpenML object ids (datasets, flows, tasks and setups).</p>"},{"location":"reference/study/#openml.study.OpenMLStudy--parameters","title":"Parameters","text":"<p>study_id : int     the study id alias : str (optional)     a string ID, unique on server (url-friendly) benchmark_suite : int (optional)     the benchmark suite (another study) upon which this study is ran.     can only be active if main entity type is runs. name : str     the name of the study (meta-info) description : str     brief description (meta-info) status : str     Whether the study is in preparation, active or deactivated creation_date : str     date of creation (meta-info) creator : int     openml user id of the owner / creator tags : list(dict)     The list of tags shows which tags are associated with the study.     Each tag is a dict of (tag) name, window_start and write_access. data : list     a list of data ids associated with this study tasks : list     a list of task ids associated with this study flows : list     a list of flow ids associated with this study runs : list     a list of run ids associated with this study setups : list     a list of setup ids associated with this study</p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    study_id: int | None,\n    alias: str | None,\n    benchmark_suite: int | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n    flows: list[int] | None,\n    runs: list[int] | None,\n    setups: list[int] | None,\n):\n    super().__init__(\n        study_id=study_id,\n        alias=alias,\n        main_entity_type=\"run\",\n        benchmark_suite=benchmark_suite,\n        name=name,\n        description=description,\n        status=status,\n        creation_date=creation_date,\n        creator=creator,\n        tags=tags,\n        data=data,\n        tasks=tasks,\n        flows=flows,\n        runs=runs,\n        setups=setups,\n    )\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLStudy.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/study/#openml.study.OpenMLStudy.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/study/#openml.study.OpenMLStudy.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLStudy.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLStudy.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLStudy.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLStudy.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/study/#openml.study.attach_to_study","title":"attach_to_study","text":"<pre><code>attach_to_study(study_id: int, run_ids: list[int]) -&gt; int\n</code></pre> <p>Attaches a set of runs to a study.</p>"},{"location":"reference/study/#openml.study.attach_to_study--parameters","title":"Parameters","text":"<p>study_id : int     OpenML id of the study</p> list (int) <p>List of entities to link to the collection</p>"},{"location":"reference/study/#openml.study.attach_to_study--returns","title":"Returns","text":"<p>int     new size of the study (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def attach_to_study(study_id: int, run_ids: list[int]) -&gt; int:\n    \"\"\"Attaches a set of runs to a study.\n\n    Parameters\n    ----------\n    study_id : int\n        OpenML id of the study\n\n    run_ids : list (int)\n        List of entities to link to the collection\n\n    Returns\n    -------\n    int\n        new size of the study (in terms of explicitly linked entities)\n    \"\"\"\n    # Interestingly, there's no need to tell the server about the entity type, it knows by itself\n    result_xml = openml._api_calls._perform_api_call(\n        call=f\"study/{study_id}/attach\",\n        request_method=\"post\",\n        data={\"ids\": \",\".join(str(x) for x in run_ids)},\n    )\n    result = xmltodict.parse(result_xml)[\"oml:study_attach\"]\n    return int(result[\"oml:linked_entities\"])\n</code></pre>"},{"location":"reference/study/#openml.study.attach_to_suite","title":"attach_to_suite","text":"<pre><code>attach_to_suite(suite_id: int, task_ids: list[int]) -&gt; int\n</code></pre> <p>Attaches a set of tasks to a benchmarking suite.</p>"},{"location":"reference/study/#openml.study.attach_to_suite--parameters","title":"Parameters","text":"<p>suite_id : int     OpenML id of the study</p> list (int) <p>List of entities to link to the collection</p>"},{"location":"reference/study/#openml.study.attach_to_suite--returns","title":"Returns","text":"<p>int     new size of the suite (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def attach_to_suite(suite_id: int, task_ids: list[int]) -&gt; int:\n    \"\"\"Attaches a set of tasks to a benchmarking suite.\n\n    Parameters\n    ----------\n    suite_id : int\n        OpenML id of the study\n\n    task_ids : list (int)\n        List of entities to link to the collection\n\n    Returns\n    -------\n    int\n        new size of the suite (in terms of explicitly linked entities)\n    \"\"\"\n    return attach_to_study(suite_id, task_ids)\n</code></pre>"},{"location":"reference/study/#openml.study.create_benchmark_suite","title":"create_benchmark_suite","text":"<pre><code>create_benchmark_suite(name: str, description: str, task_ids: list[int], alias: str | None = None) -&gt; OpenMLBenchmarkSuite\n</code></pre> <p>Creates an OpenML benchmark suite (collection of entity types, where the tasks are the linked entity)</p>"},{"location":"reference/study/#openml.study.create_benchmark_suite--parameters","title":"Parameters","text":"<p>name : str     the name of the study (meta-info) description : str     brief description (meta-info) task_ids : list     a list of task ids associated with this study     more can be added later with <code>attach_to_suite</code>. alias : str (optional)     a string ID, unique on server (url-friendly)</p>"},{"location":"reference/study/#openml.study.create_benchmark_suite--returns","title":"Returns","text":"<p>OpenMLStudy     A local OpenML study object (call publish method to upload to server)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def create_benchmark_suite(\n    name: str,\n    description: str,\n    task_ids: list[int],\n    alias: str | None = None,\n) -&gt; OpenMLBenchmarkSuite:\n    \"\"\"\n    Creates an OpenML benchmark suite (collection of entity types, where\n    the tasks are the linked entity)\n\n    Parameters\n    ----------\n    name : str\n        the name of the study (meta-info)\n    description : str\n        brief description (meta-info)\n    task_ids : list\n        a list of task ids associated with this study\n        more can be added later with ``attach_to_suite``.\n    alias : str (optional)\n        a string ID, unique on server (url-friendly)\n\n    Returns\n    -------\n    OpenMLStudy\n        A local OpenML study object (call publish method to upload to server)\n    \"\"\"\n    return OpenMLBenchmarkSuite(\n        suite_id=None,\n        alias=alias,\n        name=name,\n        description=description,\n        status=None,\n        creation_date=None,\n        creator=None,\n        tags=None,\n        data=None,\n        tasks=task_ids,\n    )\n</code></pre>"},{"location":"reference/study/#openml.study.create_study","title":"create_study","text":"<pre><code>create_study(name: str, description: str, run_ids: list[int] | None = None, alias: str | None = None, benchmark_suite: int | None = None) -&gt; OpenMLStudy\n</code></pre> <p>Creates an OpenML study (collection of data, tasks, flows, setups and run), where the runs are the main entity (collection consists of runs and all entities (flows, tasks, etc) that are related to these runs)</p>"},{"location":"reference/study/#openml.study.create_study--parameters","title":"Parameters","text":"<p>benchmark_suite : int (optional)     the benchmark suite (another study) upon which this study is ran. name : str     the name of the study (meta-info) description : str     brief description (meta-info) run_ids : list, optional     a list of run ids associated with this study,     these can also be added later with <code>attach_to_study</code>. alias : str (optional)     a string ID, unique on server (url-friendly) benchmark_suite: int (optional)     the ID of the suite for which this study contains run results</p>"},{"location":"reference/study/#openml.study.create_study--returns","title":"Returns","text":"<p>OpenMLStudy     A local OpenML study object (call publish method to upload to server)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def create_study(\n    name: str,\n    description: str,\n    run_ids: list[int] | None = None,\n    alias: str | None = None,\n    benchmark_suite: int | None = None,\n) -&gt; OpenMLStudy:\n    \"\"\"\n    Creates an OpenML study (collection of data, tasks, flows, setups and run),\n    where the runs are the main entity (collection consists of runs and all\n    entities (flows, tasks, etc) that are related to these runs)\n\n    Parameters\n    ----------\n    benchmark_suite : int (optional)\n        the benchmark suite (another study) upon which this study is ran.\n    name : str\n        the name of the study (meta-info)\n    description : str\n        brief description (meta-info)\n    run_ids : list, optional\n        a list of run ids associated with this study,\n        these can also be added later with ``attach_to_study``.\n    alias : str (optional)\n        a string ID, unique on server (url-friendly)\n    benchmark_suite: int (optional)\n        the ID of the suite for which this study contains run results\n\n    Returns\n    -------\n    OpenMLStudy\n        A local OpenML study object (call publish method to upload to server)\n    \"\"\"\n    return OpenMLStudy(\n        study_id=None,\n        alias=alias,\n        benchmark_suite=benchmark_suite,\n        name=name,\n        description=description,\n        status=None,\n        creation_date=None,\n        creator=None,\n        tags=None,\n        data=None,\n        tasks=None,\n        flows=None,\n        runs=run_ids if run_ids != [] else None,\n        setups=None,\n    )\n</code></pre>"},{"location":"reference/study/#openml.study.delete_study","title":"delete_study","text":"<pre><code>delete_study(study_id: int) -&gt; bool\n</code></pre> <p>Deletes a study from the OpenML server.</p>"},{"location":"reference/study/#openml.study.delete_study--parameters","title":"Parameters","text":"<p>study_id : int     OpenML id of the study</p>"},{"location":"reference/study/#openml.study.delete_study--returns","title":"Returns","text":"<p>bool     True iff the deletion was successful. False otherwise</p> Source code in <code>openml/study/functions.py</code> <pre><code>def delete_study(study_id: int) -&gt; bool:\n    \"\"\"Deletes a study from the OpenML server.\n\n    Parameters\n    ----------\n    study_id : int\n        OpenML id of the study\n\n    Returns\n    -------\n    bool\n        True iff the deletion was successful. False otherwise\n    \"\"\"\n    return openml.utils._delete_entity(\"study\", study_id)\n</code></pre>"},{"location":"reference/study/#openml.study.delete_suite","title":"delete_suite","text":"<pre><code>delete_suite(suite_id: int) -&gt; bool\n</code></pre> <p>Deletes a study from the OpenML server.</p>"},{"location":"reference/study/#openml.study.delete_suite--parameters","title":"Parameters","text":"<p>suite_id : int     OpenML id of the study</p>"},{"location":"reference/study/#openml.study.delete_suite--returns","title":"Returns","text":"<p>bool     True iff the deletion was successful. False otherwise</p> Source code in <code>openml/study/functions.py</code> <pre><code>def delete_suite(suite_id: int) -&gt; bool:\n    \"\"\"Deletes a study from the OpenML server.\n\n    Parameters\n    ----------\n    suite_id : int\n        OpenML id of the study\n\n    Returns\n    -------\n    bool\n        True iff the deletion was successful. False otherwise\n    \"\"\"\n    return delete_study(suite_id)\n</code></pre>"},{"location":"reference/study/#openml.study.detach_from_study","title":"detach_from_study","text":"<pre><code>detach_from_study(study_id: int, run_ids: list[int]) -&gt; int\n</code></pre> <p>Detaches a set of run ids from a study.</p>"},{"location":"reference/study/#openml.study.detach_from_study--parameters","title":"Parameters","text":"<p>study_id : int     OpenML id of the study</p> list (int) <p>List of entities to unlink from the collection</p>"},{"location":"reference/study/#openml.study.detach_from_study--returns","title":"Returns","text":"<p>int     new size of the study (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def detach_from_study(study_id: int, run_ids: list[int]) -&gt; int:\n    \"\"\"Detaches a set of run ids from a study.\n\n    Parameters\n    ----------\n    study_id : int\n        OpenML id of the study\n\n    run_ids : list (int)\n        List of entities to unlink from the collection\n\n    Returns\n    -------\n    int\n        new size of the study (in terms of explicitly linked entities)\n    \"\"\"\n    # Interestingly, there's no need to tell the server about the entity type, it knows by itself\n    uri = \"study/%d/detach\" % study_id\n    post_variables = {\"ids\": \",\".join(str(x) for x in run_ids)}  # type: openml._api_calls.DATA_TYPE\n    result_xml = openml._api_calls._perform_api_call(\n        call=uri,\n        request_method=\"post\",\n        data=post_variables,\n    )\n    result = xmltodict.parse(result_xml)[\"oml:study_detach\"]\n    return int(result[\"oml:linked_entities\"])\n</code></pre>"},{"location":"reference/study/#openml.study.detach_from_suite","title":"detach_from_suite","text":"<pre><code>detach_from_suite(suite_id: int, task_ids: list[int]) -&gt; int\n</code></pre> <p>Detaches a set of task ids from a suite.</p>"},{"location":"reference/study/#openml.study.detach_from_suite--parameters","title":"Parameters","text":"<p>suite_id : int     OpenML id of the study</p> list (int) <p>List of entities to unlink from the collection</p>"},{"location":"reference/study/#openml.study.detach_from_suite--returns","title":"Returns","text":"<p>int new size of the study (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def detach_from_suite(suite_id: int, task_ids: list[int]) -&gt; int:\n    \"\"\"Detaches a set of task ids from a suite.\n\n    Parameters\n    ----------\n    suite_id : int\n        OpenML id of the study\n\n    task_ids : list (int)\n        List of entities to unlink from the collection\n\n    Returns\n    -------\n    int\n    new size of the study (in terms of explicitly linked entities)\n    \"\"\"\n    return detach_from_study(suite_id, task_ids)\n</code></pre>"},{"location":"reference/study/#openml.study.get_study","title":"get_study","text":"<pre><code>get_study(study_id: int | str, arg_for_backwards_compat: str | None = None) -&gt; OpenMLStudy\n</code></pre> <p>Retrieves all relevant information of an OpenML study from the server.</p>"},{"location":"reference/study/#openml.study.get_study--parameters","title":"Parameters","text":"<p>study id : int, str     study id (numeric or alias)</p> str, optional <p>The example given in arxiv.org/pdf/1708.03731.pdf uses an older version of the API which required specifying the type of study, i.e. tasks. We changed the implementation of studies since then and split them up into suites (collections of tasks) and studies (collections of runs) so this argument is no longer needed.</p>"},{"location":"reference/study/#openml.study.get_study--returns","title":"Returns","text":"<p>OpenMLStudy     The OpenML study object</p> Source code in <code>openml/study/functions.py</code> <pre><code>def get_study(\n    study_id: int | str,\n    arg_for_backwards_compat: str | None = None,  # noqa: ARG001\n) -&gt; OpenMLStudy:  # F401\n    \"\"\"\n    Retrieves all relevant information of an OpenML study from the server.\n\n    Parameters\n    ----------\n    study id : int, str\n        study id (numeric or alias)\n\n    arg_for_backwards_compat : str, optional\n        The example given in https://arxiv.org/pdf/1708.03731.pdf uses an older version of the\n        API which required specifying the type of study, i.e. tasks. We changed the\n        implementation of studies since then and split them up into suites (collections of tasks)\n        and studies (collections of runs) so this argument is no longer needed.\n\n    Returns\n    -------\n    OpenMLStudy\n        The OpenML study object\n    \"\"\"\n    if study_id == \"OpenML100\":\n        message = (\n            \"It looks like you are running code from the OpenML100 paper. It still works, but lots \"\n            \"of things have changed since then. Please use `get_suite('OpenML100')` instead.\"\n        )\n        warnings.warn(message, DeprecationWarning, stacklevel=2)\n        openml.config.logger.warning(message)\n        study = _get_study(study_id, entity_type=\"task\")\n        assert isinstance(study, OpenMLBenchmarkSuite)\n\n        return study  # type: ignore\n\n    study = _get_study(study_id, entity_type=\"run\")\n    assert isinstance(study, OpenMLStudy)\n    return study\n</code></pre>"},{"location":"reference/study/#openml.study.get_suite","title":"get_suite","text":"<pre><code>get_suite(suite_id: int | str) -&gt; OpenMLBenchmarkSuite\n</code></pre> <p>Retrieves all relevant information of an OpenML benchmarking suite from the server.</p>"},{"location":"reference/study/#openml.study.get_suite--parameters","title":"Parameters","text":"<p>study id : int, str     study id (numeric or alias)</p>"},{"location":"reference/study/#openml.study.get_suite--returns","title":"Returns","text":"<p>OpenMLSuite     The OpenML suite object</p> Source code in <code>openml/study/functions.py</code> <pre><code>def get_suite(suite_id: int | str) -&gt; OpenMLBenchmarkSuite:\n    \"\"\"\n    Retrieves all relevant information of an OpenML benchmarking suite from the server.\n\n    Parameters\n    ----------\n    study id : int, str\n        study id (numeric or alias)\n\n    Returns\n    -------\n    OpenMLSuite\n        The OpenML suite object\n    \"\"\"\n    study = _get_study(suite_id, entity_type=\"task\")\n    assert isinstance(study, OpenMLBenchmarkSuite)\n\n    return study\n</code></pre>"},{"location":"reference/study/#openml.study.list_studies","title":"list_studies","text":"<pre><code>list_studies(offset: int | None = None, size: int | None = None, status: str | None = None, uploader: list[str] | None = None, benchmark_suite: int | None = None) -&gt; DataFrame\n</code></pre> <p>Return a list of all studies which are on OpenML.</p>"},{"location":"reference/study/#openml.study.list_studies--parameters","title":"Parameters","text":"<p>offset : int, optional     The number of studies to skip, starting from the first. size : int, optional     The maximum number of studies to show. status : str, optional     Should be {active, in_preparation, deactivated, all}. By default active     studies are returned. uploader : list (int), optional     Result filter. Will only return studies created by these users. benchmark_suite : int, optional</p>"},{"location":"reference/study/#openml.study.list_studies--returns","title":"Returns","text":"<p>datasets : dataframe     Every dataset is represented by a dictionary containing     the following information:     - id     - alias (optional)     - name     - benchmark_suite (optional)     - status     - creator     - creation_date     If qualities are calculated for the dataset, some of     these are also returned.</p> Source code in <code>openml/study/functions.py</code> <pre><code>def list_studies(\n    offset: int | None = None,\n    size: int | None = None,\n    status: str | None = None,\n    uploader: list[str] | None = None,\n    benchmark_suite: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a list of all studies which are on OpenML.\n\n    Parameters\n    ----------\n    offset : int, optional\n        The number of studies to skip, starting from the first.\n    size : int, optional\n        The maximum number of studies to show.\n    status : str, optional\n        Should be {active, in_preparation, deactivated, all}. By default active\n        studies are returned.\n    uploader : list (int), optional\n        Result filter. Will only return studies created by these users.\n    benchmark_suite : int, optional\n\n    Returns\n    -------\n    datasets : dataframe\n        Every dataset is represented by a dictionary containing\n        the following information:\n        - id\n        - alias (optional)\n        - name\n        - benchmark_suite (optional)\n        - status\n        - creator\n        - creation_date\n        If qualities are calculated for the dataset, some of\n        these are also returned.\n    \"\"\"\n    listing_call = partial(\n        _list_studies,\n        main_entity_type=\"run\",\n        status=status,\n        uploader=uploader,\n        benchmark_suite=benchmark_suite,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/study/#openml.study.list_suites","title":"list_suites","text":"<pre><code>list_suites(offset: int | None = None, size: int | None = None, status: str | None = None, uploader: list[int] | None = None) -&gt; DataFrame\n</code></pre> <p>Return a list of all suites which are on OpenML.</p>"},{"location":"reference/study/#openml.study.list_suites--parameters","title":"Parameters","text":"<p>offset : int, optional     The number of suites to skip, starting from the first. size : int, optional     The maximum number of suites to show. status : str, optional     Should be {active, in_preparation, deactivated, all}. By default active     suites are returned. uploader : list (int), optional     Result filter. Will only return suites created by these users.</p>"},{"location":"reference/study/#openml.study.list_suites--returns","title":"Returns","text":"<p>datasets : dataframe     Every row is represented by a dictionary containing the following information:     - id     - alias (optional)     - name     - main_entity_type     - status     - creator     - creation_date</p> Source code in <code>openml/study/functions.py</code> <pre><code>def list_suites(\n    offset: int | None = None,\n    size: int | None = None,\n    status: str | None = None,\n    uploader: list[int] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a list of all suites which are on OpenML.\n\n    Parameters\n    ----------\n    offset : int, optional\n        The number of suites to skip, starting from the first.\n    size : int, optional\n        The maximum number of suites to show.\n    status : str, optional\n        Should be {active, in_preparation, deactivated, all}. By default active\n        suites are returned.\n    uploader : list (int), optional\n        Result filter. Will only return suites created by these users.\n\n    Returns\n    -------\n    datasets : dataframe\n        Every row is represented by a dictionary containing the following information:\n        - id\n        - alias (optional)\n        - name\n        - main_entity_type\n        - status\n        - creator\n        - creation_date\n    \"\"\"\n    listing_call = partial(\n        _list_studies,\n        main_entity_type=\"task\",\n        status=status,\n        uploader=uploader,\n    )\n    batches = openml.utils._list_all(listing_call, limit=size, offset=offset)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/study/#openml.study.update_study_status","title":"update_study_status","text":"<pre><code>update_study_status(study_id: int, status: str) -&gt; None\n</code></pre> <p>Updates the status of a study to either 'active' or 'deactivated'.</p>"},{"location":"reference/study/#openml.study.update_study_status--parameters","title":"Parameters","text":"<p>study_id : int     The data id of the dataset status : str,     'active' or 'deactivated'</p> Source code in <code>openml/study/functions.py</code> <pre><code>def update_study_status(study_id: int, status: str) -&gt; None:\n    \"\"\"\n    Updates the status of a study to either 'active' or 'deactivated'.\n\n    Parameters\n    ----------\n    study_id : int\n        The data id of the dataset\n    status : str,\n        'active' or 'deactivated'\n    \"\"\"\n    legal_status = {\"active\", \"deactivated\"}\n    if status not in legal_status:\n        raise ValueError(f\"Illegal status value. Legal values: {legal_status}\")\n    data = {\"study_id\": study_id, \"status\": status}  # type: openml._api_calls.DATA_TYPE\n    result_xml = openml._api_calls._perform_api_call(\"study/status/update\", \"post\", data=data)\n    result = xmltodict.parse(result_xml)\n    server_study_id = result[\"oml:study_status_update\"][\"oml:id\"]\n    server_status = result[\"oml:study_status_update\"][\"oml:status\"]\n    if status != server_status or int(study_id) != int(server_study_id):\n        # This should never happen\n        raise ValueError(\"Study id/status does not collide\")\n</code></pre>"},{"location":"reference/study/#openml.study.update_suite_status","title":"update_suite_status","text":"<pre><code>update_suite_status(suite_id: int, status: str) -&gt; None\n</code></pre> <p>Updates the status of a study to either 'active' or 'deactivated'.</p>"},{"location":"reference/study/#openml.study.update_suite_status--parameters","title":"Parameters","text":"<p>suite_id : int     The data id of the dataset status : str,     'active' or 'deactivated'</p> Source code in <code>openml/study/functions.py</code> <pre><code>def update_suite_status(suite_id: int, status: str) -&gt; None:\n    \"\"\"\n    Updates the status of a study to either 'active' or 'deactivated'.\n\n    Parameters\n    ----------\n    suite_id : int\n        The data id of the dataset\n    status : str,\n        'active' or 'deactivated'\n    \"\"\"\n    return update_study_status(suite_id, status)\n</code></pre>"},{"location":"reference/study/functions/","title":"functions","text":""},{"location":"reference/study/functions/#openml.study.functions","title":"openml.study.functions","text":""},{"location":"reference/study/functions/#openml.study.functions.__list_studies","title":"__list_studies","text":"<pre><code>__list_studies(api_call: str) -&gt; DataFrame\n</code></pre> <p>Retrieves the list of OpenML studies and returns it in a dictionary or a Pandas DataFrame.</p>"},{"location":"reference/study/functions/#openml.study.functions.__list_studies--parameters","title":"Parameters","text":"<p>api_call : str     The API call for retrieving the list of OpenML studies.</p>"},{"location":"reference/study/functions/#openml.study.functions.__list_studies--returns","title":"Returns","text":"<p>pd.DataFrame     A Pandas DataFrame of OpenML studies</p> Source code in <code>openml/study/functions.py</code> <pre><code>def __list_studies(api_call: str) -&gt; pd.DataFrame:\n    \"\"\"Retrieves the list of OpenML studies and\n    returns it in a dictionary or a Pandas DataFrame.\n\n    Parameters\n    ----------\n    api_call : str\n        The API call for retrieving the list of OpenML studies.\n\n    Returns\n    -------\n    pd.DataFrame\n        A Pandas DataFrame of OpenML studies\n    \"\"\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    study_dict = xmltodict.parse(xml_string, force_list=(\"oml:study\",))\n\n    # Minimalistic check if the XML is useful\n    assert isinstance(study_dict[\"oml:study_list\"][\"oml:study\"], list), type(\n        study_dict[\"oml:study_list\"],\n    )\n    assert study_dict[\"oml:study_list\"][\"@xmlns:oml\"] == \"http://openml.org/openml\", study_dict[\n        \"oml:study_list\"\n    ][\"@xmlns:oml\"]\n\n    studies = {}\n    for study_ in study_dict[\"oml:study_list\"][\"oml:study\"]:\n        # maps from xml name to a tuple of (dict name, casting fn)\n        expected_fields = {\n            \"oml:id\": (\"id\", int),\n            \"oml:alias\": (\"alias\", str),\n            \"oml:main_entity_type\": (\"main_entity_type\", str),\n            \"oml:benchmark_suite\": (\"benchmark_suite\", int),\n            \"oml:name\": (\"name\", str),\n            \"oml:status\": (\"status\", str),\n            \"oml:creation_date\": (\"creation_date\", str),\n            \"oml:creator\": (\"creator\", int),\n        }\n        study_id = int(study_[\"oml:id\"])\n        current_study = {}\n        for oml_field_name, (real_field_name, cast_fn) in expected_fields.items():\n            if oml_field_name in study_:\n                current_study[real_field_name] = cast_fn(study_[oml_field_name])\n        current_study[\"id\"] = int(current_study[\"id\"])\n        studies[study_id] = current_study\n\n    return pd.DataFrame.from_dict(studies, orient=\"index\")\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.attach_to_study","title":"attach_to_study","text":"<pre><code>attach_to_study(study_id: int, run_ids: list[int]) -&gt; int\n</code></pre> <p>Attaches a set of runs to a study.</p>"},{"location":"reference/study/functions/#openml.study.functions.attach_to_study--parameters","title":"Parameters","text":"<p>study_id : int     OpenML id of the study</p> list (int) <p>List of entities to link to the collection</p>"},{"location":"reference/study/functions/#openml.study.functions.attach_to_study--returns","title":"Returns","text":"<p>int     new size of the study (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def attach_to_study(study_id: int, run_ids: list[int]) -&gt; int:\n    \"\"\"Attaches a set of runs to a study.\n\n    Parameters\n    ----------\n    study_id : int\n        OpenML id of the study\n\n    run_ids : list (int)\n        List of entities to link to the collection\n\n    Returns\n    -------\n    int\n        new size of the study (in terms of explicitly linked entities)\n    \"\"\"\n    # Interestingly, there's no need to tell the server about the entity type, it knows by itself\n    result_xml = openml._api_calls._perform_api_call(\n        call=f\"study/{study_id}/attach\",\n        request_method=\"post\",\n        data={\"ids\": \",\".join(str(x) for x in run_ids)},\n    )\n    result = xmltodict.parse(result_xml)[\"oml:study_attach\"]\n    return int(result[\"oml:linked_entities\"])\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.attach_to_suite","title":"attach_to_suite","text":"<pre><code>attach_to_suite(suite_id: int, task_ids: list[int]) -&gt; int\n</code></pre> <p>Attaches a set of tasks to a benchmarking suite.</p>"},{"location":"reference/study/functions/#openml.study.functions.attach_to_suite--parameters","title":"Parameters","text":"<p>suite_id : int     OpenML id of the study</p> list (int) <p>List of entities to link to the collection</p>"},{"location":"reference/study/functions/#openml.study.functions.attach_to_suite--returns","title":"Returns","text":"<p>int     new size of the suite (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def attach_to_suite(suite_id: int, task_ids: list[int]) -&gt; int:\n    \"\"\"Attaches a set of tasks to a benchmarking suite.\n\n    Parameters\n    ----------\n    suite_id : int\n        OpenML id of the study\n\n    task_ids : list (int)\n        List of entities to link to the collection\n\n    Returns\n    -------\n    int\n        new size of the suite (in terms of explicitly linked entities)\n    \"\"\"\n    return attach_to_study(suite_id, task_ids)\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.create_benchmark_suite","title":"create_benchmark_suite","text":"<pre><code>create_benchmark_suite(name: str, description: str, task_ids: list[int], alias: str | None = None) -&gt; OpenMLBenchmarkSuite\n</code></pre> <p>Creates an OpenML benchmark suite (collection of entity types, where the tasks are the linked entity)</p>"},{"location":"reference/study/functions/#openml.study.functions.create_benchmark_suite--parameters","title":"Parameters","text":"<p>name : str     the name of the study (meta-info) description : str     brief description (meta-info) task_ids : list     a list of task ids associated with this study     more can be added later with <code>attach_to_suite</code>. alias : str (optional)     a string ID, unique on server (url-friendly)</p>"},{"location":"reference/study/functions/#openml.study.functions.create_benchmark_suite--returns","title":"Returns","text":"<p>OpenMLStudy     A local OpenML study object (call publish method to upload to server)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def create_benchmark_suite(\n    name: str,\n    description: str,\n    task_ids: list[int],\n    alias: str | None = None,\n) -&gt; OpenMLBenchmarkSuite:\n    \"\"\"\n    Creates an OpenML benchmark suite (collection of entity types, where\n    the tasks are the linked entity)\n\n    Parameters\n    ----------\n    name : str\n        the name of the study (meta-info)\n    description : str\n        brief description (meta-info)\n    task_ids : list\n        a list of task ids associated with this study\n        more can be added later with ``attach_to_suite``.\n    alias : str (optional)\n        a string ID, unique on server (url-friendly)\n\n    Returns\n    -------\n    OpenMLStudy\n        A local OpenML study object (call publish method to upload to server)\n    \"\"\"\n    return OpenMLBenchmarkSuite(\n        suite_id=None,\n        alias=alias,\n        name=name,\n        description=description,\n        status=None,\n        creation_date=None,\n        creator=None,\n        tags=None,\n        data=None,\n        tasks=task_ids,\n    )\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.create_study","title":"create_study","text":"<pre><code>create_study(name: str, description: str, run_ids: list[int] | None = None, alias: str | None = None, benchmark_suite: int | None = None) -&gt; OpenMLStudy\n</code></pre> <p>Creates an OpenML study (collection of data, tasks, flows, setups and run), where the runs are the main entity (collection consists of runs and all entities (flows, tasks, etc) that are related to these runs)</p>"},{"location":"reference/study/functions/#openml.study.functions.create_study--parameters","title":"Parameters","text":"<p>benchmark_suite : int (optional)     the benchmark suite (another study) upon which this study is ran. name : str     the name of the study (meta-info) description : str     brief description (meta-info) run_ids : list, optional     a list of run ids associated with this study,     these can also be added later with <code>attach_to_study</code>. alias : str (optional)     a string ID, unique on server (url-friendly) benchmark_suite: int (optional)     the ID of the suite for which this study contains run results</p>"},{"location":"reference/study/functions/#openml.study.functions.create_study--returns","title":"Returns","text":"<p>OpenMLStudy     A local OpenML study object (call publish method to upload to server)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def create_study(\n    name: str,\n    description: str,\n    run_ids: list[int] | None = None,\n    alias: str | None = None,\n    benchmark_suite: int | None = None,\n) -&gt; OpenMLStudy:\n    \"\"\"\n    Creates an OpenML study (collection of data, tasks, flows, setups and run),\n    where the runs are the main entity (collection consists of runs and all\n    entities (flows, tasks, etc) that are related to these runs)\n\n    Parameters\n    ----------\n    benchmark_suite : int (optional)\n        the benchmark suite (another study) upon which this study is ran.\n    name : str\n        the name of the study (meta-info)\n    description : str\n        brief description (meta-info)\n    run_ids : list, optional\n        a list of run ids associated with this study,\n        these can also be added later with ``attach_to_study``.\n    alias : str (optional)\n        a string ID, unique on server (url-friendly)\n    benchmark_suite: int (optional)\n        the ID of the suite for which this study contains run results\n\n    Returns\n    -------\n    OpenMLStudy\n        A local OpenML study object (call publish method to upload to server)\n    \"\"\"\n    return OpenMLStudy(\n        study_id=None,\n        alias=alias,\n        benchmark_suite=benchmark_suite,\n        name=name,\n        description=description,\n        status=None,\n        creation_date=None,\n        creator=None,\n        tags=None,\n        data=None,\n        tasks=None,\n        flows=None,\n        runs=run_ids if run_ids != [] else None,\n        setups=None,\n    )\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.delete_study","title":"delete_study","text":"<pre><code>delete_study(study_id: int) -&gt; bool\n</code></pre> <p>Deletes a study from the OpenML server.</p>"},{"location":"reference/study/functions/#openml.study.functions.delete_study--parameters","title":"Parameters","text":"<p>study_id : int     OpenML id of the study</p>"},{"location":"reference/study/functions/#openml.study.functions.delete_study--returns","title":"Returns","text":"<p>bool     True iff the deletion was successful. False otherwise</p> Source code in <code>openml/study/functions.py</code> <pre><code>def delete_study(study_id: int) -&gt; bool:\n    \"\"\"Deletes a study from the OpenML server.\n\n    Parameters\n    ----------\n    study_id : int\n        OpenML id of the study\n\n    Returns\n    -------\n    bool\n        True iff the deletion was successful. False otherwise\n    \"\"\"\n    return openml.utils._delete_entity(\"study\", study_id)\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.delete_suite","title":"delete_suite","text":"<pre><code>delete_suite(suite_id: int) -&gt; bool\n</code></pre> <p>Deletes a study from the OpenML server.</p>"},{"location":"reference/study/functions/#openml.study.functions.delete_suite--parameters","title":"Parameters","text":"<p>suite_id : int     OpenML id of the study</p>"},{"location":"reference/study/functions/#openml.study.functions.delete_suite--returns","title":"Returns","text":"<p>bool     True iff the deletion was successful. False otherwise</p> Source code in <code>openml/study/functions.py</code> <pre><code>def delete_suite(suite_id: int) -&gt; bool:\n    \"\"\"Deletes a study from the OpenML server.\n\n    Parameters\n    ----------\n    suite_id : int\n        OpenML id of the study\n\n    Returns\n    -------\n    bool\n        True iff the deletion was successful. False otherwise\n    \"\"\"\n    return delete_study(suite_id)\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.detach_from_study","title":"detach_from_study","text":"<pre><code>detach_from_study(study_id: int, run_ids: list[int]) -&gt; int\n</code></pre> <p>Detaches a set of run ids from a study.</p>"},{"location":"reference/study/functions/#openml.study.functions.detach_from_study--parameters","title":"Parameters","text":"<p>study_id : int     OpenML id of the study</p> list (int) <p>List of entities to unlink from the collection</p>"},{"location":"reference/study/functions/#openml.study.functions.detach_from_study--returns","title":"Returns","text":"<p>int     new size of the study (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def detach_from_study(study_id: int, run_ids: list[int]) -&gt; int:\n    \"\"\"Detaches a set of run ids from a study.\n\n    Parameters\n    ----------\n    study_id : int\n        OpenML id of the study\n\n    run_ids : list (int)\n        List of entities to unlink from the collection\n\n    Returns\n    -------\n    int\n        new size of the study (in terms of explicitly linked entities)\n    \"\"\"\n    # Interestingly, there's no need to tell the server about the entity type, it knows by itself\n    uri = \"study/%d/detach\" % study_id\n    post_variables = {\"ids\": \",\".join(str(x) for x in run_ids)}  # type: openml._api_calls.DATA_TYPE\n    result_xml = openml._api_calls._perform_api_call(\n        call=uri,\n        request_method=\"post\",\n        data=post_variables,\n    )\n    result = xmltodict.parse(result_xml)[\"oml:study_detach\"]\n    return int(result[\"oml:linked_entities\"])\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.detach_from_suite","title":"detach_from_suite","text":"<pre><code>detach_from_suite(suite_id: int, task_ids: list[int]) -&gt; int\n</code></pre> <p>Detaches a set of task ids from a suite.</p>"},{"location":"reference/study/functions/#openml.study.functions.detach_from_suite--parameters","title":"Parameters","text":"<p>suite_id : int     OpenML id of the study</p> list (int) <p>List of entities to unlink from the collection</p>"},{"location":"reference/study/functions/#openml.study.functions.detach_from_suite--returns","title":"Returns","text":"<p>int new size of the study (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def detach_from_suite(suite_id: int, task_ids: list[int]) -&gt; int:\n    \"\"\"Detaches a set of task ids from a suite.\n\n    Parameters\n    ----------\n    suite_id : int\n        OpenML id of the study\n\n    task_ids : list (int)\n        List of entities to unlink from the collection\n\n    Returns\n    -------\n    int\n    new size of the study (in terms of explicitly linked entities)\n    \"\"\"\n    return detach_from_study(suite_id, task_ids)\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.get_study","title":"get_study","text":"<pre><code>get_study(study_id: int | str, arg_for_backwards_compat: str | None = None) -&gt; OpenMLStudy\n</code></pre> <p>Retrieves all relevant information of an OpenML study from the server.</p>"},{"location":"reference/study/functions/#openml.study.functions.get_study--parameters","title":"Parameters","text":"<p>study id : int, str     study id (numeric or alias)</p> str, optional <p>The example given in arxiv.org/pdf/1708.03731.pdf uses an older version of the API which required specifying the type of study, i.e. tasks. We changed the implementation of studies since then and split them up into suites (collections of tasks) and studies (collections of runs) so this argument is no longer needed.</p>"},{"location":"reference/study/functions/#openml.study.functions.get_study--returns","title":"Returns","text":"<p>OpenMLStudy     The OpenML study object</p> Source code in <code>openml/study/functions.py</code> <pre><code>def get_study(\n    study_id: int | str,\n    arg_for_backwards_compat: str | None = None,  # noqa: ARG001\n) -&gt; OpenMLStudy:  # F401\n    \"\"\"\n    Retrieves all relevant information of an OpenML study from the server.\n\n    Parameters\n    ----------\n    study id : int, str\n        study id (numeric or alias)\n\n    arg_for_backwards_compat : str, optional\n        The example given in https://arxiv.org/pdf/1708.03731.pdf uses an older version of the\n        API which required specifying the type of study, i.e. tasks. We changed the\n        implementation of studies since then and split them up into suites (collections of tasks)\n        and studies (collections of runs) so this argument is no longer needed.\n\n    Returns\n    -------\n    OpenMLStudy\n        The OpenML study object\n    \"\"\"\n    if study_id == \"OpenML100\":\n        message = (\n            \"It looks like you are running code from the OpenML100 paper. It still works, but lots \"\n            \"of things have changed since then. Please use `get_suite('OpenML100')` instead.\"\n        )\n        warnings.warn(message, DeprecationWarning, stacklevel=2)\n        openml.config.logger.warning(message)\n        study = _get_study(study_id, entity_type=\"task\")\n        assert isinstance(study, OpenMLBenchmarkSuite)\n\n        return study  # type: ignore\n\n    study = _get_study(study_id, entity_type=\"run\")\n    assert isinstance(study, OpenMLStudy)\n    return study\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.get_suite","title":"get_suite","text":"<pre><code>get_suite(suite_id: int | str) -&gt; OpenMLBenchmarkSuite\n</code></pre> <p>Retrieves all relevant information of an OpenML benchmarking suite from the server.</p>"},{"location":"reference/study/functions/#openml.study.functions.get_suite--parameters","title":"Parameters","text":"<p>study id : int, str     study id (numeric or alias)</p>"},{"location":"reference/study/functions/#openml.study.functions.get_suite--returns","title":"Returns","text":"<p>OpenMLSuite     The OpenML suite object</p> Source code in <code>openml/study/functions.py</code> <pre><code>def get_suite(suite_id: int | str) -&gt; OpenMLBenchmarkSuite:\n    \"\"\"\n    Retrieves all relevant information of an OpenML benchmarking suite from the server.\n\n    Parameters\n    ----------\n    study id : int, str\n        study id (numeric or alias)\n\n    Returns\n    -------\n    OpenMLSuite\n        The OpenML suite object\n    \"\"\"\n    study = _get_study(suite_id, entity_type=\"task\")\n    assert isinstance(study, OpenMLBenchmarkSuite)\n\n    return study\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.list_studies","title":"list_studies","text":"<pre><code>list_studies(offset: int | None = None, size: int | None = None, status: str | None = None, uploader: list[str] | None = None, benchmark_suite: int | None = None) -&gt; DataFrame\n</code></pre> <p>Return a list of all studies which are on OpenML.</p>"},{"location":"reference/study/functions/#openml.study.functions.list_studies--parameters","title":"Parameters","text":"<p>offset : int, optional     The number of studies to skip, starting from the first. size : int, optional     The maximum number of studies to show. status : str, optional     Should be {active, in_preparation, deactivated, all}. By default active     studies are returned. uploader : list (int), optional     Result filter. Will only return studies created by these users. benchmark_suite : int, optional</p>"},{"location":"reference/study/functions/#openml.study.functions.list_studies--returns","title":"Returns","text":"<p>datasets : dataframe     Every dataset is represented by a dictionary containing     the following information:     - id     - alias (optional)     - name     - benchmark_suite (optional)     - status     - creator     - creation_date     If qualities are calculated for the dataset, some of     these are also returned.</p> Source code in <code>openml/study/functions.py</code> <pre><code>def list_studies(\n    offset: int | None = None,\n    size: int | None = None,\n    status: str | None = None,\n    uploader: list[str] | None = None,\n    benchmark_suite: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a list of all studies which are on OpenML.\n\n    Parameters\n    ----------\n    offset : int, optional\n        The number of studies to skip, starting from the first.\n    size : int, optional\n        The maximum number of studies to show.\n    status : str, optional\n        Should be {active, in_preparation, deactivated, all}. By default active\n        studies are returned.\n    uploader : list (int), optional\n        Result filter. Will only return studies created by these users.\n    benchmark_suite : int, optional\n\n    Returns\n    -------\n    datasets : dataframe\n        Every dataset is represented by a dictionary containing\n        the following information:\n        - id\n        - alias (optional)\n        - name\n        - benchmark_suite (optional)\n        - status\n        - creator\n        - creation_date\n        If qualities are calculated for the dataset, some of\n        these are also returned.\n    \"\"\"\n    listing_call = partial(\n        _list_studies,\n        main_entity_type=\"run\",\n        status=status,\n        uploader=uploader,\n        benchmark_suite=benchmark_suite,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.list_suites","title":"list_suites","text":"<pre><code>list_suites(offset: int | None = None, size: int | None = None, status: str | None = None, uploader: list[int] | None = None) -&gt; DataFrame\n</code></pre> <p>Return a list of all suites which are on OpenML.</p>"},{"location":"reference/study/functions/#openml.study.functions.list_suites--parameters","title":"Parameters","text":"<p>offset : int, optional     The number of suites to skip, starting from the first. size : int, optional     The maximum number of suites to show. status : str, optional     Should be {active, in_preparation, deactivated, all}. By default active     suites are returned. uploader : list (int), optional     Result filter. Will only return suites created by these users.</p>"},{"location":"reference/study/functions/#openml.study.functions.list_suites--returns","title":"Returns","text":"<p>datasets : dataframe     Every row is represented by a dictionary containing the following information:     - id     - alias (optional)     - name     - main_entity_type     - status     - creator     - creation_date</p> Source code in <code>openml/study/functions.py</code> <pre><code>def list_suites(\n    offset: int | None = None,\n    size: int | None = None,\n    status: str | None = None,\n    uploader: list[int] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a list of all suites which are on OpenML.\n\n    Parameters\n    ----------\n    offset : int, optional\n        The number of suites to skip, starting from the first.\n    size : int, optional\n        The maximum number of suites to show.\n    status : str, optional\n        Should be {active, in_preparation, deactivated, all}. By default active\n        suites are returned.\n    uploader : list (int), optional\n        Result filter. Will only return suites created by these users.\n\n    Returns\n    -------\n    datasets : dataframe\n        Every row is represented by a dictionary containing the following information:\n        - id\n        - alias (optional)\n        - name\n        - main_entity_type\n        - status\n        - creator\n        - creation_date\n    \"\"\"\n    listing_call = partial(\n        _list_studies,\n        main_entity_type=\"task\",\n        status=status,\n        uploader=uploader,\n    )\n    batches = openml.utils._list_all(listing_call, limit=size, offset=offset)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.update_study_status","title":"update_study_status","text":"<pre><code>update_study_status(study_id: int, status: str) -&gt; None\n</code></pre> <p>Updates the status of a study to either 'active' or 'deactivated'.</p>"},{"location":"reference/study/functions/#openml.study.functions.update_study_status--parameters","title":"Parameters","text":"<p>study_id : int     The data id of the dataset status : str,     'active' or 'deactivated'</p> Source code in <code>openml/study/functions.py</code> <pre><code>def update_study_status(study_id: int, status: str) -&gt; None:\n    \"\"\"\n    Updates the status of a study to either 'active' or 'deactivated'.\n\n    Parameters\n    ----------\n    study_id : int\n        The data id of the dataset\n    status : str,\n        'active' or 'deactivated'\n    \"\"\"\n    legal_status = {\"active\", \"deactivated\"}\n    if status not in legal_status:\n        raise ValueError(f\"Illegal status value. Legal values: {legal_status}\")\n    data = {\"study_id\": study_id, \"status\": status}  # type: openml._api_calls.DATA_TYPE\n    result_xml = openml._api_calls._perform_api_call(\"study/status/update\", \"post\", data=data)\n    result = xmltodict.parse(result_xml)\n    server_study_id = result[\"oml:study_status_update\"][\"oml:id\"]\n    server_status = result[\"oml:study_status_update\"][\"oml:status\"]\n    if status != server_status or int(study_id) != int(server_study_id):\n        # This should never happen\n        raise ValueError(\"Study id/status does not collide\")\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.update_suite_status","title":"update_suite_status","text":"<pre><code>update_suite_status(suite_id: int, status: str) -&gt; None\n</code></pre> <p>Updates the status of a study to either 'active' or 'deactivated'.</p>"},{"location":"reference/study/functions/#openml.study.functions.update_suite_status--parameters","title":"Parameters","text":"<p>suite_id : int     The data id of the dataset status : str,     'active' or 'deactivated'</p> Source code in <code>openml/study/functions.py</code> <pre><code>def update_suite_status(suite_id: int, status: str) -&gt; None:\n    \"\"\"\n    Updates the status of a study to either 'active' or 'deactivated'.\n\n    Parameters\n    ----------\n    suite_id : int\n        The data id of the dataset\n    status : str,\n        'active' or 'deactivated'\n    \"\"\"\n    return update_study_status(suite_id, status)\n</code></pre>"},{"location":"reference/study/study/","title":"study","text":""},{"location":"reference/study/study/#openml.study.study","title":"openml.study.study","text":""},{"location":"reference/study/study/#openml.study.study.BaseStudy","title":"BaseStudy","text":"<pre><code>BaseStudy(study_id: int | None, alias: str | None, main_entity_type: str, benchmark_suite: int | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None, flows: list[int] | None, runs: list[int] | None, setups: list[int] | None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>An OpenMLStudy represents the OpenML concept of a study. It contains the following information: name, id, description, creation date, creator id and a set of tags.</p> <p>According to this list of tags, the study object receives a list of OpenML object ids (datasets, flows, tasks and setups).</p> <p>Can be used to obtain all relevant information from a study at once.</p>"},{"location":"reference/study/study/#openml.study.study.BaseStudy--parameters","title":"Parameters","text":"<p>study_id : int     the study id alias : str (optional)     a string ID, unique on server (url-friendly) main_entity_type : str     the entity type (e.g., task, run) that is core in this study.     only entities of this type can be added explicitly benchmark_suite : int (optional)     the benchmark suite (another study) upon which this study is ran.     can only be active if main entity type is runs. name : str     the name of the study (meta-info) description : str     brief description (meta-info) status : str     Whether the study is in preparation, active or deactivated creation_date : str     date of creation (meta-info) creator : int     openml user id of the owner / creator tags : list(dict)     The list of tags shows which tags are associated with the study.     Each tag is a dict of (tag) name, window_start and write_access. data : list     a list of data ids associated with this study tasks : list     a list of task ids associated with this study flows : list     a list of flow ids associated with this study runs : list     a list of run ids associated with this study setups : list     a list of setup ids associated with this study</p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    study_id: int | None,\n    alias: str | None,\n    main_entity_type: str,\n    benchmark_suite: int | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n    flows: list[int] | None,\n    runs: list[int] | None,\n    setups: list[int] | None,\n):\n    self.study_id = study_id\n    self.alias = alias\n    self.main_entity_type = main_entity_type\n    self.benchmark_suite = benchmark_suite\n    self.name = name\n    self.description = description\n    self.status = status\n    self.creation_date = creation_date\n    self.creator = creator\n    self.tags = tags  # LEGACY. Can be removed soon\n    self.data = data\n    self.tasks = tasks\n    self.flows = flows\n    self.setups = setups\n    self.runs = runs\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite","title":"OpenMLBenchmarkSuite","text":"<pre><code>OpenMLBenchmarkSuite(suite_id: int | None, alias: str | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None)\n</code></pre> <p>               Bases: <code>BaseStudy</code></p> <p>An OpenMLBenchmarkSuite represents the OpenML concept of a suite (a collection of tasks).</p> <p>It contains the following information: name, id, description, creation date, creator id and the task ids.</p> <p>According to this list of task ids, the suite object receives a list of OpenML object ids (datasets).</p>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite--parameters","title":"Parameters","text":"<p>suite_id : int     the study id alias : str (optional)     a string ID, unique on server (url-friendly) main_entity_type : str     the entity type (e.g., task, run) that is core in this study.     only entities of this type can be added explicitly name : str     the name of the study (meta-info) description : str     brief description (meta-info) status : str     Whether the study is in preparation, active or deactivated creation_date : str     date of creation (meta-info) creator : int     openml user id of the owner / creator tags : list(dict)     The list of tags shows which tags are associated with the study.     Each tag is a dict of (tag) name, window_start and write_access. data : list     a list of data ids associated with this study tasks : list     a list of task ids associated with this study</p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    suite_id: int | None,\n    alias: str | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n):\n    super().__init__(\n        study_id=suite_id,\n        alias=alias,\n        main_entity_type=\"task\",\n        benchmark_suite=None,\n        name=name,\n        description=description,\n        status=status,\n        creation_date=creation_date,\n        creator=creator,\n        tags=tags,\n        data=data,\n        tasks=tasks,\n        flows=None,\n        runs=None,\n        setups=None,\n    )\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy","title":"OpenMLStudy","text":"<pre><code>OpenMLStudy(study_id: int | None, alias: str | None, benchmark_suite: int | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None, flows: list[int] | None, runs: list[int] | None, setups: list[int] | None)\n</code></pre> <p>               Bases: <code>BaseStudy</code></p> <p>An OpenMLStudy represents the OpenML concept of a study (a collection of runs).</p> <p>It contains the following information: name, id, description, creation date, creator id and a list of run ids.</p> <p>According to this list of run ids, the study object receives a list of OpenML object ids (datasets, flows, tasks and setups).</p>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy--parameters","title":"Parameters","text":"<p>study_id : int     the study id alias : str (optional)     a string ID, unique on server (url-friendly) benchmark_suite : int (optional)     the benchmark suite (another study) upon which this study is ran.     can only be active if main entity type is runs. name : str     the name of the study (meta-info) description : str     brief description (meta-info) status : str     Whether the study is in preparation, active or deactivated creation_date : str     date of creation (meta-info) creator : int     openml user id of the owner / creator tags : list(dict)     The list of tags shows which tags are associated with the study.     Each tag is a dict of (tag) name, window_start and write_access. data : list     a list of data ids associated with this study tasks : list     a list of task ids associated with this study flows : list     a list of flow ids associated with this study runs : list     a list of run ids associated with this study setups : list     a list of setup ids associated with this study</p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    study_id: int | None,\n    alias: str | None,\n    benchmark_suite: int | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n    flows: list[int] | None,\n    runs: list[int] | None,\n    setups: list[int] | None,\n):\n    super().__init__(\n        study_id=study_id,\n        alias=alias,\n        main_entity_type=\"run\",\n        benchmark_suite=benchmark_suite,\n        name=name,\n        description=description,\n        status=status,\n        creation_date=creation_date,\n        creator=creator,\n        tags=tags,\n        data=data,\n        tasks=tasks,\n        flows=flows,\n        runs=runs,\n        setups=setups,\n    )\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/","title":"tasks","text":""},{"location":"reference/tasks/#openml.tasks","title":"openml.tasks","text":""},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask","title":"OpenMLClassificationTask","text":"<pre><code>OpenMLClassificationTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None, task_id: int | None = None, class_labels: list[str] | None = None, cost_matrix: ndarray | None = None)\n</code></pre> <p>               Bases: <code>OpenMLSupervisedTask</code></p> <p>OpenML Classification object.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     ID of the Classification task type. task_type : str     Name of the Classification task type. data_set_id : int     ID of the OpenML dataset associated with the Classification task. target_name : str     Name of the target variable. estimation_procedure_id : int, default=None     ID of the estimation procedure for the Classification task. estimation_procedure_type : str, default=None     Type of the estimation procedure. estimation_parameters : dict, default=None     Estimation parameters for the Classification task. evaluation_measure : str, default=None     Name of the evaluation measure. data_splits_url : str, default=None     URL of the data splits for the Classification task. task_id : Union[int, None]     ID of the Classification task (if it already exists on OpenML). class_labels : List of str, default=None     A list of class labels (for classification tasks). cost_matrix : array, default=None     A cost matrix (for classification tasks).</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    class_labels: list[str] | None = None,\n    cost_matrix: np.ndarray | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n    )\n    self.class_labels = class_labels\n    self.cost_matrix = cost_matrix\n\n    if cost_matrix is not None:\n        raise NotImplementedError(\"Costmatrix\")\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.get_X_and_y--returns","title":"Returns","text":"<p>tuple - X and y</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask","title":"OpenMLClusteringTask","text":"<pre><code>OpenMLClusteringTask(task_type_id: TaskType, task_type: str, data_set_id: int, estimation_procedure_id: int = 17, task_id: int | None = None, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, evaluation_measure: str | None = None, target_name: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLTask</code></p> <p>OpenML Clustering object.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     Task type ID of the OpenML clustering task. task_type : str     Task type of the OpenML clustering task. data_set_id : int     ID of the OpenML dataset used in clustering the task. estimation_procedure_id : int, default=None     ID of the OpenML estimation procedure. task_id : Union[int, None]     ID of the OpenML clustering task. estimation_procedure_type : str, default=None     Type of the OpenML estimation procedure used in the clustering task. estimation_parameters : dict, default=None     Parameters used by the OpenML estimation procedure. data_splits_url : str, default=None     URL of the OpenML data splits for the clustering task. evaluation_measure : str, default=None     Evaluation measure used in the clustering task. target_name : str, default=None     Name of the target feature (class) that is not part of the     feature set for the clustering task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    estimation_procedure_id: int = 17,\n    task_id: int | None = None,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    evaluation_measure: str | None = None,\n    target_name: str | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        evaluation_measure=evaluation_measure,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        data_splits_url=data_splits_url,\n    )\n\n    self.target_name = target_name\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.get_X","title":"get_X","text":"<pre><code>get_X() -&gt; DataFrame\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.get_X--returns","title":"Returns","text":"<p>The X data as a dataframe</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X(self) -&gt; pd.DataFrame:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    The X data as a dataframe\n    \"\"\"\n    dataset = self.get_dataset()\n    data, *_ = dataset.get_data(target=None)\n    return data\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask","title":"OpenMLLearningCurveTask","text":"<pre><code>OpenMLLearningCurveTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 13, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, task_id: int | None = None, evaluation_measure: str | None = None, class_labels: list[str] | None = None, cost_matrix: ndarray | None = None)\n</code></pre> <p>               Bases: <code>OpenMLClassificationTask</code></p> <p>OpenML Learning Curve object.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     ID of the Learning Curve task. task_type : str     Name of the Learning Curve task. data_set_id : int     ID of the dataset that this task is associated with. target_name : str     Name of the target feature in the dataset. estimation_procedure_id : int, default=None     ID of the estimation procedure to use for evaluating models. estimation_procedure_type : str, default=None     Type of the estimation procedure. estimation_parameters : dict, default=None     Additional parameters for the estimation procedure. data_splits_url : str, default=None     URL of the file containing the data splits for Learning Curve task. task_id : Union[int, None]     ID of the Learning Curve task. evaluation_measure : str, default=None     Name of the evaluation measure to use for evaluating models. class_labels : list of str, default=None     Class labels for Learning Curve tasks. cost_matrix : numpy array, default=None     Cost matrix for Learning Curve tasks.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 13,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    evaluation_measure: str | None = None,\n    class_labels: list[str] | None = None,\n    cost_matrix: np.ndarray | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n        class_labels=class_labels,\n        cost_matrix=cost_matrix,\n    )\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.get_X_and_y--returns","title":"Returns","text":"<p>tuple - X and y</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask","title":"OpenMLRegressionTask","text":"<pre><code>OpenMLRegressionTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 7, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, task_id: int | None = None, evaluation_measure: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLSupervisedTask</code></p> <p>OpenML Regression object.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     Task type ID of the OpenML Regression task. task_type : str     Task type of the OpenML Regression task. data_set_id : int     ID of the OpenML dataset. target_name : str     Name of the target feature used in the Regression task. estimation_procedure_id : int, default=None     ID of the OpenML estimation procedure. estimation_procedure_type : str, default=None     Type of the OpenML estimation procedure. estimation_parameters : dict, default=None     Parameters used by the OpenML estimation procedure. data_splits_url : str, default=None     URL of the OpenML data splits for the Regression task. task_id : Union[int, None]     ID of the OpenML Regression task. evaluation_measure : str, default=None     Evaluation measure used in the Regression task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 7,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    evaluation_measure: str | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n    )\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.get_X_and_y--returns","title":"Returns","text":"<p>tuple - X and y</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSplit","title":"OpenMLSplit","text":"<pre><code>OpenMLSplit(name: int | str, description: str, split: dict[int, dict[int, dict[int, tuple[ndarray, ndarray]]]])\n</code></pre> <p>OpenML Split object.</p> <p>This class manages train-test splits for a dataset across multiple repetitions, folds, and samples.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSplit--parameters","title":"Parameters","text":"<p>name : int or str     The name or ID of the split. description : str     A description of the split. split : dict     A dictionary containing the splits organized by repetition, fold,     and sample.</p> Source code in <code>openml/tasks/split.py</code> <pre><code>def __init__(\n    self,\n    name: int | str,\n    description: str,\n    split: dict[int, dict[int, dict[int, tuple[np.ndarray, np.ndarray]]]],\n):\n    self.description = description\n    self.name = name\n    self.split: dict[int, dict[int, dict[int, tuple[np.ndarray, np.ndarray]]]] = {}\n\n    # Add splits according to repetition\n    for repetition in split:\n        _rep = int(repetition)\n        self.split[_rep] = OrderedDict()\n        for fold in split[_rep]:\n            self.split[_rep][fold] = OrderedDict()\n            for sample in split[_rep][fold]:\n                self.split[_rep][fold][sample] = split[_rep][fold][sample]\n\n    self.repeats = len(self.split)\n\n    # TODO(eddiebergman): Better error message\n    if any(len(self.split[0]) != len(self.split[i]) for i in range(self.repeats)):\n        raise ValueError(\"\")\n\n    self.folds = len(self.split[0])\n    self.samples = len(self.split[0][0])\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSplit.get","title":"get","text":"<pre><code>get(repeat: int = 0, fold: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns the specified data split from the CrossValidationSplit object.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSplit.get--parameters","title":"Parameters","text":"<p>repeat : int     Index of the repeat to retrieve. fold : int     Index of the fold to retrieve. sample : int     Index of the sample to retrieve.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSplit.get--returns","title":"Returns","text":"<p>numpy.ndarray     The data split for the specified repeat, fold, and sample.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSplit.get--raises","title":"Raises","text":"<p>ValueError     If the specified repeat, fold, or sample is not known.</p> Source code in <code>openml/tasks/split.py</code> <pre><code>def get(self, repeat: int = 0, fold: int = 0, sample: int = 0) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns the specified data split from the CrossValidationSplit object.\n\n    Parameters\n    ----------\n    repeat : int\n        Index of the repeat to retrieve.\n    fold : int\n        Index of the fold to retrieve.\n    sample : int\n        Index of the sample to retrieve.\n\n    Returns\n    -------\n    numpy.ndarray\n        The data split for the specified repeat, fold, and sample.\n\n    Raises\n    ------\n    ValueError\n        If the specified repeat, fold, or sample is not known.\n    \"\"\"\n    if repeat not in self.split:\n        raise ValueError(f\"Repeat {repeat!s} not known\")\n    if fold not in self.split[repeat]:\n        raise ValueError(f\"Fold {fold!s} not known\")\n    if sample not in self.split[repeat][fold]:\n        raise ValueError(f\"Sample {sample!s} not known\")\n    return self.split[repeat][fold][sample]\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask","title":"OpenMLSupervisedTask","text":"<pre><code>OpenMLSupervisedTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None, task_id: int | None = None)\n</code></pre> <p>               Bases: <code>OpenMLTask</code>, <code>ABC</code></p> <p>OpenML Supervised Classification object.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     ID of the task type. task_type : str     Name of the task type. data_set_id : int     ID of the OpenML dataset associated with the task. target_name : str     Name of the target feature (the class variable). estimation_procedure_id : int, default=None     ID of the estimation procedure for the task. estimation_procedure_type : str, default=None     Type of the estimation procedure for the task. estimation_parameters : dict, default=None     Estimation parameters for the task. evaluation_measure : str, default=None     Name of the evaluation measure for the task. data_splits_url : str, default=None     URL of the data splits for the task. task_id: Union[int, None]     Refers to the unique identifier of task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        data_splits_url=data_splits_url,\n    )\n\n    self.target_name = target_name\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.get_X_and_y--returns","title":"Returns","text":"<p>tuple - X and y</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask","title":"OpenMLTask","text":"<pre><code>OpenMLTask(task_id: int | None, task_type_id: TaskType, task_type: str, data_set_id: int, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Task object.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask--parameters","title":"Parameters","text":"<p>task_id: Union[int, None]     Refers to the unique identifier of OpenML task. task_type_id: TaskType     Refers to the type of OpenML task. task_type: str     Refers to the OpenML task. data_set_id: int     Refers to the data. estimation_procedure_id: int     Refers to the type of estimates used. estimation_procedure_type: str, default=None     Refers to the type of estimation procedure used for the OpenML task. estimation_parameters: [Dict[str, str]], default=None     Estimation parameters used for the OpenML task. evaluation_measure: str, default=None     Refers to the evaluation measure. data_splits_url: str, default=None     Refers to the URL of the data splits used for the OpenML task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_id: int | None,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n):\n    self.task_id = int(task_id) if task_id is not None else None\n    self.task_type_id = task_type_id\n    self.task_type = task_type\n    self.dataset_id = int(data_set_id)\n    self.evaluation_measure = evaluation_measure\n    self.estimation_procedure: _EstimationProcedure = {\n        \"type\": estimation_procedure_type,\n        \"parameters\": estimation_parameters,\n        \"data_splits_url\": data_splits_url,\n    }\n    self.estimation_procedure_id = estimation_procedure_id\n    self.split: OpenMLSplit | None = None\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.TaskType","title":"TaskType","text":"<p>               Bases: <code>Enum</code></p> <p>Possible task types as defined in OpenML.</p>"},{"location":"reference/tasks/#openml.tasks.create_task","title":"create_task","text":"<pre><code>create_task(task_type: TaskType, dataset_id: int, estimation_procedure_id: int, target_name: str | None = None, evaluation_measure: str | None = None, **kwargs: Any) -&gt; OpenMLClassificationTask | OpenMLRegressionTask | OpenMLLearningCurveTask | OpenMLClusteringTask\n</code></pre> <p>Create a task based on different given attributes.</p> <p>Builds a task object with the function arguments as attributes. The type of the task object built is determined from the task type id. More information on how the arguments (task attributes), relate to the different possible tasks can be found in the individual task objects at the openml.tasks.task module.</p>"},{"location":"reference/tasks/#openml.tasks.create_task--parameters","title":"Parameters","text":"<p>task_type : TaskType     Id of the task type. dataset_id : int     The id of the dataset for the task. target_name : str, optional     The name of the feature used as a target.     At the moment, only optional for the clustering tasks. estimation_procedure_id : int     The id of the estimation procedure. evaluation_measure : str, optional     The name of the evaluation measure. kwargs : dict, optional     Other task attributes that are not mandatory     for task upload.</p>"},{"location":"reference/tasks/#openml.tasks.create_task--returns","title":"Returns","text":"<p>OpenMLClassificationTask, OpenMLRegressionTask, OpenMLLearningCurveTask, OpenMLClusteringTask</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def create_task(\n    task_type: TaskType,\n    dataset_id: int,\n    estimation_procedure_id: int,\n    target_name: str | None = None,\n    evaluation_measure: str | None = None,\n    **kwargs: Any,\n) -&gt; (\n    OpenMLClassificationTask | OpenMLRegressionTask | OpenMLLearningCurveTask | OpenMLClusteringTask\n):\n    \"\"\"Create a task based on different given attributes.\n\n    Builds a task object with the function arguments as\n    attributes. The type of the task object built is\n    determined from the task type id.\n    More information on how the arguments (task attributes),\n    relate to the different possible tasks can be found in\n    the individual task objects at the openml.tasks.task\n    module.\n\n    Parameters\n    ----------\n    task_type : TaskType\n        Id of the task type.\n    dataset_id : int\n        The id of the dataset for the task.\n    target_name : str, optional\n        The name of the feature used as a target.\n        At the moment, only optional for the clustering tasks.\n    estimation_procedure_id : int\n        The id of the estimation procedure.\n    evaluation_measure : str, optional\n        The name of the evaluation measure.\n    kwargs : dict, optional\n        Other task attributes that are not mandatory\n        for task upload.\n\n    Returns\n    -------\n    OpenMLClassificationTask, OpenMLRegressionTask,\n    OpenMLLearningCurveTask, OpenMLClusteringTask\n    \"\"\"\n    if task_type == TaskType.CLUSTERING:\n        task_cls = OpenMLClusteringTask\n    elif task_type == TaskType.LEARNING_CURVE:\n        task_cls = OpenMLLearningCurveTask  # type: ignore\n    elif task_type == TaskType.SUPERVISED_CLASSIFICATION:\n        task_cls = OpenMLClassificationTask  # type: ignore\n    elif task_type == TaskType.SUPERVISED_REGRESSION:\n        task_cls = OpenMLRegressionTask  # type: ignore\n    else:\n        raise NotImplementedError(f\"Task type {task_type:d} not supported.\")\n\n    return task_cls(\n        task_type_id=task_type,\n        task_type=\"None\",  # TODO: refactor to get task type string from ID.\n        data_set_id=dataset_id,\n        target_name=target_name,  # type: ignore\n        estimation_procedure_id=estimation_procedure_id,\n        evaluation_measure=evaluation_measure,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.delete_task","title":"delete_task","text":"<pre><code>delete_task(task_id: int) -&gt; bool\n</code></pre> <p>Delete task with id <code>task_id</code> from the OpenML server.</p> <p>You can only delete tasks which you created and have no runs associated with them.</p>"},{"location":"reference/tasks/#openml.tasks.delete_task--parameters","title":"Parameters","text":"<p>task_id : int     OpenML id of the task</p>"},{"location":"reference/tasks/#openml.tasks.delete_task--returns","title":"Returns","text":"<p>bool     True if the deletion was successful. False otherwise.</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def delete_task(task_id: int) -&gt; bool:\n    \"\"\"Delete task with id `task_id` from the OpenML server.\n\n    You can only delete tasks which you created and have\n    no runs associated with them.\n\n    Parameters\n    ----------\n    task_id : int\n        OpenML id of the task\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"task\", task_id)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.get_task","title":"get_task","text":"<pre><code>get_task(task_id: int, download_splits: bool = False, **get_dataset_kwargs: Any) -&gt; OpenMLTask\n</code></pre> <p>Download OpenML task for a given task ID.</p> <p>Downloads the task representation.</p> <p>Use the <code>download_splits</code> parameter to control whether the splits are downloaded. Moreover, you may pass additional parameter (args or kwargs) that are passed to :meth:<code>openml.datasets.get_dataset</code>.</p>"},{"location":"reference/tasks/#openml.tasks.get_task--parameters","title":"Parameters","text":"<p>task_id : int     The OpenML task id of the task to download. download_splits: bool (default=False)     Whether to download the splits as well. get_dataset_kwargs :     Args and kwargs can be used pass optional parameters to :meth:<code>openml.datasets.get_dataset</code>.</p>"},{"location":"reference/tasks/#openml.tasks.get_task--returns","title":"Returns","text":"<p>task: OpenMLTask</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_task(\n    task_id: int,\n    download_splits: bool = False,  # noqa: FBT001, FBT002\n    **get_dataset_kwargs: Any,\n) -&gt; OpenMLTask:\n    \"\"\"Download OpenML task for a given task ID.\n\n    Downloads the task representation.\n\n    Use the `download_splits` parameter to control whether the splits are downloaded.\n    Moreover, you may pass additional parameter (args or kwargs) that are passed to\n    :meth:`openml.datasets.get_dataset`.\n\n    Parameters\n    ----------\n    task_id : int\n        The OpenML task id of the task to download.\n    download_splits: bool (default=False)\n        Whether to download the splits as well.\n    get_dataset_kwargs :\n        Args and kwargs can be used pass optional parameters to :meth:`openml.datasets.get_dataset`.\n\n    Returns\n    -------\n    task: OpenMLTask\n    \"\"\"\n    if not isinstance(task_id, int):\n        raise TypeError(f\"Task id should be integer, is {type(task_id)}\")\n\n    tid_cache_dir = openml.utils._create_cache_directory_for_id(TASKS_CACHE_DIR_NAME, task_id)\n\n    try:\n        task = _get_task_description(task_id)\n        dataset = get_dataset(task.dataset_id, **get_dataset_kwargs)\n        # List of class labels available in dataset description\n        # Including class labels as part of task meta data handles\n        #   the case where data download was initially disabled\n        if isinstance(task, (OpenMLClassificationTask, OpenMLLearningCurveTask)):\n            task.class_labels = dataset.retrieve_class_labels(task.target_name)\n        # Clustering tasks do not have class labels\n        # and do not offer download_split\n        if download_splits and isinstance(task, OpenMLSupervisedTask):\n            task.download_split()\n    except Exception as e:\n        openml.utils._remove_cache_dir_for_id(TASKS_CACHE_DIR_NAME, tid_cache_dir)\n        raise e\n\n    return task\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.get_tasks","title":"get_tasks","text":"<pre><code>get_tasks(task_ids: list[int], download_data: bool | None = None, download_qualities: bool | None = None) -&gt; list[OpenMLTask]\n</code></pre> <p>Download tasks.</p> <p>This function iterates :meth:<code>openml.tasks.get_task</code>.</p>"},{"location":"reference/tasks/#openml.tasks.get_tasks--parameters","title":"Parameters","text":"<p>task_ids : List[int]     A list of task ids to download. download_data : bool (default = True)     Option to trigger download of data along with the meta data. download_qualities : bool (default=True)     Option to download 'qualities' meta-data in addition to the minimal dataset description.</p>"},{"location":"reference/tasks/#openml.tasks.get_tasks--returns","title":"Returns","text":"<p>list</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def get_tasks(\n    task_ids: list[int],\n    download_data: bool | None = None,\n    download_qualities: bool | None = None,\n) -&gt; list[OpenMLTask]:\n    \"\"\"Download tasks.\n\n    This function iterates :meth:`openml.tasks.get_task`.\n\n    Parameters\n    ----------\n    task_ids : List[int]\n        A list of task ids to download.\n    download_data : bool (default = True)\n        Option to trigger download of data along with the meta data.\n    download_qualities : bool (default=True)\n        Option to download 'qualities' meta-data in addition to the minimal dataset description.\n\n    Returns\n    -------\n    list\n    \"\"\"\n    if download_data is None:\n        warnings.warn(\n            \"`download_data` will default to False starting in 0.16. \"\n            \"Please set `download_data` explicitly to suppress this warning.\",\n            stacklevel=1,\n        )\n        download_data = True\n\n    if download_qualities is None:\n        warnings.warn(\n            \"`download_qualities` will default to False starting in 0.16. \"\n            \"Please set `download_qualities` explicitly to suppress this warning.\",\n            stacklevel=1,\n        )\n        download_qualities = True\n\n    tasks = []\n    for task_id in task_ids:\n        tasks.append(\n            get_task(task_id, download_data=download_data, download_qualities=download_qualities)\n        )\n    return tasks\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.list_tasks","title":"list_tasks","text":"<pre><code>list_tasks(task_type: TaskType | None = None, offset: int | None = None, size: int | None = None, tag: str | None = None, data_tag: str | None = None, status: str | None = None, data_name: str | None = None, data_id: int | None = None, number_instances: int | None = None, number_features: int | None = None, number_classes: int | None = None, number_missing_values: int | None = None) -&gt; DataFrame\n</code></pre> <p>Return a number of tasks having the given tag and task_type</p>"},{"location":"reference/tasks/#openml.tasks.list_tasks--parameters","title":"Parameters","text":"<p>Filter task_type is separated from the other filters because it is used as task_type in the task description, but it is named type when used as a filter in list tasks call. offset : int, optional     the number of tasks to skip, starting from the first task_type : TaskType, optional     Refers to the type of task. size : int, optional     the maximum number of tasks to show tag : str, optional     the tag to include data_tag : str, optional     the tag of the dataset data_id : int, optional status : str, optional data_name : str, optional number_instances : int, optional number_features : int, optional number_classes : int, optional number_missing_values : int, optional</p>"},{"location":"reference/tasks/#openml.tasks.list_tasks--returns","title":"Returns","text":"<p>dataframe     All tasks having the given task_type and the give tag. Every task is     represented by a row in the data frame containing the following information     as columns: task id, dataset id, task_type and status. If qualities are     calculated for the associated dataset, some of these are also returned.</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def list_tasks(  # noqa: PLR0913\n    task_type: TaskType | None = None,\n    offset: int | None = None,\n    size: int | None = None,\n    tag: str | None = None,\n    data_tag: str | None = None,\n    status: str | None = None,\n    data_name: str | None = None,\n    data_id: int | None = None,\n    number_instances: int | None = None,\n    number_features: int | None = None,\n    number_classes: int | None = None,\n    number_missing_values: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a number of tasks having the given tag and task_type\n\n    Parameters\n    ----------\n    Filter task_type is separated from the other filters because\n    it is used as task_type in the task description, but it is named\n    type when used as a filter in list tasks call.\n    offset : int, optional\n        the number of tasks to skip, starting from the first\n    task_type : TaskType, optional\n        Refers to the type of task.\n    size : int, optional\n        the maximum number of tasks to show\n    tag : str, optional\n        the tag to include\n    data_tag : str, optional\n        the tag of the dataset\n    data_id : int, optional\n    status : str, optional\n    data_name : str, optional\n    number_instances : int, optional\n    number_features : int, optional\n    number_classes : int, optional\n    number_missing_values : int, optional\n\n    Returns\n    -------\n    dataframe\n        All tasks having the given task_type and the give tag. Every task is\n        represented by a row in the data frame containing the following information\n        as columns: task id, dataset id, task_type and status. If qualities are\n        calculated for the associated dataset, some of these are also returned.\n    \"\"\"\n    listing_call = partial(\n        _list_tasks,\n        task_type=task_type,\n        tag=tag,\n        data_tag=data_tag,\n        status=status,\n        data_id=data_id,\n        data_name=data_name,\n        number_instances=number_instances,\n        number_features=number_features,\n        number_classes=number_classes,\n        number_missing_values=number_missing_values,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/tasks/functions/","title":"functions","text":""},{"location":"reference/tasks/functions/#openml.tasks.functions","title":"openml.tasks.functions","text":""},{"location":"reference/tasks/functions/#openml.tasks.functions.__list_tasks","title":"__list_tasks","text":"<pre><code>__list_tasks(api_call: str) -&gt; DataFrame\n</code></pre> <p>Returns a Pandas DataFrame with information about OpenML tasks.</p>"},{"location":"reference/tasks/functions/#openml.tasks.functions.__list_tasks--parameters","title":"Parameters","text":"<p>api_call : str     The API call specifying which tasks to return.</p>"},{"location":"reference/tasks/functions/#openml.tasks.functions.__list_tasks--returns","title":"Returns","text":"<pre><code>A Pandas DataFrame with information about OpenML tasks.\n</code></pre>"},{"location":"reference/tasks/functions/#openml.tasks.functions.__list_tasks--raises","title":"Raises","text":"<p>ValueError     If the XML returned by the OpenML API does not contain 'oml:tasks', '@xmlns:oml',     or has an incorrect value for '@xmlns:oml'. KeyError     If an invalid key is found in the XML for a task.</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def __list_tasks(api_call: str) -&gt; pd.DataFrame:  # noqa: C901, PLR0912\n    \"\"\"Returns a Pandas DataFrame with information about OpenML tasks.\n\n    Parameters\n    ----------\n    api_call : str\n        The API call specifying which tasks to return.\n\n    Returns\n    -------\n        A Pandas DataFrame with information about OpenML tasks.\n\n    Raises\n    ------\n    ValueError\n        If the XML returned by the OpenML API does not contain 'oml:tasks', '@xmlns:oml',\n        or has an incorrect value for '@xmlns:oml'.\n    KeyError\n        If an invalid key is found in the XML for a task.\n    \"\"\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    tasks_dict = xmltodict.parse(xml_string, force_list=(\"oml:task\", \"oml:input\"))\n    # Minimalistic check if the XML is useful\n    if \"oml:tasks\" not in tasks_dict:\n        raise ValueError(f'Error in return XML, does not contain \"oml:runs\": {tasks_dict}')\n\n    if \"@xmlns:oml\" not in tasks_dict[\"oml:tasks\"]:\n        raise ValueError(\n            f'Error in return XML, does not contain \"oml:runs\"/@xmlns:oml: {tasks_dict}'\n        )\n\n    if tasks_dict[\"oml:tasks\"][\"@xmlns:oml\"] != \"http://openml.org/openml\":\n        raise ValueError(\n            \"Error in return XML, value of  \"\n            '\"oml:runs\"/@xmlns:oml is not '\n            f'\"http://openml.org/openml\": {tasks_dict!s}',\n        )\n\n    assert isinstance(tasks_dict[\"oml:tasks\"][\"oml:task\"], list), type(tasks_dict[\"oml:tasks\"])\n\n    tasks = {}\n    procs = _get_estimation_procedure_list()\n    proc_dict = {x[\"id\"]: x for x in procs}\n\n    for task_ in tasks_dict[\"oml:tasks\"][\"oml:task\"]:\n        tid = None\n        try:\n            tid = int(task_[\"oml:task_id\"])\n            task_type_int = int(task_[\"oml:task_type_id\"])\n            try:\n                task_type_id = TaskType(task_type_int)\n            except ValueError as e:\n                warnings.warn(\n                    f\"Could not create task type id for {task_type_int} due to error {e}\",\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n                continue\n\n            task = {\n                \"tid\": tid,\n                \"ttid\": task_type_id,\n                \"did\": int(task_[\"oml:did\"]),\n                \"name\": task_[\"oml:name\"],\n                \"task_type\": task_[\"oml:task_type\"],\n                \"status\": task_[\"oml:status\"],\n            }\n\n            # Other task inputs\n            for _input in task_.get(\"oml:input\", []):\n                if _input[\"@name\"] == \"estimation_procedure\":\n                    task[_input[\"@name\"]] = proc_dict[int(_input[\"#text\"])][\"name\"]\n                else:\n                    value = _input.get(\"#text\")\n                    task[_input[\"@name\"]] = value\n\n            # The number of qualities can range from 0 to infinity\n            for quality in task_.get(\"oml:quality\", []):\n                if \"#text\" not in quality:\n                    quality_value = 0.0\n                else:\n                    quality[\"#text\"] = float(quality[\"#text\"])\n                    if abs(int(quality[\"#text\"]) - quality[\"#text\"]) &lt; 0.0000001:\n                        quality[\"#text\"] = int(quality[\"#text\"])\n                    quality_value = quality[\"#text\"]\n                task[quality[\"@name\"]] = quality_value\n            tasks[tid] = task\n        except KeyError as e:\n            if tid is not None:\n                warnings.warn(\n                    \"Invalid xml for task %d: %s\\nFrom %s\" % (tid, e, task_),\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n            else:\n                warnings.warn(f\"Could not find key {e} in {task_}!\", RuntimeWarning, stacklevel=2)\n\n    return pd.DataFrame.from_dict(tasks, orient=\"index\")\n</code></pre>"},{"location":"reference/tasks/functions/#openml.tasks.functions.create_task","title":"create_task","text":"<pre><code>create_task(task_type: TaskType, dataset_id: int, estimation_procedure_id: int, target_name: str | None = None, evaluation_measure: str | None = None, **kwargs: Any) -&gt; OpenMLClassificationTask | OpenMLRegressionTask | OpenMLLearningCurveTask | OpenMLClusteringTask\n</code></pre> <p>Create a task based on different given attributes.</p> <p>Builds a task object with the function arguments as attributes. The type of the task object built is determined from the task type id. More information on how the arguments (task attributes), relate to the different possible tasks can be found in the individual task objects at the openml.tasks.task module.</p>"},{"location":"reference/tasks/functions/#openml.tasks.functions.create_task--parameters","title":"Parameters","text":"<p>task_type : TaskType     Id of the task type. dataset_id : int     The id of the dataset for the task. target_name : str, optional     The name of the feature used as a target.     At the moment, only optional for the clustering tasks. estimation_procedure_id : int     The id of the estimation procedure. evaluation_measure : str, optional     The name of the evaluation measure. kwargs : dict, optional     Other task attributes that are not mandatory     for task upload.</p>"},{"location":"reference/tasks/functions/#openml.tasks.functions.create_task--returns","title":"Returns","text":"<p>OpenMLClassificationTask, OpenMLRegressionTask, OpenMLLearningCurveTask, OpenMLClusteringTask</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def create_task(\n    task_type: TaskType,\n    dataset_id: int,\n    estimation_procedure_id: int,\n    target_name: str | None = None,\n    evaluation_measure: str | None = None,\n    **kwargs: Any,\n) -&gt; (\n    OpenMLClassificationTask | OpenMLRegressionTask | OpenMLLearningCurveTask | OpenMLClusteringTask\n):\n    \"\"\"Create a task based on different given attributes.\n\n    Builds a task object with the function arguments as\n    attributes. The type of the task object built is\n    determined from the task type id.\n    More information on how the arguments (task attributes),\n    relate to the different possible tasks can be found in\n    the individual task objects at the openml.tasks.task\n    module.\n\n    Parameters\n    ----------\n    task_type : TaskType\n        Id of the task type.\n    dataset_id : int\n        The id of the dataset for the task.\n    target_name : str, optional\n        The name of the feature used as a target.\n        At the moment, only optional for the clustering tasks.\n    estimation_procedure_id : int\n        The id of the estimation procedure.\n    evaluation_measure : str, optional\n        The name of the evaluation measure.\n    kwargs : dict, optional\n        Other task attributes that are not mandatory\n        for task upload.\n\n    Returns\n    -------\n    OpenMLClassificationTask, OpenMLRegressionTask,\n    OpenMLLearningCurveTask, OpenMLClusteringTask\n    \"\"\"\n    if task_type == TaskType.CLUSTERING:\n        task_cls = OpenMLClusteringTask\n    elif task_type == TaskType.LEARNING_CURVE:\n        task_cls = OpenMLLearningCurveTask  # type: ignore\n    elif task_type == TaskType.SUPERVISED_CLASSIFICATION:\n        task_cls = OpenMLClassificationTask  # type: ignore\n    elif task_type == TaskType.SUPERVISED_REGRESSION:\n        task_cls = OpenMLRegressionTask  # type: ignore\n    else:\n        raise NotImplementedError(f\"Task type {task_type:d} not supported.\")\n\n    return task_cls(\n        task_type_id=task_type,\n        task_type=\"None\",  # TODO: refactor to get task type string from ID.\n        data_set_id=dataset_id,\n        target_name=target_name,  # type: ignore\n        estimation_procedure_id=estimation_procedure_id,\n        evaluation_measure=evaluation_measure,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tasks/functions/#openml.tasks.functions.delete_task","title":"delete_task","text":"<pre><code>delete_task(task_id: int) -&gt; bool\n</code></pre> <p>Delete task with id <code>task_id</code> from the OpenML server.</p> <p>You can only delete tasks which you created and have no runs associated with them.</p>"},{"location":"reference/tasks/functions/#openml.tasks.functions.delete_task--parameters","title":"Parameters","text":"<p>task_id : int     OpenML id of the task</p>"},{"location":"reference/tasks/functions/#openml.tasks.functions.delete_task--returns","title":"Returns","text":"<p>bool     True if the deletion was successful. False otherwise.</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def delete_task(task_id: int) -&gt; bool:\n    \"\"\"Delete task with id `task_id` from the OpenML server.\n\n    You can only delete tasks which you created and have\n    no runs associated with them.\n\n    Parameters\n    ----------\n    task_id : int\n        OpenML id of the task\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"task\", task_id)\n</code></pre>"},{"location":"reference/tasks/functions/#openml.tasks.functions.get_task","title":"get_task","text":"<pre><code>get_task(task_id: int, download_splits: bool = False, **get_dataset_kwargs: Any) -&gt; OpenMLTask\n</code></pre> <p>Download OpenML task for a given task ID.</p> <p>Downloads the task representation.</p> <p>Use the <code>download_splits</code> parameter to control whether the splits are downloaded. Moreover, you may pass additional parameter (args or kwargs) that are passed to :meth:<code>openml.datasets.get_dataset</code>.</p>"},{"location":"reference/tasks/functions/#openml.tasks.functions.get_task--parameters","title":"Parameters","text":"<p>task_id : int     The OpenML task id of the task to download. download_splits: bool (default=False)     Whether to download the splits as well. get_dataset_kwargs :     Args and kwargs can be used pass optional parameters to :meth:<code>openml.datasets.get_dataset</code>.</p>"},{"location":"reference/tasks/functions/#openml.tasks.functions.get_task--returns","title":"Returns","text":"<p>task: OpenMLTask</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_task(\n    task_id: int,\n    download_splits: bool = False,  # noqa: FBT001, FBT002\n    **get_dataset_kwargs: Any,\n) -&gt; OpenMLTask:\n    \"\"\"Download OpenML task for a given task ID.\n\n    Downloads the task representation.\n\n    Use the `download_splits` parameter to control whether the splits are downloaded.\n    Moreover, you may pass additional parameter (args or kwargs) that are passed to\n    :meth:`openml.datasets.get_dataset`.\n\n    Parameters\n    ----------\n    task_id : int\n        The OpenML task id of the task to download.\n    download_splits: bool (default=False)\n        Whether to download the splits as well.\n    get_dataset_kwargs :\n        Args and kwargs can be used pass optional parameters to :meth:`openml.datasets.get_dataset`.\n\n    Returns\n    -------\n    task: OpenMLTask\n    \"\"\"\n    if not isinstance(task_id, int):\n        raise TypeError(f\"Task id should be integer, is {type(task_id)}\")\n\n    tid_cache_dir = openml.utils._create_cache_directory_for_id(TASKS_CACHE_DIR_NAME, task_id)\n\n    try:\n        task = _get_task_description(task_id)\n        dataset = get_dataset(task.dataset_id, **get_dataset_kwargs)\n        # List of class labels available in dataset description\n        # Including class labels as part of task meta data handles\n        #   the case where data download was initially disabled\n        if isinstance(task, (OpenMLClassificationTask, OpenMLLearningCurveTask)):\n            task.class_labels = dataset.retrieve_class_labels(task.target_name)\n        # Clustering tasks do not have class labels\n        # and do not offer download_split\n        if download_splits and isinstance(task, OpenMLSupervisedTask):\n            task.download_split()\n    except Exception as e:\n        openml.utils._remove_cache_dir_for_id(TASKS_CACHE_DIR_NAME, tid_cache_dir)\n        raise e\n\n    return task\n</code></pre>"},{"location":"reference/tasks/functions/#openml.tasks.functions.get_tasks","title":"get_tasks","text":"<pre><code>get_tasks(task_ids: list[int], download_data: bool | None = None, download_qualities: bool | None = None) -&gt; list[OpenMLTask]\n</code></pre> <p>Download tasks.</p> <p>This function iterates :meth:<code>openml.tasks.get_task</code>.</p>"},{"location":"reference/tasks/functions/#openml.tasks.functions.get_tasks--parameters","title":"Parameters","text":"<p>task_ids : List[int]     A list of task ids to download. download_data : bool (default = True)     Option to trigger download of data along with the meta data. download_qualities : bool (default=True)     Option to download 'qualities' meta-data in addition to the minimal dataset description.</p>"},{"location":"reference/tasks/functions/#openml.tasks.functions.get_tasks--returns","title":"Returns","text":"<p>list</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def get_tasks(\n    task_ids: list[int],\n    download_data: bool | None = None,\n    download_qualities: bool | None = None,\n) -&gt; list[OpenMLTask]:\n    \"\"\"Download tasks.\n\n    This function iterates :meth:`openml.tasks.get_task`.\n\n    Parameters\n    ----------\n    task_ids : List[int]\n        A list of task ids to download.\n    download_data : bool (default = True)\n        Option to trigger download of data along with the meta data.\n    download_qualities : bool (default=True)\n        Option to download 'qualities' meta-data in addition to the minimal dataset description.\n\n    Returns\n    -------\n    list\n    \"\"\"\n    if download_data is None:\n        warnings.warn(\n            \"`download_data` will default to False starting in 0.16. \"\n            \"Please set `download_data` explicitly to suppress this warning.\",\n            stacklevel=1,\n        )\n        download_data = True\n\n    if download_qualities is None:\n        warnings.warn(\n            \"`download_qualities` will default to False starting in 0.16. \"\n            \"Please set `download_qualities` explicitly to suppress this warning.\",\n            stacklevel=1,\n        )\n        download_qualities = True\n\n    tasks = []\n    for task_id in task_ids:\n        tasks.append(\n            get_task(task_id, download_data=download_data, download_qualities=download_qualities)\n        )\n    return tasks\n</code></pre>"},{"location":"reference/tasks/functions/#openml.tasks.functions.list_tasks","title":"list_tasks","text":"<pre><code>list_tasks(task_type: TaskType | None = None, offset: int | None = None, size: int | None = None, tag: str | None = None, data_tag: str | None = None, status: str | None = None, data_name: str | None = None, data_id: int | None = None, number_instances: int | None = None, number_features: int | None = None, number_classes: int | None = None, number_missing_values: int | None = None) -&gt; DataFrame\n</code></pre> <p>Return a number of tasks having the given tag and task_type</p>"},{"location":"reference/tasks/functions/#openml.tasks.functions.list_tasks--parameters","title":"Parameters","text":"<p>Filter task_type is separated from the other filters because it is used as task_type in the task description, but it is named type when used as a filter in list tasks call. offset : int, optional     the number of tasks to skip, starting from the first task_type : TaskType, optional     Refers to the type of task. size : int, optional     the maximum number of tasks to show tag : str, optional     the tag to include data_tag : str, optional     the tag of the dataset data_id : int, optional status : str, optional data_name : str, optional number_instances : int, optional number_features : int, optional number_classes : int, optional number_missing_values : int, optional</p>"},{"location":"reference/tasks/functions/#openml.tasks.functions.list_tasks--returns","title":"Returns","text":"<p>dataframe     All tasks having the given task_type and the give tag. Every task is     represented by a row in the data frame containing the following information     as columns: task id, dataset id, task_type and status. If qualities are     calculated for the associated dataset, some of these are also returned.</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def list_tasks(  # noqa: PLR0913\n    task_type: TaskType | None = None,\n    offset: int | None = None,\n    size: int | None = None,\n    tag: str | None = None,\n    data_tag: str | None = None,\n    status: str | None = None,\n    data_name: str | None = None,\n    data_id: int | None = None,\n    number_instances: int | None = None,\n    number_features: int | None = None,\n    number_classes: int | None = None,\n    number_missing_values: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a number of tasks having the given tag and task_type\n\n    Parameters\n    ----------\n    Filter task_type is separated from the other filters because\n    it is used as task_type in the task description, but it is named\n    type when used as a filter in list tasks call.\n    offset : int, optional\n        the number of tasks to skip, starting from the first\n    task_type : TaskType, optional\n        Refers to the type of task.\n    size : int, optional\n        the maximum number of tasks to show\n    tag : str, optional\n        the tag to include\n    data_tag : str, optional\n        the tag of the dataset\n    data_id : int, optional\n    status : str, optional\n    data_name : str, optional\n    number_instances : int, optional\n    number_features : int, optional\n    number_classes : int, optional\n    number_missing_values : int, optional\n\n    Returns\n    -------\n    dataframe\n        All tasks having the given task_type and the give tag. Every task is\n        represented by a row in the data frame containing the following information\n        as columns: task id, dataset id, task_type and status. If qualities are\n        calculated for the associated dataset, some of these are also returned.\n    \"\"\"\n    listing_call = partial(\n        _list_tasks,\n        task_type=task_type,\n        tag=tag,\n        data_tag=data_tag,\n        status=status,\n        data_id=data_id,\n        data_name=data_name,\n        number_instances=number_instances,\n        number_features=number_features,\n        number_classes=number_classes,\n        number_missing_values=number_missing_values,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/tasks/split/","title":"split","text":""},{"location":"reference/tasks/split/#openml.tasks.split","title":"openml.tasks.split","text":""},{"location":"reference/tasks/split/#openml.tasks.split.OpenMLSplit","title":"OpenMLSplit","text":"<pre><code>OpenMLSplit(name: int | str, description: str, split: dict[int, dict[int, dict[int, tuple[ndarray, ndarray]]]])\n</code></pre> <p>OpenML Split object.</p> <p>This class manages train-test splits for a dataset across multiple repetitions, folds, and samples.</p>"},{"location":"reference/tasks/split/#openml.tasks.split.OpenMLSplit--parameters","title":"Parameters","text":"<p>name : int or str     The name or ID of the split. description : str     A description of the split. split : dict     A dictionary containing the splits organized by repetition, fold,     and sample.</p> Source code in <code>openml/tasks/split.py</code> <pre><code>def __init__(\n    self,\n    name: int | str,\n    description: str,\n    split: dict[int, dict[int, dict[int, tuple[np.ndarray, np.ndarray]]]],\n):\n    self.description = description\n    self.name = name\n    self.split: dict[int, dict[int, dict[int, tuple[np.ndarray, np.ndarray]]]] = {}\n\n    # Add splits according to repetition\n    for repetition in split:\n        _rep = int(repetition)\n        self.split[_rep] = OrderedDict()\n        for fold in split[_rep]:\n            self.split[_rep][fold] = OrderedDict()\n            for sample in split[_rep][fold]:\n                self.split[_rep][fold][sample] = split[_rep][fold][sample]\n\n    self.repeats = len(self.split)\n\n    # TODO(eddiebergman): Better error message\n    if any(len(self.split[0]) != len(self.split[i]) for i in range(self.repeats)):\n        raise ValueError(\"\")\n\n    self.folds = len(self.split[0])\n    self.samples = len(self.split[0][0])\n</code></pre>"},{"location":"reference/tasks/split/#openml.tasks.split.OpenMLSplit.get","title":"get","text":"<pre><code>get(repeat: int = 0, fold: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns the specified data split from the CrossValidationSplit object.</p>"},{"location":"reference/tasks/split/#openml.tasks.split.OpenMLSplit.get--parameters","title":"Parameters","text":"<p>repeat : int     Index of the repeat to retrieve. fold : int     Index of the fold to retrieve. sample : int     Index of the sample to retrieve.</p>"},{"location":"reference/tasks/split/#openml.tasks.split.OpenMLSplit.get--returns","title":"Returns","text":"<p>numpy.ndarray     The data split for the specified repeat, fold, and sample.</p>"},{"location":"reference/tasks/split/#openml.tasks.split.OpenMLSplit.get--raises","title":"Raises","text":"<p>ValueError     If the specified repeat, fold, or sample is not known.</p> Source code in <code>openml/tasks/split.py</code> <pre><code>def get(self, repeat: int = 0, fold: int = 0, sample: int = 0) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns the specified data split from the CrossValidationSplit object.\n\n    Parameters\n    ----------\n    repeat : int\n        Index of the repeat to retrieve.\n    fold : int\n        Index of the fold to retrieve.\n    sample : int\n        Index of the sample to retrieve.\n\n    Returns\n    -------\n    numpy.ndarray\n        The data split for the specified repeat, fold, and sample.\n\n    Raises\n    ------\n    ValueError\n        If the specified repeat, fold, or sample is not known.\n    \"\"\"\n    if repeat not in self.split:\n        raise ValueError(f\"Repeat {repeat!s} not known\")\n    if fold not in self.split[repeat]:\n        raise ValueError(f\"Fold {fold!s} not known\")\n    if sample not in self.split[repeat][fold]:\n        raise ValueError(f\"Sample {sample!s} not known\")\n    return self.split[repeat][fold][sample]\n</code></pre>"},{"location":"reference/tasks/split/#openml.tasks.split.Split","title":"Split","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A single split of a dataset.</p>"},{"location":"reference/tasks/task/","title":"task","text":""},{"location":"reference/tasks/task/#openml.tasks.task","title":"openml.tasks.task","text":""},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask","title":"OpenMLClassificationTask","text":"<pre><code>OpenMLClassificationTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None, task_id: int | None = None, class_labels: list[str] | None = None, cost_matrix: ndarray | None = None)\n</code></pre> <p>               Bases: <code>OpenMLSupervisedTask</code></p> <p>OpenML Classification object.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     ID of the Classification task type. task_type : str     Name of the Classification task type. data_set_id : int     ID of the OpenML dataset associated with the Classification task. target_name : str     Name of the target variable. estimation_procedure_id : int, default=None     ID of the estimation procedure for the Classification task. estimation_procedure_type : str, default=None     Type of the estimation procedure. estimation_parameters : dict, default=None     Estimation parameters for the Classification task. evaluation_measure : str, default=None     Name of the evaluation measure. data_splits_url : str, default=None     URL of the data splits for the Classification task. task_id : Union[int, None]     ID of the Classification task (if it already exists on OpenML). class_labels : List of str, default=None     A list of class labels (for classification tasks). cost_matrix : array, default=None     A cost matrix (for classification tasks).</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    class_labels: list[str] | None = None,\n    cost_matrix: np.ndarray | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n    )\n    self.class_labels = class_labels\n    self.cost_matrix = cost_matrix\n\n    if cost_matrix is not None:\n        raise NotImplementedError(\"Costmatrix\")\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.get_X_and_y--returns","title":"Returns","text":"<p>tuple - X and y</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask","title":"OpenMLClusteringTask","text":"<pre><code>OpenMLClusteringTask(task_type_id: TaskType, task_type: str, data_set_id: int, estimation_procedure_id: int = 17, task_id: int | None = None, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, evaluation_measure: str | None = None, target_name: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLTask</code></p> <p>OpenML Clustering object.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     Task type ID of the OpenML clustering task. task_type : str     Task type of the OpenML clustering task. data_set_id : int     ID of the OpenML dataset used in clustering the task. estimation_procedure_id : int, default=None     ID of the OpenML estimation procedure. task_id : Union[int, None]     ID of the OpenML clustering task. estimation_procedure_type : str, default=None     Type of the OpenML estimation procedure used in the clustering task. estimation_parameters : dict, default=None     Parameters used by the OpenML estimation procedure. data_splits_url : str, default=None     URL of the OpenML data splits for the clustering task. evaluation_measure : str, default=None     Evaluation measure used in the clustering task. target_name : str, default=None     Name of the target feature (class) that is not part of the     feature set for the clustering task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    estimation_procedure_id: int = 17,\n    task_id: int | None = None,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    evaluation_measure: str | None = None,\n    target_name: str | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        evaluation_measure=evaluation_measure,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        data_splits_url=data_splits_url,\n    )\n\n    self.target_name = target_name\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.get_X","title":"get_X","text":"<pre><code>get_X() -&gt; DataFrame\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.get_X--returns","title":"Returns","text":"<p>The X data as a dataframe</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X(self) -&gt; pd.DataFrame:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    The X data as a dataframe\n    \"\"\"\n    dataset = self.get_dataset()\n    data, *_ = dataset.get_data(target=None)\n    return data\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask","title":"OpenMLLearningCurveTask","text":"<pre><code>OpenMLLearningCurveTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 13, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, task_id: int | None = None, evaluation_measure: str | None = None, class_labels: list[str] | None = None, cost_matrix: ndarray | None = None)\n</code></pre> <p>               Bases: <code>OpenMLClassificationTask</code></p> <p>OpenML Learning Curve object.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     ID of the Learning Curve task. task_type : str     Name of the Learning Curve task. data_set_id : int     ID of the dataset that this task is associated with. target_name : str     Name of the target feature in the dataset. estimation_procedure_id : int, default=None     ID of the estimation procedure to use for evaluating models. estimation_procedure_type : str, default=None     Type of the estimation procedure. estimation_parameters : dict, default=None     Additional parameters for the estimation procedure. data_splits_url : str, default=None     URL of the file containing the data splits for Learning Curve task. task_id : Union[int, None]     ID of the Learning Curve task. evaluation_measure : str, default=None     Name of the evaluation measure to use for evaluating models. class_labels : list of str, default=None     Class labels for Learning Curve tasks. cost_matrix : numpy array, default=None     Cost matrix for Learning Curve tasks.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 13,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    evaluation_measure: str | None = None,\n    class_labels: list[str] | None = None,\n    cost_matrix: np.ndarray | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n        class_labels=class_labels,\n        cost_matrix=cost_matrix,\n    )\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.get_X_and_y--returns","title":"Returns","text":"<p>tuple - X and y</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask","title":"OpenMLRegressionTask","text":"<pre><code>OpenMLRegressionTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 7, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, task_id: int | None = None, evaluation_measure: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLSupervisedTask</code></p> <p>OpenML Regression object.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     Task type ID of the OpenML Regression task. task_type : str     Task type of the OpenML Regression task. data_set_id : int     ID of the OpenML dataset. target_name : str     Name of the target feature used in the Regression task. estimation_procedure_id : int, default=None     ID of the OpenML estimation procedure. estimation_procedure_type : str, default=None     Type of the OpenML estimation procedure. estimation_parameters : dict, default=None     Parameters used by the OpenML estimation procedure. data_splits_url : str, default=None     URL of the OpenML data splits for the Regression task. task_id : Union[int, None]     ID of the OpenML Regression task. evaluation_measure : str, default=None     Evaluation measure used in the Regression task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 7,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    evaluation_measure: str | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n    )\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.get_X_and_y--returns","title":"Returns","text":"<p>tuple - X and y</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask","title":"OpenMLSupervisedTask","text":"<pre><code>OpenMLSupervisedTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None, task_id: int | None = None)\n</code></pre> <p>               Bases: <code>OpenMLTask</code>, <code>ABC</code></p> <p>OpenML Supervised Classification object.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask--parameters","title":"Parameters","text":"<p>task_type_id : TaskType     ID of the task type. task_type : str     Name of the task type. data_set_id : int     ID of the OpenML dataset associated with the task. target_name : str     Name of the target feature (the class variable). estimation_procedure_id : int, default=None     ID of the estimation procedure for the task. estimation_procedure_type : str, default=None     Type of the estimation procedure for the task. estimation_parameters : dict, default=None     Estimation parameters for the task. evaluation_measure : str, default=None     Name of the evaluation measure for the task. data_splits_url : str, default=None     URL of the data splits for the task. task_id: Union[int, None]     Refers to the unique identifier of task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        data_splits_url=data_splits_url,\n    )\n\n    self.target_name = target_name\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.get_X_and_y--returns","title":"Returns","text":"<p>tuple - X and y</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask","title":"OpenMLTask","text":"<pre><code>OpenMLTask(task_id: int | None, task_type_id: TaskType, task_type: str, data_set_id: int, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Task object.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask--parameters","title":"Parameters","text":"<p>task_id: Union[int, None]     Refers to the unique identifier of OpenML task. task_type_id: TaskType     Refers to the type of OpenML task. task_type: str     Refers to the OpenML task. data_set_id: int     Refers to the data. estimation_procedure_id: int     Refers to the type of estimates used. estimation_procedure_type: str, default=None     Refers to the type of estimation procedure used for the OpenML task. estimation_parameters: [Dict[str, str]], default=None     Estimation parameters used for the OpenML task. evaluation_measure: str, default=None     Refers to the evaluation measure. data_splits_url: str, default=None     Refers to the URL of the data splits used for the OpenML task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_id: int | None,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n):\n    self.task_id = int(task_id) if task_id is not None else None\n    self.task_type_id = task_type_id\n    self.task_type = task_type\n    self.dataset_id = int(data_set_id)\n    self.evaluation_measure = evaluation_measure\n    self.estimation_procedure: _EstimationProcedure = {\n        \"type\": estimation_procedure_type,\n        \"parameters\": estimation_parameters,\n        \"data_splits_url\": data_splits_url,\n    }\n    self.estimation_procedure_id = estimation_procedure_id\n    self.split: OpenMLSplit | None = None\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.push_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.remove_tag--parameters","title":"Parameters","text":"<p>tag : str     Tag to attach to the flow.</p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.TaskType","title":"TaskType","text":"<p>               Bases: <code>Enum</code></p> <p>Possible task types as defined in OpenML.</p>"}]}