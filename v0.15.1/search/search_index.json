{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OpenML","text":"<p>The Python API for a World of Data and More</p> <p>Welcome to the documentation of the OpenML Python API, a connector to the collaborative machine learning platform OpenML.org.  OpenML-Python can download or upload data from OpenML, such as datasets and machine learning experiment results.</p> <p>If you are new to OpenML, we recommend checking out the OpenML documentation to get familiar with the concepts and features of OpenML. In particular, we recommend  reading more about the OpenML concepts. </p>"},{"location":"#minimal-examples","title":"Minimal Examples","text":"<p>Use the following code to get the credit-g dataset:</p> <pre><code>import openml\n\ndataset = openml.datasets.get_dataset(\"credit-g\") # or by ID get_dataset(31)\nX, y, categorical_indicator, attribute_names = dataset.get_data(target=\"class\")\n</code></pre> <p>Get a task for supervised classification on credit-g:</p> <pre><code>import openml\n\ntask = openml.tasks.get_task(31)\ndataset = task.get_dataset()\nX, y, categorical_indicator, attribute_names = dataset.get_data(target=task.target_name)\n# get splits for the first fold of 10-fold cross-validation\ntrain_indices, test_indices = task.get_train_test_split_indices(fold=0)\n</code></pre> <p>Use an OpenML benchmarking suite to get a curated list of machine-learning tasks: <pre><code>import openml\n\nsuite = openml.study.get_suite(\"amlb-classification-all\")  # Get a curated list of tasks for classification\nfor task_id in suite.tasks:\n    task = openml.tasks.get_task(task_id)\n</code></pre> Find more examples in the navbar at the top.</p>"},{"location":"#installation","title":"Installation","text":"<p>OpenML-Python is available on Linux, MacOS, and Windows.</p> <p>You can install OpenML-Python with:</p> <pre><code>pip install openml\n</code></pre> <p>For more advanced installation information, please see the \"Introduction\" example.</p>"},{"location":"#further-information","title":"Further information","text":"<ul> <li>OpenML documentation</li> <li>OpenML client APIs</li> <li>OpenML developer guide</li> <li>Contact information</li> <li>Citation request</li> <li>OpenML blog</li> <li>OpenML twitter account</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributing to the OpenML package is highly appreciated. Please see the \"Contributing\" page for more information.</p>"},{"location":"#citing-openml-python","title":"Citing OpenML-Python","text":"<p>If you use OpenML-Python in a scientific publication, we would appreciate a reference to our JMLR-MLOSS paper  \"OpenML-Python: an extensible Python API for OpenML\":</p> BibtexMLA <pre><code>@article{JMLR:v22:19-920,\n    author  = {Matthias Feurer and Jan N. van Rijn and Arlind Kadra and Pieter Gijsbers and Neeratyoy Mallik and Sahithya Ravi and Andreas M\u00c3\u00bcller and Joaquin Vanschoren and Frank Hutter},\n    title   = {OpenML-Python: an extensible Python API for OpenML},\n    journal = {Journal of Machine Learning Research},\n    year    = {2021},\n    volume  = {22},\n    number  = {100},\n    pages   = {1--5},\n    url     = {http://jmlr.org/papers/v22/19-920.html}\n}\n</code></pre> <p>Feurer, Matthias, et al.  \"OpenML-Python: an extensible Python API for OpenML.\" Journal of Machine Learning Research 22.100 (2021):1\u22125.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contribution to the OpenML package is highly appreciated in all forms. In particular, a few ways to contribute to openml-python are:</p> <ul> <li>A direct contribution to the package, by means of improving the     code, documentation or examples. To get started, see this     file     with details on how to set up your environment to develop for     openml-python.</li> <li>A contribution to an openml-python extension. An extension package     allows OpenML to interface with a machine learning package (such     as scikit-learn or keras). These extensions are hosted in separate     repositories and may have their own guidelines. For more     information, see also extensions.</li> <li>Bug reports. If something doesn't work for you or is cumbersome,     please open a new issue to let us know about the problem.</li> <li>Cite OpenML if you use it in a     scientific publication.</li> <li>Visit one of our hackathons.</li> <li>Contribute to another OpenML project, such as the main OpenML     project.</li> </ul>"},{"location":"details/","title":"Advanced User Guide","text":"<p>This document highlights some of the more advanced features of <code>openml-python</code>. </p>"},{"location":"details/#configuration","title":"Configuration","text":"<p>The configuration file resides in a directory <code>.config/openml</code> in the home directory of the user and is called config (More specifically, it resides in the configuration directory specified by the XDGB Base Directory Specification). It consists of <code>key = value</code> pairs which are separated by newlines. The following keys are defined:</p> <ul> <li>apikey: required to access the server.</li> <li>server: the server to connect to (default: <code>http://www.openml.org</code>).           For connection to the test server, set this to <code>test.openml.org</code>.</li> <li>cachedir: the root folder where the cache file directories should be created.     If not given, will default to <code>~/.openml/cache</code></li> <li>avoid_duplicate_runs: if set to <code>True</code> (default), when certain functions             are called a lookup is performed to see if there already             exists such a run on the server. If so, download those             results instead.</li> <li> <p>retry_policy: Defines how to react when the server is unavailable or             experiencing high load. It determines both how often to             attempt to reconnect and how quickly to do so. Please don't             use <code>human</code> in an automated script that you run more than             one instance of, it might increase the time to complete your             jobs and that of others. One of:             -   human (default): For people running openml in interactive                 fashion. Try only a few times, but in quick succession.             -   robot: For people using openml in an automated fashion. Keep                 trying to reconnect for a longer time, quickly increasing                 the time between retries.</p> </li> <li> <p>connection_n_retries: number of times to retry a request if they fail.  Default depends on retry_policy (5 for <code>human</code>, 50 for <code>robot</code>)</p> </li> <li>verbosity: the level of output:<ul> <li>0: normal output</li> <li>1: info output</li> <li>2: debug output</li> </ul> </li> </ul> <p>This file is easily configurable by the <code>openml</code> command line interface. To see where the file is stored, and what its values are, use openml configure none. </p>"},{"location":"details/#docker","title":"Docker","text":"<p>It is also possible to try out the latest development version of <code>openml-python</code> with docker:</p> <pre><code>docker run -it openml/openml-python\n</code></pre> <p>See the openml-python docker documentation for more information.</p>"},{"location":"details/#key-concepts","title":"Key concepts","text":"<p>OpenML contains several key concepts which it needs to make machine learning research shareable. A machine learning experiment consists of one or several runs, which describe the performance of an algorithm (called a flow in OpenML), its hyperparameter settings (called a setup) on a task. A Task is the combination of a dataset, a split and an evaluation metric. In this user guide we will go through listing and exploring existing tasks to actually running machine learning algorithms on them. In a further user guide we will examine how to search through datasets in order to curate a list of tasks.</p> <p>A further explanation is given in the OpenML user guide.</p>"},{"location":"extensions/","title":"Extensions","text":"<p>OpenML-Python provides an extension interface to connect other machine learning libraries than scikit-learn to OpenML. Please check the <code>api_extensions</code> and use the scikit-learn extension as a starting point.</p>"},{"location":"extensions/#list-of-extensions","title":"List of extensions","text":"<p>Here is a list of currently maintained OpenML extensions:</p> <ul> <li>openml-sklearn</li> <li>openml-keras</li> <li>openml-pytorch</li> <li>openml-tensorflow (for tensorflow     2+)</li> </ul>"},{"location":"extensions/#connecting-new-machine-learning-libraries","title":"Connecting new machine learning libraries","text":""},{"location":"extensions/#content-of-the-library","title":"Content of the Library","text":"<p>To leverage support from the community and to tap in the potential of OpenML, interfacing with popular machine learning libraries is essential. The OpenML-Python package is capable of downloading meta-data and results (data, flows, runs), regardless of the library that was used to upload it. However, in order to simplify the process of uploading flows and runs from a specific library, an additional interface can be built. The OpenML-Python team does not have the capacity to develop and maintain such interfaces on its own. For this reason, we have built an extension interface to allows others to contribute back. Building a suitable extension for therefore requires an understanding of the current OpenML-Python support.</p> <p>This tutorial shows how the scikit-learn  extension works with OpenML-Python.</p>"},{"location":"extensions/#api","title":"API","text":"<ul> <li>The extension scripts must import the openml-python package     and be able to interface with any function from the API.</li> <li>The extension has to be defined as a Python class and must inherit     from <code>openml.extensions.Extension</code>.</li> <li>This class needs to have all the functions from <code>openml.extensions.Extension</code> overloaded as required.</li> <li>The redefined functions should have adequate and appropriate     docstrings. The sklearn Extension API is a good example to follow.</li> </ul>"},{"location":"extensions/#interfacing-with-openml-python","title":"Interfacing with OpenML-Python","text":"<p>Once the new extension class has been defined, the openml-python module to <code>openml.extensions.register_extension</code> must be called to allow OpenML-Python to interface the new extension.</p> <p>The following methods should get implemented. Although the documentation in the extension interface should always be leading, here we list some additional information and best practices.  Note that most methods are relatively simple and can be implemented in several lines of code.</p> <ul> <li>General setup (required)<ul> <li><code>can_handle_flow</code>: Takes as     argument an OpenML flow, and checks whether this can be handled     by the current extension. The OpenML database consists of many     flows, from various workbenches (e.g., scikit-learn, Weka, mlr).     This method is called before a model is being deserialized.     Typically, the flow-dependency field is used to check whether     the specific library is present, and no unknown libraries are     present there.</li> <li><code>can_handle_model</code>: Similar as     <code>can_handle_flow</code>:, except that in     this case a Python object is given. As such, in many cases, this     method can be implemented by checking whether this adheres to a     certain base class.</li> </ul> </li> <li>Serialization and De-serialization (required)<ul> <li><code>flow_to_model</code>: deserializes the     OpenML Flow into a model (if the library can indeed handle the     flow). This method has an important interplay with     <code>model_to_flow</code>. Running these     two methods in succession should result in exactly the same     model (or flow). This property can be used for unit testing     (e.g., build a model with hyperparameters, make predictions on a     task, serialize it to a flow, deserialize it back, make it     predict on the same task, and check whether the predictions are     exactly the same.) The example in the scikit-learn interface     might seem daunting, but note that here some complicated design     choices were made, that allow for all sorts of interesting     research questions. It is probably good practice to start easy.</li> <li><code>model_to_flow</code>: The inverse of <code>flow_to_model</code>. Serializes a     model into an OpenML Flow. The flow should preserve the class,     the library version, and the tunable hyperparameters.</li> <li><code>get_version_information</code>: Return     a tuple with the version information of the important libraries.</li> <li><code>create_setup_string</code>: No longer     used, and will be deprecated soon.</li> </ul> </li> <li>Performing runs (required)<ul> <li><code>is_estimator</code>: Gets as input a     class, and checks whether it has the status of estimator in the     library (typically, whether it has a train method and a predict     method).</li> <li><code>seed_model</code>: Sets a random seed to the model.</li> <li><code>_run_model_on_fold</code>: One of the     main requirements for a library to generate run objects for the     OpenML server. Obtains a train split (with labels) and a test     split (without labels) and the goal is to train a model on the     train split and return the predictions on the test split. On top     of the actual predictions, also the class probabilities should     be determined. For classifiers that do not return class     probabilities, this can just be the hot-encoded predicted label.     The predictions will be evaluated on the OpenML server. Also,     additional information can be returned, for example,     user-defined measures (such as runtime information, as this can     not be inferred on the server). Additionally, information about     a hyperparameter optimization trace can be provided.</li> <li><code>obtain_parameter_values</code>:     Obtains the hyperparameters of a given model and the current     values. Please note that in the case of a hyperparameter     optimization procedure (e.g., random search), you only should     return the hyperparameters of this procedure (e.g., the     hyperparameter grid, budget, etc) and that the chosen model will     be inferred from the optimization trace.</li> <li><code>check_if_model_fitted</code>: Check     whether the train method of the model has been called (and as     such, whether the predict method can be used).</li> </ul> </li> <li>Hyperparameter optimization (optional)<ul> <li><code>instantiate_model_from_hpo_class</code>: If a given run has recorded the hyperparameter     optimization trace, then this method can be used to     reinstantiate the model with hyperparameters of a given     hyperparameter optimization iteration. Has some similarities     with <code>flow_to_model</code> (as this     method also sets the hyperparameters of a model). Note that     although this method is required, it is not necessary to     implement any logic if hyperparameter optimization is not     implemented. Simply raise a <code>NotImplementedError</code>     then.</li> </ul> </li> </ul>"},{"location":"extensions/#hosting-the-library","title":"Hosting the library","text":"<p>Each extension created should be a stand-alone repository, compatible with the OpenML-Python repository.  The extension repository should work off-the-shelf with OpenML-Python installed.</p> <p>Create a public Github repo with the following directory structure:</p> <pre><code>| [repo name]\n|    |-- [extension name]\n|    |    |-- __init__.py\n|    |    |-- extension.py\n|    |    |-- config.py (optionally)\n</code></pre>"},{"location":"extensions/#recommended","title":"Recommended","text":"<ul> <li>Test cases to keep the extension up to date with the     Openml-Python upstream changes.</li> <li>Documentation of the extension API, especially if any new     functionality added to OpenML-Python\\'s extension design.</li> <li>Examples to show how the new extension interfaces and works with     OpenML-Python.</li> <li>Create a PR to add the new extension to the OpenML-Python API     documentation.</li> </ul> <p>Happy contributing!</p>"},{"location":"examples/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Advanced<ul> <li>configure_logging.py</li> <li>create_upload_tutorial.py</li> <li>datasets_tutorial.py</li> <li>fetch_evaluations_tutorial.py</li> <li>study_tutorial.py</li> <li>suites_tutorial.py</li> <li>task_manual_iteration_tutorial.py</li> <li>tasks_tutorial.py</li> </ul> </li> <li>Basics<ul> <li>introduction_tutorial.py</li> <li>simple_datasets_tutorial.py</li> <li>simple_flows_and_runs_tutorial.py</li> <li>simple_suites_tutorial.py</li> <li>simple_tasks_tutorial.py</li> </ul> </li> <li>introduction.py</li> </ul>"},{"location":"examples/introduction/","title":"Overview","text":"<p>We provide a set of examples here to get started with OpenML-Python. These examples cover various aspects of using the OpenML API, including downloading datasets, uploading results, and working with tasks.</p>"},{"location":"examples/introduction/#basics","title":"Basics\u00b6","text":"<ol> <li>Installing and setting up OpenML-Python</li> <li>Downloading datasets</li> <li>Using tasks</li> <li>Uploading experiment results</li> <li>Working with collections of tasks</li> </ol>"},{"location":"examples/introduction/#advanced","title":"Advanced\u00b6","text":"<ol> <li>Getting splits for datasets from tasks</li> <li>Creating and uploading datasets</li> <li>Searching and editing datasets</li> <li>Searching and creating tasks</li> <li>Listing, downloading, and uploading suites</li> <li>Listing, downloading, and uploading studies</li> <li>Downloading evaluation results</li> <li>Configuring logging</li> </ol>"},{"location":"examples/Advanced/configure_logging/","title":"Configuring Logging","text":"<p>This tutorial explains openml-python logging, and shows how to configure it. Openml-python uses the Python logging module to provide users with log messages. Each log message is assigned a level of importance, see the table in Python's logging tutorial here.</p> <p>By default, openml-python will print log messages of level <code>WARNING</code> and above to console. All log messages (including <code>DEBUG</code> and <code>INFO</code>) are also saved in a file, which can be found in your cache directory (see also the introduction tutorial. These file logs are automatically deleted if needed, and use at most 2MB of space.</p> <p>It is possible to configure what log levels to send to console and file. When downloading a dataset from OpenML, a <code>DEBUG</code>-level message is written:</p> In\u00a0[\u00a0]: Copied! <pre>import openml\n\nopenml.datasets.get_dataset(\"iris\", version=1)\n</pre> import openml  openml.datasets.get_dataset(\"iris\", version=1) <p>With default configuration, the above example will show no output to console. However, in your cache directory you should find a file named 'openml_python.log', which has a DEBUG message written to it. It should be either like \"[DEBUG] [10:46:19:openml.datasets.dataset] Saved dataset 61: iris to file ...\" or like \"[DEBUG] [10:49:38:openml.datasets.dataset] Data pickle file already exists and is up to date.\" , depending on whether or not you had downloaded iris before. The processed log levels can be configured programmatically:</p> In\u00a0[\u00a0]: Copied! <pre>import logging\n\nopenml.config.set_console_log_level(logging.DEBUG)\nopenml.config.set_file_log_level(logging.WARNING)\nopenml.datasets.get_dataset(\"iris\", version=1)\n</pre> import logging  openml.config.set_console_log_level(logging.DEBUG) openml.config.set_file_log_level(logging.WARNING) openml.datasets.get_dataset(\"iris\", version=1) <p>Now the log level that was previously written to file should also be shown in the console. The message is now no longer written to file as the <code>file_log</code> was set to level <code>WARNING</code>.</p> <p>It is also possible to specify the desired log levels through the configuration file. This way you will not need to set them on each script separately. Add the  line verbosity = NUMBER and/or file_verbosity = NUMBER to the config file, where 'NUMBER' should be one of:</p> <ul> <li>0: <code>logging.WARNING</code> and up.</li> <li>1: <code>logging.INFO</code> and up.</li> <li>2: <code>logging.DEBUG</code> and up (i.e. all messages).</li> </ul>"},{"location":"examples/Advanced/create_upload_tutorial/","title":"Prepare dataset","text":"<p>A tutorial on how to create and upload a dataset to OpenML.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport sklearn.datasets\nfrom scipy.sparse import coo_matrix\n\nimport openml\nfrom openml.datasets.functions import create_dataset\n</pre> import numpy as np import pandas as pd import sklearn.datasets from scipy.sparse import coo_matrix  import openml from openml.datasets.functions import create_dataset In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() <p>Below we will cover the following cases of the dataset object:</p> <ul> <li>A numpy array</li> <li>A list</li> <li>A pandas dataframe</li> <li>A sparse matrix</li> <li>A pandas sparse dataframe</li> </ul> In\u00a0[\u00a0]: Copied! <pre>diabetes = sklearn.datasets.load_diabetes()\nname = \"Diabetes(scikit-learn)\"\nX = diabetes.data\ny = diabetes.target\nattribute_names = diabetes.feature_names\ndescription = diabetes.DESCR\n</pre> diabetes = sklearn.datasets.load_diabetes() name = \"Diabetes(scikit-learn)\" X = diabetes.data y = diabetes.target attribute_names = diabetes.feature_names description = diabetes.DESCR <p>OpenML does not distinguish between the attributes and targets on the data level and stores all data in a single matrix.</p> <p>The target feature is indicated as meta-data of the dataset (and tasks on that data).</p> In\u00a0[\u00a0]: Copied! <pre>data = np.concatenate((X, y.reshape((-1, 1))), axis=1)\nattribute_names = list(attribute_names)\nattributes = [(attribute_name, \"REAL\") for attribute_name in attribute_names] + [\n    (\"class\", \"INTEGER\")\n]\ncitation = (\n    \"Bradley Efron, Trevor Hastie, Iain Johnstone and \"\n    \"Robert Tibshirani (2004) (Least Angle Regression) \"\n    \"Annals of Statistics (with discussion), 407-499\"\n)\npaper_url = \"https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf\"\n</pre> data = np.concatenate((X, y.reshape((-1, 1))), axis=1) attribute_names = list(attribute_names) attributes = [(attribute_name, \"REAL\") for attribute_name in attribute_names] + [     (\"class\", \"INTEGER\") ] citation = (     \"Bradley Efron, Trevor Hastie, Iain Johnstone and \"     \"Robert Tibshirani (2004) (Least Angle Regression) \"     \"Annals of Statistics (with discussion), 407-499\" ) paper_url = \"https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf\" In\u00a0[\u00a0]: Copied! <pre>diabetes_dataset = create_dataset(\n    # The name of the dataset (needs to be unique).\n    # Must not be longer than 128 characters and only contain\n    # a-z, A-Z, 0-9 and the following special characters: _\\-\\.(),\n    name=name,\n    # Textual description of the dataset.\n    description=description,\n    # The person who created the dataset.\n    creator=\"Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani\",\n    # People who contributed to the current version of the dataset.\n    contributor=None,\n    # The date the data was originally collected, given by the uploader.\n    collection_date=\"09-01-2012\",\n    # Language in which the data is represented.\n    # Starts with 1 upper case letter, rest lower case, e.g. 'English'.\n    language=\"English\",\n    # License under which the data is/will be distributed.\n    licence=\"BSD (from scikit-learn)\",\n    # Name of the target. Can also have multiple values (comma-separated).\n    default_target_attribute=\"class\",\n    # The attribute that represents the row-id column, if present in the\n    # dataset.\n    row_id_attribute=None,\n    # Attribute or list of attributes that should be excluded in modelling, such as\n    # identifiers and indexes. E.g. \"feat1\" or [\"feat1\",\"feat2\"]\n    ignore_attribute=None,\n    # How to cite the paper.\n    citation=citation,\n    # Attributes of the data\n    attributes=attributes,\n    data=data,\n    # A version label which is provided by the user.\n    version_label=\"test\",\n    original_data_url=\"https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\",\n    paper_url=paper_url,\n)\n</pre> diabetes_dataset = create_dataset(     # The name of the dataset (needs to be unique).     # Must not be longer than 128 characters and only contain     # a-z, A-Z, 0-9 and the following special characters: _\\-\\.(),     name=name,     # Textual description of the dataset.     description=description,     # The person who created the dataset.     creator=\"Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani\",     # People who contributed to the current version of the dataset.     contributor=None,     # The date the data was originally collected, given by the uploader.     collection_date=\"09-01-2012\",     # Language in which the data is represented.     # Starts with 1 upper case letter, rest lower case, e.g. 'English'.     language=\"English\",     # License under which the data is/will be distributed.     licence=\"BSD (from scikit-learn)\",     # Name of the target. Can also have multiple values (comma-separated).     default_target_attribute=\"class\",     # The attribute that represents the row-id column, if present in the     # dataset.     row_id_attribute=None,     # Attribute or list of attributes that should be excluded in modelling, such as     # identifiers and indexes. E.g. \"feat1\" or [\"feat1\",\"feat2\"]     ignore_attribute=None,     # How to cite the paper.     citation=citation,     # Attributes of the data     attributes=attributes,     data=data,     # A version label which is provided by the user.     version_label=\"test\",     original_data_url=\"https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\",     paper_url=paper_url, ) In\u00a0[\u00a0]: Copied! <pre>diabetes_dataset.publish()\nprint(f\"URL for dataset: {diabetes_dataset.openml_url}\")\n</pre>  diabetes_dataset.publish() print(f\"URL for dataset: {diabetes_dataset.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>data = [\n    [\"sunny\", 85, 85, \"FALSE\", \"no\"],\n    [\"sunny\", 80, 90, \"TRUE\", \"no\"],\n    [\"overcast\", 83, 86, \"FALSE\", \"yes\"],\n    [\"rainy\", 70, 96, \"FALSE\", \"yes\"],\n    [\"rainy\", 68, 80, \"FALSE\", \"yes\"],\n    [\"rainy\", 65, 70, \"TRUE\", \"no\"],\n    [\"overcast\", 64, 65, \"TRUE\", \"yes\"],\n    [\"sunny\", 72, 95, \"FALSE\", \"no\"],\n    [\"sunny\", 69, 70, \"FALSE\", \"yes\"],\n    [\"rainy\", 75, 80, \"FALSE\", \"yes\"],\n    [\"sunny\", 75, 70, \"TRUE\", \"yes\"],\n    [\"overcast\", 72, 90, \"TRUE\", \"yes\"],\n    [\"overcast\", 81, 75, \"FALSE\", \"yes\"],\n    [\"rainy\", 71, 91, \"TRUE\", \"no\"],\n]\n\nattribute_names = [\n    (\"outlook\", [\"sunny\", \"overcast\", \"rainy\"]),\n    (\"temperature\", \"REAL\"),\n    (\"humidity\", \"REAL\"),\n    (\"windy\", [\"TRUE\", \"FALSE\"]),\n    (\"play\", [\"yes\", \"no\"]),\n]\n\ndescription = (\n    \"The weather problem is a tiny dataset that we will use repeatedly\"\n    \" to illustrate machine learning methods. Entirely fictitious, it \"\n    \"supposedly concerns the conditions that are suitable for playing \"\n    \"some unspecified game. In general, instances in a dataset are \"\n    \"characterized by the values of features, or attributes, that measure \"\n    \"different aspects of the instance. In this case there are four \"\n    \"attributes: outlook, temperature, humidity, and windy. \"\n    \"The outcome is whether to play or not.\"\n)\n\ncitation = (\n    \"I. H. Witten, E. Frank, M. A. Hall, and ITPro,\"\n    \"Data mining practical machine learning tools and techniques, \"\n    \"third edition. Burlington, Mass.: Morgan Kaufmann Publishers, 2011\"\n)\n\nweather_dataset = create_dataset(\n    name=\"Weather\",\n    description=description,\n    creator=\"I. H. Witten, E. Frank, M. A. Hall, and ITPro\",\n    contributor=None,\n    collection_date=\"01-01-2011\",\n    language=\"English\",\n    licence=None,\n    default_target_attribute=\"play\",\n    row_id_attribute=None,\n    ignore_attribute=None,\n    citation=citation,\n    attributes=attribute_names,\n    data=data,\n    version_label=\"example\",\n)\n</pre> data = [     [\"sunny\", 85, 85, \"FALSE\", \"no\"],     [\"sunny\", 80, 90, \"TRUE\", \"no\"],     [\"overcast\", 83, 86, \"FALSE\", \"yes\"],     [\"rainy\", 70, 96, \"FALSE\", \"yes\"],     [\"rainy\", 68, 80, \"FALSE\", \"yes\"],     [\"rainy\", 65, 70, \"TRUE\", \"no\"],     [\"overcast\", 64, 65, \"TRUE\", \"yes\"],     [\"sunny\", 72, 95, \"FALSE\", \"no\"],     [\"sunny\", 69, 70, \"FALSE\", \"yes\"],     [\"rainy\", 75, 80, \"FALSE\", \"yes\"],     [\"sunny\", 75, 70, \"TRUE\", \"yes\"],     [\"overcast\", 72, 90, \"TRUE\", \"yes\"],     [\"overcast\", 81, 75, \"FALSE\", \"yes\"],     [\"rainy\", 71, 91, \"TRUE\", \"no\"], ]  attribute_names = [     (\"outlook\", [\"sunny\", \"overcast\", \"rainy\"]),     (\"temperature\", \"REAL\"),     (\"humidity\", \"REAL\"),     (\"windy\", [\"TRUE\", \"FALSE\"]),     (\"play\", [\"yes\", \"no\"]), ]  description = (     \"The weather problem is a tiny dataset that we will use repeatedly\"     \" to illustrate machine learning methods. Entirely fictitious, it \"     \"supposedly concerns the conditions that are suitable for playing \"     \"some unspecified game. In general, instances in a dataset are \"     \"characterized by the values of features, or attributes, that measure \"     \"different aspects of the instance. In this case there are four \"     \"attributes: outlook, temperature, humidity, and windy. \"     \"The outcome is whether to play or not.\" )  citation = (     \"I. H. Witten, E. Frank, M. A. Hall, and ITPro,\"     \"Data mining practical machine learning tools and techniques, \"     \"third edition. Burlington, Mass.: Morgan Kaufmann Publishers, 2011\" )  weather_dataset = create_dataset(     name=\"Weather\",     description=description,     creator=\"I. H. Witten, E. Frank, M. A. Hall, and ITPro\",     contributor=None,     collection_date=\"01-01-2011\",     language=\"English\",     licence=None,     default_target_attribute=\"play\",     row_id_attribute=None,     ignore_attribute=None,     citation=citation,     attributes=attribute_names,     data=data,     version_label=\"example\", ) In\u00a0[\u00a0]: Copied! <pre>weather_dataset.publish()\nprint(f\"URL for dataset: {weather_dataset.openml_url}\")\n</pre> weather_dataset.publish() print(f\"URL for dataset: {weather_dataset.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>df = pd.DataFrame(data, columns=[col_name for col_name, _ in attribute_names])\n\n# enforce the categorical column to have a categorical dtype\ndf[\"outlook\"] = df[\"outlook\"].astype(\"category\")\ndf[\"windy\"] = df[\"windy\"].astype(\"bool\")\ndf[\"play\"] = df[\"play\"].astype(\"category\")\nprint(df.info())\n</pre> df = pd.DataFrame(data, columns=[col_name for col_name, _ in attribute_names])  # enforce the categorical column to have a categorical dtype df[\"outlook\"] = df[\"outlook\"].astype(\"category\") df[\"windy\"] = df[\"windy\"].astype(\"bool\") df[\"play\"] = df[\"play\"].astype(\"category\") print(df.info()) <p>We enforce the column 'outlook' and 'play' to be a categorical dtype while the column 'windy' is kept as a boolean column. 'temperature' and 'humidity' are kept as numeric columns. Then, we can call :func:<code>openml.datasets.create_dataset</code> by passing the dataframe and fixing the parameter <code>attributes</code> to <code>'auto'</code>.</p> In\u00a0[\u00a0]: Copied! <pre>weather_dataset = create_dataset(\n    name=\"Weather\",\n    description=description,\n    creator=\"I. H. Witten, E. Frank, M. A. Hall, and ITPro\",\n    contributor=None,\n    collection_date=\"01-01-2011\",\n    language=\"English\",\n    licence=None,\n    default_target_attribute=\"play\",\n    row_id_attribute=None,\n    ignore_attribute=None,\n    citation=citation,\n    attributes=\"auto\",\n    data=df,\n    version_label=\"example\",\n)\n</pre> weather_dataset = create_dataset(     name=\"Weather\",     description=description,     creator=\"I. H. Witten, E. Frank, M. A. Hall, and ITPro\",     contributor=None,     collection_date=\"01-01-2011\",     language=\"English\",     licence=None,     default_target_attribute=\"play\",     row_id_attribute=None,     ignore_attribute=None,     citation=citation,     attributes=\"auto\",     data=df,     version_label=\"example\", ) In\u00a0[\u00a0]: Copied! <pre>weather_dataset.publish()\nprint(f\"URL for dataset: {weather_dataset.openml_url}\")\n</pre> weather_dataset.publish() print(f\"URL for dataset: {weather_dataset.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>sparse_data = coo_matrix(\n    ([0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], ([0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 2, 0, 1]))\n)\n\ncolumn_names = [\n    (\"input1\", \"REAL\"),\n    (\"input2\", \"REAL\"),\n    (\"y\", \"REAL\"),\n]\n\nxor_dataset = create_dataset(\n    name=\"XOR\",\n    description=\"Dataset representing the XOR operation\",\n    creator=None,\n    contributor=None,\n    collection_date=None,\n    language=\"English\",\n    licence=None,\n    default_target_attribute=\"y\",\n    row_id_attribute=None,\n    ignore_attribute=None,\n    citation=None,\n    attributes=column_names,\n    data=sparse_data,\n    version_label=\"example\",\n)\n</pre> sparse_data = coo_matrix(     ([0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], ([0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 2, 0, 1])) )  column_names = [     (\"input1\", \"REAL\"),     (\"input2\", \"REAL\"),     (\"y\", \"REAL\"), ]  xor_dataset = create_dataset(     name=\"XOR\",     description=\"Dataset representing the XOR operation\",     creator=None,     contributor=None,     collection_date=None,     language=\"English\",     licence=None,     default_target_attribute=\"y\",     row_id_attribute=None,     ignore_attribute=None,     citation=None,     attributes=column_names,     data=sparse_data,     version_label=\"example\", ) In\u00a0[\u00a0]: Copied! <pre>xor_dataset.publish()\nprint(f\"URL for dataset: {xor_dataset.openml_url}\")\n</pre> xor_dataset.publish() print(f\"URL for dataset: {xor_dataset.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>xor_dataset.publish()\nprint(f\"URL for dataset: {xor_dataset.openml_url}\")\n</pre>  xor_dataset.publish() print(f\"URL for dataset: {xor_dataset.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n</pre> openml.config.stop_using_configuration_for_example()"},{"location":"examples/Advanced/create_upload_tutorial/#dataset-is-a-numpy-array","title":"Dataset is a numpy array\u00b6","text":"<p>A numpy array can contain lists in the case of dense data or it can contain OrderedDicts in the case of sparse data.</p>"},{"location":"examples/Advanced/create_upload_tutorial/#prepare-dataset","title":"Prepare dataset\u00b6","text":"<p>Load an example dataset from scikit-learn which we will upload to OpenML.org via the API.</p>"},{"location":"examples/Advanced/create_upload_tutorial/#create-the-dataset-object","title":"Create the dataset object\u00b6","text":"<p>The definition of all fields can be found in the XSD files describing the expected format:</p> <p>https://github.com/openml/OpenML/blob/master/openml_OS/views/pages/api_new/v1/xsd/openml.data.upload.xsd</p>"},{"location":"examples/Advanced/create_upload_tutorial/#dataset-is-a-list","title":"Dataset is a list\u00b6","text":"<p>A list can contain lists in the case of dense data or it can contain OrderedDicts in the case of sparse data.</p> <p>Weather dataset: https://storm.cis.fordham.edu/~gweiss/data-mining/datasets.html</p>"},{"location":"examples/Advanced/create_upload_tutorial/#dataset-is-a-pandas-dataframe","title":"Dataset is a pandas DataFrame\u00b6","text":"<p>It might happen that your dataset is made of heterogeneous data which can usually be stored as a Pandas DataFrame. DataFrames offer the advantage of storing the type of data for each column as well as the attribute names. Therefore, when providing a Pandas DataFrame, OpenML can infer this information without needing to explicitly provide it when calling the function :func:<code>openml.datasets.create_dataset</code>. In this regard, you only need to pass <code>'auto'</code> to the <code>attributes</code> parameter.</p>"},{"location":"examples/Advanced/create_upload_tutorial/#dataset-is-a-sparse-matrix","title":"Dataset is a sparse matrix\u00b6","text":""},{"location":"examples/Advanced/create_upload_tutorial/#dataset-is-a-pandas-dataframe-with-sparse-columns","title":"Dataset is a pandas dataframe with sparse columns\u00b6","text":"<p>sparse_data = coo_matrix( ([1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0], ([0, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 2, 0, 1])) ) column_names = [\"input1\", \"input2\", \"y\"] df = pd.DataFrame.sparse.from_spmatrix(sparse_data, columns=column_names) print(df.info())</p> <p>xor_dataset = create_dataset( name=\"XOR\", description=\"Dataset representing the XOR operation\", creator=None, contributor=None, collection_date=None, language=\"English\", licence=None, default_target_attribute=\"y\", row_id_attribute=None, ignore_attribute=None, citation=None, attributes=\"auto\", data=df, version_label=\"example\", )</p>"},{"location":"examples/Advanced/datasets_tutorial/","title":"Searching and Editing Datasets","text":"<p>How to list and download datasets.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\nimport openml\nfrom openml.datasets import edit_dataset, fork_dataset, get_dataset\n</pre> import pandas as pd  import openml from openml.datasets import edit_dataset, fork_dataset, get_dataset In\u00a0[\u00a0]: Copied! <pre>datalist = openml.datasets.list_datasets()\ndatalist = datalist[[\"did\", \"name\", \"NumberOfInstances\", \"NumberOfFeatures\", \"NumberOfClasses\"]]\n\nprint(f\"First 10 of {len(datalist)} datasets...\")\ndatalist.head(n=10)\n\n# The same can be done with lesser lines of code\nopenml_df = openml.datasets.list_datasets()\nopenml_df.head(n=10)\n</pre> datalist = openml.datasets.list_datasets() datalist = datalist[[\"did\", \"name\", \"NumberOfInstances\", \"NumberOfFeatures\", \"NumberOfClasses\"]]  print(f\"First 10 of {len(datalist)} datasets...\") datalist.head(n=10)  # The same can be done with lesser lines of code openml_df = openml.datasets.list_datasets() openml_df.head(n=10) In\u00a0[\u00a0]: Copied! <pre>datalist[datalist.NumberOfInstances &gt; 10000].sort_values([\"NumberOfInstances\"]).head(n=20)\n</pre> datalist[datalist.NumberOfInstances &gt; 10000].sort_values([\"NumberOfInstances\"]).head(n=20) In\u00a0[\u00a0]: Copied! <pre>datalist.query('name == \"eeg-eye-state\"')\n</pre> datalist.query('name == \"eeg-eye-state\"') In\u00a0[\u00a0]: Copied! <pre>datalist.query(\"NumberOfClasses &gt; 50\")\n</pre> datalist.query(\"NumberOfClasses &gt; 50\") In\u00a0[\u00a0]: Copied! <pre># This is done based on the dataset ID.\ndataset = openml.datasets.get_dataset(dataset_id=\"eeg-eye-state\", version=1)\n\n# Print a summary\nprint(\n    f\"This is dataset '{dataset.name}', the target feature is '{dataset.default_target_attribute}'\"\n)\nprint(f\"URL: {dataset.url}\")\nprint(dataset.description[:500])\n</pre> # This is done based on the dataset ID. dataset = openml.datasets.get_dataset(dataset_id=\"eeg-eye-state\", version=1)  # Print a summary print(     f\"This is dataset '{dataset.name}', the target feature is '{dataset.default_target_attribute}'\" ) print(f\"URL: {dataset.url}\") print(dataset.description[:500]) <p>Get the actual data.</p> <p>openml-python returns data as pandas dataframes (stored in the <code>eeg</code> variable below), and also some additional metadata that we don't care about right now.</p> In\u00a0[\u00a0]: Copied! <pre>eeg, *_ = dataset.get_data()\n</pre> eeg, *_ = dataset.get_data() <p>You can optionally choose to have openml separate out a column from the dataset. In particular, many datasets for supervised problems have a set <code>default_target_attribute</code> which may help identify the target variable.</p> In\u00a0[\u00a0]: Copied! <pre>X, y, categorical_indicator, attribute_names = dataset.get_data(\n    target=dataset.default_target_attribute\n)\nprint(X.head())\nprint(X.info())\n</pre> X, y, categorical_indicator, attribute_names = dataset.get_data(     target=dataset.default_target_attribute ) print(X.head()) print(X.info()) <p>Sometimes you only need access to a dataset's metadata. In those cases, you can download the dataset without downloading the data file. The dataset object can be used as normal. Whenever you use any functionality that requires the data, such as <code>get_data</code>, the data will be downloaded. Starting from 0.15, not downloading data will be the default behavior instead. The data will be downloading automatically when you try to access it through openml objects, e.g., using <code>dataset.features</code>.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = openml.datasets.get_dataset(1471)\n</pre> dataset = openml.datasets.get_dataset(1471) In\u00a0[\u00a0]: Copied! <pre>eegs = eeg.sample(n=1000)\n_ = pd.plotting.scatter_matrix(\n    X.iloc[:100, :4],\n    c=y[:100],\n    figsize=(10, 10),\n    marker=\"o\",\n    hist_kwds={\"bins\": 20},\n    alpha=0.8,\n    cmap=\"plasma\",\n)\n</pre> eegs = eeg.sample(n=1000) _ = pd.plotting.scatter_matrix(     X.iloc[:100, :4],     c=y[:100],     figsize=(10, 10),     marker=\"o\",     hist_kwds={\"bins\": 20},     alpha=0.8,     cmap=\"plasma\", ) In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() <p>Edit non-critical fields, allowed for all authorized users: description, creator, contributor, collection_date, language, citation, original_data_url, paper_url</p> In\u00a0[\u00a0]: Copied! <pre>desc = (\n    \"This data sets consists of 3 different types of irises' \"\n    \"(Setosa, Versicolour, and Virginica) petal and sepal length,\"\n    \" stored in a 150x4 numpy.ndarray\"\n)\ndid = 128\ndata_id = edit_dataset(\n    did,\n    description=desc,\n    creator=\"R.A.Fisher\",\n    collection_date=\"1937\",\n    citation=\"The use of multiple measurements in taxonomic problems\",\n    language=\"English\",\n)\nedited_dataset = get_dataset(data_id)\nprint(f\"Edited dataset ID: {data_id}\")\n</pre> desc = (     \"This data sets consists of 3 different types of irises' \"     \"(Setosa, Versicolour, and Virginica) petal and sepal length,\"     \" stored in a 150x4 numpy.ndarray\" ) did = 128 data_id = edit_dataset(     did,     description=desc,     creator=\"R.A.Fisher\",     collection_date=\"1937\",     citation=\"The use of multiple measurements in taxonomic problems\",     language=\"English\", ) edited_dataset = get_dataset(data_id) print(f\"Edited dataset ID: {data_id}\") <p>Editing critical fields (default_target_attribute, row_id_attribute, ignore_attribute) is allowed only for the dataset owner. Further, critical fields cannot be edited if the dataset has any tasks associated with it. To edit critical fields of a dataset (without tasks) owned by you, configure the API key: openml.config.apikey = 'FILL_IN_OPENML_API_KEY' This example here only shows a failure when trying to work on a dataset not owned by you:</p> In\u00a0[\u00a0]: Copied! <pre>try:\n    data_id = edit_dataset(1, default_target_attribute=\"shape\")\nexcept openml.exceptions.OpenMLServerException as e:\n    print(e)\n</pre> try:     data_id = edit_dataset(1, default_target_attribute=\"shape\") except openml.exceptions.OpenMLServerException as e:     print(e) In\u00a0[\u00a0]: Copied! <pre>data_id = fork_dataset(1)\nprint(data_id)\ndata_id = edit_dataset(data_id, default_target_attribute=\"shape\")\nprint(f\"Forked dataset ID: {data_id}\")\n</pre> data_id = fork_dataset(1) print(data_id) data_id = edit_dataset(data_id, default_target_attribute=\"shape\") print(f\"Forked dataset ID: {data_id}\") In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n</pre> openml.config.stop_using_configuration_for_example()"},{"location":"examples/Advanced/datasets_tutorial/#exercise-0","title":"Exercise 0\u00b6","text":"<ul> <li>List datasets and return a dataframe</li> </ul>"},{"location":"examples/Advanced/datasets_tutorial/#exercise-1","title":"Exercise 1\u00b6","text":"<ul> <li>Find datasets with more than 10000 examples.</li> <li>Find a dataset called 'eeg_eye_state'.</li> <li>Find all datasets with more than 50 classes.</li> </ul>"},{"location":"examples/Advanced/datasets_tutorial/#download-datasets","title":"Download datasets\u00b6","text":""},{"location":"examples/Advanced/datasets_tutorial/#exercise-2","title":"Exercise 2\u00b6","text":"<ul> <li>Explore the data visually.</li> </ul>"},{"location":"examples/Advanced/datasets_tutorial/#edit-a-created-dataset","title":"Edit a created dataset\u00b6","text":"<p>This example uses the test server, to avoid editing a dataset on the main server.</p>"},{"location":"examples/Advanced/datasets_tutorial/#fork-dataset","title":"Fork dataset\u00b6","text":"<p>Used to create a copy of the dataset with you as the owner. Use this API only if you are unable to edit the critical fields (default_target_attribute, ignore_attribute, row_id_attribute) of a dataset through the edit_dataset API. After the dataset is forked, you can edit the new version of the dataset using edit_dataset.</p>"},{"location":"examples/Advanced/fetch_evaluations_tutorial/","title":"Downloading Evaluation Results","text":"<p>Evaluations contain a concise summary of the results of all runs made. Each evaluation provides information on the dataset used, the flow applied, the setup used, the metric evaluated, and the result obtained on the metric, for each such run made. These collection of results can be used for efficient benchmarking of an algorithm and also allow transparent reuse of results from previous experiments on similar parameters.</p> <p>In this example, we shall do the following:</p> <ul> <li>Retrieve evaluations based on different metrics</li> <li>Fetch evaluations pertaining to a specific task</li> <li>Sort the obtained results in descending order of the metric</li> <li>Plot a cumulative distribution function for the evaluations</li> <li>Compare the top 10 performing flows based on the evaluation performance</li> <li>Retrieve evaluations with hyperparameter settings</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import openml\n</pre> import openml In\u00a0[\u00a0]: Copied! <pre>openml.evaluations.list_evaluations(function=\"predictive_accuracy\", size=10)\n\n# Using other evaluation metrics, 'precision' in this case\nevals = openml.evaluations.list_evaluations(\n    function=\"precision\", size=10, output_format=\"dataframe\"\n)\n\n# Querying the returned results for precision above 0.98\nprint(evals[evals.value &gt; 0.98])\n</pre> openml.evaluations.list_evaluations(function=\"predictive_accuracy\", size=10)  # Using other evaluation metrics, 'precision' in this case evals = openml.evaluations.list_evaluations(     function=\"precision\", size=10, output_format=\"dataframe\" )  # Querying the returned results for precision above 0.98 print(evals[evals.value &gt; 0.98]) In\u00a0[\u00a0]: Copied! <pre>task_id = 167140  # https://www.openml.org/t/167140\ntask = openml.tasks.get_task(task_id)\nprint(task)\n</pre> task_id = 167140  # https://www.openml.org/t/167140 task = openml.tasks.get_task(task_id) print(task) In\u00a0[\u00a0]: Copied! <pre>metric = \"predictive_accuracy\"\nevals = openml.evaluations.list_evaluations(\n    function=metric, tasks=[task_id], output_format=\"dataframe\"\n)\n# Displaying the first 10 rows\nprint(evals.head(n=10))\n# Sorting the evaluations in decreasing order of the metric chosen\nevals = evals.sort_values(by=\"value\", ascending=False)\nprint(\"\\nDisplaying head of sorted dataframe: \")\nprint(evals.head())\n</pre> metric = \"predictive_accuracy\" evals = openml.evaluations.list_evaluations(     function=metric, tasks=[task_id], output_format=\"dataframe\" ) # Displaying the first 10 rows print(evals.head(n=10)) # Sorting the evaluations in decreasing order of the metric chosen evals = evals.sort_values(by=\"value\", ascending=False) print(\"\\nDisplaying head of sorted dataframe: \") print(evals.head()) In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\n\n\ndef plot_cdf(values, metric=\"predictive_accuracy\"):\n    max_val = max(values)\n    n, bins, patches = plt.hist(values, density=True, histtype=\"step\", cumulative=True, linewidth=3)\n    patches[0].set_xy(patches[0].get_xy()[:-1])\n    plt.xlim(max(0, min(values) - 0.1), 1)\n    plt.title(\"CDF\")\n    plt.xlabel(metric)\n    plt.ylabel(\"Likelihood\")\n    plt.grid(visible=True, which=\"major\", linestyle=\"-\")\n    plt.minorticks_on()\n    plt.grid(visible=True, which=\"minor\", linestyle=\"--\")\n    plt.axvline(max_val, linestyle=\"--\", color=\"gray\")\n    plt.text(max_val, 0, f\"{max_val:.3f}\", fontsize=9)\n    plt.show()\n\n\nplot_cdf(evals.value, metric)\n</pre> from matplotlib import pyplot as plt   def plot_cdf(values, metric=\"predictive_accuracy\"):     max_val = max(values)     n, bins, patches = plt.hist(values, density=True, histtype=\"step\", cumulative=True, linewidth=3)     patches[0].set_xy(patches[0].get_xy()[:-1])     plt.xlim(max(0, min(values) - 0.1), 1)     plt.title(\"CDF\")     plt.xlabel(metric)     plt.ylabel(\"Likelihood\")     plt.grid(visible=True, which=\"major\", linestyle=\"-\")     plt.minorticks_on()     plt.grid(visible=True, which=\"minor\", linestyle=\"--\")     plt.axvline(max_val, linestyle=\"--\", color=\"gray\")     plt.text(max_val, 0, f\"{max_val:.3f}\", fontsize=9)     plt.show()   plot_cdf(evals.value, metric) <p>This CDF plot shows that for the given task, based on the results of the runs uploaded, it is almost certain to achieve an accuracy above 52%, i.e., with non-zero probability. While the maximum accuracy seen till now is 96.5%.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\n\n\ndef plot_flow_compare(evaluations, top_n=10, metric=\"predictive_accuracy\"):\n    # Collecting the top 10 performing unique flow_id\n    flow_ids = evaluations.flow_id.unique()[:top_n]\n\n    df = pd.DataFrame()\n    # Creating a data frame containing only the metric values of the selected flows\n    #   assuming evaluations is sorted in decreasing order of metric\n    for i in range(len(flow_ids)):\n        flow_values = evaluations[evaluations.flow_id == flow_ids[i]].value\n        df = pd.concat([df, flow_values], ignore_index=True, axis=1)\n    fig, axs = plt.subplots()\n    df.boxplot()\n    axs.set_title(\"Boxplot comparing \" + metric + \" for different flows\")\n    axs.set_ylabel(metric)\n    axs.set_xlabel(\"Flow ID\")\n    axs.set_xticklabels(flow_ids)\n    axs.grid(which=\"major\", linestyle=\"-\", linewidth=\"0.5\", color=\"gray\", axis=\"y\")\n    axs.minorticks_on()\n    axs.grid(which=\"minor\", linestyle=\"--\", linewidth=\"0.5\", color=\"gray\", axis=\"y\")\n    # Counting the number of entries for each flow in the data frame\n    #   which gives the number of runs for each flow\n    flow_freq = list(df.count(axis=0, numeric_only=True))\n    for i in range(len(flow_ids)):\n        axs.text(i + 1.05, np.nanmin(df.values), str(flow_freq[i]) + \"\\nrun(s)\", fontsize=7)\n    plt.show()\n\n\nplot_flow_compare(evals, metric=metric, top_n=10)\n</pre> import numpy as np import pandas as pd   def plot_flow_compare(evaluations, top_n=10, metric=\"predictive_accuracy\"):     # Collecting the top 10 performing unique flow_id     flow_ids = evaluations.flow_id.unique()[:top_n]      df = pd.DataFrame()     # Creating a data frame containing only the metric values of the selected flows     #   assuming evaluations is sorted in decreasing order of metric     for i in range(len(flow_ids)):         flow_values = evaluations[evaluations.flow_id == flow_ids[i]].value         df = pd.concat([df, flow_values], ignore_index=True, axis=1)     fig, axs = plt.subplots()     df.boxplot()     axs.set_title(\"Boxplot comparing \" + metric + \" for different flows\")     axs.set_ylabel(metric)     axs.set_xlabel(\"Flow ID\")     axs.set_xticklabels(flow_ids)     axs.grid(which=\"major\", linestyle=\"-\", linewidth=\"0.5\", color=\"gray\", axis=\"y\")     axs.minorticks_on()     axs.grid(which=\"minor\", linestyle=\"--\", linewidth=\"0.5\", color=\"gray\", axis=\"y\")     # Counting the number of entries for each flow in the data frame     #   which gives the number of runs for each flow     flow_freq = list(df.count(axis=0, numeric_only=True))     for i in range(len(flow_ids)):         axs.text(i + 1.05, np.nanmin(df.values), str(flow_freq[i]) + \"\\nrun(s)\", fontsize=7)     plt.show()   plot_flow_compare(evals, metric=metric, top_n=10) <p>The boxplots below show how the flows perform across multiple runs on the chosen task. The green horizontal lines represent the median accuracy of all the runs for that flow (number of runs denoted at the bottom of the boxplots). The higher the green line, the better the flow is for the task at hand. The ordering of the flows are in the descending order of the higest accuracy value seen under that flow.</p> <p>Printing the corresponding flow names for the top 10 performing flow IDs</p> In\u00a0[\u00a0]: Copied! <pre>top_n = 10\nflow_ids = evals.flow_id.unique()[:top_n]\nflow_names = evals.flow_name.unique()[:top_n]\nfor i in range(top_n):\n    print((flow_ids[i], flow_names[i]))\n</pre> top_n = 10 flow_ids = evals.flow_id.unique()[:top_n] flow_names = evals.flow_name.unique()[:top_n] for i in range(top_n):     print((flow_ids[i], flow_names[i])) In\u00a0[\u00a0]: Copied! <pre>evals_setups = openml.evaluations.list_evaluations_setups(\n    function=\"predictive_accuracy\",\n    tasks=[31],\n    size=100,\n    sort_order=\"desc\",\n)\n\nprint(evals_setups.head())\n</pre> evals_setups = openml.evaluations.list_evaluations_setups(     function=\"predictive_accuracy\",     tasks=[31],     size=100,     sort_order=\"desc\", )  print(evals_setups.head()) <p>Return evaluations for flow_id in descending order based on predictive_accuracy with hyperparameters. parameters_in_separate_columns returns parameters in separate columns</p> In\u00a0[\u00a0]: Copied! <pre>evals_setups = openml.evaluations.list_evaluations_setups(\n    function=\"predictive_accuracy\", flows=[6767], size=100, parameters_in_separate_columns=True\n)\n\nprint(evals_setups.head(10))\n</pre> evals_setups = openml.evaluations.list_evaluations_setups(     function=\"predictive_accuracy\", flows=[6767], size=100, parameters_in_separate_columns=True )  print(evals_setups.head(10))"},{"location":"examples/Advanced/fetch_evaluations_tutorial/#listing-evaluations","title":"Listing evaluations\u00b6","text":"<p>Evaluations can be retrieved from the database in the chosen output format. Required filters can be applied to retrieve results from runs as required.</p> <p>We shall retrieve a small set (only 10 entries) to test the listing function for evaluations</p>"},{"location":"examples/Advanced/fetch_evaluations_tutorial/#viewing-a-sample-task","title":"Viewing a sample task\u00b6","text":"<p>Over here we shall briefly take a look at the details of the task. We will start by displaying a simple supervised classification task:</p>"},{"location":"examples/Advanced/fetch_evaluations_tutorial/#obtaining-all-the-evaluations-for-the-task","title":"Obtaining all the evaluations for the task\u00b6","text":"<p>We'll now obtain all the evaluations that were uploaded for the task we displayed previously. Note that we now filter the evaluations based on another parameter 'task'.</p>"},{"location":"examples/Advanced/fetch_evaluations_tutorial/#obtaining-cdf-of-metric-for-chosen-task","title":"Obtaining CDF of metric for chosen task\u00b6","text":"<p>We shall now analyse how the performance of various flows have been on this task, by seeing the likelihood of the accuracy obtained across all runs. We shall now plot a cumulative distributive function (CDF) for the accuracies obtained.</p>"},{"location":"examples/Advanced/fetch_evaluations_tutorial/#comparing-top-10-performing-flows","title":"Comparing top 10 performing flows\u00b6","text":"<p>Let us now try to see which flows generally performed the best for this task. For this, we shall compare the top performing flows.</p>"},{"location":"examples/Advanced/fetch_evaluations_tutorial/#obtaining-evaluations-with-hyperparameter-settings","title":"Obtaining evaluations with hyperparameter settings\u00b6","text":"<p>We'll now obtain the evaluations of a task and a flow with the hyperparameters</p> <p>List evaluations in descending order based on predictive_accuracy with hyperparameters</p>"},{"location":"examples/Advanced/study_tutorial/","title":"List, Download, and Upload Studies","text":"<p>How to list, download and upload benchmark studies. In contrast to benchmark suites which hold a list of tasks, studies hold a list of runs. As runs contain all information on flows and tasks, all required information about a study can be retrieved.</p> In\u00a0[\u00a0]: Copied! <pre>import uuid\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport openml\n</pre> import uuid  from sklearn.ensemble import RandomForestClassifier  import openml In\u00a0[\u00a0]: Copied! <pre>studies = openml.study.list_studies(status=\"all\")\nprint(studies.head(n=10))\n</pre> studies = openml.study.list_studies(status=\"all\") print(studies.head(n=10)) In\u00a0[\u00a0]: Copied! <pre>study = openml.study.get_study(123)\nprint(study)\n</pre> study = openml.study.get_study(123) print(study) <p>Studies also features a description:</p> In\u00a0[\u00a0]: Copied! <pre>print(study.description)\n</pre> print(study.description) <p>Studies are a container for runs:</p> In\u00a0[\u00a0]: Copied! <pre>print(study.runs)\n</pre> print(study.runs) <p>And we can use the evaluation listing functionality to learn more about the evaluations available for the conducted runs:</p> In\u00a0[\u00a0]: Copied! <pre>evaluations = openml.evaluations.list_evaluations(\n    function=\"predictive_accuracy\",\n    study=study.study_id,\n    output_format=\"dataframe\",\n)\nprint(evaluations.head())\n</pre> evaluations = openml.evaluations.list_evaluations(     function=\"predictive_accuracy\",     study=study.study_id,     output_format=\"dataframe\", ) print(evaluations.head()) <p>We'll use the test server for the rest of this tutorial.</p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() In\u00a0[\u00a0]: Copied! <pre># Get sklearn extension to run sklearn models easily on OpenML tasks.\nfrom openml_sklearn import SklearnExtension\n\nextension = SklearnExtension()\n\n# Model to be used\nclf = RandomForestClassifier()\n\n# We'll create a study with one run on 3 datasets present in the suite\ntasks = [115, 259, 307]\n\n# To verify\n# https://test.openml.org/api/v1/study/1\nsuite = openml.study.get_suite(\"OpenML100\")\nprint(all(t_id in suite.tasks for t_id in tasks))\n\nrun_ids = []\nfor task_id in tasks:\n    task = openml.tasks.get_task(task_id)\n    run = openml.runs.run_model_on_task(clf, task)\n    run.publish()\n    run_ids.append(run.run_id)\n\n# The study needs a machine-readable and unique alias. To obtain this,\n# we simply generate a random uuid.\nalias = uuid.uuid4().hex\n\nnew_study = openml.study.create_study(\n    name=\"Test-Study\",\n    description=\"Test study for the Python tutorial on studies\",\n    run_ids=run_ids,\n    alias=alias,\n    benchmark_suite=suite.study_id,\n)\nnew_study.publish()\nprint(new_study)\n</pre> # Get sklearn extension to run sklearn models easily on OpenML tasks. from openml_sklearn import SklearnExtension  extension = SklearnExtension()  # Model to be used clf = RandomForestClassifier()  # We'll create a study with one run on 3 datasets present in the suite tasks = [115, 259, 307]  # To verify # https://test.openml.org/api/v1/study/1 suite = openml.study.get_suite(\"OpenML100\") print(all(t_id in suite.tasks for t_id in tasks))  run_ids = [] for task_id in tasks:     task = openml.tasks.get_task(task_id)     run = openml.runs.run_model_on_task(clf, task)     run.publish()     run_ids.append(run.run_id)  # The study needs a machine-readable and unique alias. To obtain this, # we simply generate a random uuid. alias = uuid.uuid4().hex  new_study = openml.study.create_study(     name=\"Test-Study\",     description=\"Test study for the Python tutorial on studies\",     run_ids=run_ids,     alias=alias,     benchmark_suite=suite.study_id, ) new_study.publish() print(new_study) In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n</pre> openml.config.stop_using_configuration_for_example()"},{"location":"examples/Advanced/study_tutorial/#listing-studies","title":"Listing studies\u00b6","text":"<ul> <li>Use the output_format parameter to select output type</li> <li>Default gives <code>dict</code>, but we'll use <code>dataframe</code> to obtain an easier-to-work-with data structure</li> </ul>"},{"location":"examples/Advanced/study_tutorial/#downloading-studies","title":"Downloading studies\u00b6","text":"<p>This is done based on the study ID.</p>"},{"location":"examples/Advanced/study_tutorial/#uploading-studies","title":"Uploading studies\u00b6","text":"<p>Creating a study is as simple as creating any kind of other OpenML entity. In this examples we'll create a few runs for the OpenML-100 benchmark suite which is available on the OpenML test server.</p> <p>Warning</p> <p>         For the rest of this tutorial, we will require the `openml-sklearn` package.         Install it with `pip install openml-sklearn`.     </p>"},{"location":"examples/Advanced/suites_tutorial/","title":"List, Download, and Upload Suites","text":"<p>How to list, download and upload benchmark suites.</p> In\u00a0[\u00a0]: Copied! <pre>import uuid\n\nimport numpy as np\n\nimport openml\n</pre> import uuid  import numpy as np  import openml In\u00a0[\u00a0]: Copied! <pre>suites = openml.study.list_suites(status=\"all\")\nprint(suites.head(n=10))\n</pre> suites = openml.study.list_suites(status=\"all\") print(suites.head(n=10)) In\u00a0[\u00a0]: Copied! <pre>suite = openml.study.get_suite(99)\nprint(suite)\n</pre> suite = openml.study.get_suite(99) print(suite) <p>Suites also feature a description:</p> In\u00a0[\u00a0]: Copied! <pre>print(suite.description)\n</pre> print(suite.description) <p>Suites are a container for tasks:</p> In\u00a0[\u00a0]: Copied! <pre>print(suite.tasks)\n</pre> print(suite.tasks) <p>And we can use the task listing functionality to learn more about them:</p> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks()\n</pre> tasks = openml.tasks.list_tasks() <p>Using <code>@</code> in pd.DataFrame.query accesses variables outside of the current dataframe.</p> In\u00a0[\u00a0]: Copied! <pre>tasks = tasks.query(\"tid in @suite.tasks\")\nprint(tasks.describe().transpose())\n</pre> tasks = tasks.query(\"tid in @suite.tasks\") print(tasks.describe().transpose()) <p>We'll use the test server for the rest of this tutorial.</p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() In\u00a0[\u00a0]: Copied! <pre>all_tasks = list(openml.tasks.list_tasks()[\"tid\"])\ntask_ids_for_suite = sorted(np.random.choice(all_tasks, replace=False, size=20))\n\n# The study needs a machine-readable and unique alias. To obtain this,\n# we simply generate a random uuid.\n\nalias = uuid.uuid4().hex\n\nnew_suite = openml.study.create_benchmark_suite(\n    name=\"Test-Suite\",\n    description=\"Test suite for the Python tutorial on benchmark suites\",\n    task_ids=task_ids_for_suite,\n    alias=alias,\n)\nnew_suite.publish()\nprint(new_suite)\n</pre> all_tasks = list(openml.tasks.list_tasks()[\"tid\"]) task_ids_for_suite = sorted(np.random.choice(all_tasks, replace=False, size=20))  # The study needs a machine-readable and unique alias. To obtain this, # we simply generate a random uuid.  alias = uuid.uuid4().hex  new_suite = openml.study.create_benchmark_suite(     name=\"Test-Suite\",     description=\"Test suite for the Python tutorial on benchmark suites\",     task_ids=task_ids_for_suite,     alias=alias, ) new_suite.publish() print(new_suite) In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n</pre> openml.config.stop_using_configuration_for_example()"},{"location":"examples/Advanced/suites_tutorial/#listing-suites","title":"Listing suites\u00b6","text":"<ul> <li>Use the output_format parameter to select output type</li> <li>Default gives <code>dict</code>, but we'll use <code>dataframe</code> to obtain an easier-to-work-with data structure</li> </ul>"},{"location":"examples/Advanced/suites_tutorial/#downloading-suites","title":"Downloading suites\u00b6","text":"<p>This is done based on the dataset ID.</p>"},{"location":"examples/Advanced/suites_tutorial/#uploading-suites","title":"Uploading suites\u00b6","text":"<p>Uploading suites is as simple as uploading any kind of other OpenML entity - the only reason why we need so much code in this example is because we upload some random data.</p> <p>We'll take a random subset of at least ten tasks of all available tasks on the test server:</p>"},{"location":"examples/Advanced/task_manual_iteration_tutorial/","title":"Dataset Splits from Tasks","text":"<p>Tasks define a target and a train/test split, which we can use for benchmarking.</p> In\u00a0[\u00a0]: Copied! <pre>import openml\n</pre> import openml <p>For this tutorial we will use the famous King+Rook versus King+Pawn on A7 dataset, which has the dataset ID 3 (dataset on OpenML), and for which there exist tasks with all important estimation procedures. It is small enough (less than 5000 samples) to efficiently use it in an example.</p> <p>We will first start with (task 233), which is a task with a holdout estimation procedure.</p> In\u00a0[\u00a0]: Copied! <pre>task_id = 233\ntask = openml.tasks.get_task(task_id)\n</pre> task_id = 233 task = openml.tasks.get_task(task_id) <p>Now that we have a task object we can obtain the number of repetitions, folds and samples as defined by the task:</p> In\u00a0[\u00a0]: Copied! <pre>n_repeats, n_folds, n_samples = task.get_split_dimensions()\n</pre> n_repeats, n_folds, n_samples = task.get_split_dimensions() <ul> <li><code>n_repeats</code>: Number of times the model quality estimation is performed</li> <li><code>n_folds</code>: Number of folds per repeat</li> <li><code>n_samples</code>: How many data points to use. This is only relevant for learning curve tasks</li> </ul> <p>A list of all available estimation procedures is available here.</p> <p>Task <code>233</code> is a simple task using the holdout estimation procedure and therefore has only a single repeat, a single fold and a single sample size:</p> In\u00a0[\u00a0]: Copied! <pre>print(\n    f\"Task {task_id}: number of repeats: {n_repeats}, number of folds: {n_folds}, number of samples {n_samples}.\"\n)\n</pre> print(     f\"Task {task_id}: number of repeats: {n_repeats}, number of folds: {n_folds}, number of samples {n_samples}.\" ) <p>We can now retrieve the train/test split for this combination of repeats, folds and number of samples (indexing is zero-based). Usually, one would loop over all repeats, folds and sample sizes, but we can neglect this here as there is only a single repetition.</p> In\u00a0[\u00a0]: Copied! <pre>train_indices, test_indices = task.get_train_test_split_indices(\n    repeat=0,\n    fold=0,\n    sample=0,\n)\n\nprint(train_indices.shape, train_indices.dtype)\nprint(test_indices.shape, test_indices.dtype)\n</pre> train_indices, test_indices = task.get_train_test_split_indices(     repeat=0,     fold=0,     sample=0, )  print(train_indices.shape, train_indices.dtype) print(test_indices.shape, test_indices.dtype) <p>And then split the data based on this:</p> In\u00a0[\u00a0]: Copied! <pre>X, y = task.get_X_and_y()\nX_train = X.iloc[train_indices]\ny_train = y.iloc[train_indices]\nX_test = X.iloc[test_indices]\ny_test = y.iloc[test_indices]\n\nprint(\n    f\"X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}, X_test.shape: {X_test.shape}, y_test.shape: {y_test.shape}\"\n)\n</pre> X, y = task.get_X_and_y() X_train = X.iloc[train_indices] y_train = y.iloc[train_indices] X_test = X.iloc[test_indices] y_test = y.iloc[test_indices]  print(     f\"X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}, X_test.shape: {X_test.shape}, y_test.shape: {y_test.shape}\" ) <p>Obviously, we can also retrieve cross-validation versions of the dataset used in task <code>233</code>:</p> In\u00a0[\u00a0]: Copied! <pre>task_id = 3\ntask = openml.tasks.get_task(task_id)\nX, y = task.get_X_and_y()\nn_repeats, n_folds, n_samples = task.get_split_dimensions()\nprint(\n    f\"Task {task_id}: number of repeats: {n_repeats}, number of folds: {n_folds}, number of samples {n_samples}.\"\n)\n</pre> task_id = 3 task = openml.tasks.get_task(task_id) X, y = task.get_X_and_y() n_repeats, n_folds, n_samples = task.get_split_dimensions() print(     f\"Task {task_id}: number of repeats: {n_repeats}, number of folds: {n_folds}, number of samples {n_samples}.\" ) <p>And then perform the aforementioned iteration over all splits:</p> In\u00a0[\u00a0]: Copied! <pre>for repeat_idx in range(n_repeats):\n    for fold_idx in range(n_folds):\n        for sample_idx in range(n_samples):\n            train_indices, test_indices = task.get_train_test_split_indices(\n                repeat=repeat_idx,\n                fold=fold_idx,\n                sample=sample_idx,\n            )\n            X_train = X.iloc[train_indices]\n            y_train = y.iloc[train_indices]\n            X_test = X.iloc[test_indices]\n            y_test = y.iloc[test_indices]\n\n            print(\n                f\"Repeat #{repeat_idx}, fold #{fold_idx}, samples {sample_idx}: X_train.shape: {X_train.shape}, \"\n                f\"y_train.shape {y_train.shape}, X_test.shape {X_test.shape}, y_test.shape {y_test.shape}\"\n            )\n</pre> for repeat_idx in range(n_repeats):     for fold_idx in range(n_folds):         for sample_idx in range(n_samples):             train_indices, test_indices = task.get_train_test_split_indices(                 repeat=repeat_idx,                 fold=fold_idx,                 sample=sample_idx,             )             X_train = X.iloc[train_indices]             y_train = y.iloc[train_indices]             X_test = X.iloc[test_indices]             y_test = y.iloc[test_indices]              print(                 f\"Repeat #{repeat_idx}, fold #{fold_idx}, samples {sample_idx}: X_train.shape: {X_train.shape}, \"                 f\"y_train.shape {y_train.shape}, X_test.shape {X_test.shape}, y_test.shape {y_test.shape}\"             ) <p>And also versions with multiple repeats:</p> In\u00a0[\u00a0]: Copied! <pre>task_id = 1767\ntask = openml.tasks.get_task(task_id)\nX, y = task.get_X_and_y()\nn_repeats, n_folds, n_samples = task.get_split_dimensions()\nprint(\n    f\"Task {task_id}: number of repeats: {n_repeats}, number of folds: {n_folds}, number of samples {n_samples}.\"\n)\n</pre> task_id = 1767 task = openml.tasks.get_task(task_id) X, y = task.get_X_and_y() n_repeats, n_folds, n_samples = task.get_split_dimensions() print(     f\"Task {task_id}: number of repeats: {n_repeats}, number of folds: {n_folds}, number of samples {n_samples}.\" ) <p>And then again perform the aforementioned iteration over all splits:</p> In\u00a0[\u00a0]: Copied! <pre>for repeat_idx in range(n_repeats):\n    for fold_idx in range(n_folds):\n        for sample_idx in range(n_samples):\n            train_indices, test_indices = task.get_train_test_split_indices(\n                repeat=repeat_idx,\n                fold=fold_idx,\n                sample=sample_idx,\n            )\n            X_train = X.iloc[train_indices]\n            y_train = y.iloc[train_indices]\n            X_test = X.iloc[test_indices]\n            y_test = y.iloc[test_indices]\n\n            print(\n                f\"Repeat #{repeat_idx}, fold #{fold_idx}, samples {sample_idx}: X_train.shape: {X_train.shape}, \"\n                f\"y_train.shape {y_train.shape}, X_test.shape {X_test.shape}, y_test.shape {y_test.shape}\"\n            )\n</pre> for repeat_idx in range(n_repeats):     for fold_idx in range(n_folds):         for sample_idx in range(n_samples):             train_indices, test_indices = task.get_train_test_split_indices(                 repeat=repeat_idx,                 fold=fold_idx,                 sample=sample_idx,             )             X_train = X.iloc[train_indices]             y_train = y.iloc[train_indices]             X_test = X.iloc[test_indices]             y_test = y.iloc[test_indices]              print(                 f\"Repeat #{repeat_idx}, fold #{fold_idx}, samples {sample_idx}: X_train.shape: {X_train.shape}, \"                 f\"y_train.shape {y_train.shape}, X_test.shape {X_test.shape}, y_test.shape {y_test.shape}\"             ) <p>And finally a task based on learning curves:</p> In\u00a0[\u00a0]: Copied! <pre>task_id = 1702\ntask = openml.tasks.get_task(task_id)\nX, y = task.get_X_and_y()\nn_repeats, n_folds, n_samples = task.get_split_dimensions()\nprint(\n    f\"Task {task_id}: number of repeats: {n_repeats}, number of folds: {n_folds}, number of samples {n_samples}.\"\n)\n</pre> task_id = 1702 task = openml.tasks.get_task(task_id) X, y = task.get_X_and_y() n_repeats, n_folds, n_samples = task.get_split_dimensions() print(     f\"Task {task_id}: number of repeats: {n_repeats}, number of folds: {n_folds}, number of samples {n_samples}.\" ) <p>And then again perform the aforementioned iteration over all splits:</p> In\u00a0[\u00a0]: Copied! <pre>for repeat_idx in range(n_repeats):\n    for fold_idx in range(n_folds):\n        for sample_idx in range(n_samples):\n            train_indices, test_indices = task.get_train_test_split_indices(\n                repeat=repeat_idx,\n                fold=fold_idx,\n                sample=sample_idx,\n            )\n            X_train = X.iloc[train_indices]\n            y_train = y.iloc[train_indices]\n            X_test = X.iloc[test_indices]\n            y_test = y.iloc[test_indices]\n\n            print(\n                f\"Repeat #{repeat_idx}, fold #{fold_idx}, samples {sample_idx}: X_train.shape: {X_train.shape}, \"\n                f\"y_train.shape {y_train.shape}, X_test.shape {X_test.shape}, y_test.shape {y_test.shape}\"\n            )\n</pre> for repeat_idx in range(n_repeats):     for fold_idx in range(n_folds):         for sample_idx in range(n_samples):             train_indices, test_indices = task.get_train_test_split_indices(                 repeat=repeat_idx,                 fold=fold_idx,                 sample=sample_idx,             )             X_train = X.iloc[train_indices]             y_train = y.iloc[train_indices]             X_test = X.iloc[test_indices]             y_test = y.iloc[test_indices]              print(                 f\"Repeat #{repeat_idx}, fold #{fold_idx}, samples {sample_idx}: X_train.shape: {X_train.shape}, \"                 f\"y_train.shape {y_train.shape}, X_test.shape {X_test.shape}, y_test.shape {y_test.shape}\"             )"},{"location":"examples/Advanced/tasks_tutorial/","title":"Searching and Creating Tasks","text":"<p>A tutorial on how to list and download tasks.</p> In\u00a0[\u00a0]: Copied! <pre>import openml\nfrom openml.tasks import TaskType\n</pre> import openml from openml.tasks import TaskType <p>Tasks are identified by IDs and can be accessed in two different ways:</p> <ol> <li>In a list providing basic information on all tasks available on OpenML. This function will not download the actual tasks, but will instead download meta data that can be used to filter the tasks and retrieve a set of IDs. We can filter this list, for example, we can only list tasks having a special tag or only tasks for a specific target such as supervised classification.</li> <li>A single task by its ID. It contains all meta information, the target metric, the splits and an iterator which can be used to access the splits in a useful manner.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks(task_type=TaskType.SUPERVISED_CLASSIFICATION)\nprint(tasks.columns)\nprint(f\"First 5 of {len(tasks)} tasks:\")\nprint(tasks.head())\n</pre> tasks = openml.tasks.list_tasks(task_type=TaskType.SUPERVISED_CLASSIFICATION) print(tasks.columns) print(f\"First 5 of {len(tasks)} tasks:\") print(tasks.head()) <p>We can filter the list of tasks to only contain datasets with more than 500 samples, but less than 1000 samples:</p> In\u00a0[\u00a0]: Copied! <pre>filtered_tasks = tasks.query(\"NumberOfInstances &gt; 500 and NumberOfInstances &lt; 1000\")\nprint(list(filtered_tasks.index))\n</pre> filtered_tasks = tasks.query(\"NumberOfInstances &gt; 500 and NumberOfInstances &lt; 1000\") print(list(filtered_tasks.index)) In\u00a0[\u00a0]: Copied! <pre># Number of tasks\nprint(len(filtered_tasks))\n</pre> # Number of tasks print(len(filtered_tasks)) <p>Then, we can further restrict the tasks to all have the same resampling strategy:</p> In\u00a0[\u00a0]: Copied! <pre>filtered_tasks = filtered_tasks.query('estimation_procedure == \"10-fold Crossvalidation\"')\nprint(list(filtered_tasks.index))\n</pre> filtered_tasks = filtered_tasks.query('estimation_procedure == \"10-fold Crossvalidation\"') print(list(filtered_tasks.index)) In\u00a0[\u00a0]: Copied! <pre># Number of tasks\nprint(len(filtered_tasks))\n</pre> # Number of tasks print(len(filtered_tasks)) <p>Resampling strategies can be found on the OpenML Website.</p> <p>Similar to listing tasks by task type, we can list tasks by tags:</p> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks(tag=\"OpenML100\")\nprint(f\"First 5 of {len(tasks)} tasks:\")\nprint(tasks.head())\n</pre> tasks = openml.tasks.list_tasks(tag=\"OpenML100\") print(f\"First 5 of {len(tasks)} tasks:\") print(tasks.head()) <p>Furthermore, we can list tasks based on the dataset id:</p> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks(data_id=1471)\nprint(f\"First 5 of {len(tasks)} tasks:\")\nprint(tasks.head())\n</pre> tasks = openml.tasks.list_tasks(data_id=1471) print(f\"First 5 of {len(tasks)} tasks:\") print(tasks.head()) <p>In addition, a size limit and an offset can be applied both separately and simultaneously:</p> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks(size=10, offset=50)\nprint(tasks)\n</pre> tasks = openml.tasks.list_tasks(size=10, offset=50) print(tasks) <p>OpenML 100 is a curated list of 100 tasks to start using OpenML. They are all supervised classification tasks with more than 500 instances and less than 50000 instances per task. To make things easier, the tasks do not contain highly unbalanced data and sparse data. However, the tasks include missing values and categorical features. You can find out more about the OpenML 100 on the OpenML benchmarking page.</p> <p>Finally, it is also possible to list all tasks on OpenML with:</p> In\u00a0[\u00a0]: Copied! <pre>tasks = openml.tasks.list_tasks()\nprint(len(tasks))\n</pre> tasks = openml.tasks.list_tasks() print(len(tasks)) In\u00a0[\u00a0]: Copied! <pre>tasks.query('name==\"eeg-eye-state\"')\n</pre> tasks.query('name==\"eeg-eye-state\"') In\u00a0[\u00a0]: Copied! <pre>task_id = 31\ntask = openml.tasks.get_task(task_id)\n</pre> task_id = 31 task = openml.tasks.get_task(task_id) In\u00a0[\u00a0]: Copied! <pre># Properties of the task are stored as member variables:\nprint(task)\n</pre> # Properties of the task are stored as member variables: print(task) In\u00a0[\u00a0]: Copied! <pre># And:\n\nids = [2, 1891, 31, 9983]\ntasks = openml.tasks.get_tasks(ids)\nprint(tasks[0])\n</pre> # And:  ids = [2, 1891, 31, 9983] tasks = openml.tasks.get_tasks(ids) print(tasks[0]) <p>We'll use the test server for the rest of this tutorial.</p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() In\u00a0[\u00a0]: Copied! <pre>try:\n    my_task = openml.tasks.create_task(\n        task_type=TaskType.SUPERVISED_CLASSIFICATION,\n        dataset_id=128,\n        target_name=\"class\",\n        evaluation_measure=\"predictive_accuracy\",\n        estimation_procedure_id=1,\n    )\n    my_task.publish()\nexcept openml.exceptions.OpenMLServerException as e:\n    # Error code for 'task already exists'\n    if e.code == 614:\n        # Lookup task\n        tasks = openml.tasks.list_tasks(data_id=128)\n        tasks = tasks.query(\n            'task_type == \"Supervised Classification\" '\n            'and estimation_procedure == \"10-fold Crossvalidation\" '\n            'and evaluation_measures == \"predictive_accuracy\"'\n        )\n        task_id = tasks.loc[:, \"tid\"].values[0]\n        print(\"Task already exists. Task ID is\", task_id)\n</pre> try:     my_task = openml.tasks.create_task(         task_type=TaskType.SUPERVISED_CLASSIFICATION,         dataset_id=128,         target_name=\"class\",         evaluation_measure=\"predictive_accuracy\",         estimation_procedure_id=1,     )     my_task.publish() except openml.exceptions.OpenMLServerException as e:     # Error code for 'task already exists'     if e.code == 614:         # Lookup task         tasks = openml.tasks.list_tasks(data_id=128)         tasks = tasks.query(             'task_type == \"Supervised Classification\" '             'and estimation_procedure == \"10-fold Crossvalidation\" '             'and evaluation_measures == \"predictive_accuracy\"'         )         task_id = tasks.loc[:, \"tid\"].values[0]         print(\"Task already exists. Task ID is\", task_id) In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n</pre> openml.config.stop_using_configuration_for_example()"},{"location":"examples/Advanced/tasks_tutorial/#listing-tasks","title":"Listing tasks\u00b6","text":"<p>We will start by simply listing only supervised classification tasks.</p> <p>openml.tasks.list_tasks() returns a dictionary of dictionaries by default, but we request a pandas dataframe instead to have better visualization capabilities and easier access:</p>"},{"location":"examples/Advanced/tasks_tutorial/#exercise","title":"Exercise\u00b6","text":"<p>Search for the tasks on the 'eeg-eye-state' dataset.</p>"},{"location":"examples/Advanced/tasks_tutorial/#downloading-tasks","title":"Downloading tasks\u00b6","text":"<p>We provide two functions to download tasks, one which downloads only a single task by its ID, and one which takes a list of IDs and downloads all of these tasks:</p>"},{"location":"examples/Advanced/tasks_tutorial/#creating-tasks","title":"Creating tasks\u00b6","text":"<p>You can also create new tasks. Take the following into account:</p> <ul> <li>You can only create tasks on active datasets</li> <li>For now, only the following tasks are supported: classification, regression, clustering, and learning curve analysis.</li> <li>For now, tasks can only be created on a single dataset.</li> <li>The exact same task must not already exist.</li> </ul> <p>Creating a task requires the following input:</p> <ul> <li>task_type: The task type ID, required (see below). Required.</li> <li>dataset_id: The dataset ID. Required.</li> <li>target_name: The name of the attribute you aim to predict. Optional.</li> <li>estimation_procedure_id : The ID of the estimation procedure used to create train-test splits. Optional.</li> <li>evaluation_measure: The name of the evaluation measure. Optional.</li> <li>Any additional inputs for specific tasks</li> </ul> <p>It is best to leave the evaluation measure open if there is no strong prerequisite for a specific measure. OpenML will always compute all appropriate measures and you can filter or sort results on your favourite measure afterwards. Only add an evaluation measure if necessary (e.g. when other measure make no sense), since it will create a new task, which scatters results across tasks.</p>"},{"location":"examples/Advanced/tasks_tutorial/#example","title":"Example\u00b6","text":"<p>Let's create a classification task on a dataset. In this example we will do this on the Iris dataset (ID=128 (on test server)). We'll use 10-fold cross-validation (ID=1), and predictive accuracy as the predefined measure (this can also be left open). If a task with these parameters exists, we will get an appropriate exception. If such a task doesn't exist, a task will be created and the corresponding task_id will be returned.</p>"},{"location":"examples/Basics/introduction_tutorial/","title":"Setup","text":"In\u00a0[\u00a0]: Copied! <pre>import openml\n\nopenml.config.apikey = \"YOURKEY\"\n</pre> import openml  openml.config.apikey = \"YOURKEY\" In\u00a0[\u00a0]: Copied! <pre>import openml\n\nopenml.config.set_root_cache_directory(\"YOURDIR\")\n</pre> import openml  openml.config.set_root_cache_directory(\"YOURDIR\")"},{"location":"examples/Basics/introduction_tutorial/#installation","title":"Installation\u00b6","text":"<p>Installation is done via <code>pip</code>:</p> <pre>pip install openml\n</pre>"},{"location":"examples/Basics/introduction_tutorial/#authentication","title":"Authentication\u00b6","text":"<p>For certain functionality, such as uploading tasks or datasets, users have to sign up. Only accessing the data on OpenML does not require an account!</p> <p>If you don\u2019t have an account yet, sign up now. You will receive an API key, which will authenticate you to the server and allow you to download and upload datasets, tasks, runs and flows.</p> <ul> <li>Create an OpenML account (free) on https://www.openml.org.</li> <li>After logging in, open your account page (avatar on the top right)</li> <li>Open 'Account Settings', then 'API authentication' to find your API key.</li> </ul> <p>There are two ways to permanently authenticate:</p> <ul> <li>Use the <code>openml</code> CLI tool with <code>openml configure apikey MYKEY</code>, replacing MYKEY with your API key.</li> <li>Create a plain text file ~/.openml/config with the line 'apikey=MYKEY', replacing MYKEY with your API key. The config file must be in the directory ~/.openml/config and exist prior to importing the openml module.</li> </ul> <p>Alternatively, by running the code below and replacing 'YOURKEY' with your API key, you authenticate for the duration of the Python process.</p>"},{"location":"examples/Basics/introduction_tutorial/#caching","title":"Caching\u00b6","text":"<p>When downloading datasets, tasks, runs and flows, they will be cached to retrieve them without calling the server later. As with the API key, the cache directory can be either specified through the config file or through the API:</p> <ul> <li>Add the  line cachedir = 'MYDIR' to the config file, replacing 'MYDIR' with the path to the cache directory. By default, OpenML will use ~/.openml/cache as the cache directory.</li> <li>Run the code below, replacing 'YOURDIR' with the path to the cache directory.</li> </ul>"},{"location":"examples/Basics/simple_datasets_tutorial/","title":"Datasets","text":"<p>A basic tutorial on how to list, load and visualize datasets.</p> <p>In general, we recommend working with tasks, so that the results can be easily reproduced. Furthermore, the results can be compared to existing results at OpenML. However, for the purposes of this tutorial, we are going to work with the datasets directly.</p> In\u00a0[\u00a0]: Copied! <pre>import openml\n</pre>  import openml In\u00a0[\u00a0]: Copied! <pre>datasets_df = openml.datasets.list_datasets()\nprint(datasets_df.head(n=10))\n</pre> datasets_df = openml.datasets.list_datasets() print(datasets_df.head(n=10)) In\u00a0[\u00a0]: Copied! <pre># Iris dataset https://www.openml.org/d/61\ndataset = openml.datasets.get_dataset(dataset_id=61)\n\n# Print a summary\nprint(\n    f\"This is dataset '{dataset.name}', the target feature is '{dataset.default_target_attribute}'\"\n)\nprint(f\"URL: {dataset.url}\")\nprint(dataset.description[:500])\n</pre> # Iris dataset https://www.openml.org/d/61 dataset = openml.datasets.get_dataset(dataset_id=61)  # Print a summary print(     f\"This is dataset '{dataset.name}', the target feature is '{dataset.default_target_attribute}'\" ) print(f\"URL: {dataset.url}\") print(dataset.description[:500]) In\u00a0[\u00a0]: Copied! <pre>X, y, categorical_indicator, attribute_names = dataset.get_data(\n    target=dataset.default_target_attribute\n)\n</pre> X, y, categorical_indicator, attribute_names = dataset.get_data(     target=dataset.default_target_attribute ) <p>Visualize the dataset</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\niris_plot = sns.pairplot(pd.concat([X, y], axis=1), hue=\"class\")\nplt.show()\n</pre> import matplotlib.pyplot as plt import pandas as pd import seaborn as sns  iris_plot = sns.pairplot(pd.concat([X, y], axis=1), hue=\"class\") plt.show()"},{"location":"examples/Basics/simple_datasets_tutorial/#list-datasets-stored-on-openml","title":"List datasets stored on OpenML\u00b6","text":""},{"location":"examples/Basics/simple_datasets_tutorial/#download-a-dataset","title":"Download a dataset\u00b6","text":""},{"location":"examples/Basics/simple_datasets_tutorial/#load-a-dataset","title":"Load a dataset\u00b6","text":"<ul> <li><code>X</code> - A dataframe where each row represents one example with the corresponding feature values.</li> <li><code>y</code> - the classes for each example</li> <li><code>categorical_indicator</code> - a list that indicates which feature is categorical</li> <li><code>attribute_names</code> - the names of the features for the examples (X) and target feature (y)</li> </ul>"},{"location":"examples/Basics/simple_flows_and_runs_tutorial/","title":"Flows and Runs","text":"<p>A simple tutorial on how to upload results from a machine learning experiment to OpenML.</p> In\u00a0[\u00a0]: Copied! <pre>import sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport openml\n</pre> import sklearn from sklearn.neighbors import KNeighborsClassifier  import openml <p>Warning</p> <p>         This example uploads data. For that reason, this example connects to the         test server at test.openml.org.         This prevents the main server from becoming overloaded with example datasets, tasks,         runs, and other submissions.         Using this test server may affect the behavior and performance of the         OpenML-Python API.     </p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() In\u00a0[\u00a0]: Copied! <pre>task = openml.tasks.get_task(119)\n\n# Get the data\ndataset = task.get_dataset()\nX, y, categorical_indicator, attribute_names = dataset.get_data(\n    target=dataset.default_target_attribute\n)\n\n# Get the holdout split from the task\ntrain_indices, test_indices = task.get_train_test_split_indices(fold=0, repeat=0)\nX_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\ny_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n\nknn_parameters = {\n    \"n_neighbors\": 3,\n}\nclf = KNeighborsClassifier(**knn_parameters)\nclf.fit(X_train, y_train)\n\n# Get experiment results\ny_pred = clf.predict(X_test)\ny_pred_proba = clf.predict_proba(X_test)\n</pre> task = openml.tasks.get_task(119)  # Get the data dataset = task.get_dataset() X, y, categorical_indicator, attribute_names = dataset.get_data(     target=dataset.default_target_attribute )  # Get the holdout split from the task train_indices, test_indices = task.get_train_test_split_indices(fold=0, repeat=0) X_train, X_test = X.iloc[train_indices], X.iloc[test_indices] y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]  knn_parameters = {     \"n_neighbors\": 3, } clf = KNeighborsClassifier(**knn_parameters) clf.fit(X_train, y_train)  # Get experiment results y_pred = clf.predict(X_test) y_pred_proba = clf.predict_proba(X_test) In\u00a0[\u00a0]: Copied! <pre>knn_flow = openml.flows.OpenMLFlow(\n    # Metadata\n    model=clf,  # or None, if you do not want to upload the model object.\n    name=\"CustomKNeighborsClassifier\",\n    description=\"A custom KNeighborsClassifier flow for OpenML.\",\n    external_version=f\"{sklearn.__version__}\",\n    language=\"English\",\n    tags=[\"openml_tutorial_knn\"],\n    dependencies=f\"{sklearn.__version__}\",\n    # Hyperparameters\n    parameters={k: str(v) for k, v in knn_parameters.items()},\n    parameters_meta_info={\n        \"n_neighbors\": {\"description\": \"number of neighbors to use\", \"data_type\": \"int\"}\n    },\n    # If you have a pipeline with subcomponents, such as preprocessing, add them here.\n    components={},\n)\nknn_flow.publish()\nprint(f\"knn_flow was published with the ID {knn_flow.flow_id}\")\n</pre> knn_flow = openml.flows.OpenMLFlow(     # Metadata     model=clf,  # or None, if you do not want to upload the model object.     name=\"CustomKNeighborsClassifier\",     description=\"A custom KNeighborsClassifier flow for OpenML.\",     external_version=f\"{sklearn.__version__}\",     language=\"English\",     tags=[\"openml_tutorial_knn\"],     dependencies=f\"{sklearn.__version__}\",     # Hyperparameters     parameters={k: str(v) for k, v in knn_parameters.items()},     parameters_meta_info={         \"n_neighbors\": {\"description\": \"number of neighbors to use\", \"data_type\": \"int\"}     },     # If you have a pipeline with subcomponents, such as preprocessing, add them here.     components={}, ) knn_flow.publish() print(f\"knn_flow was published with the ID {knn_flow.flow_id}\") <p>Second, we create a run to store the results associated with the flow.</p> In\u00a0[\u00a0]: Copied! <pre># Format the predictions for OpenML\npredictions = []\nfor test_index, y_true_i, y_pred_i, y_pred_proba_i in zip(\n    test_indices, y_test, y_pred, y_pred_proba\n):\n    predictions.append(\n        openml.runs.functions.format_prediction(\n            task=task,\n            repeat=0,\n            fold=0,\n            index=test_index,\n            prediction=y_pred_i,\n            truth=y_true_i,\n            proba=dict(zip(task.class_labels, y_pred_proba_i)),\n        )\n    )\n\n# Format the parameters for OpenML\noml_knn_parameters = [\n    {\"oml:name\": k, \"oml:value\": v, \"oml:component\": knn_flow.flow_id}\n    for k, v in knn_parameters.items()\n]\n\nknn_run = openml.runs.OpenMLRun(\n    task_id=task.task_id,\n    flow_id=knn_flow.flow_id,\n    dataset_id=dataset.dataset_id,\n    parameter_settings=oml_knn_parameters,\n    data_content=predictions,\n    tags=[\"openml_tutorial_knn\"],\n    description_text=\"Run generated by the tutorial.\",\n)\nknn_run = knn_run.publish()\nprint(f\"Run was uploaded to {knn_run.openml_url}\")\nprint(f\"The flow can be found at {knn_run.flow.openml_url}\")\n</pre>  # Format the predictions for OpenML predictions = [] for test_index, y_true_i, y_pred_i, y_pred_proba_i in zip(     test_indices, y_test, y_pred, y_pred_proba ):     predictions.append(         openml.runs.functions.format_prediction(             task=task,             repeat=0,             fold=0,             index=test_index,             prediction=y_pred_i,             truth=y_true_i,             proba=dict(zip(task.class_labels, y_pred_proba_i)),         )     )  # Format the parameters for OpenML oml_knn_parameters = [     {\"oml:name\": k, \"oml:value\": v, \"oml:component\": knn_flow.flow_id}     for k, v in knn_parameters.items() ]  knn_run = openml.runs.OpenMLRun(     task_id=task.task_id,     flow_id=knn_flow.flow_id,     dataset_id=dataset.dataset_id,     parameter_settings=oml_knn_parameters,     data_content=predictions,     tags=[\"openml_tutorial_knn\"],     description_text=\"Run generated by the tutorial.\", ) knn_run = knn_run.publish() print(f\"Run was uploaded to {knn_run.openml_url}\") print(f\"The flow can be found at {knn_run.flow.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n</pre> openml.config.stop_using_configuration_for_example()"},{"location":"examples/Basics/simple_flows_and_runs_tutorial/#train-a-machine-learning-model-and-evaluate-it","title":"Train a machine learning model and evaluate it\u00b6","text":"<p>NOTE: We are using task 119 from the test server: https://test.openml.org/d/20</p>"},{"location":"examples/Basics/simple_flows_and_runs_tutorial/#upload-the-machine-learning-experiments-to-openml","title":"Upload the machine learning experiments to OpenML\u00b6","text":"<p>First, create a fow and fill it with metadata about the machine learning model.</p>"},{"location":"examples/Basics/simple_suites_tutorial/","title":"Suites","text":"<p>This is a brief showcase of OpenML benchmark suites, which were introduced by Bischl et al. (2019). Benchmark suites standardize the datasets and splits to be used in an experiment or paper. They are fully integrated into OpenML and simplify both the sharing of the setup and the results.</p> In\u00a0[\u00a0]: Copied! <pre>import openml\n</pre> import openml In\u00a0[\u00a0]: Copied! <pre>suite = openml.study.get_suite(99)\nprint(suite)\n</pre> suite = openml.study.get_suite(99) print(suite) <p>The benchmark suite does not download the included tasks and datasets itself, but only contains a list of which tasks constitute the study.</p> <p>Tasks can then be accessed via</p> In\u00a0[\u00a0]: Copied! <pre>tasks = suite.tasks\nprint(tasks)\n</pre> tasks = suite.tasks print(tasks) <p>and iterated over for benchmarking. For speed reasons, we only iterate over the first three tasks:</p> In\u00a0[\u00a0]: Copied! <pre>for task_id in tasks[:3]:\n    task = openml.tasks.get_task(task_id)\n    print(task)\n</pre> for task_id in tasks[:3]:     task = openml.tasks.get_task(task_id)     print(task)"},{"location":"examples/Basics/simple_suites_tutorial/#openml-cc18","title":"OpenML-CC18\u00b6","text":"<p>As an example we have a look at the OpenML-CC18, which is a suite of 72 classification datasets from OpenML which were carefully selected to be usable by many algorithms. These are all datasets from mid-2018 that satisfy a large set of clear requirements for thorough yet practical benchmarking:</p> <ol> <li>the number of observations are between 500 and 100,000 to focus on medium-sized datasets,</li> <li>the number of features does not exceed 5,000 features to keep the runtime of the algorithms low</li> <li>the target attribute has at least two classes with no class having less than 20 observations</li> <li>the ratio of the minority class and the majority class is above 0.05 (to eliminate highly imbalanced datasets which require special treatment for both algorithms and evaluation measures).</li> </ol> <p>A full description can be found in the OpenML benchmarking docs.</p> <p>In this example, we'll focus on how to use benchmark suites in practice.</p>"},{"location":"examples/Basics/simple_suites_tutorial/#downloading-benchmark-suites","title":"Downloading benchmark suites\u00b6","text":""},{"location":"examples/Basics/simple_tasks_tutorial/","title":"Tasks","text":"<p>A brief example on how to use tasks from OpenML.</p> In\u00a0[\u00a0]: Copied! <pre>import openml\n</pre>  import openml <p>Get a task for supervised classification on credit-g:</p> In\u00a0[\u00a0]: Copied! <pre>task = openml.tasks.get_task(31)\n</pre> task = openml.tasks.get_task(31) <p>Get the dataset and its data from the task.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = task.get_dataset()\nX, y, categorical_indicator, attribute_names = dataset.get_data(target=task.target_name)\n</pre> dataset = task.get_dataset() X, y, categorical_indicator, attribute_names = dataset.get_data(target=task.target_name) <p>Get the first out of the 10 cross-validation splits from the task.</p> In\u00a0[\u00a0]: Copied! <pre>train_indices, test_indices = task.get_train_test_split_indices(fold=0)\nprint(train_indices[:10])  # print the first 10 indices of the training set\n</pre> train_indices, test_indices = task.get_train_test_split_indices(fold=0) print(train_indices[:10])  # print the first 10 indices of the training set"},{"location":"reference/","title":"openml","text":""},{"location":"reference/#openml","title":"openml","text":"<p>The OpenML module implements a python interface to <code>OpenML &lt;https://www.openml.org&gt;</code>_, a collaborative platform for machine learning. OpenML can be used to</p> <ul> <li>store, download and analyze datasets</li> <li>make experiments and their results (e.g. models, predictions)   accesible and reproducible for everybody</li> <li>analyze experiments (uploaded by you and other collaborators) and conduct   meta studies</li> </ul> <p>In particular, this module implements a python interface for the <code>OpenML REST API &lt;https://www.openml.org/guide#!rest_services&gt;</code> (<code>REST on wikipedia &lt;https://en.wikipedia.org/wiki/Representational_state_transfer&gt;</code>).</p>"},{"location":"reference/#openml.OpenMLBenchmarkSuite","title":"OpenMLBenchmarkSuite","text":"<pre><code>OpenMLBenchmarkSuite(suite_id: int | None, alias: str | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None)\n</code></pre> <p>               Bases: <code>BaseStudy</code></p> <p>An OpenMLBenchmarkSuite represents the OpenML concept of a suite (a collection of tasks).</p> <p>It contains the following information: name, id, description, creation date, creator id and the task ids.</p> <p>According to this list of task ids, the suite object receives a list of OpenML object ids (datasets).</p> PARAMETER DESCRIPTION <code>suite_id</code> <p>the study id</p> <p> TYPE: <code>int</code> </p> <code>alias</code> <p>a string ID, unique on server (url-friendly)</p> <p> TYPE: <code>str(optional)</code> </p> <code>main_entity_type</code> <p>the entity type (e.g., task, run) that is core in this study. only entities of this type can be added explicitly</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>the name of the study (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>brief description (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>Whether the study is in preparation, active or deactivated</p> <p> TYPE: <code>str</code> </p> <code>creation_date</code> <p>date of creation (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>creator</code> <p>openml user id of the owner / creator</p> <p> TYPE: <code>int</code> </p> <code>tags</code> <p>The list of tags shows which tags are associated with the study. Each tag is a dict of (tag) name, window_start and write_access.</p> <p> TYPE: <code>list(dict)</code> </p> <code>data</code> <p>a list of data ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>tasks</code> <p>a list of task ids associated with this study</p> <p> TYPE: <code>list</code> </p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    suite_id: int | None,\n    alias: str | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n):\n    super().__init__(\n        study_id=suite_id,\n        alias=alias,\n        main_entity_type=\"task\",\n        benchmark_suite=None,\n        name=name,\n        description=description,\n        status=status,\n        creation_date=creation_date,\n        creator=creator,\n        tags=tags,\n        data=data,\n        tasks=tasks,\n        flows=None,\n        runs=None,\n        setups=None,\n    )\n</code></pre>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/#openml.OpenMLBenchmarkSuite.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask","title":"OpenMLClassificationTask","text":"<pre><code>OpenMLClassificationTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None, task_id: int | None = None, class_labels: list[str] | None = None, cost_matrix: ndarray | None = None)\n</code></pre> <p>               Bases: <code>OpenMLSupervisedTask</code></p> <p>OpenML Classification object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>ID of the Classification task type.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Name of the Classification task type.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the OpenML dataset associated with the Classification task.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>Name of the target variable.</p> <p> TYPE: <code>str</code> </p> <code>estimation_procedure_id</code> <p>ID of the estimation procedure for the Classification task.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the estimation procedure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Estimation parameters for the Classification task.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Name of the evaluation measure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the data splits for the Classification task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID of the Classification task (if it already exists on OpenML).</p> <p> TYPE: <code>Union[int, None]</code> DEFAULT: <code>None</code> </p> <code>class_labels</code> <p>A list of class labels (for classification tasks).</p> <p> TYPE: <code>List of str</code> DEFAULT: <code>None</code> </p> <code>cost_matrix</code> <p>A cost matrix (for classification tasks).</p> <p> TYPE: <code>array</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    class_labels: list[str] | None = None,\n    cost_matrix: np.ndarray | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n    )\n    self.class_labels = class_labels\n    self.cost_matrix = cost_matrix\n\n    if cost_matrix is not None:\n        raise NotImplementedError(\"Costmatrix\")\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/#openml.OpenMLClassificationTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/#openml.OpenMLClassificationTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLClassificationTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>tuple - X and y</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLClassificationTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask","title":"OpenMLClusteringTask","text":"<pre><code>OpenMLClusteringTask(task_type_id: TaskType, task_type: str, data_set_id: int, estimation_procedure_id: int = 17, task_id: int | None = None, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, evaluation_measure: str | None = None, target_name: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLTask</code></p> <p>OpenML Clustering object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>Task type ID of the OpenML clustering task.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Task type of the OpenML clustering task.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the OpenML dataset used in clustering the task.</p> <p> TYPE: <code>int</code> </p> <code>estimation_procedure_id</code> <p>ID of the OpenML estimation procedure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID of the OpenML clustering task.</p> <p> TYPE: <code>Union[int, None]</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the OpenML estimation procedure used in the clustering task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Parameters used by the OpenML estimation procedure.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the OpenML data splits for the clustering task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Evaluation measure used in the clustering task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>target_name</code> <p>Name of the target feature (class) that is not part of the feature set for the clustering task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    estimation_procedure_id: int = 17,\n    task_id: int | None = None,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    evaluation_measure: str | None = None,\n    target_name: str | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        evaluation_measure=evaluation_measure,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        data_splits_url=data_splits_url,\n    )\n\n    self.target_name = target_name\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/#openml.OpenMLClusteringTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLClusteringTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.get_X","title":"get_X","text":"<pre><code>get_X() -&gt; DataFrame\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>The X data as a dataframe</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X(self) -&gt; pd.DataFrame:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    The X data as a dataframe\n    \"\"\"\n    dataset = self.get_dataset()\n    data, *_ = dataset.get_data(target=None)\n    return data\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLClusteringTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLDataFeature","title":"OpenMLDataFeature","text":"<pre><code>OpenMLDataFeature(index: int, name: str, data_type: str, nominal_values: list[str], number_missing_values: int, ontologies: list[str] | None = None)\n</code></pre> <p>Data Feature (a.k.a. Attribute) object.</p> PARAMETER DESCRIPTION <code>index</code> <p>The index of this feature</p> <p> TYPE: <code>int</code> </p> <code>name</code> <p>Name of the feature</p> <p> TYPE: <code>str</code> </p> <code>data_type</code> <p>can be nominal, numeric, string, date (corresponds to arff)</p> <p> TYPE: <code>str</code> </p> <code>nominal_values</code> <p>list of the possible values, in case of nominal attribute</p> <p> TYPE: <code>list(str)</code> </p> <code>number_missing_values</code> <p>Number of rows that have a missing value for this feature.</p> <p> TYPE: <code>int</code> </p> <code>ontologies</code> <p>list of ontologies attached to this feature. An ontology describes the concept that are described in a feature. An ontology is defined by an URL where the information is provided.</p> <p> TYPE: <code>list(str)</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/datasets/data_feature.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    index: int,\n    name: str,\n    data_type: str,\n    nominal_values: list[str],\n    number_missing_values: int,\n    ontologies: list[str] | None = None,\n):\n    if not isinstance(index, int):\n        raise TypeError(f\"Index must be `int` but is {type(index)}\")\n\n    if data_type not in self.LEGAL_DATA_TYPES:\n        raise ValueError(\n            f\"data type should be in {self.LEGAL_DATA_TYPES!s}, found: {data_type}\",\n        )\n\n    if data_type == \"nominal\":\n        if nominal_values is None:\n            raise TypeError(\n                \"Dataset features require attribute `nominal_values` for nominal \"\n                \"feature type.\",\n            )\n\n        if not isinstance(nominal_values, list):\n            raise TypeError(\n                \"Argument `nominal_values` is of wrong datatype, should be list, \"\n                f\"but is {type(nominal_values)}\",\n            )\n    elif nominal_values is not None:\n        raise TypeError(\"Argument `nominal_values` must be None for non-nominal feature.\")\n\n    if not isinstance(number_missing_values, int):\n        msg = f\"number_missing_values must be int but is {type(number_missing_values)}\"\n        raise TypeError(msg)\n\n    self.index = index\n    self.name = str(name)\n    self.data_type = str(data_type)\n    self.nominal_values = nominal_values\n    self.number_missing_values = number_missing_values\n    self.ontologies = ontologies\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset","title":"OpenMLDataset","text":"<pre><code>OpenMLDataset(name: str, description: str | None, data_format: Literal['arff', 'sparse_arff'] = 'arff', cache_format: Literal['feather', 'pickle'] = 'pickle', dataset_id: int | None = None, version: int | None = None, creator: str | None = None, contributor: str | None = None, collection_date: str | None = None, upload_date: str | None = None, language: str | None = None, licence: str | None = None, url: str | None = None, default_target_attribute: str | None = None, row_id_attribute: str | None = None, ignore_attribute: str | list[str] | None = None, version_label: str | None = None, citation: str | None = None, tag: str | None = None, visibility: str | None = None, original_data_url: str | None = None, paper_url: str | None = None, update_comment: str | None = None, md5_checksum: str | None = None, data_file: str | None = None, features_file: str | None = None, qualities_file: str | None = None, dataset: str | None = None, parquet_url: str | None = None, parquet_file: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>Dataset object.</p> <p>Allows fetching and uploading datasets to OpenML.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Description of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>data_format</code> <p>Format of the dataset which can be either 'arff' or 'sparse_arff'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'arff'</code> </p> <code>cache_format</code> <p>Format for caching the dataset which can be either 'feather' or 'pickle'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pickle'</code> </p> <code>dataset_id</code> <p>Id autogenerated by the server.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>version</code> <p>Version of this dataset. '1' for original version. Auto-incremented by server.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>creator</code> <p>The person who created the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>contributor</code> <p>People who contributed to the current version of the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>collection_date</code> <p>The date the data was originally collected, given by the uploader.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>upload_date</code> <p>The date-time when the dataset was uploaded, generated by server.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>language</code> <p>Language in which the data is represented. Starts with 1 upper case letter, rest lower case, e.g. 'English'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>licence</code> <p>License of the data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>url</code> <p>Valid URL, points to actual data file. The file can be on the OpenML server or another dataset repository.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>default_target_attribute</code> <p>The default target attribute, if it exists. Can have multiple values, comma separated.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>row_id_attribute</code> <p>The attribute that represents the row-id column, if present in the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>ignore_attribute</code> <p>Attributes that should be excluded in modelling, such as identifiers and indexes.</p> <p> TYPE: <code>str | list</code> DEFAULT: <code>None</code> </p> <code>version_label</code> <p>Version label provided by user. Can be a date, hash, or some other type of id.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>citation</code> <p>Reference(s) that should be cited when building on this data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p>Tags, describing the algorithms.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>visibility</code> <p>Who can see the dataset. Typical values: 'Everyone','All my friends','Only me'. Can also be any of the user's circles.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>original_data_url</code> <p>For derived data, the url to the original dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>paper_url</code> <p>Link to a paper describing the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>update_comment</code> <p>An explanation for when the dataset is uploaded.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>md5_checksum</code> <p>MD5 checksum to check if the dataset is downloaded without corruption.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_file</code> <p>Path to where the dataset is located.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>features_file</code> <p>A dictionary of dataset features, which maps a feature index to a OpenMLDataFeature.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>qualities_file</code> <p>A dictionary of dataset qualities, which maps a quality name to a quality value.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>dataset</code> <p>Serialized arff dataset string.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>parquet_url</code> <p>This is the URL to the storage location where the dataset files are hosted. This can be a MinIO bucket URL. If specified, the data will be accessed from this URL when reading the files.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>parquet_file</code> <p>Path to the local file.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def __init__(  # noqa: C901, PLR0912, PLR0913, PLR0915\n    self,\n    name: str,\n    description: str | None,\n    data_format: Literal[\"arff\", \"sparse_arff\"] = \"arff\",\n    cache_format: Literal[\"feather\", \"pickle\"] = \"pickle\",\n    dataset_id: int | None = None,\n    version: int | None = None,\n    creator: str | None = None,\n    contributor: str | None = None,\n    collection_date: str | None = None,\n    upload_date: str | None = None,\n    language: str | None = None,\n    licence: str | None = None,\n    url: str | None = None,\n    default_target_attribute: str | None = None,\n    row_id_attribute: str | None = None,\n    ignore_attribute: str | list[str] | None = None,\n    version_label: str | None = None,\n    citation: str | None = None,\n    tag: str | None = None,\n    visibility: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n    update_comment: str | None = None,\n    md5_checksum: str | None = None,\n    data_file: str | None = None,\n    features_file: str | None = None,\n    qualities_file: str | None = None,\n    dataset: str | None = None,\n    parquet_url: str | None = None,\n    parquet_file: str | None = None,\n):\n    if cache_format not in [\"feather\", \"pickle\"]:\n        raise ValueError(\n            \"cache_format must be one of 'feather' or 'pickle. \"\n            f\"Invalid format specified: {cache_format}\",\n        )\n\n    def find_invalid_characters(string: str, pattern: str) -&gt; str:\n        invalid_chars = set()\n        regex = re.compile(pattern)\n        for char in string:\n            if not regex.match(char):\n                invalid_chars.add(char)\n        return \",\".join(\n            [f\"'{char}'\" if char != \"'\" else f'\"{char}\"' for char in invalid_chars],\n        )\n\n    if dataset_id is None:\n        pattern = \"^[\\x00-\\x7f]*$\"\n        if description and not re.match(pattern, description):\n            # not basiclatin (XSD complains)\n            invalid_characters = find_invalid_characters(description, pattern)\n            raise ValueError(\n                f\"Invalid symbols {invalid_characters} in description: {description}\",\n            )\n        pattern = \"^[\\x00-\\x7f]*$\"\n        if citation and not re.match(pattern, citation):\n            # not basiclatin (XSD complains)\n            invalid_characters = find_invalid_characters(citation, pattern)\n            raise ValueError(\n                f\"Invalid symbols {invalid_characters} in citation: {citation}\",\n            )\n        pattern = \"^[a-zA-Z0-9_\\\\-\\\\.\\\\(\\\\),]+$\"\n        if not re.match(pattern, name):\n            # regex given by server in error message\n            invalid_characters = find_invalid_characters(name, pattern)\n            raise ValueError(f\"Invalid symbols {invalid_characters} in name: {name}\")\n\n    self.ignore_attribute: list[str] | None = None\n    if isinstance(ignore_attribute, str):\n        self.ignore_attribute = [ignore_attribute]\n    elif isinstance(ignore_attribute, list) or ignore_attribute is None:\n        self.ignore_attribute = ignore_attribute\n    else:\n        raise ValueError(\"Wrong data type for ignore_attribute. Should be list.\")\n\n    # TODO add function to check if the name is casual_string128\n    # Attributes received by querying the RESTful API\n    self.dataset_id = int(dataset_id) if dataset_id is not None else None\n    self.name = name\n    self.version = int(version) if version is not None else None\n    self.description = description\n    self.cache_format = cache_format\n    # Has to be called format, otherwise there will be an XML upload error\n    self.format = data_format\n    self.creator = creator\n    self.contributor = contributor\n    self.collection_date = collection_date\n    self.upload_date = upload_date\n    self.language = language\n    self.licence = licence\n    self.url = url\n    self.default_target_attribute = default_target_attribute\n    self.row_id_attribute = row_id_attribute\n\n    self.version_label = version_label\n    self.citation = citation\n    self.tag = tag\n    self.visibility = visibility\n    self.original_data_url = original_data_url\n    self.paper_url = paper_url\n    self.update_comment = update_comment\n    self.md5_checksum = md5_checksum\n    self.data_file = data_file\n    self.parquet_file = parquet_file\n    self._dataset = dataset\n    self._parquet_url = parquet_url\n\n    self._features: dict[int, OpenMLDataFeature] | None = None\n    self._qualities: dict[str, float] | None = None\n    self._no_qualities_found = False\n\n    if features_file is not None:\n        self._features = _read_features(Path(features_file))\n\n    # \"\" was the old default value by `get_dataset` and maybe still used by some\n    if qualities_file == \"\":\n        # TODO(0.15): to switch to \"qualities_file is not None\" below and remove warning\n        warnings.warn(\n            \"Starting from Version 0.15 `qualities_file` must be None and not an empty string \"\n            \"to avoid reading the qualities from file. Set `qualities_file` to None to avoid \"\n            \"this warning.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        qualities_file = None\n\n    if qualities_file is not None:\n        self._qualities = _read_qualities(Path(qualities_file))\n\n    if data_file is not None:\n        data_pickle, data_feather, feather_attribute = self._compressed_cache_file_paths(\n            Path(data_file)\n        )\n        self.data_pickle_file = data_pickle if Path(data_pickle).exists() else None\n        self.data_feather_file = data_feather if Path(data_feather).exists() else None\n        self.feather_attribute_file = feather_attribute if Path(feather_attribute) else None\n    else:\n        self.data_pickle_file = None\n        self.data_feather_file = None\n        self.feather_attribute_file = None\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.features","title":"features  <code>property</code>","text":"<pre><code>features: dict[int, OpenMLDataFeature]\n</code></pre> <p>Get the features of this dataset.</p>"},{"location":"reference/#openml.OpenMLDataset.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Get the dataset numeric id.</p>"},{"location":"reference/#openml.OpenMLDataset.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLDataset.qualities","title":"qualities  <code>property</code>","text":"<pre><code>qualities: dict[str, float] | None\n</code></pre> <p>Get the qualities of this dataset.</p>"},{"location":"reference/#openml.OpenMLDataset.get_data","title":"get_data","text":"<pre><code>get_data(target: list[str] | str | None = None, include_row_id: bool = False, include_ignore_attribute: bool = False) -&gt; tuple[DataFrame, Series | None, list[bool], list[str]]\n</code></pre> <p>Returns dataset content as dataframes.</p> PARAMETER DESCRIPTION <code>target</code> <p>Name of target column to separate from the data. Splitting multiple columns is currently not supported.</p> <p> TYPE: <code>(string, List[str] or None(default=None))</code> DEFAULT: <code>None</code> </p> <code>include_row_id</code> <p>Whether to include row ids in the returned dataset.</p> <p> TYPE: <code>boolean(default=False)</code> DEFAULT: <code>False</code> </p> <code>include_ignore_attribute</code> <p>Whether to include columns that are marked as \"ignore\" on the server in the dataset.</p> <p> TYPE: <code>boolean(default=False)</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>X</code> <p>Dataset, may have sparse dtypes in the columns if required.</p> <p> TYPE: <code>(dataframe, shape(n_samples, n_columns))</code> </p> <code>y</code> <p>Target column</p> <p> TYPE: <code>(Series, shape(n_samples) or None)</code> </p> <code>categorical_indicator</code> <p>Mask that indicate categorical features.</p> <p> TYPE: <code>list[bool]</code> </p> <code>attribute_names</code> <p>List of attribute names.</p> <p> TYPE: <code>list[str]</code> </p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def get_data(  # noqa: C901\n    self,\n    target: list[str] | str | None = None,\n    include_row_id: bool = False,  # noqa: FBT001, FBT002\n    include_ignore_attribute: bool = False,  # noqa: FBT001, FBT002\n) -&gt; tuple[pd.DataFrame, pd.Series | None, list[bool], list[str]]:\n    \"\"\"Returns dataset content as dataframes.\n\n    Parameters\n    ----------\n    target : string, List[str] or None (default=None)\n        Name of target column to separate from the data.\n        Splitting multiple columns is currently not supported.\n    include_row_id : boolean (default=False)\n        Whether to include row ids in the returned dataset.\n    include_ignore_attribute : boolean (default=False)\n        Whether to include columns that are marked as \"ignore\"\n        on the server in the dataset.\n\n\n    Returns\n    -------\n    X : dataframe, shape (n_samples, n_columns)\n        Dataset, may have sparse dtypes in the columns if required.\n    y : pd.Series, shape (n_samples, ) or None\n        Target column\n    categorical_indicator : list[bool]\n        Mask that indicate categorical features.\n    attribute_names : list[str]\n        List of attribute names.\n    \"\"\"\n    data, categorical_mask, attribute_names = self._load_data()\n\n    to_exclude = []\n    if not include_row_id and self.row_id_attribute is not None:\n        if isinstance(self.row_id_attribute, str):\n            to_exclude.append(self.row_id_attribute)\n        elif isinstance(self.row_id_attribute, Iterable):\n            to_exclude.extend(self.row_id_attribute)\n\n    if not include_ignore_attribute and self.ignore_attribute is not None:\n        if isinstance(self.ignore_attribute, str):\n            to_exclude.append(self.ignore_attribute)\n        elif isinstance(self.ignore_attribute, Iterable):\n            to_exclude.extend(self.ignore_attribute)\n\n    if len(to_exclude) &gt; 0:\n        logger.info(f\"Going to remove the following attributes: {to_exclude}\")\n        keep = np.array([column not in to_exclude for column in attribute_names])\n        data = data.drop(columns=to_exclude)\n        categorical_mask = [cat for cat, k in zip(categorical_mask, keep) if k]\n        attribute_names = [att for att, k in zip(attribute_names, keep) if k]\n\n    if target is None:\n        return data, None, categorical_mask, attribute_names\n\n    if isinstance(target, str):\n        target_names = target.split(\",\") if \",\" in target else [target]\n    else:\n        target_names = target\n\n    # All the assumptions below for the target are dependant on the number of targets being 1\n    n_targets = len(target_names)\n    if n_targets &gt; 1:\n        raise NotImplementedError(f\"Number of targets {n_targets} not implemented.\")\n\n    target_name = target_names[0]\n    x = data.drop(columns=[target_name])\n    y = data[target_name].squeeze()\n\n    # Finally, remove the target from the list of attributes and categorical mask\n    target_index = attribute_names.index(target_name)\n    categorical_mask.pop(target_index)\n    attribute_names.remove(target_name)\n\n    assert isinstance(y, pd.Series)\n    return x, y, categorical_mask, attribute_names\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.get_features_by_type","title":"get_features_by_type","text":"<pre><code>get_features_by_type(data_type: str, exclude: list[str] | None = None, exclude_ignore_attribute: bool = True, exclude_row_id_attribute: bool = True) -&gt; list[int]\n</code></pre> <p>Return indices of features of a given type, e.g. all nominal features. Optional parameters to exclude various features by index or ontology.</p> PARAMETER DESCRIPTION <code>data_type</code> <p>The data type to return (e.g., nominal, numeric, date, string)</p> <p> TYPE: <code>str</code> </p> <code>exclude</code> <p>List of columns to exclude from the return value</p> <p> TYPE: <code>list(int)</code> DEFAULT: <code>None</code> </p> <code>exclude_ignore_attribute</code> <p>Whether to exclude the defined ignore attributes (and adapt the return values as if these indices are not present)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>exclude_row_id_attribute</code> <p>Whether to exclude the defined row id attributes (and adapt the return values as if these indices are not present)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>result</code> <p>a list of indices that have the specified data type</p> <p> TYPE: <code>list</code> </p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def get_features_by_type(  # noqa: C901\n    self,\n    data_type: str,\n    exclude: list[str] | None = None,\n    exclude_ignore_attribute: bool = True,  # noqa: FBT002, FBT001\n    exclude_row_id_attribute: bool = True,  # noqa: FBT002, FBT001\n) -&gt; list[int]:\n    \"\"\"\n    Return indices of features of a given type, e.g. all nominal features.\n    Optional parameters to exclude various features by index or ontology.\n\n    Parameters\n    ----------\n    data_type : str\n        The data type to return (e.g., nominal, numeric, date, string)\n    exclude : list(int)\n        List of columns to exclude from the return value\n    exclude_ignore_attribute : bool\n        Whether to exclude the defined ignore attributes (and adapt the\n        return values as if these indices are not present)\n    exclude_row_id_attribute : bool\n        Whether to exclude the defined row id attributes (and adapt the\n        return values as if these indices are not present)\n\n    Returns\n    -------\n    result : list\n        a list of indices that have the specified data type\n    \"\"\"\n    if data_type not in OpenMLDataFeature.LEGAL_DATA_TYPES:\n        raise TypeError(\"Illegal feature type requested\")\n    if self.ignore_attribute is not None and not isinstance(self.ignore_attribute, list):\n        raise TypeError(\"ignore_attribute should be a list\")\n    if self.row_id_attribute is not None and not isinstance(self.row_id_attribute, str):\n        raise TypeError(\"row id attribute should be a str\")\n    if exclude is not None and not isinstance(exclude, list):\n        raise TypeError(\"Exclude should be a list\")\n        # assert all(isinstance(elem, str) for elem in exclude),\n        #            \"Exclude should be a list of strings\"\n    to_exclude = []\n    if exclude is not None:\n        to_exclude.extend(exclude)\n    if exclude_ignore_attribute and self.ignore_attribute is not None:\n        to_exclude.extend(self.ignore_attribute)\n    if exclude_row_id_attribute and self.row_id_attribute is not None:\n        to_exclude.append(self.row_id_attribute)\n\n    result = []\n    offset = 0\n    # this function assumes that everything in to_exclude will\n    # be 'excluded' from the dataset (hence the offset)\n    for idx in self.features:\n        name = self.features[idx].name\n        if name in to_exclude:\n            offset += 1\n        elif self.features[idx].data_type == data_type:\n            result.append(idx - offset)\n    return result\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.retrieve_class_labels","title":"retrieve_class_labels","text":"<pre><code>retrieve_class_labels(target_name: str = 'class') -&gt; None | list[str]\n</code></pre> <p>Reads the datasets arff to determine the class-labels.</p> <p>If the task has no class labels (for example a regression problem) it returns None. Necessary because the data returned by get_data only contains the indices of the classes, while OpenML needs the real classname when uploading the results of a run.</p> PARAMETER DESCRIPTION <code>target_name</code> <p>Name of the target attribute</p> <p> TYPE: <code>str</code> DEFAULT: <code>'class'</code> </p> RETURNS DESCRIPTION <code>list</code> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def retrieve_class_labels(self, target_name: str = \"class\") -&gt; None | list[str]:\n    \"\"\"Reads the datasets arff to determine the class-labels.\n\n    If the task has no class labels (for example a regression problem)\n    it returns None. Necessary because the data returned by get_data\n    only contains the indices of the classes, while OpenML needs the real\n    classname when uploading the results of a run.\n\n    Parameters\n    ----------\n    target_name : str\n        Name of the target attribute\n\n    Returns\n    -------\n    list\n    \"\"\"\n    for feature in self.features.values():\n        if feature.name == target_name:\n            if feature.data_type == \"nominal\":\n                return feature.nominal_values\n\n            if feature.data_type == \"string\":\n                # Rel.: #1311\n                # The target is invalid for a classification task if the feature type is string\n                # and not nominal. For such miss-configured tasks, we silently fix it here as\n                # we can safely interpreter string as nominal.\n                df, *_ = self.get_data()\n                return list(df[feature.name].unique())\n\n    return None\n</code></pre>"},{"location":"reference/#openml.OpenMLDataset.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLEvaluation","title":"OpenMLEvaluation","text":"<pre><code>OpenMLEvaluation(run_id: int, task_id: int, setup_id: int, flow_id: int, flow_name: str, data_id: int, data_name: str, function: str, upload_time: str, uploader: int, uploader_name: str, value: float | None, values: list[float] | None, array_data: str | None = None)\n</code></pre> <p>Contains all meta-information about a run / evaluation combination, according to the evaluation/list function</p> PARAMETER DESCRIPTION <code>run_id</code> <p>Refers to the run.</p> <p> TYPE: <code>int</code> </p> <code>task_id</code> <p>Refers to the task.</p> <p> TYPE: <code>int</code> </p> <code>setup_id</code> <p>Refers to the setup.</p> <p> TYPE: <code>int</code> </p> <code>flow_id</code> <p>Refers to the flow.</p> <p> TYPE: <code>int</code> </p> <code>flow_name</code> <p>Name of the referred flow.</p> <p> TYPE: <code>str</code> </p> <code>data_id</code> <p>Refers to the dataset.</p> <p> TYPE: <code>int</code> </p> <code>data_name</code> <p>The name of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>function</code> <p>The evaluation metric of this item (e.g., accuracy).</p> <p> TYPE: <code>str</code> </p> <code>upload_time</code> <p>The time of evaluation.</p> <p> TYPE: <code>str</code> </p> <code>uploader</code> <p>Uploader ID (user ID)</p> <p> TYPE: <code>int</code> </p> <code>upload_name</code> <p>Name of the uploader of this evaluation</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>The value (score) of this evaluation.</p> <p> TYPE: <code>float</code> </p> <code>values</code> <p>The values (scores) per repeat and fold (if requested)</p> <p> TYPE: <code>List[float]</code> </p> <code>array_data</code> <p>list of information per class. (e.g., in case of precision, auroc, recall)</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/evaluations/evaluation.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    run_id: int,\n    task_id: int,\n    setup_id: int,\n    flow_id: int,\n    flow_name: str,\n    data_id: int,\n    data_name: str,\n    function: str,\n    upload_time: str,\n    uploader: int,\n    uploader_name: str,\n    value: float | None,\n    values: list[float] | None,\n    array_data: str | None = None,\n):\n    self.run_id = run_id\n    self.task_id = task_id\n    self.setup_id = setup_id\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.data_id = data_id\n    self.data_name = data_name\n    self.function = function\n    self.upload_time = upload_time\n    self.uploader = uploader\n    self.uploader_name = uploader_name\n    self.value = value\n    self.values = values\n    self.array_data = array_data\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow","title":"OpenMLFlow","text":"<pre><code>OpenMLFlow(name: str, description: str, model: object, components: dict, parameters: dict, parameters_meta_info: dict, external_version: str, tags: list, language: str, dependencies: str, class_name: str | None = None, custom_name: str | None = None, binary_url: str | None = None, binary_format: str | None = None, binary_md5: str | None = None, uploader: str | None = None, upload_date: str | None = None, flow_id: int | None = None, extension: Extension | None = None, version: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Flow. Stores machine learning models.</p> <p>Flows should not be generated manually, but by the function :meth:<code>openml.flows.create_flow_from_model</code>. Using this helper function ensures that all relevant fields are filled in.</p> <p>Implements <code>openml.implementation.upload.xsd &lt;https://github.com/openml/openml/blob/master/openml_OS/views/pages/api_new/v1/xsd/ openml.implementation.upload.xsd&gt;</code>_.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the flow. Is used together with the attribute <code>external_version</code> as a unique identifier of the flow.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Human-readable description of the flow (free text).</p> <p> TYPE: <code>str</code> </p> <code>model</code> <p>ML model which is described by this flow.</p> <p> TYPE: <code>object</code> </p> <code>components</code> <p>Mapping from component identifier to an OpenMLFlow object. Components are usually subfunctions of an algorithm (e.g. kernels), base learners in ensemble algorithms (decision tree in adaboost) or building blocks of a machine learning pipeline. Components are modeled as independent flows and can be shared between flows (different pipelines can use the same components).</p> <p> TYPE: <code>OrderedDict</code> </p> <code>parameters</code> <p>Mapping from parameter name to the parameter default value. The parameter default value must be of type <code>str</code>, so that the respective toolbox plugin can take care of casting the parameter default value to the correct type.</p> <p> TYPE: <code>OrderedDict</code> </p> <code>parameters_meta_info</code> <p>Mapping from parameter name to <code>dict</code>. Stores additional information for each parameter. Required keys are <code>data_type</code> and <code>description</code>.</p> <p> TYPE: <code>OrderedDict</code> </p> <code>external_version</code> <p>Version number of the software the flow is implemented in. Is used together with the attribute <code>name</code> as a uniquer identifier of the flow.</p> <p> TYPE: <code>str</code> </p> <code>tags</code> <p>List of tags. Created on the server by other API calls.</p> <p> TYPE: <code>list</code> </p> <code>language</code> <p>Natural language the flow is described in (not the programming language).</p> <p> TYPE: <code>str</code> </p> <code>dependencies</code> <p>A list of dependencies necessary to run the flow. This field should contain all libraries the flow depends on. To allow reproducibility it should also specify the exact version numbers.</p> <p> TYPE: <code>str</code> </p> <code>class_name</code> <p>The development language name of the class which is described by this flow.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>custom_name</code> <p>Custom name of the flow given by the owner.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>binary_url</code> <p>Url from which the binary can be downloaded. Added by the server. Ignored when uploaded manually. Will not be used by the python API because binaries aren't compatible across machines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>binary_format</code> <p>Format in which the binary code was uploaded. Will not be used by the python API because binaries aren't compatible across machines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>binary_md5</code> <p>MD5 checksum to check if the binary code was correctly downloaded. Will not be used by the python API because binaries aren't compatible across machines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>uploader</code> <p>OpenML user ID of the uploader. Filled in by the server.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>upload_date</code> <p>Date the flow was uploaded. Filled in by the server.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>flow_id</code> <p>Flow ID. Assigned by the server.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>extension</code> <p>The extension for a flow (e.g., sklearn).</p> <p> TYPE: <code>Extension</code> DEFAULT: <code>None</code> </p> <code>version</code> <p>OpenML version of the flow. Assigned by the server.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/flows/flow.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    name: str,\n    description: str,\n    model: object,\n    components: dict,\n    parameters: dict,\n    parameters_meta_info: dict,\n    external_version: str,\n    tags: list,\n    language: str,\n    dependencies: str,\n    class_name: str | None = None,\n    custom_name: str | None = None,\n    binary_url: str | None = None,\n    binary_format: str | None = None,\n    binary_md5: str | None = None,\n    uploader: str | None = None,\n    upload_date: str | None = None,\n    flow_id: int | None = None,\n    extension: Extension | None = None,\n    version: str | None = None,\n):\n    self.name = name\n    self.description = description\n    self.model = model\n\n    for variable, variable_name in [\n        [components, \"components\"],\n        [parameters, \"parameters\"],\n        [parameters_meta_info, \"parameters_meta_info\"],\n    ]:\n        if not isinstance(variable, (OrderedDict, dict)):\n            raise TypeError(\n                f\"{variable_name} must be of type OrderedDict or dict, \"\n                f\"but is {type(variable)}.\",\n            )\n\n    self.components = components\n    self.parameters = parameters\n    self.parameters_meta_info = parameters_meta_info\n    self.class_name = class_name\n\n    keys_parameters = set(parameters.keys())\n    keys_parameters_meta_info = set(parameters_meta_info.keys())\n    if len(keys_parameters.difference(keys_parameters_meta_info)) &gt; 0:\n        raise ValueError(\n            f\"Parameter {keys_parameters.difference(keys_parameters_meta_info)!s} only in \"\n            \"parameters, but not in parameters_meta_info.\",\n        )\n    if len(keys_parameters_meta_info.difference(keys_parameters)) &gt; 0:\n        raise ValueError(\n            f\"Parameter {keys_parameters_meta_info.difference(keys_parameters)!s} only in \"\n            \" parameters_meta_info, but not in parameters.\",\n        )\n\n    self.external_version = external_version\n    self.uploader = uploader\n\n    self.custom_name = custom_name\n    self.tags = tags if tags is not None else []\n    self.binary_url = binary_url\n    self.binary_format = binary_format\n    self.binary_md5 = binary_md5\n    self.version = version\n    self.upload_date = upload_date\n    self.language = language\n    self.dependencies = dependencies\n    self.flow_id = flow_id\n    self._extension = extension\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.extension","title":"extension  <code>property</code>","text":"<pre><code>extension: Extension\n</code></pre> <p>The extension of the flow (e.g., sklearn).</p>"},{"location":"reference/#openml.OpenMLFlow.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The ID of the flow.</p>"},{"location":"reference/#openml.OpenMLFlow.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLFlow.from_filesystem","title":"from_filesystem  <code>classmethod</code>","text":"<pre><code>from_filesystem(input_directory: str | Path) -&gt; OpenMLFlow\n</code></pre> <p>Read a flow from an XML in input_directory on the filesystem.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>@classmethod\ndef from_filesystem(cls, input_directory: str | Path) -&gt; OpenMLFlow:\n    \"\"\"Read a flow from an XML in input_directory on the filesystem.\"\"\"\n    input_directory = Path(input_directory) / \"flow.xml\"\n    with input_directory.open() as f:\n        xml_string = f.read()\n    return OpenMLFlow._from_dict(xmltodict.parse(xml_string))\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.get_structure","title":"get_structure","text":"<pre><code>get_structure(key_item: str) -&gt; dict[str, list[str]]\n</code></pre> <p>Returns for each sub-component of the flow the path of identifiers that should be traversed to reach this component. The resulting dict maps a key (identifying a flow by either its id, name or fullname) to the parameter prefix.</p> PARAMETER DESCRIPTION <code>key_item</code> <p>The flow attribute that will be used to identify flows in the structure. Allowed values {flow_id, name}</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict[str, List[str]]</code> <p>The flow structure</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def get_structure(self, key_item: str) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Returns for each sub-component of the flow the path of identifiers\n    that should be traversed to reach this component. The resulting dict\n    maps a key (identifying a flow by either its id, name or fullname) to\n    the parameter prefix.\n\n    Parameters\n    ----------\n    key_item: str\n        The flow attribute that will be used to identify flows in the\n        structure. Allowed values {flow_id, name}\n\n    Returns\n    -------\n    dict[str, List[str]]\n        The flow structure\n    \"\"\"\n    if key_item not in [\"flow_id\", \"name\"]:\n        raise ValueError(\"key_item should be in {flow_id, name}\")\n    structure = {}\n    for key, sub_flow in self.components.items():\n        sub_structure = sub_flow.get_structure(key_item)\n        for flow_name, flow_sub_structure in sub_structure.items():\n            structure[flow_name] = [key, *flow_sub_structure]\n    structure[getattr(self, key_item)] = []\n    return structure\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.get_subflow","title":"get_subflow","text":"<pre><code>get_subflow(structure: list[str]) -&gt; OpenMLFlow\n</code></pre> <p>Returns a subflow from the tree of dependencies.</p> PARAMETER DESCRIPTION <code>structure</code> <p>A list of strings, indicating the location of the subflow</p> <p> TYPE: <code>list[str]</code> </p> RETURNS DESCRIPTION <code>OpenMLFlow</code> <p>The OpenMLFlow that corresponds to the structure</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def get_subflow(self, structure: list[str]) -&gt; OpenMLFlow:\n    \"\"\"\n    Returns a subflow from the tree of dependencies.\n\n    Parameters\n    ----------\n    structure: list[str]\n        A list of strings, indicating the location of the subflow\n\n    Returns\n    -------\n    OpenMLFlow\n        The OpenMLFlow that corresponds to the structure\n    \"\"\"\n    # make a copy of structure, as we don't want to change it in the\n    # outer scope\n    structure = list(structure)\n    if len(structure) &lt; 1:\n        raise ValueError(\"Please provide a structure list of size &gt;= 1\")\n    sub_identifier = structure[0]\n    if sub_identifier not in self.components:\n        raise ValueError(\n            f\"Flow {self.name} does not contain component with \" f\"identifier {sub_identifier}\",\n        )\n    if len(structure) == 1:\n        return self.components[sub_identifier]  # type: ignore\n\n    structure.pop(0)\n    return self.components[sub_identifier].get_subflow(structure)  # type: ignore\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.publish","title":"publish","text":"<pre><code>publish(raise_error_if_exists: bool = False) -&gt; OpenMLFlow\n</code></pre> <p>Publish this flow to OpenML server.</p> <p>Raises a PyOpenMLError if the flow exists on the server, but <code>self.flow_id</code> does not match the server known flow id.</p> PARAMETER DESCRIPTION <code>raise_error_if_exists</code> <p>If True, raise PyOpenMLError if the flow exists on the server. If False, update the local flow to match the server flow.</p> <p> TYPE: <code>(bool, optional(default=False))</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>self</code> <p> TYPE: <code>OpenMLFlow</code> </p> Source code in <code>openml/flows/flow.py</code> <pre><code>def publish(self, raise_error_if_exists: bool = False) -&gt; OpenMLFlow:  # noqa: FBT001, FBT002\n    \"\"\"Publish this flow to OpenML server.\n\n    Raises a PyOpenMLError if the flow exists on the server, but\n    `self.flow_id` does not match the server known flow id.\n\n    Parameters\n    ----------\n    raise_error_if_exists : bool, optional (default=False)\n        If True, raise PyOpenMLError if the flow exists on the server.\n        If False, update the local flow to match the server flow.\n\n    Returns\n    -------\n    self : OpenMLFlow\n\n    \"\"\"\n    # Import at top not possible because of cyclic dependencies. In\n    # particular, flow.py tries to import functions.py in order to call\n    # get_flow(), while functions.py tries to import flow.py in order to\n    # instantiate an OpenMLFlow.\n    import openml.flows.functions\n\n    flow_id = openml.flows.functions.flow_exists(self.name, self.external_version)\n    if not flow_id:\n        if self.flow_id:\n            raise openml.exceptions.PyOpenMLError(\n                \"Flow does not exist on the server, \" \"but 'flow.flow_id' is not None.\",\n            )\n        super().publish()\n        assert self.flow_id is not None  # for mypy\n        flow_id = self.flow_id\n    elif raise_error_if_exists:\n        error_message = f\"This OpenMLFlow already exists with id: {flow_id}.\"\n        raise openml.exceptions.PyOpenMLError(error_message)\n    elif self.flow_id is not None and self.flow_id != flow_id:\n        raise openml.exceptions.PyOpenMLError(\n            \"Local flow_id does not match server flow_id: \" f\"'{self.flow_id}' vs '{flow_id}'\",\n        )\n\n    flow = openml.flows.functions.get_flow(flow_id)\n    _copy_server_fields(flow, self)\n    try:\n        openml.flows.functions.assert_flows_equal(\n            self,\n            flow,\n            flow.upload_date,\n            ignore_parameter_values=True,\n            ignore_custom_name_if_none=True,\n        )\n    except ValueError as e:\n        message = e.args[0]\n        raise ValueError(\n            \"The flow on the server is inconsistent with the local flow. \"\n            f\"The server flow ID is {flow_id}. Please check manually and remove \"\n            f\"the flow if necessary! Error is:\\n'{message}'\",\n        ) from e\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.to_filesystem","title":"to_filesystem","text":"<pre><code>to_filesystem(output_directory: str | Path) -&gt; None\n</code></pre> <p>Write a flow to the filesystem as XML to output_directory.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def to_filesystem(self, output_directory: str | Path) -&gt; None:\n    \"\"\"Write a flow to the filesystem as XML to output_directory.\"\"\"\n    output_directory = Path(output_directory)\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    output_path = output_directory / \"flow.xml\"\n    if output_path.exists():\n        raise ValueError(\"Output directory already contains a flow.xml file.\")\n\n    run_xml = self._to_xml()\n    with output_path.open(\"w\") as f:\n        f.write(run_xml)\n</code></pre>"},{"location":"reference/#openml.OpenMLFlow.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask","title":"OpenMLLearningCurveTask","text":"<pre><code>OpenMLLearningCurveTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 13, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, task_id: int | None = None, evaluation_measure: str | None = None, class_labels: list[str] | None = None, cost_matrix: ndarray | None = None)\n</code></pre> <p>               Bases: <code>OpenMLClassificationTask</code></p> <p>OpenML Learning Curve object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>ID of the Learning Curve task.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Name of the Learning Curve task.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the dataset that this task is associated with.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>Name of the target feature in the dataset.</p> <p> TYPE: <code>str</code> </p> <code>estimation_procedure_id</code> <p>ID of the estimation procedure to use for evaluating models.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the estimation procedure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Additional parameters for the estimation procedure.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the file containing the data splits for Learning Curve task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID of the Learning Curve task.</p> <p> TYPE: <code>Union[int, None]</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Name of the evaluation measure to use for evaluating models.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>class_labels</code> <p>Class labels for Learning Curve tasks.</p> <p> TYPE: <code>list of str</code> DEFAULT: <code>None</code> </p> <code>cost_matrix</code> <p>Cost matrix for Learning Curve tasks.</p> <p> TYPE: <code>numpy array</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 13,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    evaluation_measure: str | None = None,\n    class_labels: list[str] | None = None,\n    cost_matrix: np.ndarray | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n        class_labels=class_labels,\n        cost_matrix=cost_matrix,\n    )\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/#openml.OpenMLLearningCurveTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/#openml.OpenMLLearningCurveTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLLearningCurveTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>tuple - X and y</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLLearningCurveTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLParameter","title":"OpenMLParameter","text":"<pre><code>OpenMLParameter(input_id: int, flow_id: int, flow_name: str, full_name: str, parameter_name: str, data_type: str, default_value: str, value: str)\n</code></pre> <p>Parameter object (used in setup).</p> PARAMETER DESCRIPTION <code>input_id</code> <p>The input id from the openml database</p> <p> TYPE: <code>int</code> </p> <code>flow</code> <p>The flow to which this parameter is associated</p> <p> </p> <code>flow</code> <p>The name of the flow (no version number) to which this parameter is associated</p> <p> </p> <code>full_name</code> <p>The name of the flow and parameter combined</p> <p> TYPE: <code>str</code> </p> <code>parameter_name</code> <p>The name of the parameter</p> <p> TYPE: <code>str</code> </p> <code>data_type</code> <p>The datatype of the parameter. generally unused for sklearn flows</p> <p> TYPE: <code>str</code> </p> <code>default_value</code> <p>The default value. For sklearn parameters, this is unknown and a default value is selected arbitrarily</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>If the parameter was set, the value that it was set to.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/setups/setup.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    input_id: int,\n    flow_id: int,\n    flow_name: str,\n    full_name: str,\n    parameter_name: str,\n    data_type: str,\n    default_value: str,\n    value: str,\n):\n    self.id = input_id\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.full_name = full_name\n    self.parameter_name = parameter_name\n    self.data_type = data_type\n    self.default_value = default_value\n    self.value = value\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask","title":"OpenMLRegressionTask","text":"<pre><code>OpenMLRegressionTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 7, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, task_id: int | None = None, evaluation_measure: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLSupervisedTask</code></p> <p>OpenML Regression object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>Task type ID of the OpenML Regression task.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Task type of the OpenML Regression task.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the OpenML dataset.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>Name of the target feature used in the Regression task.</p> <p> TYPE: <code>str</code> </p> <code>estimation_procedure_id</code> <p>ID of the OpenML estimation procedure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the OpenML estimation procedure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Parameters used by the OpenML estimation procedure.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the OpenML data splits for the Regression task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID of the OpenML Regression task.</p> <p> TYPE: <code>Union[int, None]</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Evaluation measure used in the Regression task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 7,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    evaluation_measure: str | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n    )\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/#openml.OpenMLRegressionTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/#openml.OpenMLRegressionTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLRegressionTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>tuple - X and y</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLRegressionTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLRun","title":"OpenMLRun","text":"<pre><code>OpenMLRun(task_id: int, flow_id: int | None, dataset_id: int | None, setup_string: str | None = None, output_files: dict[str, int] | None = None, setup_id: int | None = None, tags: list[str] | None = None, uploader: int | None = None, uploader_name: str | None = None, evaluations: dict | None = None, fold_evaluations: dict | None = None, sample_evaluations: dict | None = None, data_content: list[list] | None = None, trace: OpenMLRunTrace | None = None, model: object | None = None, task_type: str | None = None, task_evaluation_measure: str | None = None, flow_name: str | None = None, parameter_settings: list[dict[str, Any]] | None = None, predictions_url: str | None = None, task: OpenMLTask | None = None, flow: OpenMLFlow | None = None, run_id: int | None = None, description_text: str | None = None, run_details: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Run: result of running a model on an OpenML dataset.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>The ID of the OpenML task associated with the run.</p> <p> TYPE: <code>int</code> </p> <code>flow_id</code> <p>The ID of the OpenML flow associated with the run.</p> <p> TYPE: <code>int | None</code> </p> <code>dataset_id</code> <p>The ID of the OpenML dataset used for the run.</p> <p> TYPE: <code>int | None</code> </p> <code>setup_string</code> <p>The setup string of the run.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>output_files</code> <p>Specifies where each related file can be found.</p> <p> TYPE: <code>dict[str, int] | None</code> DEFAULT: <code>None</code> </p> <code>setup_id</code> <p>An integer representing the ID of the setup used for the run.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Representing the tags associated with the run.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>uploader</code> <p>User ID of the uploader.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>uploader_name</code> <p>The name of the person who uploaded the run.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>evaluations</code> <p>Representing the evaluations of the run.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>fold_evaluations</code> <p>The evaluations of the run for each fold.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>sample_evaluations</code> <p>The evaluations of the run for each sample.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>data_content</code> <p>The predictions generated from executing this run.</p> <p> TYPE: <code>list[list] | None</code> DEFAULT: <code>None</code> </p> <code>trace</code> <p>The trace containing information on internal model evaluations of this run.</p> <p> TYPE: <code>OpenMLRunTrace | None</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>The untrained model that was evaluated in the run.</p> <p> TYPE: <code>object | None</code> DEFAULT: <code>None</code> </p> <code>task_type</code> <p>The type of the OpenML task associated with the run.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>task_evaluation_measure</code> <p>The evaluation measure used for the task.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>flow_name</code> <p>The name of the OpenML flow associated with the run.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>parameter_settings</code> <p>Representing the parameter settings used for the run.</p> <p> TYPE: <code>list[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>predictions_url</code> <p>The URL of the predictions file.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>task</code> <p>An instance of the OpenMLTask class, representing the OpenML task associated with the run.</p> <p> TYPE: <code>OpenMLTask | None</code> DEFAULT: <code>None</code> </p> <code>flow</code> <p>An instance of the OpenMLFlow class, representing the OpenML flow associated with the run.</p> <p> TYPE: <code>OpenMLFlow | None</code> DEFAULT: <code>None</code> </p> <code>run_id</code> <p>The ID of the run.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>description_text</code> <p>Description text to add to the predictions file. If left None, is set to the time the arff file is generated.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>run_details</code> <p>Description of the run stored in the run meta-data.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/runs/run.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_id: int,\n    flow_id: int | None,\n    dataset_id: int | None,\n    setup_string: str | None = None,\n    output_files: dict[str, int] | None = None,\n    setup_id: int | None = None,\n    tags: list[str] | None = None,\n    uploader: int | None = None,\n    uploader_name: str | None = None,\n    evaluations: dict | None = None,\n    fold_evaluations: dict | None = None,\n    sample_evaluations: dict | None = None,\n    data_content: list[list] | None = None,\n    trace: OpenMLRunTrace | None = None,\n    model: object | None = None,\n    task_type: str | None = None,\n    task_evaluation_measure: str | None = None,\n    flow_name: str | None = None,\n    parameter_settings: list[dict[str, Any]] | None = None,\n    predictions_url: str | None = None,\n    task: OpenMLTask | None = None,\n    flow: OpenMLFlow | None = None,\n    run_id: int | None = None,\n    description_text: str | None = None,\n    run_details: str | None = None,\n):\n    self.uploader = uploader\n    self.uploader_name = uploader_name\n    self.task_id = task_id\n    self.task_type = task_type\n    self.task_evaluation_measure = task_evaluation_measure\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.setup_id = setup_id\n    self.setup_string = setup_string\n    self.parameter_settings = parameter_settings\n    self.dataset_id = dataset_id\n    self.evaluations = evaluations\n    self.fold_evaluations = fold_evaluations\n    self.sample_evaluations = sample_evaluations\n    self.data_content = data_content\n    self.output_files = output_files\n    self.trace = trace\n    self.error_message = None\n    self.task = task\n    self.flow = flow\n    self.run_id = run_id\n    self.model = model\n    self.tags = tags\n    self.predictions_url = predictions_url\n    self.description_text = description_text\n    self.run_details = run_details\n    self._predictions = None\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The ID of the run, None if not uploaded to the server yet.</p>"},{"location":"reference/#openml.OpenMLRun.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLRun.predictions","title":"predictions  <code>property</code>","text":"<pre><code>predictions: DataFrame\n</code></pre> <p>Return a DataFrame with predictions for this run</p>"},{"location":"reference/#openml.OpenMLRun.from_filesystem","title":"from_filesystem  <code>classmethod</code>","text":"<pre><code>from_filesystem(directory: str | Path, expect_model: bool = True) -&gt; OpenMLRun\n</code></pre> <p>The inverse of the to_filesystem method. Instantiates an OpenMLRun object based on files stored on the file system.</p> PARAMETER DESCRIPTION <code>directory</code> <p>a path leading to the folder where the results are stored</p> <p> TYPE: <code>str</code> </p> <code>expect_model</code> <p>if True, it requires the model pickle to be present, and an error will be thrown if not. Otherwise, the model might or might not be present.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>run</code> <p>the re-instantiated run object</p> <p> TYPE: <code>OpenMLRun</code> </p> Source code in <code>openml/runs/run.py</code> <pre><code>@classmethod\ndef from_filesystem(cls, directory: str | Path, expect_model: bool = True) -&gt; OpenMLRun:  # noqa: FBT001, FBT002\n    \"\"\"\n    The inverse of the to_filesystem method. Instantiates an OpenMLRun\n    object based on files stored on the file system.\n\n    Parameters\n    ----------\n    directory : str\n        a path leading to the folder where the results\n        are stored\n\n    expect_model : bool\n        if True, it requires the model pickle to be present, and an error\n        will be thrown if not. Otherwise, the model might or might not\n        be present.\n\n    Returns\n    -------\n    run : OpenMLRun\n        the re-instantiated run object\n    \"\"\"\n    # Avoiding cyclic imports\n    import openml.runs.functions\n\n    directory = Path(directory)\n    if not directory.is_dir():\n        raise ValueError(\"Could not find folder\")\n\n    description_path = directory / \"description.xml\"\n    predictions_path = directory / \"predictions.arff\"\n    trace_path = directory / \"trace.arff\"\n    model_path = directory / \"model.pkl\"\n\n    if not description_path.is_file():\n        raise ValueError(\"Could not find description.xml\")\n    if not predictions_path.is_file():\n        raise ValueError(\"Could not find predictions.arff\")\n    if (not model_path.is_file()) and expect_model:\n        raise ValueError(\"Could not find model.pkl\")\n\n    with description_path.open() as fht:\n        xml_string = fht.read()\n    run = openml.runs.functions._create_run_from_xml(xml_string, from_server=False)\n\n    if run.flow_id is None:\n        flow = openml.flows.OpenMLFlow.from_filesystem(directory)\n        run.flow = flow\n        run.flow_name = flow.name\n\n    with predictions_path.open() as fht:\n        predictions = arff.load(fht)\n        run.data_content = predictions[\"data\"]\n\n    if model_path.is_file():\n        # note that it will load the model if the file exists, even if\n        # expect_model is False\n        with model_path.open(\"rb\") as fhb:\n            run.model = pickle.load(fhb)  # noqa: S301\n\n    if trace_path.is_file():\n        run.trace = openml.runs.OpenMLRunTrace._from_filesystem(trace_path)\n\n    return run\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.get_metric_fn","title":"get_metric_fn","text":"<pre><code>get_metric_fn(sklearn_fn: Callable, kwargs: dict | None = None) -&gt; ndarray\n</code></pre> <p>Calculates metric scores based on predicted values. Assumes the run has been executed locally (and contains run_data). Furthermore, it assumes that the 'correct' or 'truth' attribute is specified in the arff (which is an optional field, but always the case for openml-python runs)</p> PARAMETER DESCRIPTION <code>sklearn_fn</code> <p>a function pointer to a sklearn function that accepts <code>y_true</code>, <code>y_pred</code> and <code>**kwargs</code></p> <p> TYPE: <code>function</code> </p> <code>kwargs</code> <p>kwargs for the function</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>scores</code> <p>metric results</p> <p> TYPE: <code>ndarray of scores of length num_folds * num_repeats</code> </p> Source code in <code>openml/runs/run.py</code> <pre><code>def get_metric_fn(self, sklearn_fn: Callable, kwargs: dict | None = None) -&gt; np.ndarray:  # noqa: PLR0915, PLR0912, C901\n    \"\"\"Calculates metric scores based on predicted values. Assumes the\n    run has been executed locally (and contains run_data). Furthermore,\n    it assumes that the 'correct' or 'truth' attribute is specified in\n    the arff (which is an optional field, but always the case for\n    openml-python runs)\n\n    Parameters\n    ----------\n    sklearn_fn : function\n        a function pointer to a sklearn function that\n        accepts ``y_true``, ``y_pred`` and ``**kwargs``\n    kwargs : dict\n        kwargs for the function\n\n    Returns\n    -------\n    scores : ndarray of scores of length num_folds * num_repeats\n        metric results\n    \"\"\"\n    kwargs = kwargs if kwargs else {}\n    if self.data_content is not None and self.task_id is not None:\n        predictions_arff = self._generate_arff_dict()\n    elif (self.output_files is not None) and (\"predictions\" in self.output_files):\n        predictions_file_url = openml._api_calls._file_id_to_url(\n            self.output_files[\"predictions\"],\n            \"predictions.arff\",\n        )\n        response = openml._api_calls._download_text_file(predictions_file_url)\n        predictions_arff = arff.loads(response)\n        # TODO: make this a stream reader\n    else:\n        raise ValueError(\n            \"Run should have been locally executed or \" \"contain outputfile reference.\",\n        )\n\n    # Need to know more about the task to compute scores correctly\n    task = get_task(self.task_id)\n\n    attribute_names = [att[0] for att in predictions_arff[\"attributes\"]]\n    if (\n        task.task_type_id in [TaskType.SUPERVISED_CLASSIFICATION, TaskType.LEARNING_CURVE]\n        and \"correct\" not in attribute_names\n    ):\n        raise ValueError('Attribute \"correct\" should be set for ' \"classification task runs\")\n    if task.task_type_id == TaskType.SUPERVISED_REGRESSION and \"truth\" not in attribute_names:\n        raise ValueError('Attribute \"truth\" should be set for ' \"regression task runs\")\n    if task.task_type_id != TaskType.CLUSTERING and \"prediction\" not in attribute_names:\n        raise ValueError('Attribute \"predict\" should be set for ' \"supervised task runs\")\n\n    def _attribute_list_to_dict(attribute_list):  # type: ignore\n        # convenience function: Creates a mapping to map from the name of\n        # attributes present in the arff prediction file to their index.\n        # This is necessary because the number of classes can be different\n        # for different tasks.\n        res = OrderedDict()\n        for idx in range(len(attribute_list)):\n            res[attribute_list[idx][0]] = idx\n        return res\n\n    attribute_dict = _attribute_list_to_dict(predictions_arff[\"attributes\"])\n\n    repeat_idx = attribute_dict[\"repeat\"]\n    fold_idx = attribute_dict[\"fold\"]\n    predicted_idx = attribute_dict[\"prediction\"]  # Assume supervised task\n\n    if task.task_type_id in (TaskType.SUPERVISED_CLASSIFICATION, TaskType.LEARNING_CURVE):\n        correct_idx = attribute_dict[\"correct\"]\n    elif task.task_type_id == TaskType.SUPERVISED_REGRESSION:\n        correct_idx = attribute_dict[\"truth\"]\n    has_samples = False\n    if \"sample\" in attribute_dict:\n        sample_idx = attribute_dict[\"sample\"]\n        has_samples = True\n\n    if (\n        predictions_arff[\"attributes\"][predicted_idx][1]\n        != predictions_arff[\"attributes\"][correct_idx][1]\n    ):\n        pred = predictions_arff[\"attributes\"][predicted_idx][1]\n        corr = predictions_arff[\"attributes\"][correct_idx][1]\n        raise ValueError(\n            \"Predicted and Correct do not have equal values:\" f\" {pred!s} Vs. {corr!s}\",\n        )\n\n    # TODO: these could be cached\n    values_predict: dict[int, dict[int, dict[int, list[float]]]] = {}\n    values_correct: dict[int, dict[int, dict[int, list[float]]]] = {}\n    for _line_idx, line in enumerate(predictions_arff[\"data\"]):\n        rep = line[repeat_idx]\n        fold = line[fold_idx]\n        samp = line[sample_idx] if has_samples else 0\n\n        if task.task_type_id in [\n            TaskType.SUPERVISED_CLASSIFICATION,\n            TaskType.LEARNING_CURVE,\n        ]:\n            prediction = predictions_arff[\"attributes\"][predicted_idx][1].index(\n                line[predicted_idx],\n            )\n            correct = predictions_arff[\"attributes\"][predicted_idx][1].index(line[correct_idx])\n        elif task.task_type_id == TaskType.SUPERVISED_REGRESSION:\n            prediction = line[predicted_idx]\n            correct = line[correct_idx]\n        if rep not in values_predict:\n            values_predict[rep] = OrderedDict()\n            values_correct[rep] = OrderedDict()\n        if fold not in values_predict[rep]:\n            values_predict[rep][fold] = OrderedDict()\n            values_correct[rep][fold] = OrderedDict()\n        if samp not in values_predict[rep][fold]:\n            values_predict[rep][fold][samp] = []\n            values_correct[rep][fold][samp] = []\n\n        values_predict[rep][fold][samp].append(prediction)\n        values_correct[rep][fold][samp].append(correct)\n\n    scores = []\n    for rep in values_predict:\n        for fold in values_predict[rep]:\n            last_sample = len(values_predict[rep][fold]) - 1\n            y_pred = values_predict[rep][fold][last_sample]\n            y_true = values_correct[rep][fold][last_sample]\n            scores.append(sklearn_fn(y_true, y_pred, **kwargs))\n    return np.array(scores)\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.to_filesystem","title":"to_filesystem","text":"<pre><code>to_filesystem(directory: str | Path, store_model: bool = True) -&gt; None\n</code></pre> <p>The inverse of the from_filesystem method. Serializes a run on the filesystem, to be uploaded later.</p> PARAMETER DESCRIPTION <code>directory</code> <p>a path leading to the folder where the results will be stored. Should be empty</p> <p> TYPE: <code>str</code> </p> <code>store_model</code> <p>if True, a model will be pickled as well. As this is the most storage expensive part, it is often desirable to not store the model.</p> <p> TYPE: <code>(bool, optional(default=True))</code> DEFAULT: <code>True</code> </p> Source code in <code>openml/runs/run.py</code> <pre><code>def to_filesystem(\n    self,\n    directory: str | Path,\n    store_model: bool = True,  # noqa: FBT001, FBT002\n) -&gt; None:\n    \"\"\"\n    The inverse of the from_filesystem method. Serializes a run\n    on the filesystem, to be uploaded later.\n\n    Parameters\n    ----------\n    directory : str\n        a path leading to the folder where the results\n        will be stored. Should be empty\n\n    store_model : bool, optional (default=True)\n        if True, a model will be pickled as well. As this is the most\n        storage expensive part, it is often desirable to not store the\n        model.\n    \"\"\"\n    if self.data_content is None or self.model is None:\n        raise ValueError(\"Run should have been executed (and contain \" \"model / predictions)\")\n    directory = Path(directory)\n    directory.mkdir(exist_ok=True, parents=True)\n\n    if any(directory.iterdir()):\n        raise ValueError(f\"Output directory {directory.expanduser().resolve()} should be empty\")\n\n    run_xml = self._to_xml()\n    predictions_arff = arff.dumps(self._generate_arff_dict())\n\n    # It seems like typing does not allow to define the same variable multiple times\n    with (directory / \"description.xml\").open(\"w\") as fh:\n        fh.write(run_xml)\n    with (directory / \"predictions.arff\").open(\"w\") as fh:\n        fh.write(predictions_arff)\n    if store_model:\n        with (directory / \"model.pkl\").open(\"wb\") as fh_b:\n            pickle.dump(self.model, fh_b)\n\n    if self.flow_id is None and self.flow is not None:\n        self.flow.to_filesystem(directory)\n\n    if self.trace is not None:\n        self.trace._to_filesystem(directory)\n</code></pre>"},{"location":"reference/#openml.OpenMLRun.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLSetup","title":"OpenMLSetup","text":"<pre><code>OpenMLSetup(setup_id: int, flow_id: int, parameters: dict[int, Any] | None)\n</code></pre> <p>Setup object (a.k.a. Configuration).</p> PARAMETER DESCRIPTION <code>setup_id</code> <p>The OpenML setup id</p> <p> TYPE: <code>int</code> </p> <code>flow_id</code> <p>The flow that it is build upon</p> <p> TYPE: <code>int</code> </p> <code>parameters</code> <p>The setting of the parameters</p> <p> TYPE: <code>dict</code> </p> Source code in <code>openml/setups/setup.py</code> <pre><code>def __init__(self, setup_id: int, flow_id: int, parameters: dict[int, Any] | None):\n    if not isinstance(setup_id, int):\n        raise ValueError(\"setup id should be int\")\n\n    if not isinstance(flow_id, int):\n        raise ValueError(\"flow id should be int\")\n\n    if parameters is not None and not isinstance(parameters, dict):\n        raise ValueError(\"parameters should be dict\")\n\n    self.setup_id = setup_id\n    self.flow_id = flow_id\n    self.parameters = parameters\n</code></pre>"},{"location":"reference/#openml.OpenMLSplit","title":"OpenMLSplit","text":"<pre><code>OpenMLSplit(name: int | str, description: str, split: dict[int, dict[int, dict[int, tuple[ndarray, ndarray]]]])\n</code></pre> <p>OpenML Split object.</p> <p>This class manages train-test splits for a dataset across multiple repetitions, folds, and samples.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name or ID of the split.</p> <p> TYPE: <code>int or str</code> </p> <code>description</code> <p>A description of the split.</p> <p> TYPE: <code>str</code> </p> <code>split</code> <p>A dictionary containing the splits organized by repetition, fold, and sample.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>openml/tasks/split.py</code> <pre><code>def __init__(\n    self,\n    name: int | str,\n    description: str,\n    split: dict[int, dict[int, dict[int, tuple[np.ndarray, np.ndarray]]]],\n):\n    self.description = description\n    self.name = name\n    self.split: dict[int, dict[int, dict[int, tuple[np.ndarray, np.ndarray]]]] = {}\n\n    # Add splits according to repetition\n    for repetition in split:\n        _rep = int(repetition)\n        self.split[_rep] = OrderedDict()\n        for fold in split[_rep]:\n            self.split[_rep][fold] = OrderedDict()\n            for sample in split[_rep][fold]:\n                self.split[_rep][fold][sample] = split[_rep][fold][sample]\n\n    self.repeats = len(self.split)\n\n    # TODO(eddiebergman): Better error message\n    if any(len(self.split[0]) != len(self.split[i]) for i in range(self.repeats)):\n        raise ValueError(\"\")\n\n    self.folds = len(self.split[0])\n    self.samples = len(self.split[0][0])\n</code></pre>"},{"location":"reference/#openml.OpenMLSplit.get","title":"get","text":"<pre><code>get(repeat: int = 0, fold: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns the specified data split from the CrossValidationSplit object.</p> PARAMETER DESCRIPTION <code>repeat</code> <p>Index of the repeat to retrieve.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>fold</code> <p>Index of the fold to retrieve.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sample</code> <p>Index of the sample to retrieve.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The data split for the specified repeat, fold, and sample.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the specified repeat, fold, or sample is not known.</p> Source code in <code>openml/tasks/split.py</code> <pre><code>def get(self, repeat: int = 0, fold: int = 0, sample: int = 0) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns the specified data split from the CrossValidationSplit object.\n\n    Parameters\n    ----------\n    repeat : int\n        Index of the repeat to retrieve.\n    fold : int\n        Index of the fold to retrieve.\n    sample : int\n        Index of the sample to retrieve.\n\n    Returns\n    -------\n    numpy.ndarray\n        The data split for the specified repeat, fold, and sample.\n\n    Raises\n    ------\n    ValueError\n        If the specified repeat, fold, or sample is not known.\n    \"\"\"\n    if repeat not in self.split:\n        raise ValueError(f\"Repeat {repeat!s} not known\")\n    if fold not in self.split[repeat]:\n        raise ValueError(f\"Fold {fold!s} not known\")\n    if sample not in self.split[repeat][fold]:\n        raise ValueError(f\"Sample {sample!s} not known\")\n    return self.split[repeat][fold][sample]\n</code></pre>"},{"location":"reference/#openml.OpenMLStudy","title":"OpenMLStudy","text":"<pre><code>OpenMLStudy(study_id: int | None, alias: str | None, benchmark_suite: int | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None, flows: list[int] | None, runs: list[int] | None, setups: list[int] | None)\n</code></pre> <p>               Bases: <code>BaseStudy</code></p> <p>An OpenMLStudy represents the OpenML concept of a study (a collection of runs).</p> <p>It contains the following information: name, id, description, creation date, creator id and a list of run ids.</p> <p>According to this list of run ids, the study object receives a list of OpenML object ids (datasets, flows, tasks and setups).</p> PARAMETER DESCRIPTION <code>study_id</code> <p>the study id</p> <p> TYPE: <code>int</code> </p> <code>alias</code> <p>a string ID, unique on server (url-friendly)</p> <p> TYPE: <code>str(optional)</code> </p> <code>benchmark_suite</code> <p>the benchmark suite (another study) upon which this study is ran. can only be active if main entity type is runs.</p> <p> TYPE: <code>int(optional)</code> </p> <code>name</code> <p>the name of the study (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>brief description (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>Whether the study is in preparation, active or deactivated</p> <p> TYPE: <code>str</code> </p> <code>creation_date</code> <p>date of creation (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>creator</code> <p>openml user id of the owner / creator</p> <p> TYPE: <code>int</code> </p> <code>tags</code> <p>The list of tags shows which tags are associated with the study. Each tag is a dict of (tag) name, window_start and write_access.</p> <p> TYPE: <code>list(dict)</code> </p> <code>data</code> <p>a list of data ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>tasks</code> <p>a list of task ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>flows</code> <p>a list of flow ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>runs</code> <p>a list of run ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>setups</code> <p>a list of setup ids associated with this study</p> <p> TYPE: <code>list</code> </p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    study_id: int | None,\n    alias: str | None,\n    benchmark_suite: int | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n    flows: list[int] | None,\n    runs: list[int] | None,\n    setups: list[int] | None,\n):\n    super().__init__(\n        study_id=study_id,\n        alias=alias,\n        main_entity_type=\"run\",\n        benchmark_suite=benchmark_suite,\n        name=name,\n        description=description,\n        status=status,\n        creation_date=creation_date,\n        creator=creator,\n        tags=tags,\n        data=data,\n        tasks=tasks,\n        flows=flows,\n        runs=runs,\n        setups=setups,\n    )\n</code></pre>"},{"location":"reference/#openml.OpenMLStudy.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/#openml.OpenMLStudy.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLStudy.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLStudy.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLStudy.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/#openml.OpenMLStudy.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/#openml.OpenMLStudy.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask","title":"OpenMLSupervisedTask","text":"<pre><code>OpenMLSupervisedTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None, task_id: int | None = None)\n</code></pre> <p>               Bases: <code>OpenMLTask</code>, <code>ABC</code></p> <p>OpenML Supervised Classification object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>ID of the task type.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Name of the task type.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the OpenML dataset associated with the task.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>Name of the target feature (the class variable).</p> <p> TYPE: <code>str</code> </p> <code>estimation_procedure_id</code> <p>ID of the estimation procedure for the task.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the estimation procedure for the task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Estimation parameters for the task.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Name of the evaluation measure for the task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the data splits for the task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>Refers to the unique identifier of task.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        data_splits_url=data_splits_url,\n    )\n\n    self.target_name = target_name\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/#openml.OpenMLSupervisedTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/#openml.OpenMLSupervisedTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLSupervisedTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>tuple - X and y</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLSupervisedTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.OpenMLTask","title":"OpenMLTask","text":"<pre><code>OpenMLTask(task_id: int | None, task_type_id: TaskType, task_type: str, data_set_id: int, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Task object.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>Refers to the unique identifier of OpenML task.</p> <p> TYPE: <code>int | None</code> </p> <code>task_type_id</code> <p>Refers to the type of OpenML task.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Refers to the OpenML task.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>Refers to the data.</p> <p> TYPE: <code>int</code> </p> <code>estimation_procedure_id</code> <p>Refers to the type of estimates used.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>estimation_procedure_type</code> <p>Refers to the type of estimation procedure used for the OpenML task.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Estimation parameters used for the OpenML task.</p> <p> TYPE: <code>dict[str, str] | None</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Refers to the evaluation measure.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>Refers to the URL of the data splits used for the OpenML task.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_id: int | None,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n):\n    self.task_id = int(task_id) if task_id is not None else None\n    self.task_type_id = task_type_id\n    self.task_type = task_type\n    self.dataset_id = int(data_set_id)\n    self.evaluation_measure = evaluation_measure\n    self.estimation_procedure: _EstimationProcedure = {\n        \"type\": estimation_procedure_type,\n        \"parameters\": estimation_parameters,\n        \"data_splits_url\": data_splits_url,\n    }\n    self.estimation_procedure_id = estimation_procedure_id\n    self.split: OpenMLSplit | None = None\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/#openml.OpenMLTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/#openml.OpenMLTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/#openml.OpenMLTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/#openml.populate_cache","title":"populate_cache","text":"<pre><code>populate_cache(task_ids: list[int] | None = None, dataset_ids: list[int | str] | None = None, flow_ids: list[int] | None = None, run_ids: list[int] | None = None) -&gt; None\n</code></pre> <p>Populate a cache for offline and parallel usage of the OpenML connector.</p> PARAMETER DESCRIPTION <code>task_ids</code> <p> TYPE: <code>iterable</code> DEFAULT: <code>None</code> </p> <code>dataset_ids</code> <p> TYPE: <code>iterable</code> DEFAULT: <code>None</code> </p> <code>flow_ids</code> <p> TYPE: <code>iterable</code> DEFAULT: <code>None</code> </p> <code>run_ids</code> <p> TYPE: <code>iterable</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>None</code> Source code in <code>openml/__init__.py</code> <pre><code>def populate_cache(\n    task_ids: list[int] | None = None,\n    dataset_ids: list[int | str] | None = None,\n    flow_ids: list[int] | None = None,\n    run_ids: list[int] | None = None,\n) -&gt; None:\n    \"\"\"\n    Populate a cache for offline and parallel usage of the OpenML connector.\n\n    Parameters\n    ----------\n    task_ids : iterable\n\n    dataset_ids : iterable\n\n    flow_ids : iterable\n\n    run_ids : iterable\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if task_ids is not None:\n        for task_id in task_ids:\n            tasks.functions.get_task(task_id)\n\n    if dataset_ids is not None:\n        for dataset_id in dataset_ids:\n            datasets.functions.get_dataset(dataset_id)\n\n    if flow_ids is not None:\n        for flow_id in flow_ids:\n            flows.functions.get_flow(flow_id)\n\n    if run_ids is not None:\n        for run_id in run_ids:\n            runs.functions.get_run(run_id)\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>openml<ul> <li>__version__</li> <li>_api_calls</li> <li>base</li> <li>cli</li> <li>config</li> <li>datasets<ul> <li>data_feature</li> <li>dataset</li> <li>functions</li> </ul> </li> <li>evaluations<ul> <li>evaluation</li> <li>functions</li> </ul> </li> <li>exceptions</li> <li>extensions<ul> <li>extension_interface</li> <li>functions</li> </ul> </li> <li>flows<ul> <li>flow</li> <li>functions</li> </ul> </li> <li>runs<ul> <li>functions</li> <li>run</li> <li>trace</li> </ul> </li> <li>setups<ul> <li>functions</li> <li>setup</li> </ul> </li> <li>study<ul> <li>functions</li> <li>study</li> </ul> </li> <li>tasks<ul> <li>functions</li> <li>split</li> <li>task</li> </ul> </li> <li>testing</li> <li>utils</li> </ul> </li> </ul>"},{"location":"reference/__version__/","title":"__version__","text":""},{"location":"reference/__version__/#openml.__version__","title":"openml.__version__","text":"<p>Version information.</p>"},{"location":"reference/_api_calls/","title":"_api_calls","text":""},{"location":"reference/_api_calls/#openml._api_calls","title":"openml._api_calls","text":""},{"location":"reference/_api_calls/#openml._api_calls.resolve_env_proxies","title":"resolve_env_proxies","text":"<pre><code>resolve_env_proxies(url: str) -&gt; str | None\n</code></pre> <p>Attempt to find a suitable proxy for this url.</p> <p>Relies on <code>requests</code> internals to remain consistent. To disable this from the environment, please set the enviornment varialbe <code>no_proxy=\"*\"</code>.</p> PARAMETER DESCRIPTION <code>url</code> <p>The url endpoint</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[str]</code> <p>The proxy url if found, else None</p> Source code in <code>openml/_api_calls.py</code> <pre><code>def resolve_env_proxies(url: str) -&gt; str | None:\n    \"\"\"Attempt to find a suitable proxy for this url.\n\n    Relies on ``requests`` internals to remain consistent. To disable this from the\n    environment, please set the enviornment varialbe ``no_proxy=\"*\"``.\n\n    Parameters\n    ----------\n    url : str\n        The url endpoint\n\n    Returns\n    -------\n    Optional[str]\n        The proxy url if found, else None\n    \"\"\"\n    resolved_proxies = requests.utils.get_environ_proxies(url)\n    return requests.utils.select_proxy(url, resolved_proxies)  # type: ignore\n</code></pre>"},{"location":"reference/base/","title":"base","text":""},{"location":"reference/base/#openml.base","title":"openml.base","text":""},{"location":"reference/base/#openml.base.OpenMLBase","title":"OpenMLBase","text":"<p>               Bases: <code>ABC</code></p> <p>Base object for functionality that is shared across entities.</p>"},{"location":"reference/base/#openml.base.OpenMLBase.id","title":"id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The id of the entity, it is unique for its entity type.</p>"},{"location":"reference/base/#openml.base.OpenMLBase.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/base/#openml.base.OpenMLBase.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/base/#openml.base.OpenMLBase.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/base/#openml.base.OpenMLBase.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/base/#openml.base.OpenMLBase.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/base/#openml.base.OpenMLBase.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/cli/","title":"cli","text":""},{"location":"reference/cli/#openml.cli","title":"openml.cli","text":"<p>Command Line Interface for <code>openml</code> to configure its settings.</p>"},{"location":"reference/cli/#openml.cli.configure","title":"configure","text":"<pre><code>configure(args: Namespace) -&gt; None\n</code></pre> <p>Calls the right submenu(s) to edit <code>args.field</code> in the configuration file.</p> Source code in <code>openml/cli.py</code> <pre><code>def configure(args: argparse.Namespace) -&gt; None:\n    \"\"\"Calls the right submenu(s) to edit `args.field` in the configuration file.\"\"\"\n    set_functions = {\n        \"apikey\": configure_apikey,\n        \"server\": configure_server,\n        \"cachedir\": configure_cachedir,\n        \"retry_policy\": configure_retry_policy,\n        \"connection_n_retries\": configure_connection_n_retries,\n        \"avoid_duplicate_runs\": configure_avoid_duplicate_runs,\n        \"verbosity\": configure_verbosity,\n    }\n\n    def not_supported_yet(_: str) -&gt; None:\n        print(f\"Setting '{args.field}' is not supported yet.\")\n\n    if args.field not in [\"all\", \"none\"]:\n        set_functions.get(args.field, not_supported_yet)(args.value)\n    else:\n        if args.value is not None:\n            print(f\"Can not set value ('{args.value}') when field is specified as '{args.field}'.\")\n            sys.exit()\n        print_configuration()\n\n    if args.field == \"all\":\n        for set_field_function in set_functions.values():\n            set_field_function(args.value)\n</code></pre>"},{"location":"reference/cli/#openml.cli.configure_field","title":"configure_field","text":"<pre><code>configure_field(field: str, value: None | str, check_with_message: Callable[[str], str], intro_message: str, input_message: str, sanitize: Callable[[str], str] | None = None) -&gt; None\n</code></pre> <p>Configure <code>field</code> with <code>value</code>. If <code>value</code> is None ask the user for input.</p> <p><code>value</code> and user input are first corrected/auto-completed with <code>convert_value</code> if provided, then validated with <code>check_with_message</code> function. If the user input a wrong value in interactive mode, the user gets to input a new value. The new valid value is saved in the openml configuration file. In case an invalid <code>value</code> is supplied directly (non-interactive), no changes are made.</p> PARAMETER DESCRIPTION <code>field</code> <p>Field to set.</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>Value to field to. If <code>None</code> will ask user for input.</p> <p> TYPE: <code>None | str</code> </p> <code>check_with_message</code> <p>Function which validates <code>value</code> or user input, and returns either an error message if it is invalid, or a False-like value if <code>value</code> is valid.</p> <p> TYPE: <code>Callable[[str], str]</code> </p> <code>intro_message</code> <p>Message that is printed once if user input is requested (e.g. instructions).</p> <p> TYPE: <code>str</code> </p> <code>input_message</code> <p>Message that comes with the input prompt.</p> <p> TYPE: <code>str</code> </p> <code>sanitize</code> <p>A function to convert user input to 'more acceptable' input, e.g. for auto-complete. If no correction of user input is possible, return the original value. If no function is provided, don't attempt to correct/auto-complete input.</p> <p> TYPE: <code>Callable[[str], str] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/cli.py</code> <pre><code>def configure_field(  # noqa: PLR0913\n    field: str,\n    value: None | str,\n    check_with_message: Callable[[str], str],\n    intro_message: str,\n    input_message: str,\n    sanitize: Callable[[str], str] | None = None,\n) -&gt; None:\n    \"\"\"Configure `field` with `value`. If `value` is None ask the user for input.\n\n    `value` and user input are first corrected/auto-completed with `convert_value` if provided,\n    then validated with `check_with_message` function.\n    If the user input a wrong value in interactive mode, the user gets to input a new value.\n    The new valid value is saved in the openml configuration file.\n    In case an invalid `value` is supplied directly (non-interactive), no changes are made.\n\n    Parameters\n    ----------\n    field: str\n        Field to set.\n    value: str, None\n        Value to field to. If `None` will ask user for input.\n    check_with_message: Callable[[str], str]\n        Function which validates `value` or user input, and returns either an error message if it\n        is invalid, or a False-like value if `value` is valid.\n    intro_message: str\n        Message that is printed once if user input is requested (e.g. instructions).\n    input_message: str\n        Message that comes with the input prompt.\n    sanitize: Union[Callable[[str], str], None]\n        A function to convert user input to 'more acceptable' input, e.g. for auto-complete.\n        If no correction of user input is possible, return the original value.\n        If no function is provided, don't attempt to correct/auto-complete input.\n    \"\"\"\n    if value is not None:\n        if sanitize:\n            value = sanitize(value)\n        malformed_input = check_with_message(value)\n        if malformed_input:\n            print(malformed_input)\n            sys.exit()\n    else:\n        print(intro_message)\n        value = wait_until_valid_input(\n            prompt=input_message,\n            check=check_with_message,\n            sanitize=sanitize,\n        )\n    verbose_set(field, value)\n</code></pre>"},{"location":"reference/cli/#openml.cli.wait_until_valid_input","title":"wait_until_valid_input","text":"<pre><code>wait_until_valid_input(prompt: str, check: Callable[[str], str], sanitize: Callable[[str], str] | None) -&gt; str\n</code></pre> <p>Asks <code>prompt</code> until an input is received which returns True for <code>check</code>.</p> PARAMETER DESCRIPTION <code>prompt</code> <p>message to display</p> <p> TYPE: <code>str</code> </p> <code>check</code> <p>function to call with the given input, that provides an error message if the input is not valid otherwise, and False-like otherwise.</p> <p> TYPE: <code>Callable[[str], str]</code> </p> <code>sanitize</code> <p>A function which attempts to sanitize the user input (e.g. auto-complete).</p> <p> TYPE: <code>Callable[[str], str] | None</code> </p> RETURNS DESCRIPTION <code>valid input</code> Source code in <code>openml/cli.py</code> <pre><code>def wait_until_valid_input(\n    prompt: str,\n    check: Callable[[str], str],\n    sanitize: Callable[[str], str] | None,\n) -&gt; str:\n    \"\"\"Asks `prompt` until an input is received which returns True for `check`.\n\n    Parameters\n    ----------\n    prompt: str\n        message to display\n    check: Callable[[str], str]\n        function to call with the given input, that provides an error message if the input is not\n        valid otherwise, and False-like otherwise.\n    sanitize: Callable[[str], str], optional\n        A function which attempts to sanitize the user input (e.g. auto-complete).\n\n    Returns\n    -------\n    valid input\n\n    \"\"\"\n    while True:\n        response = input(prompt)\n        if sanitize:\n            response = sanitize(response)\n        error_message = check(response)\n        if error_message:\n            print(error_message, end=\"\\n\\n\")\n        else:\n            return response\n</code></pre>"},{"location":"reference/config/","title":"config","text":""},{"location":"reference/config/#openml.config","title":"openml.config","text":"<p>Store module level information like the API key, cache directory and the server</p>"},{"location":"reference/config/#openml.config.ConfigurationForExamples","title":"ConfigurationForExamples","text":"<p>Allows easy switching to and from a test configuration, used for examples.</p>"},{"location":"reference/config/#openml.config.ConfigurationForExamples.start_using_configuration_for_example","title":"start_using_configuration_for_example  <code>classmethod</code>","text":"<pre><code>start_using_configuration_for_example() -&gt; None\n</code></pre> <p>Sets the configuration to connect to the test server with valid apikey.</p> <p>To configuration as was before this call is stored, and can be recovered by using the <code>stop_use_example_configuration</code> method.</p> Source code in <code>openml/config.py</code> <pre><code>@classmethod\ndef start_using_configuration_for_example(cls) -&gt; None:\n    \"\"\"Sets the configuration to connect to the test server with valid apikey.\n\n    To configuration as was before this call is stored, and can be recovered\n    by using the `stop_use_example_configuration` method.\n    \"\"\"\n    global server  # noqa: PLW0603\n    global apikey  # noqa: PLW0603\n\n    if cls._start_last_called and server == cls._test_server and apikey == cls._test_apikey:\n        # Method is called more than once in a row without modifying the server or apikey.\n        # We don't want to save the current test configuration as a last used configuration.\n        return\n\n    cls._last_used_server = server\n    cls._last_used_key = apikey\n    cls._start_last_called = True\n\n    # Test server key for examples\n    server = cls._test_server\n    apikey = cls._test_apikey\n    warnings.warn(\n        f\"Switching to the test server {server} to not upload results to the live server. \"\n        \"Using the test server may result in reduced performance of the API!\",\n        stacklevel=2,\n    )\n</code></pre>"},{"location":"reference/config/#openml.config.ConfigurationForExamples.stop_using_configuration_for_example","title":"stop_using_configuration_for_example  <code>classmethod</code>","text":"<pre><code>stop_using_configuration_for_example() -&gt; None\n</code></pre> <p>Return to configuration as it was before <code>start_use_example_configuration</code>.</p> Source code in <code>openml/config.py</code> <pre><code>@classmethod\ndef stop_using_configuration_for_example(cls) -&gt; None:\n    \"\"\"Return to configuration as it was before `start_use_example_configuration`.\"\"\"\n    if not cls._start_last_called:\n        # We don't want to allow this because it will (likely) result in the `server` and\n        # `apikey` variables being set to None.\n        raise RuntimeError(\n            \"`stop_use_example_configuration` called without a saved config.\"\n            \"`start_use_example_configuration` must be called first.\",\n        )\n\n    global server  # noqa: PLW0603\n    global apikey  # noqa: PLW0603\n\n    server = cast(str, cls._last_used_server)\n    apikey = cast(str, cls._last_used_key)\n    cls._start_last_called = False\n</code></pre>"},{"location":"reference/config/#openml.config.get_cache_directory","title":"get_cache_directory","text":"<pre><code>get_cache_directory() -&gt; str\n</code></pre> <p>Get the current cache directory.</p> <p>This gets the cache directory for the current server relative to the root cache directory that can be set via <code>set_root_cache_directory()</code>. The cache directory is the <code>root_cache_directory</code> with additional information on which subdirectory to use based on the server name. By default it is <code>root_cache_directory / org / openml / www</code> for the standard OpenML.org server and is defined as <code>root_cache_directory / top-level domain / second-level domain / hostname</code> ```</p>"},{"location":"reference/config/#openml.config.get_cache_directory--returns","title":"Returns","text":"<p>cachedir : string     The current cache directory.</p> Source code in <code>openml/config.py</code> <pre><code>def get_cache_directory() -&gt; str:\n    \"\"\"Get the current cache directory.\n\n    This gets the cache directory for the current server relative\n    to the root cache directory that can be set via\n    ``set_root_cache_directory()``. The cache directory is the\n    ``root_cache_directory`` with additional information on which\n    subdirectory to use based on the server name. By default it is\n    ``root_cache_directory / org / openml / www`` for the standard\n    OpenML.org server and is defined as\n    ``root_cache_directory / top-level domain / second-level domain /\n    hostname``\n    ```\n\n    Returns\n    -------\n    cachedir : string\n        The current cache directory.\n\n    \"\"\"\n    url_suffix = urlparse(server).netloc\n    reversed_url_suffix = os.sep.join(url_suffix.split(\".\")[::-1])  # noqa: PTH118\n    return os.path.join(_root_cache_directory, reversed_url_suffix)  # noqa: PTH118\n</code></pre>"},{"location":"reference/config/#openml.config.get_server_base_url","title":"get_server_base_url","text":"<pre><code>get_server_base_url() -&gt; str\n</code></pre> <p>Return the base URL of the currently configured server.</p> <p>Turns <code>\"https://api.openml.org/api/v1/xml\"</code> in <code>\"https://www.openml.org/\"</code> and <code>\"https://test.openml.org/api/v1/xml\"</code> in <code>\"https://test.openml.org/\"</code></p> RETURNS DESCRIPTION <code>str</code> Source code in <code>openml/config.py</code> <pre><code>def get_server_base_url() -&gt; str:\n    \"\"\"Return the base URL of the currently configured server.\n\n    Turns ``\"https://api.openml.org/api/v1/xml\"`` in ``\"https://www.openml.org/\"``\n    and ``\"https://test.openml.org/api/v1/xml\"`` in ``\"https://test.openml.org/\"``\n\n    Returns\n    -------\n    str\n    \"\"\"\n    domain, path = server.split(\"/api\", maxsplit=1)\n    return domain.replace(\"api\", \"www\")\n</code></pre>"},{"location":"reference/config/#openml.config.overwrite_config_context","title":"overwrite_config_context","text":"<pre><code>overwrite_config_context(config: dict[str, Any]) -&gt; Iterator[_Config]\n</code></pre> <p>A context manager to temporarily override variables in the configuration.</p> Source code in <code>openml/config.py</code> <pre><code>@contextmanager\ndef overwrite_config_context(config: dict[str, Any]) -&gt; Iterator[_Config]:\n    \"\"\"A context manager to temporarily override variables in the configuration.\"\"\"\n    existing_config = get_config_as_dict()\n    merged_config = {**existing_config, **config}\n\n    _setup(merged_config)  # type: ignore\n    yield merged_config  # type: ignore\n\n    _setup(existing_config)\n</code></pre>"},{"location":"reference/config/#openml.config.set_console_log_level","title":"set_console_log_level","text":"<pre><code>set_console_log_level(console_output_level: int) -&gt; None\n</code></pre> <p>Set console output to the desired level and register it with openml logger if needed.</p> Source code in <code>openml/config.py</code> <pre><code>def set_console_log_level(console_output_level: int) -&gt; None:\n    \"\"\"Set console output to the desired level and register it with openml logger if needed.\"\"\"\n    global console_handler  # noqa: PLW0602\n    assert console_handler is not None\n    _set_level_register_and_store(console_handler, console_output_level)\n</code></pre>"},{"location":"reference/config/#openml.config.set_field_in_config_file","title":"set_field_in_config_file","text":"<pre><code>set_field_in_config_file(field: str, value: Any) -&gt; None\n</code></pre> <p>Overwrites the <code>field</code> in the configuration file with the new <code>value</code>.</p> Source code in <code>openml/config.py</code> <pre><code>def set_field_in_config_file(field: str, value: Any) -&gt; None:\n    \"\"\"Overwrites the `field` in the configuration file with the new `value`.\"\"\"\n    if field not in _defaults:\n        raise ValueError(f\"Field '{field}' is not valid and must be one of '{_defaults.keys()}'.\")\n\n    # TODO(eddiebergman): This use of globals has gone too far\n    globals()[field] = value\n    config_file = determine_config_file_path()\n    config = _parse_config(config_file)\n    with config_file.open(\"w\") as fh:\n        for f in _defaults:\n            # We can't blindly set all values based on globals() because when the user\n            # sets it through config.FIELD it should not be stored to file.\n            # There doesn't seem to be a way to avoid writing defaults to file with configparser,\n            # because it is impossible to distinguish from an explicitly set value that matches\n            # the default value, to one that was set to its default because it was omitted.\n            value = globals()[f] if f == field else config.get(f)  # type: ignore\n            if value is not None:\n                fh.write(f\"{f} = {value}\\n\")\n</code></pre>"},{"location":"reference/config/#openml.config.set_file_log_level","title":"set_file_log_level","text":"<pre><code>set_file_log_level(file_output_level: int) -&gt; None\n</code></pre> <p>Set file output to the desired level and register it with openml logger if needed.</p> Source code in <code>openml/config.py</code> <pre><code>def set_file_log_level(file_output_level: int) -&gt; None:\n    \"\"\"Set file output to the desired level and register it with openml logger if needed.\"\"\"\n    global file_handler  # noqa: PLW0602\n    assert file_handler is not None\n    _set_level_register_and_store(file_handler, file_output_level)\n</code></pre>"},{"location":"reference/config/#openml.config.set_root_cache_directory","title":"set_root_cache_directory","text":"<pre><code>set_root_cache_directory(root_cache_directory: str | Path) -&gt; None\n</code></pre> <p>Set module-wide base cache directory.</p> <p>Sets the root cache directory, wherin the cache directories are created to store content from different OpenML servers. For example, by default, cached data for the standard OpenML.org server is stored at <code>root_cache_directory / org / openml / www</code>, and the general pattern is <code>root_cache_directory / top-level domain / second-level domain / hostname</code>.</p> PARAMETER DESCRIPTION <code>root_cache_directory</code> <p>Path to use as cache directory.</p> <p> TYPE: <code>string</code> </p> See Also <p>get_cache_directory</p> Source code in <code>openml/config.py</code> <pre><code>def set_root_cache_directory(root_cache_directory: str | Path) -&gt; None:\n    \"\"\"Set module-wide base cache directory.\n\n    Sets the root cache directory, wherin the cache directories are\n    created to store content from different OpenML servers. For example,\n    by default, cached data for the standard OpenML.org server is stored\n    at ``root_cache_directory / org / openml / www``, and the general\n    pattern is ``root_cache_directory / top-level domain / second-level\n    domain / hostname``.\n\n    Parameters\n    ----------\n    root_cache_directory : string\n         Path to use as cache directory.\n\n    See Also\n    --------\n    get_cache_directory\n    \"\"\"\n    global _root_cache_directory  # noqa: PLW0603\n    _root_cache_directory = Path(root_cache_directory)\n</code></pre>"},{"location":"reference/exceptions/","title":"exceptions","text":""},{"location":"reference/exceptions/#openml.exceptions","title":"openml.exceptions","text":""},{"location":"reference/exceptions/#openml.exceptions.ObjectNotPublishedError","title":"ObjectNotPublishedError","text":"<pre><code>ObjectNotPublishedError(message: str)\n</code></pre> <p>               Bases: <code>PyOpenMLError</code></p> <p>Indicates an object has not been published yet.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLCacheException","title":"OpenMLCacheException","text":"<pre><code>OpenMLCacheException(message: str)\n</code></pre> <p>               Bases: <code>PyOpenMLError</code></p> <p>Dataset / task etc not found in cache</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLHashException","title":"OpenMLHashException","text":"<pre><code>OpenMLHashException(message: str)\n</code></pre> <p>               Bases: <code>PyOpenMLError</code></p> <p>Locally computed hash is different than hash announced by the server.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLNotAuthorizedError","title":"OpenMLNotAuthorizedError","text":"<pre><code>OpenMLNotAuthorizedError(message: str)\n</code></pre> <p>               Bases: <code>OpenMLServerError</code></p> <p>Indicates an authenticated user is not authorized to execute the requested action.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLPrivateDatasetError","title":"OpenMLPrivateDatasetError","text":"<pre><code>OpenMLPrivateDatasetError(message: str)\n</code></pre> <p>               Bases: <code>PyOpenMLError</code></p> <p>Exception thrown when the user has no rights to access the dataset.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLRunsExistError","title":"OpenMLRunsExistError","text":"<pre><code>OpenMLRunsExistError(run_ids: set[int], message: str)\n</code></pre> <p>               Bases: <code>PyOpenMLError</code></p> <p>Indicates run(s) already exists on the server when they should not be duplicated.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, run_ids: set[int], message: str) -&gt; None:\n    if len(run_ids) &lt; 1:\n        raise ValueError(\"Set of run ids must be non-empty.\")\n    self.run_ids = run_ids\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLServerError","title":"OpenMLServerError","text":"<pre><code>OpenMLServerError(message: str)\n</code></pre> <p>               Bases: <code>PyOpenMLError</code></p> <p>class for when something is really wrong on the server (result did not parse to dict), contains unparsed error.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLServerException","title":"OpenMLServerException","text":"<pre><code>OpenMLServerException(message: str, code: int | None = None, url: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLServerError</code></p> <p>exception for when the result of the server was not 200 (e.g., listing call w/o results).</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str, code: int | None = None, url: str | None = None):\n    self.message = message\n    self.code = code\n    self.url = url\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.OpenMLServerNoResult","title":"OpenMLServerNoResult","text":"<pre><code>OpenMLServerNoResult(message: str, code: int | None = None, url: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLServerException</code></p> <p>Exception for when the result of the server is empty.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str, code: int | None = None, url: str | None = None):\n    self.message = message\n    self.code = code\n    self.url = url\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exceptions/#openml.exceptions.PyOpenMLError","title":"PyOpenMLError","text":"<pre><code>PyOpenMLError(message: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Base class for all exceptions in OpenML-Python.</p> Source code in <code>openml/exceptions.py</code> <pre><code>def __init__(self, message: str):\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/testing/","title":"testing","text":""},{"location":"reference/testing/#openml.testing","title":"openml.testing","text":""},{"location":"reference/testing/#openml.testing.CustomImputer","title":"CustomImputer","text":"<p>               Bases: <code>Imputer</code></p> <p>Duplicate class alias for sklearn's SimpleImputer</p> <p>Helps bypass the sklearn extension duplicate operation check</p>"},{"location":"reference/testing/#openml.testing.TestBase","title":"TestBase","text":"<p>               Bases: <code>TestCase</code></p> <p>Base class for tests</p> Note <p>Currently hard-codes a read-write key. Hopefully soon allows using a test server, not the production server.</p>"},{"location":"reference/testing/#openml.testing.TestBase.setUp","title":"setUp","text":"<pre><code>setUp(n_levels: int = 1, tmpdir_suffix: str = '') -&gt; None\n</code></pre> <p>Setup variables and temporary directories.</p> <p>In particular, this methods:</p> <ul> <li>creates a temporary working directory</li> <li>figures out a path to a few static test files</li> <li>set the default server to be the test server</li> <li>set a static API key for the test server</li> <li>increases the maximal number of retries</li> </ul> PARAMETER DESCRIPTION <code>n_levels</code> <p>Number of nested directories the test is in. Necessary to resolve the path to the <code>files</code> directory, which is located directly under the <code>tests</code> directory.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>openml/testing.py</code> <pre><code>def setUp(self, n_levels: int = 1, tmpdir_suffix: str = \"\") -&gt; None:\n    \"\"\"Setup variables and temporary directories.\n\n    In particular, this methods:\n\n    * creates a temporary working directory\n    * figures out a path to a few static test files\n    * set the default server to be the test server\n    * set a static API key for the test server\n    * increases the maximal number of retries\n\n    Parameters\n    ----------\n    n_levels : int\n        Number of nested directories the test is in. Necessary to resolve the path to the\n        ``files`` directory, which is located directly under the ``tests`` directory.\n    \"\"\"\n    # This cache directory is checked in to git to simulate a populated\n    # cache\n    self.maxDiff = None\n    abspath_this_file = Path(inspect.getfile(self.__class__)).absolute()\n    static_cache_dir = abspath_this_file.parent\n    for _ in range(n_levels):\n        static_cache_dir = static_cache_dir.parent.absolute()\n\n    content = os.listdir(static_cache_dir)\n    if \"files\" in content:\n        static_cache_dir = static_cache_dir / \"files\"\n    else:\n        raise ValueError(\n            f\"Cannot find test cache dir, expected it to be {static_cache_dir}!\",\n        )\n\n    self.static_cache_dir = static_cache_dir\n    self.cwd = Path.cwd()\n    workdir = Path(__file__).parent.absolute()\n    tmp_dir_name = self.id() + tmpdir_suffix\n    self.workdir = workdir / tmp_dir_name\n    shutil.rmtree(self.workdir, ignore_errors=True)\n\n    self.workdir.mkdir(exist_ok=True)\n    os.chdir(self.workdir)\n\n    self.cached = True\n    openml.config.apikey = TestBase.user_key\n    self.production_server = \"https://www.openml.org/api/v1/xml\"\n    openml.config.set_root_cache_directory(str(self.workdir))\n\n    # Increase the number of retries to avoid spurious server failures\n    self.retry_policy = openml.config.retry_policy\n    self.connection_n_retries = openml.config.connection_n_retries\n    openml.config.set_retry_policy(\"robot\", n_retries=20)\n</code></pre>"},{"location":"reference/testing/#openml.testing.TestBase.tearDown","title":"tearDown","text":"<pre><code>tearDown() -&gt; None\n</code></pre> <p>Tear down the test</p> Source code in <code>openml/testing.py</code> <pre><code>def tearDown(self) -&gt; None:\n    \"\"\"Tear down the test\"\"\"\n    os.chdir(self.cwd)\n    try:\n        shutil.rmtree(self.workdir)\n    except PermissionError as e:\n        if os.name != \"nt\":\n            # one of the files may still be used by another process\n            raise e\n\n    openml.config.connection_n_retries = self.connection_n_retries\n    openml.config.retry_policy = self.retry_policy\n</code></pre>"},{"location":"reference/testing/#openml.testing.TestBase.use_production_server","title":"use_production_server","text":"<pre><code>use_production_server() -&gt; None\n</code></pre> <p>Use the production server for the OpenML API calls.</p> <p>Please use this sparingly - it is better to use the test server.</p> Source code in <code>openml/testing.py</code> <pre><code>def use_production_server(self) -&gt; None:\n    \"\"\"\n    Use the production server for the OpenML API calls.\n\n    Please use this sparingly - it is better to use the test server.\n    \"\"\"\n    openml.config.server = self.production_server\n    openml.config.apikey = \"\"\n</code></pre>"},{"location":"reference/testing/#openml.testing.check_task_existence","title":"check_task_existence","text":"<pre><code>check_task_existence(task_type: TaskType, dataset_id: int, target_name: str, **kwargs: dict[str, str | int | dict[str, str | int | TaskType]]) -&gt; int | None\n</code></pre> <p>Checks if any task with exists on test server that matches the meta data.</p> Parameter <p>task_type : openml.tasks.TaskType dataset_id : int target_name : str</p> Return <p>int, None</p> Source code in <code>openml/testing.py</code> <pre><code>def check_task_existence(\n    task_type: TaskType,\n    dataset_id: int,\n    target_name: str,\n    **kwargs: dict[str, str | int | dict[str, str | int | openml.tasks.TaskType]],\n) -&gt; int | None:\n    \"\"\"Checks if any task with exists on test server that matches the meta data.\n\n    Parameter\n    ---------\n    task_type : openml.tasks.TaskType\n    dataset_id : int\n    target_name : str\n\n    Return\n    ------\n    int, None\n    \"\"\"\n    return_val = None\n    tasks = openml.tasks.list_tasks(task_type=task_type)\n    if len(tasks) == 0:\n        return None\n    tasks = tasks.loc[tasks[\"did\"] == dataset_id]\n    if len(tasks) == 0:\n        return None\n    tasks = tasks.loc[tasks[\"target_feature\"] == target_name]\n    if len(tasks) == 0:\n        return None\n    task_match = []\n    for task_id in tasks[\"tid\"].to_list():\n        task_match.append(task_id)\n        try:\n            task = openml.tasks.get_task(task_id)\n        except OpenMLServerException:\n            # can fail if task_id deleted by another parallely run unit test\n            task_match.pop(-1)\n            return_val = None\n            continue\n        for k, v in kwargs.items():\n            if getattr(task, k) != v:\n                # even if one of the meta-data key mismatches, then task_id is not a match\n                task_match.pop(-1)\n                break\n        # if task_id is retained in the task_match list, it passed all meta key-value matches\n        if len(task_match) == 1:\n            return_val = task_id\n            break\n    if len(task_match) == 0:\n        return_val = None\n    return return_val\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#openml.utils","title":"openml.utils","text":""},{"location":"reference/utils/#openml.utils.ProgressBar","title":"ProgressBar","text":"<pre><code>ProgressBar()\n</code></pre> <p>               Bases: <code>ProgressType</code></p> <p>Progressbar for MinIO function's <code>progress</code> parameter.</p> Source code in <code>openml/utils.py</code> <pre><code>def __init__(self) -&gt; None:\n    self._object_name = \"\"\n    self._progress_bar: tqdm | None = None\n</code></pre>"},{"location":"reference/utils/#openml.utils.ProgressBar.set_meta","title":"set_meta","text":"<pre><code>set_meta(object_name: str, total_length: int) -&gt; None\n</code></pre> <p>Initializes the progress bar.</p> PARAMETER DESCRIPTION <code>object_name</code> <p>Not used.</p> <p> TYPE: <code>str</code> </p> <code>total_length</code> <p>File size of the object in bytes.</p> <p> TYPE: <code>int</code> </p> Source code in <code>openml/utils.py</code> <pre><code>def set_meta(self, object_name: str, total_length: int) -&gt; None:\n    \"\"\"Initializes the progress bar.\n\n    Parameters\n    ----------\n    object_name: str\n      Not used.\n\n    total_length: int\n      File size of the object in bytes.\n    \"\"\"\n    self._object_name = object_name\n    self._progress_bar = tqdm(total=total_length, unit_scale=True, unit=\"B\")\n</code></pre>"},{"location":"reference/utils/#openml.utils.ProgressBar.update","title":"update","text":"<pre><code>update(length: int) -&gt; None\n</code></pre> <p>Updates the progress bar.</p> PARAMETER DESCRIPTION <code>length</code> <p>Number of bytes downloaded since last <code>update</code> call.</p> <p> TYPE: <code>int</code> </p> Source code in <code>openml/utils.py</code> <pre><code>def update(self, length: int) -&gt; None:\n    \"\"\"Updates the progress bar.\n\n    Parameters\n    ----------\n    length: int\n      Number of bytes downloaded since last `update` call.\n    \"\"\"\n    if not self._progress_bar:\n        raise RuntimeError(\"Call `set_meta` before calling `update`.\")\n    self._progress_bar.update(length)\n    if self._progress_bar.total &lt;= self._progress_bar.n:\n        self._progress_bar.close()\n</code></pre>"},{"location":"reference/utils/#openml.utils.extract_xml_tags","title":"extract_xml_tags","text":"<pre><code>extract_xml_tags(xml_tag_name: str, node: Mapping[str, Any], *, allow_none: bool = True) -&gt; Any | None\n</code></pre> <p>Helper to extract xml tags from xmltodict.</p> PARAMETER DESCRIPTION <code>xml_tag_name</code> <p>Name of the xml tag to extract from the node.</p> <p> TYPE: <code>str</code> </p> <code>node</code> <p>Node object returned by <code>xmltodict</code> from which <code>xml_tag_name</code> should be extracted.</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> <code>allow_none</code> <p>If <code>False</code>, the tag needs to exist in the node. Will raise a <code>ValueError</code> if it does not.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>object</code> Source code in <code>openml/utils.py</code> <pre><code>def extract_xml_tags(\n    xml_tag_name: str,\n    node: Mapping[str, Any],\n    *,\n    allow_none: bool = True,\n) -&gt; Any | None:\n    \"\"\"Helper to extract xml tags from xmltodict.\n\n    Parameters\n    ----------\n    xml_tag_name : str\n        Name of the xml tag to extract from the node.\n\n    node : Mapping[str, Any]\n        Node object returned by ``xmltodict`` from which ``xml_tag_name``\n        should be extracted.\n\n    allow_none : bool\n        If ``False``, the tag needs to exist in the node. Will raise a\n        ``ValueError`` if it does not.\n\n    Returns\n    -------\n    object\n    \"\"\"\n    if xml_tag_name in node and node[xml_tag_name] is not None:\n        if isinstance(node[xml_tag_name], (dict, str)):\n            return [node[xml_tag_name]]\n        if isinstance(node[xml_tag_name], list):\n            return node[xml_tag_name]\n\n        raise ValueError(\"Received not string and non list as tag item\")\n\n    if allow_none:\n        return None\n\n    raise ValueError(f\"Could not find tag '{xml_tag_name}' in node '{node!s}'\")\n</code></pre>"},{"location":"reference/datasets/","title":"datasets","text":""},{"location":"reference/datasets/#openml.datasets","title":"openml.datasets","text":""},{"location":"reference/datasets/#openml.datasets.OpenMLDataFeature","title":"OpenMLDataFeature","text":"<pre><code>OpenMLDataFeature(index: int, name: str, data_type: str, nominal_values: list[str], number_missing_values: int, ontologies: list[str] | None = None)\n</code></pre> <p>Data Feature (a.k.a. Attribute) object.</p> PARAMETER DESCRIPTION <code>index</code> <p>The index of this feature</p> <p> TYPE: <code>int</code> </p> <code>name</code> <p>Name of the feature</p> <p> TYPE: <code>str</code> </p> <code>data_type</code> <p>can be nominal, numeric, string, date (corresponds to arff)</p> <p> TYPE: <code>str</code> </p> <code>nominal_values</code> <p>list of the possible values, in case of nominal attribute</p> <p> TYPE: <code>list(str)</code> </p> <code>number_missing_values</code> <p>Number of rows that have a missing value for this feature.</p> <p> TYPE: <code>int</code> </p> <code>ontologies</code> <p>list of ontologies attached to this feature. An ontology describes the concept that are described in a feature. An ontology is defined by an URL where the information is provided.</p> <p> TYPE: <code>list(str)</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/datasets/data_feature.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    index: int,\n    name: str,\n    data_type: str,\n    nominal_values: list[str],\n    number_missing_values: int,\n    ontologies: list[str] | None = None,\n):\n    if not isinstance(index, int):\n        raise TypeError(f\"Index must be `int` but is {type(index)}\")\n\n    if data_type not in self.LEGAL_DATA_TYPES:\n        raise ValueError(\n            f\"data type should be in {self.LEGAL_DATA_TYPES!s}, found: {data_type}\",\n        )\n\n    if data_type == \"nominal\":\n        if nominal_values is None:\n            raise TypeError(\n                \"Dataset features require attribute `nominal_values` for nominal \"\n                \"feature type.\",\n            )\n\n        if not isinstance(nominal_values, list):\n            raise TypeError(\n                \"Argument `nominal_values` is of wrong datatype, should be list, \"\n                f\"but is {type(nominal_values)}\",\n            )\n    elif nominal_values is not None:\n        raise TypeError(\"Argument `nominal_values` must be None for non-nominal feature.\")\n\n    if not isinstance(number_missing_values, int):\n        msg = f\"number_missing_values must be int but is {type(number_missing_values)}\"\n        raise TypeError(msg)\n\n    self.index = index\n    self.name = str(name)\n    self.data_type = str(data_type)\n    self.nominal_values = nominal_values\n    self.number_missing_values = number_missing_values\n    self.ontologies = ontologies\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset","title":"OpenMLDataset","text":"<pre><code>OpenMLDataset(name: str, description: str | None, data_format: Literal['arff', 'sparse_arff'] = 'arff', cache_format: Literal['feather', 'pickle'] = 'pickle', dataset_id: int | None = None, version: int | None = None, creator: str | None = None, contributor: str | None = None, collection_date: str | None = None, upload_date: str | None = None, language: str | None = None, licence: str | None = None, url: str | None = None, default_target_attribute: str | None = None, row_id_attribute: str | None = None, ignore_attribute: str | list[str] | None = None, version_label: str | None = None, citation: str | None = None, tag: str | None = None, visibility: str | None = None, original_data_url: str | None = None, paper_url: str | None = None, update_comment: str | None = None, md5_checksum: str | None = None, data_file: str | None = None, features_file: str | None = None, qualities_file: str | None = None, dataset: str | None = None, parquet_url: str | None = None, parquet_file: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>Dataset object.</p> <p>Allows fetching and uploading datasets to OpenML.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Description of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>data_format</code> <p>Format of the dataset which can be either 'arff' or 'sparse_arff'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'arff'</code> </p> <code>cache_format</code> <p>Format for caching the dataset which can be either 'feather' or 'pickle'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pickle'</code> </p> <code>dataset_id</code> <p>Id autogenerated by the server.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>version</code> <p>Version of this dataset. '1' for original version. Auto-incremented by server.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>creator</code> <p>The person who created the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>contributor</code> <p>People who contributed to the current version of the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>collection_date</code> <p>The date the data was originally collected, given by the uploader.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>upload_date</code> <p>The date-time when the dataset was uploaded, generated by server.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>language</code> <p>Language in which the data is represented. Starts with 1 upper case letter, rest lower case, e.g. 'English'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>licence</code> <p>License of the data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>url</code> <p>Valid URL, points to actual data file. The file can be on the OpenML server or another dataset repository.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>default_target_attribute</code> <p>The default target attribute, if it exists. Can have multiple values, comma separated.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>row_id_attribute</code> <p>The attribute that represents the row-id column, if present in the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>ignore_attribute</code> <p>Attributes that should be excluded in modelling, such as identifiers and indexes.</p> <p> TYPE: <code>str | list</code> DEFAULT: <code>None</code> </p> <code>version_label</code> <p>Version label provided by user. Can be a date, hash, or some other type of id.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>citation</code> <p>Reference(s) that should be cited when building on this data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p>Tags, describing the algorithms.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>visibility</code> <p>Who can see the dataset. Typical values: 'Everyone','All my friends','Only me'. Can also be any of the user's circles.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>original_data_url</code> <p>For derived data, the url to the original dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>paper_url</code> <p>Link to a paper describing the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>update_comment</code> <p>An explanation for when the dataset is uploaded.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>md5_checksum</code> <p>MD5 checksum to check if the dataset is downloaded without corruption.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_file</code> <p>Path to where the dataset is located.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>features_file</code> <p>A dictionary of dataset features, which maps a feature index to a OpenMLDataFeature.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>qualities_file</code> <p>A dictionary of dataset qualities, which maps a quality name to a quality value.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>dataset</code> <p>Serialized arff dataset string.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>parquet_url</code> <p>This is the URL to the storage location where the dataset files are hosted. This can be a MinIO bucket URL. If specified, the data will be accessed from this URL when reading the files.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>parquet_file</code> <p>Path to the local file.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def __init__(  # noqa: C901, PLR0912, PLR0913, PLR0915\n    self,\n    name: str,\n    description: str | None,\n    data_format: Literal[\"arff\", \"sparse_arff\"] = \"arff\",\n    cache_format: Literal[\"feather\", \"pickle\"] = \"pickle\",\n    dataset_id: int | None = None,\n    version: int | None = None,\n    creator: str | None = None,\n    contributor: str | None = None,\n    collection_date: str | None = None,\n    upload_date: str | None = None,\n    language: str | None = None,\n    licence: str | None = None,\n    url: str | None = None,\n    default_target_attribute: str | None = None,\n    row_id_attribute: str | None = None,\n    ignore_attribute: str | list[str] | None = None,\n    version_label: str | None = None,\n    citation: str | None = None,\n    tag: str | None = None,\n    visibility: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n    update_comment: str | None = None,\n    md5_checksum: str | None = None,\n    data_file: str | None = None,\n    features_file: str | None = None,\n    qualities_file: str | None = None,\n    dataset: str | None = None,\n    parquet_url: str | None = None,\n    parquet_file: str | None = None,\n):\n    if cache_format not in [\"feather\", \"pickle\"]:\n        raise ValueError(\n            \"cache_format must be one of 'feather' or 'pickle. \"\n            f\"Invalid format specified: {cache_format}\",\n        )\n\n    def find_invalid_characters(string: str, pattern: str) -&gt; str:\n        invalid_chars = set()\n        regex = re.compile(pattern)\n        for char in string:\n            if not regex.match(char):\n                invalid_chars.add(char)\n        return \",\".join(\n            [f\"'{char}'\" if char != \"'\" else f'\"{char}\"' for char in invalid_chars],\n        )\n\n    if dataset_id is None:\n        pattern = \"^[\\x00-\\x7f]*$\"\n        if description and not re.match(pattern, description):\n            # not basiclatin (XSD complains)\n            invalid_characters = find_invalid_characters(description, pattern)\n            raise ValueError(\n                f\"Invalid symbols {invalid_characters} in description: {description}\",\n            )\n        pattern = \"^[\\x00-\\x7f]*$\"\n        if citation and not re.match(pattern, citation):\n            # not basiclatin (XSD complains)\n            invalid_characters = find_invalid_characters(citation, pattern)\n            raise ValueError(\n                f\"Invalid symbols {invalid_characters} in citation: {citation}\",\n            )\n        pattern = \"^[a-zA-Z0-9_\\\\-\\\\.\\\\(\\\\),]+$\"\n        if not re.match(pattern, name):\n            # regex given by server in error message\n            invalid_characters = find_invalid_characters(name, pattern)\n            raise ValueError(f\"Invalid symbols {invalid_characters} in name: {name}\")\n\n    self.ignore_attribute: list[str] | None = None\n    if isinstance(ignore_attribute, str):\n        self.ignore_attribute = [ignore_attribute]\n    elif isinstance(ignore_attribute, list) or ignore_attribute is None:\n        self.ignore_attribute = ignore_attribute\n    else:\n        raise ValueError(\"Wrong data type for ignore_attribute. Should be list.\")\n\n    # TODO add function to check if the name is casual_string128\n    # Attributes received by querying the RESTful API\n    self.dataset_id = int(dataset_id) if dataset_id is not None else None\n    self.name = name\n    self.version = int(version) if version is not None else None\n    self.description = description\n    self.cache_format = cache_format\n    # Has to be called format, otherwise there will be an XML upload error\n    self.format = data_format\n    self.creator = creator\n    self.contributor = contributor\n    self.collection_date = collection_date\n    self.upload_date = upload_date\n    self.language = language\n    self.licence = licence\n    self.url = url\n    self.default_target_attribute = default_target_attribute\n    self.row_id_attribute = row_id_attribute\n\n    self.version_label = version_label\n    self.citation = citation\n    self.tag = tag\n    self.visibility = visibility\n    self.original_data_url = original_data_url\n    self.paper_url = paper_url\n    self.update_comment = update_comment\n    self.md5_checksum = md5_checksum\n    self.data_file = data_file\n    self.parquet_file = parquet_file\n    self._dataset = dataset\n    self._parquet_url = parquet_url\n\n    self._features: dict[int, OpenMLDataFeature] | None = None\n    self._qualities: dict[str, float] | None = None\n    self._no_qualities_found = False\n\n    if features_file is not None:\n        self._features = _read_features(Path(features_file))\n\n    # \"\" was the old default value by `get_dataset` and maybe still used by some\n    if qualities_file == \"\":\n        # TODO(0.15): to switch to \"qualities_file is not None\" below and remove warning\n        warnings.warn(\n            \"Starting from Version 0.15 `qualities_file` must be None and not an empty string \"\n            \"to avoid reading the qualities from file. Set `qualities_file` to None to avoid \"\n            \"this warning.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        qualities_file = None\n\n    if qualities_file is not None:\n        self._qualities = _read_qualities(Path(qualities_file))\n\n    if data_file is not None:\n        data_pickle, data_feather, feather_attribute = self._compressed_cache_file_paths(\n            Path(data_file)\n        )\n        self.data_pickle_file = data_pickle if Path(data_pickle).exists() else None\n        self.data_feather_file = data_feather if Path(data_feather).exists() else None\n        self.feather_attribute_file = feather_attribute if Path(feather_attribute) else None\n    else:\n        self.data_pickle_file = None\n        self.data_feather_file = None\n        self.feather_attribute_file = None\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.features","title":"features  <code>property</code>","text":"<pre><code>features: dict[int, OpenMLDataFeature]\n</code></pre> <p>Get the features of this dataset.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Get the dataset numeric id.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.qualities","title":"qualities  <code>property</code>","text":"<pre><code>qualities: dict[str, float] | None\n</code></pre> <p>Get the qualities of this dataset.</p>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.get_data","title":"get_data","text":"<pre><code>get_data(target: list[str] | str | None = None, include_row_id: bool = False, include_ignore_attribute: bool = False) -&gt; tuple[DataFrame, Series | None, list[bool], list[str]]\n</code></pre> <p>Returns dataset content as dataframes.</p> PARAMETER DESCRIPTION <code>target</code> <p>Name of target column to separate from the data. Splitting multiple columns is currently not supported.</p> <p> TYPE: <code>(string, List[str] or None(default=None))</code> DEFAULT: <code>None</code> </p> <code>include_row_id</code> <p>Whether to include row ids in the returned dataset.</p> <p> TYPE: <code>boolean(default=False)</code> DEFAULT: <code>False</code> </p> <code>include_ignore_attribute</code> <p>Whether to include columns that are marked as \"ignore\" on the server in the dataset.</p> <p> TYPE: <code>boolean(default=False)</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>X</code> <p>Dataset, may have sparse dtypes in the columns if required.</p> <p> TYPE: <code>(dataframe, shape(n_samples, n_columns))</code> </p> <code>y</code> <p>Target column</p> <p> TYPE: <code>(Series, shape(n_samples) or None)</code> </p> <code>categorical_indicator</code> <p>Mask that indicate categorical features.</p> <p> TYPE: <code>list[bool]</code> </p> <code>attribute_names</code> <p>List of attribute names.</p> <p> TYPE: <code>list[str]</code> </p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def get_data(  # noqa: C901\n    self,\n    target: list[str] | str | None = None,\n    include_row_id: bool = False,  # noqa: FBT001, FBT002\n    include_ignore_attribute: bool = False,  # noqa: FBT001, FBT002\n) -&gt; tuple[pd.DataFrame, pd.Series | None, list[bool], list[str]]:\n    \"\"\"Returns dataset content as dataframes.\n\n    Parameters\n    ----------\n    target : string, List[str] or None (default=None)\n        Name of target column to separate from the data.\n        Splitting multiple columns is currently not supported.\n    include_row_id : boolean (default=False)\n        Whether to include row ids in the returned dataset.\n    include_ignore_attribute : boolean (default=False)\n        Whether to include columns that are marked as \"ignore\"\n        on the server in the dataset.\n\n\n    Returns\n    -------\n    X : dataframe, shape (n_samples, n_columns)\n        Dataset, may have sparse dtypes in the columns if required.\n    y : pd.Series, shape (n_samples, ) or None\n        Target column\n    categorical_indicator : list[bool]\n        Mask that indicate categorical features.\n    attribute_names : list[str]\n        List of attribute names.\n    \"\"\"\n    data, categorical_mask, attribute_names = self._load_data()\n\n    to_exclude = []\n    if not include_row_id and self.row_id_attribute is not None:\n        if isinstance(self.row_id_attribute, str):\n            to_exclude.append(self.row_id_attribute)\n        elif isinstance(self.row_id_attribute, Iterable):\n            to_exclude.extend(self.row_id_attribute)\n\n    if not include_ignore_attribute and self.ignore_attribute is not None:\n        if isinstance(self.ignore_attribute, str):\n            to_exclude.append(self.ignore_attribute)\n        elif isinstance(self.ignore_attribute, Iterable):\n            to_exclude.extend(self.ignore_attribute)\n\n    if len(to_exclude) &gt; 0:\n        logger.info(f\"Going to remove the following attributes: {to_exclude}\")\n        keep = np.array([column not in to_exclude for column in attribute_names])\n        data = data.drop(columns=to_exclude)\n        categorical_mask = [cat for cat, k in zip(categorical_mask, keep) if k]\n        attribute_names = [att for att, k in zip(attribute_names, keep) if k]\n\n    if target is None:\n        return data, None, categorical_mask, attribute_names\n\n    if isinstance(target, str):\n        target_names = target.split(\",\") if \",\" in target else [target]\n    else:\n        target_names = target\n\n    # All the assumptions below for the target are dependant on the number of targets being 1\n    n_targets = len(target_names)\n    if n_targets &gt; 1:\n        raise NotImplementedError(f\"Number of targets {n_targets} not implemented.\")\n\n    target_name = target_names[0]\n    x = data.drop(columns=[target_name])\n    y = data[target_name].squeeze()\n\n    # Finally, remove the target from the list of attributes and categorical mask\n    target_index = attribute_names.index(target_name)\n    categorical_mask.pop(target_index)\n    attribute_names.remove(target_name)\n\n    assert isinstance(y, pd.Series)\n    return x, y, categorical_mask, attribute_names\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.get_features_by_type","title":"get_features_by_type","text":"<pre><code>get_features_by_type(data_type: str, exclude: list[str] | None = None, exclude_ignore_attribute: bool = True, exclude_row_id_attribute: bool = True) -&gt; list[int]\n</code></pre> <p>Return indices of features of a given type, e.g. all nominal features. Optional parameters to exclude various features by index or ontology.</p> PARAMETER DESCRIPTION <code>data_type</code> <p>The data type to return (e.g., nominal, numeric, date, string)</p> <p> TYPE: <code>str</code> </p> <code>exclude</code> <p>List of columns to exclude from the return value</p> <p> TYPE: <code>list(int)</code> DEFAULT: <code>None</code> </p> <code>exclude_ignore_attribute</code> <p>Whether to exclude the defined ignore attributes (and adapt the return values as if these indices are not present)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>exclude_row_id_attribute</code> <p>Whether to exclude the defined row id attributes (and adapt the return values as if these indices are not present)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>result</code> <p>a list of indices that have the specified data type</p> <p> TYPE: <code>list</code> </p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def get_features_by_type(  # noqa: C901\n    self,\n    data_type: str,\n    exclude: list[str] | None = None,\n    exclude_ignore_attribute: bool = True,  # noqa: FBT002, FBT001\n    exclude_row_id_attribute: bool = True,  # noqa: FBT002, FBT001\n) -&gt; list[int]:\n    \"\"\"\n    Return indices of features of a given type, e.g. all nominal features.\n    Optional parameters to exclude various features by index or ontology.\n\n    Parameters\n    ----------\n    data_type : str\n        The data type to return (e.g., nominal, numeric, date, string)\n    exclude : list(int)\n        List of columns to exclude from the return value\n    exclude_ignore_attribute : bool\n        Whether to exclude the defined ignore attributes (and adapt the\n        return values as if these indices are not present)\n    exclude_row_id_attribute : bool\n        Whether to exclude the defined row id attributes (and adapt the\n        return values as if these indices are not present)\n\n    Returns\n    -------\n    result : list\n        a list of indices that have the specified data type\n    \"\"\"\n    if data_type not in OpenMLDataFeature.LEGAL_DATA_TYPES:\n        raise TypeError(\"Illegal feature type requested\")\n    if self.ignore_attribute is not None and not isinstance(self.ignore_attribute, list):\n        raise TypeError(\"ignore_attribute should be a list\")\n    if self.row_id_attribute is not None and not isinstance(self.row_id_attribute, str):\n        raise TypeError(\"row id attribute should be a str\")\n    if exclude is not None and not isinstance(exclude, list):\n        raise TypeError(\"Exclude should be a list\")\n        # assert all(isinstance(elem, str) for elem in exclude),\n        #            \"Exclude should be a list of strings\"\n    to_exclude = []\n    if exclude is not None:\n        to_exclude.extend(exclude)\n    if exclude_ignore_attribute and self.ignore_attribute is not None:\n        to_exclude.extend(self.ignore_attribute)\n    if exclude_row_id_attribute and self.row_id_attribute is not None:\n        to_exclude.append(self.row_id_attribute)\n\n    result = []\n    offset = 0\n    # this function assumes that everything in to_exclude will\n    # be 'excluded' from the dataset (hence the offset)\n    for idx in self.features:\n        name = self.features[idx].name\n        if name in to_exclude:\n            offset += 1\n        elif self.features[idx].data_type == data_type:\n            result.append(idx - offset)\n    return result\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.retrieve_class_labels","title":"retrieve_class_labels","text":"<pre><code>retrieve_class_labels(target_name: str = 'class') -&gt; None | list[str]\n</code></pre> <p>Reads the datasets arff to determine the class-labels.</p> <p>If the task has no class labels (for example a regression problem) it returns None. Necessary because the data returned by get_data only contains the indices of the classes, while OpenML needs the real classname when uploading the results of a run.</p> PARAMETER DESCRIPTION <code>target_name</code> <p>Name of the target attribute</p> <p> TYPE: <code>str</code> DEFAULT: <code>'class'</code> </p> RETURNS DESCRIPTION <code>list</code> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def retrieve_class_labels(self, target_name: str = \"class\") -&gt; None | list[str]:\n    \"\"\"Reads the datasets arff to determine the class-labels.\n\n    If the task has no class labels (for example a regression problem)\n    it returns None. Necessary because the data returned by get_data\n    only contains the indices of the classes, while OpenML needs the real\n    classname when uploading the results of a run.\n\n    Parameters\n    ----------\n    target_name : str\n        Name of the target attribute\n\n    Returns\n    -------\n    list\n    \"\"\"\n    for feature in self.features.values():\n        if feature.name == target_name:\n            if feature.data_type == \"nominal\":\n                return feature.nominal_values\n\n            if feature.data_type == \"string\":\n                # Rel.: #1311\n                # The target is invalid for a classification task if the feature type is string\n                # and not nominal. For such miss-configured tasks, we silently fix it here as\n                # we can safely interpreter string as nominal.\n                df, *_ = self.get_data()\n                return list(df[feature.name].unique())\n\n    return None\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.OpenMLDataset.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.attributes_arff_from_df","title":"attributes_arff_from_df","text":"<pre><code>attributes_arff_from_df(df: DataFrame) -&gt; list[tuple[str, list[str] | str]]\n</code></pre> <p>Describe attributes of the dataframe according to ARFF specification.</p> PARAMETER DESCRIPTION <code>df</code> <p>The dataframe containing the data set.</p> <p> TYPE: <code>(DataFrame, shape(n_samples, n_features))</code> </p> RETURNS DESCRIPTION <code>attributes_arff</code> <p>The data set attributes as required by the ARFF format.</p> <p> TYPE: <code>list[str]</code> </p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def attributes_arff_from_df(df: pd.DataFrame) -&gt; list[tuple[str, list[str] | str]]:\n    \"\"\"Describe attributes of the dataframe according to ARFF specification.\n\n    Parameters\n    ----------\n    df : DataFrame, shape (n_samples, n_features)\n        The dataframe containing the data set.\n\n    Returns\n    -------\n    attributes_arff : list[str]\n        The data set attributes as required by the ARFF format.\n    \"\"\"\n    PD_DTYPES_TO_ARFF_DTYPE = {\"integer\": \"INTEGER\", \"floating\": \"REAL\", \"string\": \"STRING\"}\n    attributes_arff: list[tuple[str, list[str] | str]] = []\n\n    if not all(isinstance(column_name, str) for column_name in df.columns):\n        logger.warning(\"Converting non-str column names to str.\")\n        df.columns = [str(column_name) for column_name in df.columns]\n\n    for column_name in df:\n        # skipna=True does not infer properly the dtype. The NA values are\n        # dropped before the inference instead.\n        column_dtype = pd.api.types.infer_dtype(df[column_name].dropna(), skipna=False)\n\n        if column_dtype == \"categorical\":\n            # for categorical feature, arff expects a list string. However, a\n            # categorical column can contain mixed type and should therefore\n            # raise an error asking to convert all entries to string.\n            categories = df[column_name].cat.categories\n            categories_dtype = pd.api.types.infer_dtype(categories)\n            if categories_dtype not in (\"string\", \"unicode\"):\n                raise ValueError(\n                    f\"The column '{column_name}' of the dataframe is of \"\n                    \"'category' dtype. Therefore, all values in \"\n                    \"this columns should be string. Please \"\n                    \"convert the entries which are not string. \"\n                    f\"Got {categories_dtype} dtype in this column.\",\n                )\n            attributes_arff.append((column_name, categories.tolist()))\n        elif column_dtype == \"boolean\":\n            # boolean are encoded as categorical.\n            attributes_arff.append((column_name, [\"True\", \"False\"]))\n        elif column_dtype in PD_DTYPES_TO_ARFF_DTYPE:\n            attributes_arff.append((column_name, PD_DTYPES_TO_ARFF_DTYPE[column_dtype]))\n        else:\n            raise ValueError(\n                f\"The dtype '{column_dtype}' of the column '{column_name}' is not \"\n                \"currently supported by liac-arff. Supported \"\n                \"dtypes are categorical, string, integer, \"\n                \"floating, and boolean.\",\n            )\n    return attributes_arff\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.check_datasets_active","title":"check_datasets_active","text":"<pre><code>check_datasets_active(dataset_ids: list[int], raise_error_if_not_exist: bool = True) -&gt; dict[int, bool]\n</code></pre> <p>Check if the dataset ids provided are active.</p> <p>Raises an error if a dataset_id in the given list of dataset_ids does not exist on the server and <code>raise_error_if_not_exist</code> is set to True (default).</p> PARAMETER DESCRIPTION <code>dataset_ids</code> <p>A list of integers representing dataset ids.</p> <p> TYPE: <code>List[int]</code> </p> <code>raise_error_if_not_exist</code> <p>Flag that if activated can raise an error, if one or more of the given dataset ids do not exist on the server.</p> <p> TYPE: <code>bool(default=True)</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary with items {did: bool}</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def check_datasets_active(\n    dataset_ids: list[int],\n    raise_error_if_not_exist: bool = True,  # noqa: FBT001, FBT002\n) -&gt; dict[int, bool]:\n    \"\"\"\n    Check if the dataset ids provided are active.\n\n    Raises an error if a dataset_id in the given list\n    of dataset_ids does not exist on the server and\n    `raise_error_if_not_exist` is set to True (default).\n\n    Parameters\n    ----------\n    dataset_ids : List[int]\n        A list of integers representing dataset ids.\n    raise_error_if_not_exist : bool (default=True)\n        Flag that if activated can raise an error, if one or more of the\n        given dataset ids do not exist on the server.\n\n    Returns\n    -------\n    dict\n        A dictionary with items {did: bool}\n    \"\"\"\n    datasets = list_datasets(status=\"all\", data_id=dataset_ids)\n    missing = set(dataset_ids) - set(datasets.index)\n    if raise_error_if_not_exist and missing:\n        missing_str = \", \".join(str(did) for did in missing)\n        raise ValueError(f\"Could not find dataset(s) {missing_str} in OpenML dataset list.\")\n    mask = datasets[\"status\"] == \"active\"\n    return dict(mask)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(name: str, description: str | None, creator: str | None, contributor: str | None, collection_date: str | None, language: str | None, licence: str | None, attributes: list[tuple[str, str | list[str]]] | dict[str, str | list[str]] | Literal['auto'], data: DataFrame | ndarray | coo_matrix, default_target_attribute: str, ignore_attribute: str | list[str] | None, citation: str, row_id_attribute: str | None = None, original_data_url: str | None = None, paper_url: str | None = None, update_comment: str | None = None, version_label: str | None = None) -&gt; OpenMLDataset\n</code></pre> <p>Create a dataset.</p> <p>This function creates an OpenMLDataset object. The OpenMLDataset object contains information related to the dataset and the actual data file.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Description of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>creator</code> <p>The person who created the dataset.</p> <p> TYPE: <code>str</code> </p> <code>contributor</code> <p>People who contributed to the current version of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>collection_date</code> <p>The date the data was originally collected, given by the uploader.</p> <p> TYPE: <code>str</code> </p> <code>language</code> <p>Language in which the data is represented. Starts with 1 upper case letter, rest lower case, e.g. 'English'.</p> <p> TYPE: <code>str</code> </p> <code>licence</code> <p>License of the data.</p> <p> TYPE: <code>str</code> </p> <code>attributes</code> <p>A list of tuples. Each tuple consists of the attribute name and type. If passing a pandas DataFrame, the attributes can be automatically inferred by passing <code>'auto'</code>. Specific attributes can be manually specified by a passing a dictionary where the key is the name of the attribute and the value is the data type of the attribute.</p> <p> TYPE: <code>list, dict, or 'auto'</code> </p> <code>data</code> <p>An array that contains both the attributes and the targets. When providing a dataframe, the attribute names and type can be inferred by passing <code>attributes='auto'</code>. The target feature is indicated as meta-data of the dataset.</p> <p> TYPE: <code>(ndarray, list, dataframe, coo_matrix, shape(n_samples, n_features))</code> </p> <code>default_target_attribute</code> <p>The default target attribute, if it exists. Can have multiple values, comma separated.</p> <p> TYPE: <code>str</code> </p> <code>ignore_attribute</code> <p>Attributes that should be excluded in modelling, such as identifiers and indexes. Can have multiple values, comma separated.</p> <p> TYPE: <code>str | list</code> </p> <code>citation</code> <p>Reference(s) that should be cited when building on this data.</p> <p> TYPE: <code>str</code> </p> <code>version_label</code> <p>Version label provided by user.  Can be a date, hash, or some other type of id.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>row_id_attribute</code> <p>The attribute that represents the row-id column, if present in the dataset. If <code>data</code> is a dataframe and <code>row_id_attribute</code> is not specified, the index of the dataframe will be used as the <code>row_id_attribute</code>. If the name of the index is <code>None</code>, it will be discarded.</p> <p>.. versionadded: 0.8     Inference of <code>row_id_attribute</code> from a dataframe.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>original_data_url</code> <p>For derived data, the url to the original dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>paper_url</code> <p>Link to a paper describing the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>update_comment</code> <p>An explanation for when the dataset is uploaded.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>class</code> <p> TYPE: <code>`openml.OpenMLDataset`</code> </p> <code>Dataset description.</code> Source code in <code>openml/datasets/functions.py</code> <pre><code>def create_dataset(  # noqa: C901, PLR0912, PLR0915\n    name: str,\n    description: str | None,\n    creator: str | None,\n    contributor: str | None,\n    collection_date: str | None,\n    language: str | None,\n    licence: str | None,\n    # TODO(eddiebergman): Docstring says `type` but I don't know what this is other than strings\n    # Edit: Found it could also be like [\"True\", \"False\"]\n    attributes: list[tuple[str, str | list[str]]] | dict[str, str | list[str]] | Literal[\"auto\"],\n    data: pd.DataFrame | np.ndarray | scipy.sparse.coo_matrix,\n    # TODO(eddiebergman): Function requires `default_target_attribute` exist but API allows None\n    default_target_attribute: str,\n    ignore_attribute: str | list[str] | None,\n    citation: str,\n    row_id_attribute: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n    update_comment: str | None = None,\n    version_label: str | None = None,\n) -&gt; OpenMLDataset:\n    \"\"\"Create a dataset.\n\n    This function creates an OpenMLDataset object.\n    The OpenMLDataset object contains information related to the dataset\n    and the actual data file.\n\n    Parameters\n    ----------\n    name : str\n        Name of the dataset.\n    description : str\n        Description of the dataset.\n    creator : str\n        The person who created the dataset.\n    contributor : str\n        People who contributed to the current version of the dataset.\n    collection_date : str\n        The date the data was originally collected, given by the uploader.\n    language : str\n        Language in which the data is represented.\n        Starts with 1 upper case letter, rest lower case, e.g. 'English'.\n    licence : str\n        License of the data.\n    attributes : list, dict, or 'auto'\n        A list of tuples. Each tuple consists of the attribute name and type.\n        If passing a pandas DataFrame, the attributes can be automatically\n        inferred by passing ``'auto'``. Specific attributes can be manually\n        specified by a passing a dictionary where the key is the name of the\n        attribute and the value is the data type of the attribute.\n    data : ndarray, list, dataframe, coo_matrix, shape (n_samples, n_features)\n        An array that contains both the attributes and the targets. When\n        providing a dataframe, the attribute names and type can be inferred by\n        passing ``attributes='auto'``.\n        The target feature is indicated as meta-data of the dataset.\n    default_target_attribute : str\n        The default target attribute, if it exists.\n        Can have multiple values, comma separated.\n    ignore_attribute : str | list\n        Attributes that should be excluded in modelling,\n        such as identifiers and indexes.\n        Can have multiple values, comma separated.\n    citation : str\n        Reference(s) that should be cited when building on this data.\n    version_label : str, optional\n        Version label provided by user.\n         Can be a date, hash, or some other type of id.\n    row_id_attribute : str, optional\n        The attribute that represents the row-id column, if present in the\n        dataset. If ``data`` is a dataframe and ``row_id_attribute`` is not\n        specified, the index of the dataframe will be used as the\n        ``row_id_attribute``. If the name of the index is ``None``, it will\n        be discarded.\n\n        .. versionadded: 0.8\n            Inference of ``row_id_attribute`` from a dataframe.\n    original_data_url : str, optional\n        For derived data, the url to the original dataset.\n    paper_url : str, optional\n        Link to a paper describing the dataset.\n    update_comment : str, optional\n        An explanation for when the dataset is uploaded.\n\n    Returns\n    -------\n    class:`openml.OpenMLDataset`\n    Dataset description.\n    \"\"\"\n    if isinstance(data, pd.DataFrame):\n        # infer the row id from the index of the dataset\n        if row_id_attribute is None:\n            row_id_attribute = data.index.name\n        # When calling data.values, the index will be skipped.\n        # We need to reset the index such that it is part of the data.\n        if data.index.name is not None:\n            data = data.reset_index()\n\n    if attributes == \"auto\" or isinstance(attributes, dict):\n        if not isinstance(data, pd.DataFrame):\n            raise ValueError(\n                \"Automatically inferring attributes requires \"\n                f\"a pandas DataFrame. A {data!r} was given instead.\",\n            )\n        # infer the type of data for each column of the DataFrame\n        attributes_ = attributes_arff_from_df(data)\n        if isinstance(attributes, dict):\n            # override the attributes which was specified by the user\n            for attr_idx in range(len(attributes_)):\n                attr_name = attributes_[attr_idx][0]\n                if attr_name in attributes:\n                    attributes_[attr_idx] = (attr_name, attributes[attr_name])\n    else:\n        attributes_ = attributes\n    ignore_attributes = _expand_parameter(ignore_attribute)\n    _validated_data_attributes(ignore_attributes, attributes_, \"ignore_attribute\")\n\n    default_target_attributes = _expand_parameter(default_target_attribute)\n    _validated_data_attributes(default_target_attributes, attributes_, \"default_target_attribute\")\n\n    if row_id_attribute is not None:\n        is_row_id_an_attribute = any(attr[0] == row_id_attribute for attr in attributes_)\n        if not is_row_id_an_attribute:\n            raise ValueError(\n                \"'row_id_attribute' should be one of the data attribute. \"\n                f\" Got '{row_id_attribute}' while candidates are\"\n                f\" {[attr[0] for attr in attributes_]}.\",\n            )\n\n    if isinstance(data, pd.DataFrame):\n        if all(isinstance(dtype, pd.SparseDtype) for dtype in data.dtypes):\n            data = data.sparse.to_coo()\n            # liac-arff only support COO matrices with sorted rows\n            row_idx_sorted = np.argsort(data.row)  # type: ignore\n            data.row = data.row[row_idx_sorted]  # type: ignore\n            data.col = data.col[row_idx_sorted]  # type: ignore\n            data.data = data.data[row_idx_sorted]  # type: ignore\n        else:\n            data = data.to_numpy()\n\n    data_format: Literal[\"arff\", \"sparse_arff\"]\n    if isinstance(data, (list, np.ndarray)):\n        if isinstance(data[0], (list, np.ndarray)):\n            data_format = \"arff\"\n        elif isinstance(data[0], dict):\n            data_format = \"sparse_arff\"\n        else:\n            raise ValueError(\n                \"When giving a list or a numpy.ndarray, \"\n                \"they should contain a list/ numpy.ndarray \"\n                \"for dense data or a dictionary for sparse \"\n                f\"data. Got {data[0]!r} instead.\",\n            )\n    elif isinstance(data, coo_matrix):\n        data_format = \"sparse_arff\"\n    else:\n        raise ValueError(\n            \"When giving a list or a numpy.ndarray, \"\n            \"they should contain a list/ numpy.ndarray \"\n            \"for dense data or a dictionary for sparse \"\n            f\"data. Got {data[0]!r} instead.\",\n        )\n\n    arff_object = {\n        \"relation\": name,\n        \"description\": description,\n        \"attributes\": attributes_,\n        \"data\": data,\n    }\n\n    # serializes the ARFF dataset object and returns a string\n    arff_dataset = arff.dumps(arff_object)\n    try:\n        # check if ARFF is valid\n        decoder = arff.ArffDecoder()\n        return_type = arff.COO if data_format == \"sparse_arff\" else arff.DENSE\n        decoder.decode(arff_dataset, encode_nominal=True, return_type=return_type)\n    except arff.ArffException as e:\n        raise ValueError(\n            \"The arguments you have provided do not construct a valid ARFF file\"\n        ) from e\n\n    return OpenMLDataset(\n        name=name,\n        description=description,\n        data_format=data_format,\n        creator=creator,\n        contributor=contributor,\n        collection_date=collection_date,\n        language=language,\n        licence=licence,\n        default_target_attribute=default_target_attribute,\n        row_id_attribute=row_id_attribute,\n        ignore_attribute=ignore_attribute,\n        citation=citation,\n        version_label=version_label,\n        original_data_url=original_data_url,\n        paper_url=paper_url,\n        update_comment=update_comment,\n        dataset=arff_dataset,\n    )\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.delete_dataset","title":"delete_dataset","text":"<pre><code>delete_dataset(dataset_id: int) -&gt; bool\n</code></pre> <p>Delete dataset with id <code>dataset_id</code> from the OpenML server.</p> <p>This can only be done if you are the owner of the dataset and no tasks are attached to the dataset.</p> PARAMETER DESCRIPTION <code>dataset_id</code> <p>OpenML id of the dataset</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the deletion was successful. False otherwise.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def delete_dataset(dataset_id: int) -&gt; bool:\n    \"\"\"Delete dataset with id `dataset_id` from the OpenML server.\n\n    This can only be done if you are the owner of the dataset and\n    no tasks are attached to the dataset.\n\n    Parameters\n    ----------\n    dataset_id : int\n        OpenML id of the dataset\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"data\", dataset_id)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.edit_dataset","title":"edit_dataset","text":"<pre><code>edit_dataset(data_id: int, description: str | None = None, creator: str | None = None, contributor: str | None = None, collection_date: str | None = None, language: str | None = None, default_target_attribute: str | None = None, ignore_attribute: str | list[str] | None = None, citation: str | None = None, row_id_attribute: str | None = None, original_data_url: str | None = None, paper_url: str | None = None) -&gt; int\n</code></pre> <p>Edits an OpenMLDataset.</p> <p>In addition to providing the dataset id of the dataset to edit (through data_id), you must specify a value for at least one of the optional function arguments, i.e. one value for a field to edit.</p> <p>This function allows editing of both non-critical and critical fields. Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.</p> <ul> <li>Editing non-critical data fields is allowed for all authenticated users.</li> <li>Editing critical fields is allowed only for the owner, provided there are no tasks    associated with this dataset.</li> </ul> <p>If dataset has tasks or if the user is not the owner, the only way to edit critical fields is to use fork_dataset followed by edit_dataset.</p> PARAMETER DESCRIPTION <code>data_id</code> <p>ID of the dataset.</p> <p> TYPE: <code>int</code> </p> <code>description</code> <p>Description of the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>creator</code> <p>The person who created the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>contributor</code> <p>People who contributed to the current version of the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>collection_date</code> <p>The date the data was originally collected, given by the uploader.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>language</code> <p>Language in which the data is represented. Starts with 1 upper case letter, rest lower case, e.g. 'English'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>default_target_attribute</code> <p>The default target attribute, if it exists. Can have multiple values, comma separated.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>ignore_attribute</code> <p>Attributes that should be excluded in modelling, such as identifiers and indexes.</p> <p> TYPE: <code>str | list</code> DEFAULT: <code>None</code> </p> <code>citation</code> <p>Reference(s) that should be cited when building on this data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>row_id_attribute</code> <p>The attribute that represents the row-id column, if present in the dataset. If <code>data</code> is a dataframe and <code>row_id_attribute</code> is not specified, the index of the dataframe will be used as the <code>row_id_attribute</code>. If the name of the index is <code>None</code>, it will be discarded.</p> <p>.. versionadded: 0.8     Inference of <code>row_id_attribute</code> from a dataframe.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>original_data_url</code> <p>For derived data, the url to the original dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>paper_url</code> <p>Link to a paper describing the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dataset id</code> Source code in <code>openml/datasets/functions.py</code> <pre><code>def edit_dataset(\n    data_id: int,\n    description: str | None = None,\n    creator: str | None = None,\n    contributor: str | None = None,\n    collection_date: str | None = None,\n    language: str | None = None,\n    default_target_attribute: str | None = None,\n    ignore_attribute: str | list[str] | None = None,\n    citation: str | None = None,\n    row_id_attribute: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n) -&gt; int:\n    \"\"\"Edits an OpenMLDataset.\n\n    In addition to providing the dataset id of the dataset to edit (through data_id),\n    you must specify a value for at least one of the optional function arguments,\n    i.e. one value for a field to edit.\n\n    This function allows editing of both non-critical and critical fields.\n    Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.\n\n     - Editing non-critical data fields is allowed for all authenticated users.\n     - Editing critical fields is allowed only for the owner, provided there are no tasks\n       associated with this dataset.\n\n    If dataset has tasks or if the user is not the owner, the only way\n    to edit critical fields is to use fork_dataset followed by edit_dataset.\n\n    Parameters\n    ----------\n    data_id : int\n        ID of the dataset.\n    description : str\n        Description of the dataset.\n    creator : str\n        The person who created the dataset.\n    contributor : str\n        People who contributed to the current version of the dataset.\n    collection_date : str\n        The date the data was originally collected, given by the uploader.\n    language : str\n        Language in which the data is represented.\n        Starts with 1 upper case letter, rest lower case, e.g. 'English'.\n    default_target_attribute : str\n        The default target attribute, if it exists.\n        Can have multiple values, comma separated.\n    ignore_attribute : str | list\n        Attributes that should be excluded in modelling,\n        such as identifiers and indexes.\n    citation : str\n        Reference(s) that should be cited when building on this data.\n    row_id_attribute : str, optional\n        The attribute that represents the row-id column, if present in the\n        dataset. If ``data`` is a dataframe and ``row_id_attribute`` is not\n        specified, the index of the dataframe will be used as the\n        ``row_id_attribute``. If the name of the index is ``None``, it will\n        be discarded.\n\n        .. versionadded: 0.8\n            Inference of ``row_id_attribute`` from a dataframe.\n    original_data_url : str, optional\n        For derived data, the url to the original dataset.\n    paper_url : str, optional\n        Link to a paper describing the dataset.\n\n    Returns\n    -------\n    Dataset id\n    \"\"\"\n    if not isinstance(data_id, int):\n        raise TypeError(f\"`data_id` must be of type `int`, not {type(data_id)}.\")\n\n    # compose data edit parameters as xml\n    form_data = {\"data_id\": data_id}  # type: openml._api_calls.DATA_TYPE\n    xml = OrderedDict()  # type: 'OrderedDict[str, OrderedDict]'\n    xml[\"oml:data_edit_parameters\"] = OrderedDict()\n    xml[\"oml:data_edit_parameters\"][\"@xmlns:oml\"] = \"http://openml.org/openml\"\n    xml[\"oml:data_edit_parameters\"][\"oml:description\"] = description\n    xml[\"oml:data_edit_parameters\"][\"oml:creator\"] = creator\n    xml[\"oml:data_edit_parameters\"][\"oml:contributor\"] = contributor\n    xml[\"oml:data_edit_parameters\"][\"oml:collection_date\"] = collection_date\n    xml[\"oml:data_edit_parameters\"][\"oml:language\"] = language\n    xml[\"oml:data_edit_parameters\"][\"oml:default_target_attribute\"] = default_target_attribute\n    xml[\"oml:data_edit_parameters\"][\"oml:row_id_attribute\"] = row_id_attribute\n    xml[\"oml:data_edit_parameters\"][\"oml:ignore_attribute\"] = ignore_attribute\n    xml[\"oml:data_edit_parameters\"][\"oml:citation\"] = citation\n    xml[\"oml:data_edit_parameters\"][\"oml:original_data_url\"] = original_data_url\n    xml[\"oml:data_edit_parameters\"][\"oml:paper_url\"] = paper_url\n\n    # delete None inputs\n    for k in list(xml[\"oml:data_edit_parameters\"]):\n        if not xml[\"oml:data_edit_parameters\"][k]:\n            del xml[\"oml:data_edit_parameters\"][k]\n\n    file_elements = {\n        \"edit_parameters\": (\"description.xml\", xmltodict.unparse(xml)),\n    }  # type: openml._api_calls.FILE_ELEMENTS_TYPE\n    result_xml = openml._api_calls._perform_api_call(\n        \"data/edit\",\n        \"post\",\n        data=form_data,\n        file_elements=file_elements,\n    )\n    result = xmltodict.parse(result_xml)\n    data_id = result[\"oml:data_edit\"][\"oml:id\"]\n    return int(data_id)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.fork_dataset","title":"fork_dataset","text":"<pre><code>fork_dataset(data_id: int) -&gt; int\n</code></pre> <p>Creates a new dataset version, with the authenticated user as the new owner.  The forked dataset can have distinct dataset meta-data,  but the actual data itself is shared with the original version.</p> <p>This API is intended for use when a user is unable to edit the critical fields of a dataset  through the edit_dataset API.  (Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.)</p> <p>Specifically, this happens when the user is:         1. Not the owner of the dataset.         2. User is the owner of the dataset, but the dataset has tasks.</p> <p>In these two cases the only way to edit critical fields is:         1. STEP 1: Fork the dataset using fork_dataset API         2. STEP 2: Call edit_dataset API on the forked version.</p> PARAMETER DESCRIPTION <code>data_id</code> <p>id of the dataset to be forked</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Dataset id of the forked dataset</code> Source code in <code>openml/datasets/functions.py</code> <pre><code>def fork_dataset(data_id: int) -&gt; int:\n    \"\"\"\n     Creates a new dataset version, with the authenticated user as the new owner.\n     The forked dataset can have distinct dataset meta-data,\n     but the actual data itself is shared with the original version.\n\n     This API is intended for use when a user is unable to edit the critical fields of a dataset\n     through the edit_dataset API.\n     (Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.)\n\n     Specifically, this happens when the user is:\n            1. Not the owner of the dataset.\n            2. User is the owner of the dataset, but the dataset has tasks.\n\n     In these two cases the only way to edit critical fields is:\n            1. STEP 1: Fork the dataset using fork_dataset API\n            2. STEP 2: Call edit_dataset API on the forked version.\n\n\n    Parameters\n    ----------\n    data_id : int\n        id of the dataset to be forked\n\n    Returns\n    -------\n    Dataset id of the forked dataset\n\n    \"\"\"\n    if not isinstance(data_id, int):\n        raise TypeError(f\"`data_id` must be of type `int`, not {type(data_id)}.\")\n    # compose data fork parameters\n    form_data = {\"data_id\": data_id}  # type: openml._api_calls.DATA_TYPE\n    result_xml = openml._api_calls._perform_api_call(\"data/fork\", \"post\", data=form_data)\n    result = xmltodict.parse(result_xml)\n    data_id = result[\"oml:data_fork\"][\"oml:id\"]\n    return int(data_id)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(dataset_id: int | str, download_data: bool = False, version: int | None = None, error_if_multiple: bool = False, cache_format: Literal['pickle', 'feather'] = 'pickle', download_qualities: bool = False, download_features_meta_data: bool = False, download_all_files: bool = False, force_refresh_cache: bool = False) -&gt; OpenMLDataset\n</code></pre> <p>Download the OpenML dataset representation, optionally also download actual data file.</p> <p>This function is by default NOT thread/multiprocessing safe, as this function uses caching. A check will be performed to determine if the information has previously been downloaded to a cache, and if so be loaded from disk instead of retrieved from the server.</p> <p>To make this function thread safe, you can install the python package <code>oslo.concurrency</code>. If <code>oslo.concurrency</code> is installed <code>get_dataset</code> becomes thread safe.</p> <p>Alternatively, to make this function thread/multiprocessing safe initialize the cache first by calling <code>get_dataset(args)</code> once before calling <code>get_dataset(args)</code> many times in parallel. This will initialize the cache and later calls will use the cache in a thread/multiprocessing safe way.</p> <p>If dataset is retrieved by name, a version may be specified. If no version is specified and multiple versions of the dataset exist, the earliest version of the dataset that is still active will be returned. If no version is specified, multiple versions of the dataset exist and <code>exception_if_multiple</code> is set to <code>True</code>, this function will raise an exception.</p> PARAMETER DESCRIPTION <code>dataset_id</code> <p>Dataset ID (integer) or dataset name (string) of the dataset to download.</p> <p> TYPE: <code>int or str</code> </p> <code>download_data</code> <p>If True, also download the data file. Beware that some datasets are large and it might make the operation noticeably slower. Metadata is also still retrieved. If False, create the OpenMLDataset and only populate it with the metadata. The data may later be retrieved through the <code>OpenMLDataset.get_data</code> method.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>version</code> <p>Specifies the version if <code>dataset_id</code> is specified by name. If no version is specified, retrieve the least recent still active version.</p> <p> TYPE: <code>(int, optional(default=None))</code> DEFAULT: <code>None</code> </p> <code>error_if_multiple</code> <p>If <code>True</code> raise an error if multiple datasets are found with matching criteria.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>cache_format</code> <p>Format for caching the dataset - may be feather or pickle Note that the default 'pickle' option may load slower than feather when no.of.rows is very high.</p> <p> TYPE: <code>str(default='pickle') in {'pickle', 'feather'}</code> DEFAULT: <code>'pickle'</code> </p> <code>download_qualities</code> <p>Option to download 'qualities' meta-data in addition to the minimal dataset description. If True, download and cache the qualities file. If False, create the OpenMLDataset without qualities metadata. The data may later be added to the OpenMLDataset through the <code>OpenMLDataset.load_metadata(qualities=True)</code> method.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>download_features_meta_data</code> <p>Option to download 'features' meta-data in addition to the minimal dataset description. If True, download and cache the features file. If False, create the OpenMLDataset without features metadata. The data may later be added to the OpenMLDataset through the <code>OpenMLDataset.load_metadata(features=True)</code> method.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>download_all_files</code> <p>EXPERIMENTAL. Download all files related to the dataset that reside on the server. Useful for datasets which refer to auxiliary files (e.g., meta-album).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>force_refresh_cache</code> <p>Force the cache to refreshed by deleting the cache directory and re-downloading the data. Note, if <code>force_refresh_cache</code> is True, <code>get_dataset</code> is NOT thread/multiprocessing safe, because this creates a race condition to creating and deleting the cache; as in general with the cache.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dataset</code> <p>The downloaded dataset.</p> <p> TYPE: <code>:class:`openml.OpenMLDataset`</code> </p> Source code in <code>openml/datasets/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_dataset(  # noqa: C901, PLR0912\n    dataset_id: int | str,\n    download_data: bool = False,  # noqa: FBT002, FBT001\n    version: int | None = None,\n    error_if_multiple: bool = False,  # noqa: FBT002, FBT001\n    cache_format: Literal[\"pickle\", \"feather\"] = \"pickle\",\n    download_qualities: bool = False,  # noqa: FBT002, FBT001\n    download_features_meta_data: bool = False,  # noqa: FBT002, FBT001\n    download_all_files: bool = False,  # noqa: FBT002, FBT001\n    force_refresh_cache: bool = False,  # noqa: FBT001, FBT002\n) -&gt; OpenMLDataset:\n    \"\"\"Download the OpenML dataset representation, optionally also download actual data file.\n\n    This function is by default NOT thread/multiprocessing safe, as this function uses caching.\n    A check will be performed to determine if the information has previously been downloaded to a\n    cache, and if so be loaded from disk instead of retrieved from the server.\n\n    To make this function thread safe, you can install the python package ``oslo.concurrency``.\n    If ``oslo.concurrency`` is installed `get_dataset` becomes thread safe.\n\n    Alternatively, to make this function thread/multiprocessing safe initialize the cache first by\n    calling `get_dataset(args)` once before calling `get_dataset(args)` many times in parallel.\n    This will initialize the cache and later calls will use the cache in a thread/multiprocessing\n    safe way.\n\n    If dataset is retrieved by name, a version may be specified.\n    If no version is specified and multiple versions of the dataset exist,\n    the earliest version of the dataset that is still active will be returned.\n    If no version is specified, multiple versions of the dataset exist and\n    ``exception_if_multiple`` is set to ``True``, this function will raise an exception.\n\n    Parameters\n    ----------\n    dataset_id : int or str\n        Dataset ID (integer) or dataset name (string) of the dataset to download.\n    download_data : bool (default=False)\n        If True, also download the data file. Beware that some datasets are large and it might\n        make the operation noticeably slower. Metadata is also still retrieved.\n        If False, create the OpenMLDataset and only populate it with the metadata.\n        The data may later be retrieved through the `OpenMLDataset.get_data` method.\n    version : int, optional (default=None)\n        Specifies the version if `dataset_id` is specified by name.\n        If no version is specified, retrieve the least recent still active version.\n    error_if_multiple : bool (default=False)\n        If ``True`` raise an error if multiple datasets are found with matching criteria.\n    cache_format : str (default='pickle') in {'pickle', 'feather'}\n        Format for caching the dataset - may be feather or pickle\n        Note that the default 'pickle' option may load slower than feather when\n        no.of.rows is very high.\n    download_qualities : bool (default=False)\n        Option to download 'qualities' meta-data in addition to the minimal dataset description.\n        If True, download and cache the qualities file.\n        If False, create the OpenMLDataset without qualities metadata. The data may later be added\n        to the OpenMLDataset through the `OpenMLDataset.load_metadata(qualities=True)` method.\n    download_features_meta_data : bool (default=False)\n        Option to download 'features' meta-data in addition to the minimal dataset description.\n        If True, download and cache the features file.\n        If False, create the OpenMLDataset without features metadata. The data may later be added\n        to the OpenMLDataset through the `OpenMLDataset.load_metadata(features=True)` method.\n    download_all_files: bool (default=False)\n        EXPERIMENTAL. Download all files related to the dataset that reside on the server.\n        Useful for datasets which refer to auxiliary files (e.g., meta-album).\n    force_refresh_cache : bool (default=False)\n        Force the cache to refreshed by deleting the cache directory and re-downloading the data.\n        Note, if `force_refresh_cache` is True, `get_dataset` is NOT thread/multiprocessing safe,\n        because this creates a race condition to creating and deleting the cache; as in general with\n        the cache.\n\n    Returns\n    -------\n    dataset : :class:`openml.OpenMLDataset`\n        The downloaded dataset.\n    \"\"\"\n    if download_all_files:\n        warnings.warn(\n            \"``download_all_files`` is experimental and is likely to break with new releases.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n    if cache_format not in [\"feather\", \"pickle\"]:\n        raise ValueError(\n            \"cache_format must be one of 'feather' or 'pickle. \"\n            f\"Invalid format specified: {cache_format}\",\n        )\n\n    if isinstance(dataset_id, str):\n        try:\n            dataset_id = int(dataset_id)\n        except ValueError:\n            dataset_id = _name_to_id(dataset_id, version, error_if_multiple)  # type: ignore\n    elif not isinstance(dataset_id, int):\n        raise TypeError(\n            f\"`dataset_id` must be one of `str` or `int`, not {type(dataset_id)}.\",\n        )\n\n    if force_refresh_cache:\n        did_cache_dir = _get_cache_dir_for_id(DATASETS_CACHE_DIR_NAME, dataset_id)\n        if did_cache_dir.exists():\n            _remove_cache_dir_for_id(DATASETS_CACHE_DIR_NAME, did_cache_dir)\n\n    did_cache_dir = _create_cache_directory_for_id(\n        DATASETS_CACHE_DIR_NAME,\n        dataset_id,\n    )\n\n    remove_dataset_cache = True\n    try:\n        description = _get_dataset_description(did_cache_dir, dataset_id)\n        features_file = None\n        qualities_file = None\n\n        if download_features_meta_data:\n            features_file = _get_dataset_features_file(did_cache_dir, dataset_id)\n        if download_qualities:\n            qualities_file = _get_dataset_qualities_file(did_cache_dir, dataset_id)\n\n        parquet_file = None\n        skip_parquet = os.environ.get(OPENML_SKIP_PARQUET_ENV_VAR, \"false\").casefold() == \"true\"\n        download_parquet = \"oml:parquet_url\" in description and not skip_parquet\n        if download_parquet and (download_data or download_all_files):\n            try:\n                parquet_file = _get_dataset_parquet(\n                    description,\n                    download_all_files=download_all_files,\n                )\n            except urllib3.exceptions.MaxRetryError:\n                parquet_file = None\n\n        arff_file = None\n        if parquet_file is None and download_data:\n            if download_parquet:\n                logger.warning(\"Failed to download parquet, fallback on ARFF.\")\n            arff_file = _get_dataset_arff(description)\n\n        remove_dataset_cache = False\n    except OpenMLServerException as e:\n        # if there was an exception\n        # check if the user had access to the dataset\n        if e.code == NO_ACCESS_GRANTED_ERRCODE:\n            raise OpenMLPrivateDatasetError(e.message) from None\n\n        raise e\n    finally:\n        if remove_dataset_cache:\n            _remove_cache_dir_for_id(DATASETS_CACHE_DIR_NAME, did_cache_dir)\n\n    return _create_dataset_from_description(\n        description,\n        features_file,\n        qualities_file,\n        arff_file,\n        parquet_file,\n        cache_format,\n    )\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.get_datasets","title":"get_datasets","text":"<pre><code>get_datasets(dataset_ids: list[str | int], download_data: bool = False, download_qualities: bool = False) -&gt; list[OpenMLDataset]\n</code></pre> <p>Download datasets.</p> <p>This function iterates :meth:<code>openml.datasets.get_dataset</code>.</p> PARAMETER DESCRIPTION <code>dataset_ids</code> <p>Integers or strings representing dataset ids or dataset names. If dataset names are specified, the least recent still active dataset version is returned.</p> <p> TYPE: <code>iterable</code> </p> <code>download_data</code> <p>If True, also download the data file. Beware that some datasets are large and it might make the operation noticeably slower. Metadata is also still retrieved. If False, create the OpenMLDataset and only populate it with the metadata. The data may later be retrieved through the <code>OpenMLDataset.get_data</code> method.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>download_qualities</code> <p>If True, also download qualities.xml file. If False it skip the qualities.xml.</p> <p> TYPE: <code>(bool, optional(default=True))</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>datasets</code> <p>A list of dataset objects.</p> <p> TYPE: <code>list of datasets</code> </p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def get_datasets(\n    dataset_ids: list[str | int],\n    download_data: bool = False,  # noqa: FBT001, FBT002\n    download_qualities: bool = False,  # noqa: FBT001, FBT002\n) -&gt; list[OpenMLDataset]:\n    \"\"\"Download datasets.\n\n    This function iterates :meth:`openml.datasets.get_dataset`.\n\n    Parameters\n    ----------\n    dataset_ids : iterable\n        Integers or strings representing dataset ids or dataset names.\n        If dataset names are specified, the least recent still active dataset version is returned.\n    download_data : bool, optional\n        If True, also download the data file. Beware that some datasets are large and it might\n        make the operation noticeably slower. Metadata is also still retrieved.\n        If False, create the OpenMLDataset and only populate it with the metadata.\n        The data may later be retrieved through the `OpenMLDataset.get_data` method.\n    download_qualities : bool, optional (default=True)\n        If True, also download qualities.xml file. If False it skip the qualities.xml.\n\n    Returns\n    -------\n    datasets : list of datasets\n        A list of dataset objects.\n    \"\"\"\n    datasets = []\n    for dataset_id in dataset_ids:\n        datasets.append(\n            get_dataset(dataset_id, download_data, download_qualities=download_qualities),\n        )\n    return datasets\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.list_datasets","title":"list_datasets","text":"<pre><code>list_datasets(data_id: list[int] | None = None, offset: int | None = None, size: int | None = None, status: str | None = None, tag: str | None = None, data_name: str | None = None, data_version: int | None = None, number_instances: int | str | None = None, number_features: int | str | None = None, number_classes: int | str | None = None, number_missing_values: int | str | None = None) -&gt; DataFrame\n</code></pre> <p>Return a dataframe of all dataset which are on OpenML.</p> <p>Supports large amount of results.</p> PARAMETER DESCRIPTION <code>data_id</code> <p>A list of data ids, to specify which datasets should be listed</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>offset</code> <p>The number of datasets to skip, starting from the first.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>The maximum number of datasets to show.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>status</code> <p>Should be {active, in_preparation, deactivated}. By default active datasets are returned, but also datasets from another status can be requested.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_name</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_version</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>number_instances</code> <p> TYPE: <code>int | str</code> DEFAULT: <code>None</code> </p> <code>number_features</code> <p> TYPE: <code>int | str</code> DEFAULT: <code>None</code> </p> <code>number_classes</code> <p> TYPE: <code>int | str</code> DEFAULT: <code>None</code> </p> <code>number_missing_values</code> <p> TYPE: <code>int | str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>datasets</code> <p>Each row maps to a dataset Each column contains the following information: - dataset id - name - format - status If qualities are calculated for the dataset, some of these are also included as columns.</p> <p> TYPE: <code>dataframe</code> </p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def list_datasets(\n    data_id: list[int] | None = None,\n    offset: int | None = None,\n    size: int | None = None,\n    status: str | None = None,\n    tag: str | None = None,\n    data_name: str | None = None,\n    data_version: int | None = None,\n    number_instances: int | str | None = None,\n    number_features: int | str | None = None,\n    number_classes: int | str | None = None,\n    number_missing_values: int | str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Return a dataframe of all dataset which are on OpenML.\n\n    Supports large amount of results.\n\n    Parameters\n    ----------\n    data_id : list, optional\n        A list of data ids, to specify which datasets should be\n        listed\n    offset : int, optional\n        The number of datasets to skip, starting from the first.\n    size : int, optional\n        The maximum number of datasets to show.\n    status : str, optional\n        Should be {active, in_preparation, deactivated}. By\n        default active datasets are returned, but also datasets\n        from another status can be requested.\n    tag : str, optional\n    data_name : str, optional\n    data_version : int, optional\n    number_instances : int | str, optional\n    number_features : int | str, optional\n    number_classes : int | str, optional\n    number_missing_values : int | str, optional\n\n    Returns\n    -------\n    datasets: dataframe\n        Each row maps to a dataset\n        Each column contains the following information:\n        - dataset id\n        - name\n        - format\n        - status\n        If qualities are calculated for the dataset, some of\n        these are also included as columns.\n    \"\"\"\n    listing_call = partial(\n        _list_datasets,\n        data_id=data_id,\n        status=status,\n        tag=tag,\n        data_name=data_name,\n        data_version=data_version,\n        number_instances=number_instances,\n        number_features=number_features,\n        number_classes=number_classes,\n        number_missing_values=number_missing_values,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.list_qualities","title":"list_qualities","text":"<pre><code>list_qualities() -&gt; list[str]\n</code></pre> <p>Return list of data qualities available.</p> <p>The function performs an API call to retrieve the entire list of data qualities that are computed on the datasets uploaded.</p> RETURNS DESCRIPTION <code>list</code> Source code in <code>openml/datasets/functions.py</code> <pre><code>def list_qualities() -&gt; list[str]:\n    \"\"\"Return list of data qualities available.\n\n    The function performs an API call to retrieve the entire list of\n    data qualities that are computed on the datasets uploaded.\n\n    Returns\n    -------\n    list\n    \"\"\"\n    api_call = \"data/qualities/list\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    qualities = xmltodict.parse(xml_string, force_list=(\"oml:quality\"))\n    # Minimalistic check if the XML is useful\n    if \"oml:data_qualities_list\" not in qualities:\n        raise ValueError('Error in return XML, does not contain \"oml:data_qualities_list\"')\n\n    if not isinstance(qualities[\"oml:data_qualities_list\"][\"oml:quality\"], list):\n        raise TypeError('Error in return XML, does not contain \"oml:quality\" as a list')\n\n    return qualities[\"oml:data_qualities_list\"][\"oml:quality\"]\n</code></pre>"},{"location":"reference/datasets/#openml.datasets.status_update","title":"status_update","text":"<pre><code>status_update(data_id: int, status: Literal['active', 'deactivated']) -&gt; None\n</code></pre> <p>Updates the status of a dataset to either 'active' or 'deactivated'. Please see the OpenML API documentation for a description of the status and all legal status transitions: docs.openml.org/concepts/data/#dataset-status</p> PARAMETER DESCRIPTION <code>data_id</code> <p>The data id of the dataset</p> <p> TYPE: <code>int</code> </p> <code>status</code> <p>'active' or 'deactivated'</p> <p> TYPE: <code>(str)</code> </p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def status_update(data_id: int, status: Literal[\"active\", \"deactivated\"]) -&gt; None:\n    \"\"\"\n    Updates the status of a dataset to either 'active' or 'deactivated'.\n    Please see the OpenML API documentation for a description of the status\n    and all legal status transitions:\n    https://docs.openml.org/concepts/data/#dataset-status\n\n    Parameters\n    ----------\n    data_id : int\n        The data id of the dataset\n    status : str,\n        'active' or 'deactivated'\n    \"\"\"\n    legal_status = {\"active\", \"deactivated\"}\n    if status not in legal_status:\n        raise ValueError(f\"Illegal status value. Legal values: {legal_status}\")\n\n    data: openml._api_calls.DATA_TYPE = {\"data_id\": data_id, \"status\": status}\n    result_xml = openml._api_calls._perform_api_call(\"data/status/update\", \"post\", data=data)\n    result = xmltodict.parse(result_xml)\n    server_data_id = result[\"oml:data_status_update\"][\"oml:id\"]\n    server_status = result[\"oml:data_status_update\"][\"oml:status\"]\n    if status != server_status or int(data_id) != int(server_data_id):\n        # This should never happen\n        raise ValueError(\"Data id/status does not collide\")\n</code></pre>"},{"location":"reference/datasets/data_feature/","title":"data_feature","text":""},{"location":"reference/datasets/data_feature/#openml.datasets.data_feature","title":"openml.datasets.data_feature","text":""},{"location":"reference/datasets/data_feature/#openml.datasets.data_feature.OpenMLDataFeature","title":"OpenMLDataFeature","text":"<pre><code>OpenMLDataFeature(index: int, name: str, data_type: str, nominal_values: list[str], number_missing_values: int, ontologies: list[str] | None = None)\n</code></pre> <p>Data Feature (a.k.a. Attribute) object.</p> PARAMETER DESCRIPTION <code>index</code> <p>The index of this feature</p> <p> TYPE: <code>int</code> </p> <code>name</code> <p>Name of the feature</p> <p> TYPE: <code>str</code> </p> <code>data_type</code> <p>can be nominal, numeric, string, date (corresponds to arff)</p> <p> TYPE: <code>str</code> </p> <code>nominal_values</code> <p>list of the possible values, in case of nominal attribute</p> <p> TYPE: <code>list(str)</code> </p> <code>number_missing_values</code> <p>Number of rows that have a missing value for this feature.</p> <p> TYPE: <code>int</code> </p> <code>ontologies</code> <p>list of ontologies attached to this feature. An ontology describes the concept that are described in a feature. An ontology is defined by an URL where the information is provided.</p> <p> TYPE: <code>list(str)</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/datasets/data_feature.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    index: int,\n    name: str,\n    data_type: str,\n    nominal_values: list[str],\n    number_missing_values: int,\n    ontologies: list[str] | None = None,\n):\n    if not isinstance(index, int):\n        raise TypeError(f\"Index must be `int` but is {type(index)}\")\n\n    if data_type not in self.LEGAL_DATA_TYPES:\n        raise ValueError(\n            f\"data type should be in {self.LEGAL_DATA_TYPES!s}, found: {data_type}\",\n        )\n\n    if data_type == \"nominal\":\n        if nominal_values is None:\n            raise TypeError(\n                \"Dataset features require attribute `nominal_values` for nominal \"\n                \"feature type.\",\n            )\n\n        if not isinstance(nominal_values, list):\n            raise TypeError(\n                \"Argument `nominal_values` is of wrong datatype, should be list, \"\n                f\"but is {type(nominal_values)}\",\n            )\n    elif nominal_values is not None:\n        raise TypeError(\"Argument `nominal_values` must be None for non-nominal feature.\")\n\n    if not isinstance(number_missing_values, int):\n        msg = f\"number_missing_values must be int but is {type(number_missing_values)}\"\n        raise TypeError(msg)\n\n    self.index = index\n    self.name = str(name)\n    self.data_type = str(data_type)\n    self.nominal_values = nominal_values\n    self.number_missing_values = number_missing_values\n    self.ontologies = ontologies\n</code></pre>"},{"location":"reference/datasets/dataset/","title":"dataset","text":""},{"location":"reference/datasets/dataset/#openml.datasets.dataset","title":"openml.datasets.dataset","text":""},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset","title":"OpenMLDataset","text":"<pre><code>OpenMLDataset(name: str, description: str | None, data_format: Literal['arff', 'sparse_arff'] = 'arff', cache_format: Literal['feather', 'pickle'] = 'pickle', dataset_id: int | None = None, version: int | None = None, creator: str | None = None, contributor: str | None = None, collection_date: str | None = None, upload_date: str | None = None, language: str | None = None, licence: str | None = None, url: str | None = None, default_target_attribute: str | None = None, row_id_attribute: str | None = None, ignore_attribute: str | list[str] | None = None, version_label: str | None = None, citation: str | None = None, tag: str | None = None, visibility: str | None = None, original_data_url: str | None = None, paper_url: str | None = None, update_comment: str | None = None, md5_checksum: str | None = None, data_file: str | None = None, features_file: str | None = None, qualities_file: str | None = None, dataset: str | None = None, parquet_url: str | None = None, parquet_file: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>Dataset object.</p> <p>Allows fetching and uploading datasets to OpenML.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Description of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>data_format</code> <p>Format of the dataset which can be either 'arff' or 'sparse_arff'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'arff'</code> </p> <code>cache_format</code> <p>Format for caching the dataset which can be either 'feather' or 'pickle'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pickle'</code> </p> <code>dataset_id</code> <p>Id autogenerated by the server.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>version</code> <p>Version of this dataset. '1' for original version. Auto-incremented by server.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>creator</code> <p>The person who created the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>contributor</code> <p>People who contributed to the current version of the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>collection_date</code> <p>The date the data was originally collected, given by the uploader.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>upload_date</code> <p>The date-time when the dataset was uploaded, generated by server.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>language</code> <p>Language in which the data is represented. Starts with 1 upper case letter, rest lower case, e.g. 'English'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>licence</code> <p>License of the data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>url</code> <p>Valid URL, points to actual data file. The file can be on the OpenML server or another dataset repository.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>default_target_attribute</code> <p>The default target attribute, if it exists. Can have multiple values, comma separated.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>row_id_attribute</code> <p>The attribute that represents the row-id column, if present in the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>ignore_attribute</code> <p>Attributes that should be excluded in modelling, such as identifiers and indexes.</p> <p> TYPE: <code>str | list</code> DEFAULT: <code>None</code> </p> <code>version_label</code> <p>Version label provided by user. Can be a date, hash, or some other type of id.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>citation</code> <p>Reference(s) that should be cited when building on this data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p>Tags, describing the algorithms.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>visibility</code> <p>Who can see the dataset. Typical values: 'Everyone','All my friends','Only me'. Can also be any of the user's circles.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>original_data_url</code> <p>For derived data, the url to the original dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>paper_url</code> <p>Link to a paper describing the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>update_comment</code> <p>An explanation for when the dataset is uploaded.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>md5_checksum</code> <p>MD5 checksum to check if the dataset is downloaded without corruption.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_file</code> <p>Path to where the dataset is located.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>features_file</code> <p>A dictionary of dataset features, which maps a feature index to a OpenMLDataFeature.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>qualities_file</code> <p>A dictionary of dataset qualities, which maps a quality name to a quality value.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>dataset</code> <p>Serialized arff dataset string.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>parquet_url</code> <p>This is the URL to the storage location where the dataset files are hosted. This can be a MinIO bucket URL. If specified, the data will be accessed from this URL when reading the files.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>parquet_file</code> <p>Path to the local file.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def __init__(  # noqa: C901, PLR0912, PLR0913, PLR0915\n    self,\n    name: str,\n    description: str | None,\n    data_format: Literal[\"arff\", \"sparse_arff\"] = \"arff\",\n    cache_format: Literal[\"feather\", \"pickle\"] = \"pickle\",\n    dataset_id: int | None = None,\n    version: int | None = None,\n    creator: str | None = None,\n    contributor: str | None = None,\n    collection_date: str | None = None,\n    upload_date: str | None = None,\n    language: str | None = None,\n    licence: str | None = None,\n    url: str | None = None,\n    default_target_attribute: str | None = None,\n    row_id_attribute: str | None = None,\n    ignore_attribute: str | list[str] | None = None,\n    version_label: str | None = None,\n    citation: str | None = None,\n    tag: str | None = None,\n    visibility: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n    update_comment: str | None = None,\n    md5_checksum: str | None = None,\n    data_file: str | None = None,\n    features_file: str | None = None,\n    qualities_file: str | None = None,\n    dataset: str | None = None,\n    parquet_url: str | None = None,\n    parquet_file: str | None = None,\n):\n    if cache_format not in [\"feather\", \"pickle\"]:\n        raise ValueError(\n            \"cache_format must be one of 'feather' or 'pickle. \"\n            f\"Invalid format specified: {cache_format}\",\n        )\n\n    def find_invalid_characters(string: str, pattern: str) -&gt; str:\n        invalid_chars = set()\n        regex = re.compile(pattern)\n        for char in string:\n            if not regex.match(char):\n                invalid_chars.add(char)\n        return \",\".join(\n            [f\"'{char}'\" if char != \"'\" else f'\"{char}\"' for char in invalid_chars],\n        )\n\n    if dataset_id is None:\n        pattern = \"^[\\x00-\\x7f]*$\"\n        if description and not re.match(pattern, description):\n            # not basiclatin (XSD complains)\n            invalid_characters = find_invalid_characters(description, pattern)\n            raise ValueError(\n                f\"Invalid symbols {invalid_characters} in description: {description}\",\n            )\n        pattern = \"^[\\x00-\\x7f]*$\"\n        if citation and not re.match(pattern, citation):\n            # not basiclatin (XSD complains)\n            invalid_characters = find_invalid_characters(citation, pattern)\n            raise ValueError(\n                f\"Invalid symbols {invalid_characters} in citation: {citation}\",\n            )\n        pattern = \"^[a-zA-Z0-9_\\\\-\\\\.\\\\(\\\\),]+$\"\n        if not re.match(pattern, name):\n            # regex given by server in error message\n            invalid_characters = find_invalid_characters(name, pattern)\n            raise ValueError(f\"Invalid symbols {invalid_characters} in name: {name}\")\n\n    self.ignore_attribute: list[str] | None = None\n    if isinstance(ignore_attribute, str):\n        self.ignore_attribute = [ignore_attribute]\n    elif isinstance(ignore_attribute, list) or ignore_attribute is None:\n        self.ignore_attribute = ignore_attribute\n    else:\n        raise ValueError(\"Wrong data type for ignore_attribute. Should be list.\")\n\n    # TODO add function to check if the name is casual_string128\n    # Attributes received by querying the RESTful API\n    self.dataset_id = int(dataset_id) if dataset_id is not None else None\n    self.name = name\n    self.version = int(version) if version is not None else None\n    self.description = description\n    self.cache_format = cache_format\n    # Has to be called format, otherwise there will be an XML upload error\n    self.format = data_format\n    self.creator = creator\n    self.contributor = contributor\n    self.collection_date = collection_date\n    self.upload_date = upload_date\n    self.language = language\n    self.licence = licence\n    self.url = url\n    self.default_target_attribute = default_target_attribute\n    self.row_id_attribute = row_id_attribute\n\n    self.version_label = version_label\n    self.citation = citation\n    self.tag = tag\n    self.visibility = visibility\n    self.original_data_url = original_data_url\n    self.paper_url = paper_url\n    self.update_comment = update_comment\n    self.md5_checksum = md5_checksum\n    self.data_file = data_file\n    self.parquet_file = parquet_file\n    self._dataset = dataset\n    self._parquet_url = parquet_url\n\n    self._features: dict[int, OpenMLDataFeature] | None = None\n    self._qualities: dict[str, float] | None = None\n    self._no_qualities_found = False\n\n    if features_file is not None:\n        self._features = _read_features(Path(features_file))\n\n    # \"\" was the old default value by `get_dataset` and maybe still used by some\n    if qualities_file == \"\":\n        # TODO(0.15): to switch to \"qualities_file is not None\" below and remove warning\n        warnings.warn(\n            \"Starting from Version 0.15 `qualities_file` must be None and not an empty string \"\n            \"to avoid reading the qualities from file. Set `qualities_file` to None to avoid \"\n            \"this warning.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        qualities_file = None\n\n    if qualities_file is not None:\n        self._qualities = _read_qualities(Path(qualities_file))\n\n    if data_file is not None:\n        data_pickle, data_feather, feather_attribute = self._compressed_cache_file_paths(\n            Path(data_file)\n        )\n        self.data_pickle_file = data_pickle if Path(data_pickle).exists() else None\n        self.data_feather_file = data_feather if Path(data_feather).exists() else None\n        self.feather_attribute_file = feather_attribute if Path(feather_attribute) else None\n    else:\n        self.data_pickle_file = None\n        self.data_feather_file = None\n        self.feather_attribute_file = None\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.features","title":"features  <code>property</code>","text":"<pre><code>features: dict[int, OpenMLDataFeature]\n</code></pre> <p>Get the features of this dataset.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Get the dataset numeric id.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.qualities","title":"qualities  <code>property</code>","text":"<pre><code>qualities: dict[str, float] | None\n</code></pre> <p>Get the qualities of this dataset.</p>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.get_data","title":"get_data","text":"<pre><code>get_data(target: list[str] | str | None = None, include_row_id: bool = False, include_ignore_attribute: bool = False) -&gt; tuple[DataFrame, Series | None, list[bool], list[str]]\n</code></pre> <p>Returns dataset content as dataframes.</p> PARAMETER DESCRIPTION <code>target</code> <p>Name of target column to separate from the data. Splitting multiple columns is currently not supported.</p> <p> TYPE: <code>(string, List[str] or None(default=None))</code> DEFAULT: <code>None</code> </p> <code>include_row_id</code> <p>Whether to include row ids in the returned dataset.</p> <p> TYPE: <code>boolean(default=False)</code> DEFAULT: <code>False</code> </p> <code>include_ignore_attribute</code> <p>Whether to include columns that are marked as \"ignore\" on the server in the dataset.</p> <p> TYPE: <code>boolean(default=False)</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>X</code> <p>Dataset, may have sparse dtypes in the columns if required.</p> <p> TYPE: <code>(dataframe, shape(n_samples, n_columns))</code> </p> <code>y</code> <p>Target column</p> <p> TYPE: <code>(Series, shape(n_samples) or None)</code> </p> <code>categorical_indicator</code> <p>Mask that indicate categorical features.</p> <p> TYPE: <code>list[bool]</code> </p> <code>attribute_names</code> <p>List of attribute names.</p> <p> TYPE: <code>list[str]</code> </p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def get_data(  # noqa: C901\n    self,\n    target: list[str] | str | None = None,\n    include_row_id: bool = False,  # noqa: FBT001, FBT002\n    include_ignore_attribute: bool = False,  # noqa: FBT001, FBT002\n) -&gt; tuple[pd.DataFrame, pd.Series | None, list[bool], list[str]]:\n    \"\"\"Returns dataset content as dataframes.\n\n    Parameters\n    ----------\n    target : string, List[str] or None (default=None)\n        Name of target column to separate from the data.\n        Splitting multiple columns is currently not supported.\n    include_row_id : boolean (default=False)\n        Whether to include row ids in the returned dataset.\n    include_ignore_attribute : boolean (default=False)\n        Whether to include columns that are marked as \"ignore\"\n        on the server in the dataset.\n\n\n    Returns\n    -------\n    X : dataframe, shape (n_samples, n_columns)\n        Dataset, may have sparse dtypes in the columns if required.\n    y : pd.Series, shape (n_samples, ) or None\n        Target column\n    categorical_indicator : list[bool]\n        Mask that indicate categorical features.\n    attribute_names : list[str]\n        List of attribute names.\n    \"\"\"\n    data, categorical_mask, attribute_names = self._load_data()\n\n    to_exclude = []\n    if not include_row_id and self.row_id_attribute is not None:\n        if isinstance(self.row_id_attribute, str):\n            to_exclude.append(self.row_id_attribute)\n        elif isinstance(self.row_id_attribute, Iterable):\n            to_exclude.extend(self.row_id_attribute)\n\n    if not include_ignore_attribute and self.ignore_attribute is not None:\n        if isinstance(self.ignore_attribute, str):\n            to_exclude.append(self.ignore_attribute)\n        elif isinstance(self.ignore_attribute, Iterable):\n            to_exclude.extend(self.ignore_attribute)\n\n    if len(to_exclude) &gt; 0:\n        logger.info(f\"Going to remove the following attributes: {to_exclude}\")\n        keep = np.array([column not in to_exclude for column in attribute_names])\n        data = data.drop(columns=to_exclude)\n        categorical_mask = [cat for cat, k in zip(categorical_mask, keep) if k]\n        attribute_names = [att for att, k in zip(attribute_names, keep) if k]\n\n    if target is None:\n        return data, None, categorical_mask, attribute_names\n\n    if isinstance(target, str):\n        target_names = target.split(\",\") if \",\" in target else [target]\n    else:\n        target_names = target\n\n    # All the assumptions below for the target are dependant on the number of targets being 1\n    n_targets = len(target_names)\n    if n_targets &gt; 1:\n        raise NotImplementedError(f\"Number of targets {n_targets} not implemented.\")\n\n    target_name = target_names[0]\n    x = data.drop(columns=[target_name])\n    y = data[target_name].squeeze()\n\n    # Finally, remove the target from the list of attributes and categorical mask\n    target_index = attribute_names.index(target_name)\n    categorical_mask.pop(target_index)\n    attribute_names.remove(target_name)\n\n    assert isinstance(y, pd.Series)\n    return x, y, categorical_mask, attribute_names\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.get_features_by_type","title":"get_features_by_type","text":"<pre><code>get_features_by_type(data_type: str, exclude: list[str] | None = None, exclude_ignore_attribute: bool = True, exclude_row_id_attribute: bool = True) -&gt; list[int]\n</code></pre> <p>Return indices of features of a given type, e.g. all nominal features. Optional parameters to exclude various features by index or ontology.</p> PARAMETER DESCRIPTION <code>data_type</code> <p>The data type to return (e.g., nominal, numeric, date, string)</p> <p> TYPE: <code>str</code> </p> <code>exclude</code> <p>List of columns to exclude from the return value</p> <p> TYPE: <code>list(int)</code> DEFAULT: <code>None</code> </p> <code>exclude_ignore_attribute</code> <p>Whether to exclude the defined ignore attributes (and adapt the return values as if these indices are not present)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>exclude_row_id_attribute</code> <p>Whether to exclude the defined row id attributes (and adapt the return values as if these indices are not present)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>result</code> <p>a list of indices that have the specified data type</p> <p> TYPE: <code>list</code> </p> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def get_features_by_type(  # noqa: C901\n    self,\n    data_type: str,\n    exclude: list[str] | None = None,\n    exclude_ignore_attribute: bool = True,  # noqa: FBT002, FBT001\n    exclude_row_id_attribute: bool = True,  # noqa: FBT002, FBT001\n) -&gt; list[int]:\n    \"\"\"\n    Return indices of features of a given type, e.g. all nominal features.\n    Optional parameters to exclude various features by index or ontology.\n\n    Parameters\n    ----------\n    data_type : str\n        The data type to return (e.g., nominal, numeric, date, string)\n    exclude : list(int)\n        List of columns to exclude from the return value\n    exclude_ignore_attribute : bool\n        Whether to exclude the defined ignore attributes (and adapt the\n        return values as if these indices are not present)\n    exclude_row_id_attribute : bool\n        Whether to exclude the defined row id attributes (and adapt the\n        return values as if these indices are not present)\n\n    Returns\n    -------\n    result : list\n        a list of indices that have the specified data type\n    \"\"\"\n    if data_type not in OpenMLDataFeature.LEGAL_DATA_TYPES:\n        raise TypeError(\"Illegal feature type requested\")\n    if self.ignore_attribute is not None and not isinstance(self.ignore_attribute, list):\n        raise TypeError(\"ignore_attribute should be a list\")\n    if self.row_id_attribute is not None and not isinstance(self.row_id_attribute, str):\n        raise TypeError(\"row id attribute should be a str\")\n    if exclude is not None and not isinstance(exclude, list):\n        raise TypeError(\"Exclude should be a list\")\n        # assert all(isinstance(elem, str) for elem in exclude),\n        #            \"Exclude should be a list of strings\"\n    to_exclude = []\n    if exclude is not None:\n        to_exclude.extend(exclude)\n    if exclude_ignore_attribute and self.ignore_attribute is not None:\n        to_exclude.extend(self.ignore_attribute)\n    if exclude_row_id_attribute and self.row_id_attribute is not None:\n        to_exclude.append(self.row_id_attribute)\n\n    result = []\n    offset = 0\n    # this function assumes that everything in to_exclude will\n    # be 'excluded' from the dataset (hence the offset)\n    for idx in self.features:\n        name = self.features[idx].name\n        if name in to_exclude:\n            offset += 1\n        elif self.features[idx].data_type == data_type:\n            result.append(idx - offset)\n    return result\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.retrieve_class_labels","title":"retrieve_class_labels","text":"<pre><code>retrieve_class_labels(target_name: str = 'class') -&gt; None | list[str]\n</code></pre> <p>Reads the datasets arff to determine the class-labels.</p> <p>If the task has no class labels (for example a regression problem) it returns None. Necessary because the data returned by get_data only contains the indices of the classes, while OpenML needs the real classname when uploading the results of a run.</p> PARAMETER DESCRIPTION <code>target_name</code> <p>Name of the target attribute</p> <p> TYPE: <code>str</code> DEFAULT: <code>'class'</code> </p> RETURNS DESCRIPTION <code>list</code> Source code in <code>openml/datasets/dataset.py</code> <pre><code>def retrieve_class_labels(self, target_name: str = \"class\") -&gt; None | list[str]:\n    \"\"\"Reads the datasets arff to determine the class-labels.\n\n    If the task has no class labels (for example a regression problem)\n    it returns None. Necessary because the data returned by get_data\n    only contains the indices of the classes, while OpenML needs the real\n    classname when uploading the results of a run.\n\n    Parameters\n    ----------\n    target_name : str\n        Name of the target attribute\n\n    Returns\n    -------\n    list\n    \"\"\"\n    for feature in self.features.values():\n        if feature.name == target_name:\n            if feature.data_type == \"nominal\":\n                return feature.nominal_values\n\n            if feature.data_type == \"string\":\n                # Rel.: #1311\n                # The target is invalid for a classification task if the feature type is string\n                # and not nominal. For such miss-configured tasks, we silently fix it here as\n                # we can safely interpreter string as nominal.\n                df, *_ = self.get_data()\n                return list(df[feature.name].unique())\n\n    return None\n</code></pre>"},{"location":"reference/datasets/dataset/#openml.datasets.dataset.OpenMLDataset.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/datasets/functions/","title":"functions","text":""},{"location":"reference/datasets/functions/#openml.datasets.functions","title":"openml.datasets.functions","text":""},{"location":"reference/datasets/functions/#openml.datasets.functions.attributes_arff_from_df","title":"attributes_arff_from_df","text":"<pre><code>attributes_arff_from_df(df: DataFrame) -&gt; list[tuple[str, list[str] | str]]\n</code></pre> <p>Describe attributes of the dataframe according to ARFF specification.</p> PARAMETER DESCRIPTION <code>df</code> <p>The dataframe containing the data set.</p> <p> TYPE: <code>(DataFrame, shape(n_samples, n_features))</code> </p> RETURNS DESCRIPTION <code>attributes_arff</code> <p>The data set attributes as required by the ARFF format.</p> <p> TYPE: <code>list[str]</code> </p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def attributes_arff_from_df(df: pd.DataFrame) -&gt; list[tuple[str, list[str] | str]]:\n    \"\"\"Describe attributes of the dataframe according to ARFF specification.\n\n    Parameters\n    ----------\n    df : DataFrame, shape (n_samples, n_features)\n        The dataframe containing the data set.\n\n    Returns\n    -------\n    attributes_arff : list[str]\n        The data set attributes as required by the ARFF format.\n    \"\"\"\n    PD_DTYPES_TO_ARFF_DTYPE = {\"integer\": \"INTEGER\", \"floating\": \"REAL\", \"string\": \"STRING\"}\n    attributes_arff: list[tuple[str, list[str] | str]] = []\n\n    if not all(isinstance(column_name, str) for column_name in df.columns):\n        logger.warning(\"Converting non-str column names to str.\")\n        df.columns = [str(column_name) for column_name in df.columns]\n\n    for column_name in df:\n        # skipna=True does not infer properly the dtype. The NA values are\n        # dropped before the inference instead.\n        column_dtype = pd.api.types.infer_dtype(df[column_name].dropna(), skipna=False)\n\n        if column_dtype == \"categorical\":\n            # for categorical feature, arff expects a list string. However, a\n            # categorical column can contain mixed type and should therefore\n            # raise an error asking to convert all entries to string.\n            categories = df[column_name].cat.categories\n            categories_dtype = pd.api.types.infer_dtype(categories)\n            if categories_dtype not in (\"string\", \"unicode\"):\n                raise ValueError(\n                    f\"The column '{column_name}' of the dataframe is of \"\n                    \"'category' dtype. Therefore, all values in \"\n                    \"this columns should be string. Please \"\n                    \"convert the entries which are not string. \"\n                    f\"Got {categories_dtype} dtype in this column.\",\n                )\n            attributes_arff.append((column_name, categories.tolist()))\n        elif column_dtype == \"boolean\":\n            # boolean are encoded as categorical.\n            attributes_arff.append((column_name, [\"True\", \"False\"]))\n        elif column_dtype in PD_DTYPES_TO_ARFF_DTYPE:\n            attributes_arff.append((column_name, PD_DTYPES_TO_ARFF_DTYPE[column_dtype]))\n        else:\n            raise ValueError(\n                f\"The dtype '{column_dtype}' of the column '{column_name}' is not \"\n                \"currently supported by liac-arff. Supported \"\n                \"dtypes are categorical, string, integer, \"\n                \"floating, and boolean.\",\n            )\n    return attributes_arff\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.check_datasets_active","title":"check_datasets_active","text":"<pre><code>check_datasets_active(dataset_ids: list[int], raise_error_if_not_exist: bool = True) -&gt; dict[int, bool]\n</code></pre> <p>Check if the dataset ids provided are active.</p> <p>Raises an error if a dataset_id in the given list of dataset_ids does not exist on the server and <code>raise_error_if_not_exist</code> is set to True (default).</p> PARAMETER DESCRIPTION <code>dataset_ids</code> <p>A list of integers representing dataset ids.</p> <p> TYPE: <code>List[int]</code> </p> <code>raise_error_if_not_exist</code> <p>Flag that if activated can raise an error, if one or more of the given dataset ids do not exist on the server.</p> <p> TYPE: <code>bool(default=True)</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary with items {did: bool}</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def check_datasets_active(\n    dataset_ids: list[int],\n    raise_error_if_not_exist: bool = True,  # noqa: FBT001, FBT002\n) -&gt; dict[int, bool]:\n    \"\"\"\n    Check if the dataset ids provided are active.\n\n    Raises an error if a dataset_id in the given list\n    of dataset_ids does not exist on the server and\n    `raise_error_if_not_exist` is set to True (default).\n\n    Parameters\n    ----------\n    dataset_ids : List[int]\n        A list of integers representing dataset ids.\n    raise_error_if_not_exist : bool (default=True)\n        Flag that if activated can raise an error, if one or more of the\n        given dataset ids do not exist on the server.\n\n    Returns\n    -------\n    dict\n        A dictionary with items {did: bool}\n    \"\"\"\n    datasets = list_datasets(status=\"all\", data_id=dataset_ids)\n    missing = set(dataset_ids) - set(datasets.index)\n    if raise_error_if_not_exist and missing:\n        missing_str = \", \".join(str(did) for did in missing)\n        raise ValueError(f\"Could not find dataset(s) {missing_str} in OpenML dataset list.\")\n    mask = datasets[\"status\"] == \"active\"\n    return dict(mask)\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(name: str, description: str | None, creator: str | None, contributor: str | None, collection_date: str | None, language: str | None, licence: str | None, attributes: list[tuple[str, str | list[str]]] | dict[str, str | list[str]] | Literal['auto'], data: DataFrame | ndarray | coo_matrix, default_target_attribute: str, ignore_attribute: str | list[str] | None, citation: str, row_id_attribute: str | None = None, original_data_url: str | None = None, paper_url: str | None = None, update_comment: str | None = None, version_label: str | None = None) -&gt; OpenMLDataset\n</code></pre> <p>Create a dataset.</p> <p>This function creates an OpenMLDataset object. The OpenMLDataset object contains information related to the dataset and the actual data file.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Description of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>creator</code> <p>The person who created the dataset.</p> <p> TYPE: <code>str</code> </p> <code>contributor</code> <p>People who contributed to the current version of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>collection_date</code> <p>The date the data was originally collected, given by the uploader.</p> <p> TYPE: <code>str</code> </p> <code>language</code> <p>Language in which the data is represented. Starts with 1 upper case letter, rest lower case, e.g. 'English'.</p> <p> TYPE: <code>str</code> </p> <code>licence</code> <p>License of the data.</p> <p> TYPE: <code>str</code> </p> <code>attributes</code> <p>A list of tuples. Each tuple consists of the attribute name and type. If passing a pandas DataFrame, the attributes can be automatically inferred by passing <code>'auto'</code>. Specific attributes can be manually specified by a passing a dictionary where the key is the name of the attribute and the value is the data type of the attribute.</p> <p> TYPE: <code>list, dict, or 'auto'</code> </p> <code>data</code> <p>An array that contains both the attributes and the targets. When providing a dataframe, the attribute names and type can be inferred by passing <code>attributes='auto'</code>. The target feature is indicated as meta-data of the dataset.</p> <p> TYPE: <code>(ndarray, list, dataframe, coo_matrix, shape(n_samples, n_features))</code> </p> <code>default_target_attribute</code> <p>The default target attribute, if it exists. Can have multiple values, comma separated.</p> <p> TYPE: <code>str</code> </p> <code>ignore_attribute</code> <p>Attributes that should be excluded in modelling, such as identifiers and indexes. Can have multiple values, comma separated.</p> <p> TYPE: <code>str | list</code> </p> <code>citation</code> <p>Reference(s) that should be cited when building on this data.</p> <p> TYPE: <code>str</code> </p> <code>version_label</code> <p>Version label provided by user.  Can be a date, hash, or some other type of id.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>row_id_attribute</code> <p>The attribute that represents the row-id column, if present in the dataset. If <code>data</code> is a dataframe and <code>row_id_attribute</code> is not specified, the index of the dataframe will be used as the <code>row_id_attribute</code>. If the name of the index is <code>None</code>, it will be discarded.</p> <p>.. versionadded: 0.8     Inference of <code>row_id_attribute</code> from a dataframe.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>original_data_url</code> <p>For derived data, the url to the original dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>paper_url</code> <p>Link to a paper describing the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>update_comment</code> <p>An explanation for when the dataset is uploaded.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>class</code> <p> TYPE: <code>`openml.OpenMLDataset`</code> </p> <code>Dataset description.</code> Source code in <code>openml/datasets/functions.py</code> <pre><code>def create_dataset(  # noqa: C901, PLR0912, PLR0915\n    name: str,\n    description: str | None,\n    creator: str | None,\n    contributor: str | None,\n    collection_date: str | None,\n    language: str | None,\n    licence: str | None,\n    # TODO(eddiebergman): Docstring says `type` but I don't know what this is other than strings\n    # Edit: Found it could also be like [\"True\", \"False\"]\n    attributes: list[tuple[str, str | list[str]]] | dict[str, str | list[str]] | Literal[\"auto\"],\n    data: pd.DataFrame | np.ndarray | scipy.sparse.coo_matrix,\n    # TODO(eddiebergman): Function requires `default_target_attribute` exist but API allows None\n    default_target_attribute: str,\n    ignore_attribute: str | list[str] | None,\n    citation: str,\n    row_id_attribute: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n    update_comment: str | None = None,\n    version_label: str | None = None,\n) -&gt; OpenMLDataset:\n    \"\"\"Create a dataset.\n\n    This function creates an OpenMLDataset object.\n    The OpenMLDataset object contains information related to the dataset\n    and the actual data file.\n\n    Parameters\n    ----------\n    name : str\n        Name of the dataset.\n    description : str\n        Description of the dataset.\n    creator : str\n        The person who created the dataset.\n    contributor : str\n        People who contributed to the current version of the dataset.\n    collection_date : str\n        The date the data was originally collected, given by the uploader.\n    language : str\n        Language in which the data is represented.\n        Starts with 1 upper case letter, rest lower case, e.g. 'English'.\n    licence : str\n        License of the data.\n    attributes : list, dict, or 'auto'\n        A list of tuples. Each tuple consists of the attribute name and type.\n        If passing a pandas DataFrame, the attributes can be automatically\n        inferred by passing ``'auto'``. Specific attributes can be manually\n        specified by a passing a dictionary where the key is the name of the\n        attribute and the value is the data type of the attribute.\n    data : ndarray, list, dataframe, coo_matrix, shape (n_samples, n_features)\n        An array that contains both the attributes and the targets. When\n        providing a dataframe, the attribute names and type can be inferred by\n        passing ``attributes='auto'``.\n        The target feature is indicated as meta-data of the dataset.\n    default_target_attribute : str\n        The default target attribute, if it exists.\n        Can have multiple values, comma separated.\n    ignore_attribute : str | list\n        Attributes that should be excluded in modelling,\n        such as identifiers and indexes.\n        Can have multiple values, comma separated.\n    citation : str\n        Reference(s) that should be cited when building on this data.\n    version_label : str, optional\n        Version label provided by user.\n         Can be a date, hash, or some other type of id.\n    row_id_attribute : str, optional\n        The attribute that represents the row-id column, if present in the\n        dataset. If ``data`` is a dataframe and ``row_id_attribute`` is not\n        specified, the index of the dataframe will be used as the\n        ``row_id_attribute``. If the name of the index is ``None``, it will\n        be discarded.\n\n        .. versionadded: 0.8\n            Inference of ``row_id_attribute`` from a dataframe.\n    original_data_url : str, optional\n        For derived data, the url to the original dataset.\n    paper_url : str, optional\n        Link to a paper describing the dataset.\n    update_comment : str, optional\n        An explanation for when the dataset is uploaded.\n\n    Returns\n    -------\n    class:`openml.OpenMLDataset`\n    Dataset description.\n    \"\"\"\n    if isinstance(data, pd.DataFrame):\n        # infer the row id from the index of the dataset\n        if row_id_attribute is None:\n            row_id_attribute = data.index.name\n        # When calling data.values, the index will be skipped.\n        # We need to reset the index such that it is part of the data.\n        if data.index.name is not None:\n            data = data.reset_index()\n\n    if attributes == \"auto\" or isinstance(attributes, dict):\n        if not isinstance(data, pd.DataFrame):\n            raise ValueError(\n                \"Automatically inferring attributes requires \"\n                f\"a pandas DataFrame. A {data!r} was given instead.\",\n            )\n        # infer the type of data for each column of the DataFrame\n        attributes_ = attributes_arff_from_df(data)\n        if isinstance(attributes, dict):\n            # override the attributes which was specified by the user\n            for attr_idx in range(len(attributes_)):\n                attr_name = attributes_[attr_idx][0]\n                if attr_name in attributes:\n                    attributes_[attr_idx] = (attr_name, attributes[attr_name])\n    else:\n        attributes_ = attributes\n    ignore_attributes = _expand_parameter(ignore_attribute)\n    _validated_data_attributes(ignore_attributes, attributes_, \"ignore_attribute\")\n\n    default_target_attributes = _expand_parameter(default_target_attribute)\n    _validated_data_attributes(default_target_attributes, attributes_, \"default_target_attribute\")\n\n    if row_id_attribute is not None:\n        is_row_id_an_attribute = any(attr[0] == row_id_attribute for attr in attributes_)\n        if not is_row_id_an_attribute:\n            raise ValueError(\n                \"'row_id_attribute' should be one of the data attribute. \"\n                f\" Got '{row_id_attribute}' while candidates are\"\n                f\" {[attr[0] for attr in attributes_]}.\",\n            )\n\n    if isinstance(data, pd.DataFrame):\n        if all(isinstance(dtype, pd.SparseDtype) for dtype in data.dtypes):\n            data = data.sparse.to_coo()\n            # liac-arff only support COO matrices with sorted rows\n            row_idx_sorted = np.argsort(data.row)  # type: ignore\n            data.row = data.row[row_idx_sorted]  # type: ignore\n            data.col = data.col[row_idx_sorted]  # type: ignore\n            data.data = data.data[row_idx_sorted]  # type: ignore\n        else:\n            data = data.to_numpy()\n\n    data_format: Literal[\"arff\", \"sparse_arff\"]\n    if isinstance(data, (list, np.ndarray)):\n        if isinstance(data[0], (list, np.ndarray)):\n            data_format = \"arff\"\n        elif isinstance(data[0], dict):\n            data_format = \"sparse_arff\"\n        else:\n            raise ValueError(\n                \"When giving a list or a numpy.ndarray, \"\n                \"they should contain a list/ numpy.ndarray \"\n                \"for dense data or a dictionary for sparse \"\n                f\"data. Got {data[0]!r} instead.\",\n            )\n    elif isinstance(data, coo_matrix):\n        data_format = \"sparse_arff\"\n    else:\n        raise ValueError(\n            \"When giving a list or a numpy.ndarray, \"\n            \"they should contain a list/ numpy.ndarray \"\n            \"for dense data or a dictionary for sparse \"\n            f\"data. Got {data[0]!r} instead.\",\n        )\n\n    arff_object = {\n        \"relation\": name,\n        \"description\": description,\n        \"attributes\": attributes_,\n        \"data\": data,\n    }\n\n    # serializes the ARFF dataset object and returns a string\n    arff_dataset = arff.dumps(arff_object)\n    try:\n        # check if ARFF is valid\n        decoder = arff.ArffDecoder()\n        return_type = arff.COO if data_format == \"sparse_arff\" else arff.DENSE\n        decoder.decode(arff_dataset, encode_nominal=True, return_type=return_type)\n    except arff.ArffException as e:\n        raise ValueError(\n            \"The arguments you have provided do not construct a valid ARFF file\"\n        ) from e\n\n    return OpenMLDataset(\n        name=name,\n        description=description,\n        data_format=data_format,\n        creator=creator,\n        contributor=contributor,\n        collection_date=collection_date,\n        language=language,\n        licence=licence,\n        default_target_attribute=default_target_attribute,\n        row_id_attribute=row_id_attribute,\n        ignore_attribute=ignore_attribute,\n        citation=citation,\n        version_label=version_label,\n        original_data_url=original_data_url,\n        paper_url=paper_url,\n        update_comment=update_comment,\n        dataset=arff_dataset,\n    )\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.data_feature_add_ontology","title":"data_feature_add_ontology","text":"<pre><code>data_feature_add_ontology(data_id: int, index: int, ontology: str) -&gt; bool\n</code></pre> <p>An ontology describes the concept that are described in a feature. An ontology is defined by an URL where the information is provided. Adds an ontology (URL) to a given dataset feature (defined by a dataset id and index). The dataset has to exists on OpenML and needs to have been processed by the evaluation engine.</p> PARAMETER DESCRIPTION <code>data_id</code> <p>id of the dataset to which the feature belongs</p> <p> TYPE: <code>int</code> </p> <code>index</code> <p>index of the feature in dataset (0-based)</p> <p> TYPE: <code>int</code> </p> <code>ontology</code> <p>URL to ontology (max. 256 characters)</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>True or throws an OpenML server exception</code> Source code in <code>openml/datasets/functions.py</code> <pre><code>def data_feature_add_ontology(data_id: int, index: int, ontology: str) -&gt; bool:\n    \"\"\"\n    An ontology describes the concept that are described in a feature. An\n    ontology is defined by an URL where the information is provided. Adds\n    an ontology (URL) to a given dataset feature (defined by a dataset id\n    and index). The dataset has to exists on OpenML and needs to have been\n    processed by the evaluation engine.\n\n    Parameters\n    ----------\n    data_id : int\n        id of the dataset to which the feature belongs\n    index : int\n        index of the feature in dataset (0-based)\n    ontology : str\n        URL to ontology (max. 256 characters)\n\n    Returns\n    -------\n    True or throws an OpenML server exception\n    \"\"\"\n    upload_data: dict[str, int | str] = {\"data_id\": data_id, \"index\": index, \"ontology\": ontology}\n    openml._api_calls._perform_api_call(\"data/feature/ontology/add\", \"post\", data=upload_data)\n    # an error will be thrown in case the request was unsuccessful\n    return True\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.data_feature_remove_ontology","title":"data_feature_remove_ontology","text":"<pre><code>data_feature_remove_ontology(data_id: int, index: int, ontology: str) -&gt; bool\n</code></pre> <p>Removes an existing ontology (URL) from a given dataset feature (defined by a dataset id and index). The dataset has to exists on OpenML and needs to have been processed by the evaluation engine. Ontology needs to be attached to the specific fearure.</p> PARAMETER DESCRIPTION <code>data_id</code> <p>id of the dataset to which the feature belongs</p> <p> TYPE: <code>int</code> </p> <code>index</code> <p>index of the feature in dataset (0-based)</p> <p> TYPE: <code>int</code> </p> <code>ontology</code> <p>URL to ontology (max. 256 characters)</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>True or throws an OpenML server exception</code> Source code in <code>openml/datasets/functions.py</code> <pre><code>def data_feature_remove_ontology(data_id: int, index: int, ontology: str) -&gt; bool:\n    \"\"\"\n    Removes an existing ontology (URL) from a given dataset feature (defined\n    by a dataset id and index). The dataset has to exists on OpenML and needs\n    to have been processed by the evaluation engine. Ontology needs to be\n    attached to the specific fearure.\n\n    Parameters\n    ----------\n    data_id : int\n        id of the dataset to which the feature belongs\n    index : int\n        index of the feature in dataset (0-based)\n    ontology : str\n        URL to ontology (max. 256 characters)\n\n    Returns\n    -------\n    True or throws an OpenML server exception\n    \"\"\"\n    upload_data: dict[str, int | str] = {\"data_id\": data_id, \"index\": index, \"ontology\": ontology}\n    openml._api_calls._perform_api_call(\"data/feature/ontology/remove\", \"post\", data=upload_data)\n    # an error will be thrown in case the request was unsuccessful\n    return True\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.delete_dataset","title":"delete_dataset","text":"<pre><code>delete_dataset(dataset_id: int) -&gt; bool\n</code></pre> <p>Delete dataset with id <code>dataset_id</code> from the OpenML server.</p> <p>This can only be done if you are the owner of the dataset and no tasks are attached to the dataset.</p> PARAMETER DESCRIPTION <code>dataset_id</code> <p>OpenML id of the dataset</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the deletion was successful. False otherwise.</p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def delete_dataset(dataset_id: int) -&gt; bool:\n    \"\"\"Delete dataset with id `dataset_id` from the OpenML server.\n\n    This can only be done if you are the owner of the dataset and\n    no tasks are attached to the dataset.\n\n    Parameters\n    ----------\n    dataset_id : int\n        OpenML id of the dataset\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"data\", dataset_id)\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.edit_dataset","title":"edit_dataset","text":"<pre><code>edit_dataset(data_id: int, description: str | None = None, creator: str | None = None, contributor: str | None = None, collection_date: str | None = None, language: str | None = None, default_target_attribute: str | None = None, ignore_attribute: str | list[str] | None = None, citation: str | None = None, row_id_attribute: str | None = None, original_data_url: str | None = None, paper_url: str | None = None) -&gt; int\n</code></pre> <p>Edits an OpenMLDataset.</p> <p>In addition to providing the dataset id of the dataset to edit (through data_id), you must specify a value for at least one of the optional function arguments, i.e. one value for a field to edit.</p> <p>This function allows editing of both non-critical and critical fields. Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.</p> <ul> <li>Editing non-critical data fields is allowed for all authenticated users.</li> <li>Editing critical fields is allowed only for the owner, provided there are no tasks    associated with this dataset.</li> </ul> <p>If dataset has tasks or if the user is not the owner, the only way to edit critical fields is to use fork_dataset followed by edit_dataset.</p> PARAMETER DESCRIPTION <code>data_id</code> <p>ID of the dataset.</p> <p> TYPE: <code>int</code> </p> <code>description</code> <p>Description of the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>creator</code> <p>The person who created the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>contributor</code> <p>People who contributed to the current version of the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>collection_date</code> <p>The date the data was originally collected, given by the uploader.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>language</code> <p>Language in which the data is represented. Starts with 1 upper case letter, rest lower case, e.g. 'English'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>default_target_attribute</code> <p>The default target attribute, if it exists. Can have multiple values, comma separated.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>ignore_attribute</code> <p>Attributes that should be excluded in modelling, such as identifiers and indexes.</p> <p> TYPE: <code>str | list</code> DEFAULT: <code>None</code> </p> <code>citation</code> <p>Reference(s) that should be cited when building on this data.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>row_id_attribute</code> <p>The attribute that represents the row-id column, if present in the dataset. If <code>data</code> is a dataframe and <code>row_id_attribute</code> is not specified, the index of the dataframe will be used as the <code>row_id_attribute</code>. If the name of the index is <code>None</code>, it will be discarded.</p> <p>.. versionadded: 0.8     Inference of <code>row_id_attribute</code> from a dataframe.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>original_data_url</code> <p>For derived data, the url to the original dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>paper_url</code> <p>Link to a paper describing the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dataset id</code> Source code in <code>openml/datasets/functions.py</code> <pre><code>def edit_dataset(\n    data_id: int,\n    description: str | None = None,\n    creator: str | None = None,\n    contributor: str | None = None,\n    collection_date: str | None = None,\n    language: str | None = None,\n    default_target_attribute: str | None = None,\n    ignore_attribute: str | list[str] | None = None,\n    citation: str | None = None,\n    row_id_attribute: str | None = None,\n    original_data_url: str | None = None,\n    paper_url: str | None = None,\n) -&gt; int:\n    \"\"\"Edits an OpenMLDataset.\n\n    In addition to providing the dataset id of the dataset to edit (through data_id),\n    you must specify a value for at least one of the optional function arguments,\n    i.e. one value for a field to edit.\n\n    This function allows editing of both non-critical and critical fields.\n    Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.\n\n     - Editing non-critical data fields is allowed for all authenticated users.\n     - Editing critical fields is allowed only for the owner, provided there are no tasks\n       associated with this dataset.\n\n    If dataset has tasks or if the user is not the owner, the only way\n    to edit critical fields is to use fork_dataset followed by edit_dataset.\n\n    Parameters\n    ----------\n    data_id : int\n        ID of the dataset.\n    description : str\n        Description of the dataset.\n    creator : str\n        The person who created the dataset.\n    contributor : str\n        People who contributed to the current version of the dataset.\n    collection_date : str\n        The date the data was originally collected, given by the uploader.\n    language : str\n        Language in which the data is represented.\n        Starts with 1 upper case letter, rest lower case, e.g. 'English'.\n    default_target_attribute : str\n        The default target attribute, if it exists.\n        Can have multiple values, comma separated.\n    ignore_attribute : str | list\n        Attributes that should be excluded in modelling,\n        such as identifiers and indexes.\n    citation : str\n        Reference(s) that should be cited when building on this data.\n    row_id_attribute : str, optional\n        The attribute that represents the row-id column, if present in the\n        dataset. If ``data`` is a dataframe and ``row_id_attribute`` is not\n        specified, the index of the dataframe will be used as the\n        ``row_id_attribute``. If the name of the index is ``None``, it will\n        be discarded.\n\n        .. versionadded: 0.8\n            Inference of ``row_id_attribute`` from a dataframe.\n    original_data_url : str, optional\n        For derived data, the url to the original dataset.\n    paper_url : str, optional\n        Link to a paper describing the dataset.\n\n    Returns\n    -------\n    Dataset id\n    \"\"\"\n    if not isinstance(data_id, int):\n        raise TypeError(f\"`data_id` must be of type `int`, not {type(data_id)}.\")\n\n    # compose data edit parameters as xml\n    form_data = {\"data_id\": data_id}  # type: openml._api_calls.DATA_TYPE\n    xml = OrderedDict()  # type: 'OrderedDict[str, OrderedDict]'\n    xml[\"oml:data_edit_parameters\"] = OrderedDict()\n    xml[\"oml:data_edit_parameters\"][\"@xmlns:oml\"] = \"http://openml.org/openml\"\n    xml[\"oml:data_edit_parameters\"][\"oml:description\"] = description\n    xml[\"oml:data_edit_parameters\"][\"oml:creator\"] = creator\n    xml[\"oml:data_edit_parameters\"][\"oml:contributor\"] = contributor\n    xml[\"oml:data_edit_parameters\"][\"oml:collection_date\"] = collection_date\n    xml[\"oml:data_edit_parameters\"][\"oml:language\"] = language\n    xml[\"oml:data_edit_parameters\"][\"oml:default_target_attribute\"] = default_target_attribute\n    xml[\"oml:data_edit_parameters\"][\"oml:row_id_attribute\"] = row_id_attribute\n    xml[\"oml:data_edit_parameters\"][\"oml:ignore_attribute\"] = ignore_attribute\n    xml[\"oml:data_edit_parameters\"][\"oml:citation\"] = citation\n    xml[\"oml:data_edit_parameters\"][\"oml:original_data_url\"] = original_data_url\n    xml[\"oml:data_edit_parameters\"][\"oml:paper_url\"] = paper_url\n\n    # delete None inputs\n    for k in list(xml[\"oml:data_edit_parameters\"]):\n        if not xml[\"oml:data_edit_parameters\"][k]:\n            del xml[\"oml:data_edit_parameters\"][k]\n\n    file_elements = {\n        \"edit_parameters\": (\"description.xml\", xmltodict.unparse(xml)),\n    }  # type: openml._api_calls.FILE_ELEMENTS_TYPE\n    result_xml = openml._api_calls._perform_api_call(\n        \"data/edit\",\n        \"post\",\n        data=form_data,\n        file_elements=file_elements,\n    )\n    result = xmltodict.parse(result_xml)\n    data_id = result[\"oml:data_edit\"][\"oml:id\"]\n    return int(data_id)\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.fork_dataset","title":"fork_dataset","text":"<pre><code>fork_dataset(data_id: int) -&gt; int\n</code></pre> <p>Creates a new dataset version, with the authenticated user as the new owner.  The forked dataset can have distinct dataset meta-data,  but the actual data itself is shared with the original version.</p> <p>This API is intended for use when a user is unable to edit the critical fields of a dataset  through the edit_dataset API.  (Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.)</p> <p>Specifically, this happens when the user is:         1. Not the owner of the dataset.         2. User is the owner of the dataset, but the dataset has tasks.</p> <p>In these two cases the only way to edit critical fields is:         1. STEP 1: Fork the dataset using fork_dataset API         2. STEP 2: Call edit_dataset API on the forked version.</p> PARAMETER DESCRIPTION <code>data_id</code> <p>id of the dataset to be forked</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Dataset id of the forked dataset</code> Source code in <code>openml/datasets/functions.py</code> <pre><code>def fork_dataset(data_id: int) -&gt; int:\n    \"\"\"\n     Creates a new dataset version, with the authenticated user as the new owner.\n     The forked dataset can have distinct dataset meta-data,\n     but the actual data itself is shared with the original version.\n\n     This API is intended for use when a user is unable to edit the critical fields of a dataset\n     through the edit_dataset API.\n     (Critical fields are default_target_attribute, ignore_attribute, row_id_attribute.)\n\n     Specifically, this happens when the user is:\n            1. Not the owner of the dataset.\n            2. User is the owner of the dataset, but the dataset has tasks.\n\n     In these two cases the only way to edit critical fields is:\n            1. STEP 1: Fork the dataset using fork_dataset API\n            2. STEP 2: Call edit_dataset API on the forked version.\n\n\n    Parameters\n    ----------\n    data_id : int\n        id of the dataset to be forked\n\n    Returns\n    -------\n    Dataset id of the forked dataset\n\n    \"\"\"\n    if not isinstance(data_id, int):\n        raise TypeError(f\"`data_id` must be of type `int`, not {type(data_id)}.\")\n    # compose data fork parameters\n    form_data = {\"data_id\": data_id}  # type: openml._api_calls.DATA_TYPE\n    result_xml = openml._api_calls._perform_api_call(\"data/fork\", \"post\", data=form_data)\n    result = xmltodict.parse(result_xml)\n    data_id = result[\"oml:data_fork\"][\"oml:id\"]\n    return int(data_id)\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(dataset_id: int | str, download_data: bool = False, version: int | None = None, error_if_multiple: bool = False, cache_format: Literal['pickle', 'feather'] = 'pickle', download_qualities: bool = False, download_features_meta_data: bool = False, download_all_files: bool = False, force_refresh_cache: bool = False) -&gt; OpenMLDataset\n</code></pre> <p>Download the OpenML dataset representation, optionally also download actual data file.</p> <p>This function is by default NOT thread/multiprocessing safe, as this function uses caching. A check will be performed to determine if the information has previously been downloaded to a cache, and if so be loaded from disk instead of retrieved from the server.</p> <p>To make this function thread safe, you can install the python package <code>oslo.concurrency</code>. If <code>oslo.concurrency</code> is installed <code>get_dataset</code> becomes thread safe.</p> <p>Alternatively, to make this function thread/multiprocessing safe initialize the cache first by calling <code>get_dataset(args)</code> once before calling <code>get_dataset(args)</code> many times in parallel. This will initialize the cache and later calls will use the cache in a thread/multiprocessing safe way.</p> <p>If dataset is retrieved by name, a version may be specified. If no version is specified and multiple versions of the dataset exist, the earliest version of the dataset that is still active will be returned. If no version is specified, multiple versions of the dataset exist and <code>exception_if_multiple</code> is set to <code>True</code>, this function will raise an exception.</p> PARAMETER DESCRIPTION <code>dataset_id</code> <p>Dataset ID (integer) or dataset name (string) of the dataset to download.</p> <p> TYPE: <code>int or str</code> </p> <code>download_data</code> <p>If True, also download the data file. Beware that some datasets are large and it might make the operation noticeably slower. Metadata is also still retrieved. If False, create the OpenMLDataset and only populate it with the metadata. The data may later be retrieved through the <code>OpenMLDataset.get_data</code> method.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>version</code> <p>Specifies the version if <code>dataset_id</code> is specified by name. If no version is specified, retrieve the least recent still active version.</p> <p> TYPE: <code>(int, optional(default=None))</code> DEFAULT: <code>None</code> </p> <code>error_if_multiple</code> <p>If <code>True</code> raise an error if multiple datasets are found with matching criteria.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>cache_format</code> <p>Format for caching the dataset - may be feather or pickle Note that the default 'pickle' option may load slower than feather when no.of.rows is very high.</p> <p> TYPE: <code>str(default='pickle') in {'pickle', 'feather'}</code> DEFAULT: <code>'pickle'</code> </p> <code>download_qualities</code> <p>Option to download 'qualities' meta-data in addition to the minimal dataset description. If True, download and cache the qualities file. If False, create the OpenMLDataset without qualities metadata. The data may later be added to the OpenMLDataset through the <code>OpenMLDataset.load_metadata(qualities=True)</code> method.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>download_features_meta_data</code> <p>Option to download 'features' meta-data in addition to the minimal dataset description. If True, download and cache the features file. If False, create the OpenMLDataset without features metadata. The data may later be added to the OpenMLDataset through the <code>OpenMLDataset.load_metadata(features=True)</code> method.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>download_all_files</code> <p>EXPERIMENTAL. Download all files related to the dataset that reside on the server. Useful for datasets which refer to auxiliary files (e.g., meta-album).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>force_refresh_cache</code> <p>Force the cache to refreshed by deleting the cache directory and re-downloading the data. Note, if <code>force_refresh_cache</code> is True, <code>get_dataset</code> is NOT thread/multiprocessing safe, because this creates a race condition to creating and deleting the cache; as in general with the cache.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dataset</code> <p>The downloaded dataset.</p> <p> TYPE: <code>:class:`openml.OpenMLDataset`</code> </p> Source code in <code>openml/datasets/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_dataset(  # noqa: C901, PLR0912\n    dataset_id: int | str,\n    download_data: bool = False,  # noqa: FBT002, FBT001\n    version: int | None = None,\n    error_if_multiple: bool = False,  # noqa: FBT002, FBT001\n    cache_format: Literal[\"pickle\", \"feather\"] = \"pickle\",\n    download_qualities: bool = False,  # noqa: FBT002, FBT001\n    download_features_meta_data: bool = False,  # noqa: FBT002, FBT001\n    download_all_files: bool = False,  # noqa: FBT002, FBT001\n    force_refresh_cache: bool = False,  # noqa: FBT001, FBT002\n) -&gt; OpenMLDataset:\n    \"\"\"Download the OpenML dataset representation, optionally also download actual data file.\n\n    This function is by default NOT thread/multiprocessing safe, as this function uses caching.\n    A check will be performed to determine if the information has previously been downloaded to a\n    cache, and if so be loaded from disk instead of retrieved from the server.\n\n    To make this function thread safe, you can install the python package ``oslo.concurrency``.\n    If ``oslo.concurrency`` is installed `get_dataset` becomes thread safe.\n\n    Alternatively, to make this function thread/multiprocessing safe initialize the cache first by\n    calling `get_dataset(args)` once before calling `get_dataset(args)` many times in parallel.\n    This will initialize the cache and later calls will use the cache in a thread/multiprocessing\n    safe way.\n\n    If dataset is retrieved by name, a version may be specified.\n    If no version is specified and multiple versions of the dataset exist,\n    the earliest version of the dataset that is still active will be returned.\n    If no version is specified, multiple versions of the dataset exist and\n    ``exception_if_multiple`` is set to ``True``, this function will raise an exception.\n\n    Parameters\n    ----------\n    dataset_id : int or str\n        Dataset ID (integer) or dataset name (string) of the dataset to download.\n    download_data : bool (default=False)\n        If True, also download the data file. Beware that some datasets are large and it might\n        make the operation noticeably slower. Metadata is also still retrieved.\n        If False, create the OpenMLDataset and only populate it with the metadata.\n        The data may later be retrieved through the `OpenMLDataset.get_data` method.\n    version : int, optional (default=None)\n        Specifies the version if `dataset_id` is specified by name.\n        If no version is specified, retrieve the least recent still active version.\n    error_if_multiple : bool (default=False)\n        If ``True`` raise an error if multiple datasets are found with matching criteria.\n    cache_format : str (default='pickle') in {'pickle', 'feather'}\n        Format for caching the dataset - may be feather or pickle\n        Note that the default 'pickle' option may load slower than feather when\n        no.of.rows is very high.\n    download_qualities : bool (default=False)\n        Option to download 'qualities' meta-data in addition to the minimal dataset description.\n        If True, download and cache the qualities file.\n        If False, create the OpenMLDataset without qualities metadata. The data may later be added\n        to the OpenMLDataset through the `OpenMLDataset.load_metadata(qualities=True)` method.\n    download_features_meta_data : bool (default=False)\n        Option to download 'features' meta-data in addition to the minimal dataset description.\n        If True, download and cache the features file.\n        If False, create the OpenMLDataset without features metadata. The data may later be added\n        to the OpenMLDataset through the `OpenMLDataset.load_metadata(features=True)` method.\n    download_all_files: bool (default=False)\n        EXPERIMENTAL. Download all files related to the dataset that reside on the server.\n        Useful for datasets which refer to auxiliary files (e.g., meta-album).\n    force_refresh_cache : bool (default=False)\n        Force the cache to refreshed by deleting the cache directory and re-downloading the data.\n        Note, if `force_refresh_cache` is True, `get_dataset` is NOT thread/multiprocessing safe,\n        because this creates a race condition to creating and deleting the cache; as in general with\n        the cache.\n\n    Returns\n    -------\n    dataset : :class:`openml.OpenMLDataset`\n        The downloaded dataset.\n    \"\"\"\n    if download_all_files:\n        warnings.warn(\n            \"``download_all_files`` is experimental and is likely to break with new releases.\",\n            FutureWarning,\n            stacklevel=2,\n        )\n\n    if cache_format not in [\"feather\", \"pickle\"]:\n        raise ValueError(\n            \"cache_format must be one of 'feather' or 'pickle. \"\n            f\"Invalid format specified: {cache_format}\",\n        )\n\n    if isinstance(dataset_id, str):\n        try:\n            dataset_id = int(dataset_id)\n        except ValueError:\n            dataset_id = _name_to_id(dataset_id, version, error_if_multiple)  # type: ignore\n    elif not isinstance(dataset_id, int):\n        raise TypeError(\n            f\"`dataset_id` must be one of `str` or `int`, not {type(dataset_id)}.\",\n        )\n\n    if force_refresh_cache:\n        did_cache_dir = _get_cache_dir_for_id(DATASETS_CACHE_DIR_NAME, dataset_id)\n        if did_cache_dir.exists():\n            _remove_cache_dir_for_id(DATASETS_CACHE_DIR_NAME, did_cache_dir)\n\n    did_cache_dir = _create_cache_directory_for_id(\n        DATASETS_CACHE_DIR_NAME,\n        dataset_id,\n    )\n\n    remove_dataset_cache = True\n    try:\n        description = _get_dataset_description(did_cache_dir, dataset_id)\n        features_file = None\n        qualities_file = None\n\n        if download_features_meta_data:\n            features_file = _get_dataset_features_file(did_cache_dir, dataset_id)\n        if download_qualities:\n            qualities_file = _get_dataset_qualities_file(did_cache_dir, dataset_id)\n\n        parquet_file = None\n        skip_parquet = os.environ.get(OPENML_SKIP_PARQUET_ENV_VAR, \"false\").casefold() == \"true\"\n        download_parquet = \"oml:parquet_url\" in description and not skip_parquet\n        if download_parquet and (download_data or download_all_files):\n            try:\n                parquet_file = _get_dataset_parquet(\n                    description,\n                    download_all_files=download_all_files,\n                )\n            except urllib3.exceptions.MaxRetryError:\n                parquet_file = None\n\n        arff_file = None\n        if parquet_file is None and download_data:\n            if download_parquet:\n                logger.warning(\"Failed to download parquet, fallback on ARFF.\")\n            arff_file = _get_dataset_arff(description)\n\n        remove_dataset_cache = False\n    except OpenMLServerException as e:\n        # if there was an exception\n        # check if the user had access to the dataset\n        if e.code == NO_ACCESS_GRANTED_ERRCODE:\n            raise OpenMLPrivateDatasetError(e.message) from None\n\n        raise e\n    finally:\n        if remove_dataset_cache:\n            _remove_cache_dir_for_id(DATASETS_CACHE_DIR_NAME, did_cache_dir)\n\n    return _create_dataset_from_description(\n        description,\n        features_file,\n        qualities_file,\n        arff_file,\n        parquet_file,\n        cache_format,\n    )\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.get_datasets","title":"get_datasets","text":"<pre><code>get_datasets(dataset_ids: list[str | int], download_data: bool = False, download_qualities: bool = False) -&gt; list[OpenMLDataset]\n</code></pre> <p>Download datasets.</p> <p>This function iterates :meth:<code>openml.datasets.get_dataset</code>.</p> PARAMETER DESCRIPTION <code>dataset_ids</code> <p>Integers or strings representing dataset ids or dataset names. If dataset names are specified, the least recent still active dataset version is returned.</p> <p> TYPE: <code>iterable</code> </p> <code>download_data</code> <p>If True, also download the data file. Beware that some datasets are large and it might make the operation noticeably slower. Metadata is also still retrieved. If False, create the OpenMLDataset and only populate it with the metadata. The data may later be retrieved through the <code>OpenMLDataset.get_data</code> method.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>download_qualities</code> <p>If True, also download qualities.xml file. If False it skip the qualities.xml.</p> <p> TYPE: <code>(bool, optional(default=True))</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>datasets</code> <p>A list of dataset objects.</p> <p> TYPE: <code>list of datasets</code> </p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def get_datasets(\n    dataset_ids: list[str | int],\n    download_data: bool = False,  # noqa: FBT001, FBT002\n    download_qualities: bool = False,  # noqa: FBT001, FBT002\n) -&gt; list[OpenMLDataset]:\n    \"\"\"Download datasets.\n\n    This function iterates :meth:`openml.datasets.get_dataset`.\n\n    Parameters\n    ----------\n    dataset_ids : iterable\n        Integers or strings representing dataset ids or dataset names.\n        If dataset names are specified, the least recent still active dataset version is returned.\n    download_data : bool, optional\n        If True, also download the data file. Beware that some datasets are large and it might\n        make the operation noticeably slower. Metadata is also still retrieved.\n        If False, create the OpenMLDataset and only populate it with the metadata.\n        The data may later be retrieved through the `OpenMLDataset.get_data` method.\n    download_qualities : bool, optional (default=True)\n        If True, also download qualities.xml file. If False it skip the qualities.xml.\n\n    Returns\n    -------\n    datasets : list of datasets\n        A list of dataset objects.\n    \"\"\"\n    datasets = []\n    for dataset_id in dataset_ids:\n        datasets.append(\n            get_dataset(dataset_id, download_data, download_qualities=download_qualities),\n        )\n    return datasets\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.list_datasets","title":"list_datasets","text":"<pre><code>list_datasets(data_id: list[int] | None = None, offset: int | None = None, size: int | None = None, status: str | None = None, tag: str | None = None, data_name: str | None = None, data_version: int | None = None, number_instances: int | str | None = None, number_features: int | str | None = None, number_classes: int | str | None = None, number_missing_values: int | str | None = None) -&gt; DataFrame\n</code></pre> <p>Return a dataframe of all dataset which are on OpenML.</p> <p>Supports large amount of results.</p> PARAMETER DESCRIPTION <code>data_id</code> <p>A list of data ids, to specify which datasets should be listed</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>offset</code> <p>The number of datasets to skip, starting from the first.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>The maximum number of datasets to show.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>status</code> <p>Should be {active, in_preparation, deactivated}. By default active datasets are returned, but also datasets from another status can be requested.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_name</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_version</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>number_instances</code> <p> TYPE: <code>int | str</code> DEFAULT: <code>None</code> </p> <code>number_features</code> <p> TYPE: <code>int | str</code> DEFAULT: <code>None</code> </p> <code>number_classes</code> <p> TYPE: <code>int | str</code> DEFAULT: <code>None</code> </p> <code>number_missing_values</code> <p> TYPE: <code>int | str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>datasets</code> <p>Each row maps to a dataset Each column contains the following information: - dataset id - name - format - status If qualities are calculated for the dataset, some of these are also included as columns.</p> <p> TYPE: <code>dataframe</code> </p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def list_datasets(\n    data_id: list[int] | None = None,\n    offset: int | None = None,\n    size: int | None = None,\n    status: str | None = None,\n    tag: str | None = None,\n    data_name: str | None = None,\n    data_version: int | None = None,\n    number_instances: int | str | None = None,\n    number_features: int | str | None = None,\n    number_classes: int | str | None = None,\n    number_missing_values: int | str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Return a dataframe of all dataset which are on OpenML.\n\n    Supports large amount of results.\n\n    Parameters\n    ----------\n    data_id : list, optional\n        A list of data ids, to specify which datasets should be\n        listed\n    offset : int, optional\n        The number of datasets to skip, starting from the first.\n    size : int, optional\n        The maximum number of datasets to show.\n    status : str, optional\n        Should be {active, in_preparation, deactivated}. By\n        default active datasets are returned, but also datasets\n        from another status can be requested.\n    tag : str, optional\n    data_name : str, optional\n    data_version : int, optional\n    number_instances : int | str, optional\n    number_features : int | str, optional\n    number_classes : int | str, optional\n    number_missing_values : int | str, optional\n\n    Returns\n    -------\n    datasets: dataframe\n        Each row maps to a dataset\n        Each column contains the following information:\n        - dataset id\n        - name\n        - format\n        - status\n        If qualities are calculated for the dataset, some of\n        these are also included as columns.\n    \"\"\"\n    listing_call = partial(\n        _list_datasets,\n        data_id=data_id,\n        status=status,\n        tag=tag,\n        data_name=data_name,\n        data_version=data_version,\n        number_instances=number_instances,\n        number_features=number_features,\n        number_classes=number_classes,\n        number_missing_values=number_missing_values,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.list_qualities","title":"list_qualities","text":"<pre><code>list_qualities() -&gt; list[str]\n</code></pre> <p>Return list of data qualities available.</p> <p>The function performs an API call to retrieve the entire list of data qualities that are computed on the datasets uploaded.</p> RETURNS DESCRIPTION <code>list</code> Source code in <code>openml/datasets/functions.py</code> <pre><code>def list_qualities() -&gt; list[str]:\n    \"\"\"Return list of data qualities available.\n\n    The function performs an API call to retrieve the entire list of\n    data qualities that are computed on the datasets uploaded.\n\n    Returns\n    -------\n    list\n    \"\"\"\n    api_call = \"data/qualities/list\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    qualities = xmltodict.parse(xml_string, force_list=(\"oml:quality\"))\n    # Minimalistic check if the XML is useful\n    if \"oml:data_qualities_list\" not in qualities:\n        raise ValueError('Error in return XML, does not contain \"oml:data_qualities_list\"')\n\n    if not isinstance(qualities[\"oml:data_qualities_list\"][\"oml:quality\"], list):\n        raise TypeError('Error in return XML, does not contain \"oml:quality\" as a list')\n\n    return qualities[\"oml:data_qualities_list\"][\"oml:quality\"]\n</code></pre>"},{"location":"reference/datasets/functions/#openml.datasets.functions.status_update","title":"status_update","text":"<pre><code>status_update(data_id: int, status: Literal['active', 'deactivated']) -&gt; None\n</code></pre> <p>Updates the status of a dataset to either 'active' or 'deactivated'. Please see the OpenML API documentation for a description of the status and all legal status transitions: docs.openml.org/concepts/data/#dataset-status</p> PARAMETER DESCRIPTION <code>data_id</code> <p>The data id of the dataset</p> <p> TYPE: <code>int</code> </p> <code>status</code> <p>'active' or 'deactivated'</p> <p> TYPE: <code>(str)</code> </p> Source code in <code>openml/datasets/functions.py</code> <pre><code>def status_update(data_id: int, status: Literal[\"active\", \"deactivated\"]) -&gt; None:\n    \"\"\"\n    Updates the status of a dataset to either 'active' or 'deactivated'.\n    Please see the OpenML API documentation for a description of the status\n    and all legal status transitions:\n    https://docs.openml.org/concepts/data/#dataset-status\n\n    Parameters\n    ----------\n    data_id : int\n        The data id of the dataset\n    status : str,\n        'active' or 'deactivated'\n    \"\"\"\n    legal_status = {\"active\", \"deactivated\"}\n    if status not in legal_status:\n        raise ValueError(f\"Illegal status value. Legal values: {legal_status}\")\n\n    data: openml._api_calls.DATA_TYPE = {\"data_id\": data_id, \"status\": status}\n    result_xml = openml._api_calls._perform_api_call(\"data/status/update\", \"post\", data=data)\n    result = xmltodict.parse(result_xml)\n    server_data_id = result[\"oml:data_status_update\"][\"oml:id\"]\n    server_status = result[\"oml:data_status_update\"][\"oml:status\"]\n    if status != server_status or int(data_id) != int(server_data_id):\n        # This should never happen\n        raise ValueError(\"Data id/status does not collide\")\n</code></pre>"},{"location":"reference/evaluations/","title":"evaluations","text":""},{"location":"reference/evaluations/#openml.evaluations","title":"openml.evaluations","text":""},{"location":"reference/evaluations/#openml.evaluations.OpenMLEvaluation","title":"OpenMLEvaluation","text":"<pre><code>OpenMLEvaluation(run_id: int, task_id: int, setup_id: int, flow_id: int, flow_name: str, data_id: int, data_name: str, function: str, upload_time: str, uploader: int, uploader_name: str, value: float | None, values: list[float] | None, array_data: str | None = None)\n</code></pre> <p>Contains all meta-information about a run / evaluation combination, according to the evaluation/list function</p> PARAMETER DESCRIPTION <code>run_id</code> <p>Refers to the run.</p> <p> TYPE: <code>int</code> </p> <code>task_id</code> <p>Refers to the task.</p> <p> TYPE: <code>int</code> </p> <code>setup_id</code> <p>Refers to the setup.</p> <p> TYPE: <code>int</code> </p> <code>flow_id</code> <p>Refers to the flow.</p> <p> TYPE: <code>int</code> </p> <code>flow_name</code> <p>Name of the referred flow.</p> <p> TYPE: <code>str</code> </p> <code>data_id</code> <p>Refers to the dataset.</p> <p> TYPE: <code>int</code> </p> <code>data_name</code> <p>The name of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>function</code> <p>The evaluation metric of this item (e.g., accuracy).</p> <p> TYPE: <code>str</code> </p> <code>upload_time</code> <p>The time of evaluation.</p> <p> TYPE: <code>str</code> </p> <code>uploader</code> <p>Uploader ID (user ID)</p> <p> TYPE: <code>int</code> </p> <code>upload_name</code> <p>Name of the uploader of this evaluation</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>The value (score) of this evaluation.</p> <p> TYPE: <code>float</code> </p> <code>values</code> <p>The values (scores) per repeat and fold (if requested)</p> <p> TYPE: <code>List[float]</code> </p> <code>array_data</code> <p>list of information per class. (e.g., in case of precision, auroc, recall)</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/evaluations/evaluation.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    run_id: int,\n    task_id: int,\n    setup_id: int,\n    flow_id: int,\n    flow_name: str,\n    data_id: int,\n    data_name: str,\n    function: str,\n    upload_time: str,\n    uploader: int,\n    uploader_name: str,\n    value: float | None,\n    values: list[float] | None,\n    array_data: str | None = None,\n):\n    self.run_id = run_id\n    self.task_id = task_id\n    self.setup_id = setup_id\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.data_id = data_id\n    self.data_name = data_name\n    self.function = function\n    self.upload_time = upload_time\n    self.uploader = uploader\n    self.uploader_name = uploader_name\n    self.value = value\n    self.values = values\n    self.array_data = array_data\n</code></pre>"},{"location":"reference/evaluations/#openml.evaluations.list_evaluation_measures","title":"list_evaluation_measures","text":"<pre><code>list_evaluation_measures() -&gt; list[str]\n</code></pre> <p>Return list of evaluation measures available.</p> <p>The function performs an API call to retrieve the entire list of evaluation measures that are available.</p> RETURNS DESCRIPTION <code>list</code> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_evaluation_measures() -&gt; list[str]:\n    \"\"\"Return list of evaluation measures available.\n\n    The function performs an API call to retrieve the entire list of\n    evaluation measures that are available.\n\n    Returns\n    -------\n    list\n\n    \"\"\"\n    api_call = \"evaluationmeasure/list\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    qualities = xmltodict.parse(xml_string, force_list=(\"oml:measures\"))\n    # Minimalistic check if the XML is useful\n    if \"oml:evaluation_measures\" not in qualities:\n        raise ValueError('Error in return XML, does not contain \"oml:evaluation_measures\"')\n\n    if not isinstance(qualities[\"oml:evaluation_measures\"][\"oml:measures\"][0][\"oml:measure\"], list):\n        raise TypeError('Error in return XML, does not contain \"oml:measure\" as a list')\n\n    return qualities[\"oml:evaluation_measures\"][\"oml:measures\"][0][\"oml:measure\"]\n</code></pre>"},{"location":"reference/evaluations/#openml.evaluations.list_evaluations","title":"list_evaluations","text":"<pre><code>list_evaluations(function: str, offset: int | None = None, size: int | None = None, tasks: list[str | int] | None = None, setups: list[str | int] | None = None, flows: list[str | int] | None = None, runs: list[str | int] | None = None, uploaders: list[str | int] | None = None, tag: str | None = None, study: int | None = None, per_fold: bool | None = None, sort_order: str | None = None, output_format: Literal['object', 'dataframe'] = 'object') -&gt; dict[int, OpenMLEvaluation] | DataFrame\n</code></pre> <p>List all run-evaluation pairs matching all of the given filters.</p> <p>(Supports large amount of results)</p> PARAMETER DESCRIPTION <code>function</code> <p>the evaluation function. e.g., predictive_accuracy</p> <p> TYPE: <code>str</code> </p> <code>offset</code> <p>the number of runs to skip, starting from the first</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>The maximum number of runs to show. If set to <code>None</code>, it returns all the results.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>tasks</code> <p>the list of task IDs</p> <p> TYPE: <code>list[int, str]</code> DEFAULT: <code>None</code> </p> <code>setups</code> <p>the list of setup IDs</p> <p> TYPE: <code>list[str | int] | None</code> DEFAULT: <code>None</code> </p> <code>flows</code> <p>the list of flow IDs</p> <p> TYPE: <code>list[int, str]</code> DEFAULT: <code>None</code> </p> <code>runs</code> <p>the list of run IDs</p> <p> TYPE: <code>list[str | int] | None</code> DEFAULT: <code>None</code> </p> <code>uploaders</code> <p>the list of uploader IDs</p> <p> TYPE: <code>list[int, str]</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p>filter evaluation based on given tag</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>study</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>per_fold</code> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>sort_order</code> <p>order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>output_format</code> <p>The parameter decides the format of the output. - If 'object' the output is a dict of OpenMLEvaluation objects - If 'dataframe' the output is a pandas DataFrame</p> <p> TYPE: <code>Literal['object', 'dataframe']</code> DEFAULT: <code>'object'</code> </p> RETURNS DESCRIPTION <code>dict or dataframe</code> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_evaluations(\n    function: str,\n    offset: int | None = None,\n    size: int | None = None,\n    tasks: list[str | int] | None = None,\n    setups: list[str | int] | None = None,\n    flows: list[str | int] | None = None,\n    runs: list[str | int] | None = None,\n    uploaders: list[str | int] | None = None,\n    tag: str | None = None,\n    study: int | None = None,\n    per_fold: bool | None = None,\n    sort_order: str | None = None,\n    output_format: Literal[\"object\", \"dataframe\"] = \"object\",\n) -&gt; dict[int, OpenMLEvaluation] | pd.DataFrame:\n    \"\"\"List all run-evaluation pairs matching all of the given filters.\n\n    (Supports large amount of results)\n\n    Parameters\n    ----------\n    function : str\n        the evaluation function. e.g., predictive_accuracy\n    offset : int, optional\n        the number of runs to skip, starting from the first\n    size : int, default 10000\n        The maximum number of runs to show.\n        If set to ``None``, it returns all the results.\n\n    tasks : list[int,str], optional\n        the list of task IDs\n    setups: list[int,str], optional\n        the list of setup IDs\n    flows : list[int,str], optional\n        the list of flow IDs\n    runs :list[int,str], optional\n        the list of run IDs\n    uploaders : list[int,str], optional\n        the list of uploader IDs\n    tag : str, optional\n        filter evaluation based on given tag\n\n    study : int, optional\n\n    per_fold : bool, optional\n\n    sort_order : str, optional\n       order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")\n\n    output_format: str, optional (default='object')\n        The parameter decides the format of the output.\n        - If 'object' the output is a dict of OpenMLEvaluation objects\n        - If 'dataframe' the output is a pandas DataFrame\n\n    Returns\n    -------\n    dict or dataframe\n    \"\"\"\n    if output_format not in (\"dataframe\", \"object\"):\n        raise ValueError(\"Invalid output format. Only 'object', 'dataframe'.\")\n\n    per_fold_str = None\n    if per_fold is not None:\n        per_fold_str = str(per_fold).lower()\n\n    listing_call = partial(\n        _list_evaluations,\n        function=function,\n        tasks=tasks,\n        setups=setups,\n        flows=flows,\n        runs=runs,\n        uploaders=uploaders,\n        tag=tag,\n        study=study,\n        sort_order=sort_order,\n        per_fold=per_fold_str,\n    )\n    eval_collection = openml.utils._list_all(listing_call, offset=offset, limit=size)\n\n    flattened = list(chain.from_iterable(eval_collection))\n    if output_format == \"dataframe\":\n        records = [item._to_dict() for item in flattened]\n        return pd.DataFrame.from_records(records)  # No index...\n\n    return {e.run_id: e for e in flattened}\n</code></pre>"},{"location":"reference/evaluations/#openml.evaluations.list_evaluations_setups","title":"list_evaluations_setups","text":"<pre><code>list_evaluations_setups(function: str, offset: int | None = None, size: int | None = None, tasks: list | None = None, setups: list | None = None, flows: list | None = None, runs: list | None = None, uploaders: list | None = None, tag: str | None = None, per_fold: bool | None = None, sort_order: str | None = None, parameters_in_separate_columns: bool = False) -&gt; DataFrame\n</code></pre> <p>List all run-evaluation pairs matching all of the given filters and their hyperparameter settings.</p> PARAMETER DESCRIPTION <code>function</code> <p>the evaluation function. e.g., predictive_accuracy</p> <p> TYPE: <code>str</code> </p> <code>offset</code> <p>the number of runs to skip, starting from the first</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>the maximum number of runs to show</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>tasks</code> <p>the list of task IDs</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>None</code> </p> <code>setups</code> <p>the list of setup IDs</p> <p> TYPE: <code>list | None</code> DEFAULT: <code>None</code> </p> <code>flows</code> <p>the list of flow IDs</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>None</code> </p> <code>runs</code> <p>the list of run IDs</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>None</code> </p> <code>uploaders</code> <p>the list of uploader IDs</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p>filter evaluation based on given tag</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>per_fold</code> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>sort_order</code> <p>order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>parameters_in_separate_columns</code> <p>Returns hyperparameters in separate columns if set to True. Valid only for a single flow</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dataframe with hyperparameter settings as a list of tuples.</code> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_evaluations_setups(\n    function: str,\n    offset: int | None = None,\n    size: int | None = None,\n    tasks: list | None = None,\n    setups: list | None = None,\n    flows: list | None = None,\n    runs: list | None = None,\n    uploaders: list | None = None,\n    tag: str | None = None,\n    per_fold: bool | None = None,\n    sort_order: str | None = None,\n    parameters_in_separate_columns: bool = False,  # noqa: FBT001, FBT002\n) -&gt; pd.DataFrame:\n    \"\"\"List all run-evaluation pairs matching all of the given filters\n    and their hyperparameter settings.\n\n    Parameters\n    ----------\n    function : str\n        the evaluation function. e.g., predictive_accuracy\n    offset : int, optional\n        the number of runs to skip, starting from the first\n    size : int, optional\n        the maximum number of runs to show\n    tasks : list[int], optional\n        the list of task IDs\n    setups: list[int], optional\n        the list of setup IDs\n    flows : list[int], optional\n        the list of flow IDs\n    runs : list[int], optional\n        the list of run IDs\n    uploaders : list[int], optional\n        the list of uploader IDs\n    tag : str, optional\n        filter evaluation based on given tag\n    per_fold : bool, optional\n    sort_order : str, optional\n       order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")\n    parameters_in_separate_columns: bool, optional (default= False)\n        Returns hyperparameters in separate columns if set to True.\n        Valid only for a single flow\n\n    Returns\n    -------\n    dataframe with hyperparameter settings as a list of tuples.\n    \"\"\"\n    if parameters_in_separate_columns and (flows is None or len(flows) != 1):\n        raise ValueError(\"Can set parameters_in_separate_columns to true only for single flow_id\")\n\n    # List evaluations\n    evals = list_evaluations(\n        function=function,\n        offset=offset,\n        size=size,\n        runs=runs,\n        tasks=tasks,\n        setups=setups,\n        flows=flows,\n        uploaders=uploaders,\n        tag=tag,\n        per_fold=per_fold,\n        sort_order=sort_order,\n        output_format=\"dataframe\",\n    )\n    # List setups\n    # list_setups by setup id does not support large sizes (exceeds URL length limit)\n    # Hence we split the list of unique setup ids returned by list_evaluations into chunks of size N\n    _df = pd.DataFrame()\n    if len(evals) != 0:\n        N = 100  # size of section\n        uniq = np.asarray(evals[\"setup_id\"].unique())\n        length = len(uniq)\n\n        # array_split - allows indices_or_sections to not equally divide the array\n        # array_split -length % N sub-arrays of size length//N + 1 and the rest of size length//N.\n        split_size = ((length - 1) // N) + 1\n        setup_chunks = np.array_split(uniq, split_size)\n\n        setup_data = pd.DataFrame()\n        for _setups in setup_chunks:\n            result = openml.setups.list_setups(setup=_setups, output_format=\"dataframe\")\n            assert isinstance(result, pd.DataFrame)\n            result = result.drop(\"flow_id\", axis=1)\n            # concat resulting setup chunks into single datframe\n            setup_data = pd.concat([setup_data, result])\n\n        parameters = []\n        # Convert parameters of setup into dict of (hyperparameter, value)\n        for parameter_dict in setup_data[\"parameters\"]:\n            if parameter_dict is not None:\n                parameters.append(\n                    {param[\"full_name\"]: param[\"value\"] for param in parameter_dict.values()},\n                )\n            else:\n                parameters.append({})\n        setup_data[\"parameters\"] = parameters\n        # Merge setups with evaluations\n        _df = evals.merge(setup_data, on=\"setup_id\", how=\"left\")\n\n    if parameters_in_separate_columns:\n        _df = pd.concat(\n            [_df.drop(\"parameters\", axis=1), _df[\"parameters\"].apply(pd.Series)],\n            axis=1,\n        )\n\n    return _df\n</code></pre>"},{"location":"reference/evaluations/evaluation/","title":"evaluation","text":""},{"location":"reference/evaluations/evaluation/#openml.evaluations.evaluation","title":"openml.evaluations.evaluation","text":""},{"location":"reference/evaluations/evaluation/#openml.evaluations.evaluation.OpenMLEvaluation","title":"OpenMLEvaluation","text":"<pre><code>OpenMLEvaluation(run_id: int, task_id: int, setup_id: int, flow_id: int, flow_name: str, data_id: int, data_name: str, function: str, upload_time: str, uploader: int, uploader_name: str, value: float | None, values: list[float] | None, array_data: str | None = None)\n</code></pre> <p>Contains all meta-information about a run / evaluation combination, according to the evaluation/list function</p> PARAMETER DESCRIPTION <code>run_id</code> <p>Refers to the run.</p> <p> TYPE: <code>int</code> </p> <code>task_id</code> <p>Refers to the task.</p> <p> TYPE: <code>int</code> </p> <code>setup_id</code> <p>Refers to the setup.</p> <p> TYPE: <code>int</code> </p> <code>flow_id</code> <p>Refers to the flow.</p> <p> TYPE: <code>int</code> </p> <code>flow_name</code> <p>Name of the referred flow.</p> <p> TYPE: <code>str</code> </p> <code>data_id</code> <p>Refers to the dataset.</p> <p> TYPE: <code>int</code> </p> <code>data_name</code> <p>The name of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>function</code> <p>The evaluation metric of this item (e.g., accuracy).</p> <p> TYPE: <code>str</code> </p> <code>upload_time</code> <p>The time of evaluation.</p> <p> TYPE: <code>str</code> </p> <code>uploader</code> <p>Uploader ID (user ID)</p> <p> TYPE: <code>int</code> </p> <code>upload_name</code> <p>Name of the uploader of this evaluation</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>The value (score) of this evaluation.</p> <p> TYPE: <code>float</code> </p> <code>values</code> <p>The values (scores) per repeat and fold (if requested)</p> <p> TYPE: <code>List[float]</code> </p> <code>array_data</code> <p>list of information per class. (e.g., in case of precision, auroc, recall)</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/evaluations/evaluation.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    run_id: int,\n    task_id: int,\n    setup_id: int,\n    flow_id: int,\n    flow_name: str,\n    data_id: int,\n    data_name: str,\n    function: str,\n    upload_time: str,\n    uploader: int,\n    uploader_name: str,\n    value: float | None,\n    values: list[float] | None,\n    array_data: str | None = None,\n):\n    self.run_id = run_id\n    self.task_id = task_id\n    self.setup_id = setup_id\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.data_id = data_id\n    self.data_name = data_name\n    self.function = function\n    self.upload_time = upload_time\n    self.uploader = uploader\n    self.uploader_name = uploader_name\n    self.value = value\n    self.values = values\n    self.array_data = array_data\n</code></pre>"},{"location":"reference/evaluations/functions/","title":"functions","text":""},{"location":"reference/evaluations/functions/#openml.evaluations.functions","title":"openml.evaluations.functions","text":""},{"location":"reference/evaluations/functions/#openml.evaluations.functions.__list_evaluations","title":"__list_evaluations","text":"<pre><code>__list_evaluations(api_call: str) -&gt; list[OpenMLEvaluation]\n</code></pre> <p>Helper function to parse API calls which are lists of runs</p> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def __list_evaluations(api_call: str) -&gt; list[OpenMLEvaluation]:\n    \"\"\"Helper function to parse API calls which are lists of runs\"\"\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    evals_dict = xmltodict.parse(xml_string, force_list=(\"oml:evaluation\",))\n    # Minimalistic check if the XML is useful\n    if \"oml:evaluations\" not in evals_dict:\n        raise ValueError(\n            \"Error in return XML, does not contain \" f'\"oml:evaluations\": {evals_dict!s}',\n        )\n\n    assert isinstance(evals_dict[\"oml:evaluations\"][\"oml:evaluation\"], list), type(\n        evals_dict[\"oml:evaluations\"],\n    )\n\n    uploader_ids = list(\n        {eval_[\"oml:uploader\"] for eval_ in evals_dict[\"oml:evaluations\"][\"oml:evaluation\"]},\n    )\n    api_users = \"user/list/user_id/\" + \",\".join(uploader_ids)\n    xml_string_user = openml._api_calls._perform_api_call(api_users, \"get\")\n\n    users = xmltodict.parse(xml_string_user, force_list=(\"oml:user\",))\n    user_dict = {user[\"oml:id\"]: user[\"oml:username\"] for user in users[\"oml:users\"][\"oml:user\"]}\n\n    evals = []\n    for eval_ in evals_dict[\"oml:evaluations\"][\"oml:evaluation\"]:\n        run_id = int(eval_[\"oml:run_id\"])\n        value = float(eval_[\"oml:value\"]) if \"oml:value\" in eval_ else None\n        values = json.loads(eval_[\"oml:values\"]) if eval_.get(\"oml:values\", None) else None\n        array_data = eval_.get(\"oml:array_data\")\n\n        evals.append(\n            OpenMLEvaluation(\n                run_id=run_id,\n                task_id=int(eval_[\"oml:task_id\"]),\n                setup_id=int(eval_[\"oml:setup_id\"]),\n                flow_id=int(eval_[\"oml:flow_id\"]),\n                flow_name=eval_[\"oml:flow_name\"],\n                data_id=int(eval_[\"oml:data_id\"]),\n                data_name=eval_[\"oml:data_name\"],\n                function=eval_[\"oml:function\"],\n                upload_time=eval_[\"oml:upload_time\"],\n                uploader=int(eval_[\"oml:uploader\"]),\n                uploader_name=user_dict[eval_[\"oml:uploader\"]],\n                value=value,\n                values=values,\n                array_data=array_data,\n            )\n        )\n\n    return evals\n</code></pre>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_estimation_procedures","title":"list_estimation_procedures","text":"<pre><code>list_estimation_procedures() -&gt; list[str]\n</code></pre> <p>Return list of evaluation procedures available.</p> <p>The function performs an API call to retrieve the entire list of evaluation procedures' names that are available.</p> RETURNS DESCRIPTION <code>list</code> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_estimation_procedures() -&gt; list[str]:\n    \"\"\"Return list of evaluation procedures available.\n\n    The function performs an API call to retrieve the entire list of\n    evaluation procedures' names that are available.\n\n    Returns\n    -------\n    list\n    \"\"\"\n    api_call = \"estimationprocedure/list\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    api_results = xmltodict.parse(xml_string)\n\n    # Minimalistic check if the XML is useful\n    if \"oml:estimationprocedures\" not in api_results:\n        raise ValueError('Error in return XML, does not contain \"oml:estimationprocedures\"')\n\n    if \"oml:estimationprocedure\" not in api_results[\"oml:estimationprocedures\"]:\n        raise ValueError('Error in return XML, does not contain \"oml:estimationprocedure\"')\n\n    if not isinstance(api_results[\"oml:estimationprocedures\"][\"oml:estimationprocedure\"], list):\n        raise TypeError('Error in return XML, does not contain \"oml:estimationprocedure\" as a list')\n\n    return [\n        prod[\"oml:name\"]\n        for prod in api_results[\"oml:estimationprocedures\"][\"oml:estimationprocedure\"]\n    ]\n</code></pre>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_evaluation_measures","title":"list_evaluation_measures","text":"<pre><code>list_evaluation_measures() -&gt; list[str]\n</code></pre> <p>Return list of evaluation measures available.</p> <p>The function performs an API call to retrieve the entire list of evaluation measures that are available.</p> RETURNS DESCRIPTION <code>list</code> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_evaluation_measures() -&gt; list[str]:\n    \"\"\"Return list of evaluation measures available.\n\n    The function performs an API call to retrieve the entire list of\n    evaluation measures that are available.\n\n    Returns\n    -------\n    list\n\n    \"\"\"\n    api_call = \"evaluationmeasure/list\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    qualities = xmltodict.parse(xml_string, force_list=(\"oml:measures\"))\n    # Minimalistic check if the XML is useful\n    if \"oml:evaluation_measures\" not in qualities:\n        raise ValueError('Error in return XML, does not contain \"oml:evaluation_measures\"')\n\n    if not isinstance(qualities[\"oml:evaluation_measures\"][\"oml:measures\"][0][\"oml:measure\"], list):\n        raise TypeError('Error in return XML, does not contain \"oml:measure\" as a list')\n\n    return qualities[\"oml:evaluation_measures\"][\"oml:measures\"][0][\"oml:measure\"]\n</code></pre>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_evaluations","title":"list_evaluations","text":"<pre><code>list_evaluations(function: str, offset: int | None = None, size: int | None = None, tasks: list[str | int] | None = None, setups: list[str | int] | None = None, flows: list[str | int] | None = None, runs: list[str | int] | None = None, uploaders: list[str | int] | None = None, tag: str | None = None, study: int | None = None, per_fold: bool | None = None, sort_order: str | None = None, output_format: Literal['object', 'dataframe'] = 'object') -&gt; dict[int, OpenMLEvaluation] | DataFrame\n</code></pre> <p>List all run-evaluation pairs matching all of the given filters.</p> <p>(Supports large amount of results)</p> PARAMETER DESCRIPTION <code>function</code> <p>the evaluation function. e.g., predictive_accuracy</p> <p> TYPE: <code>str</code> </p> <code>offset</code> <p>the number of runs to skip, starting from the first</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>The maximum number of runs to show. If set to <code>None</code>, it returns all the results.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>tasks</code> <p>the list of task IDs</p> <p> TYPE: <code>list[int, str]</code> DEFAULT: <code>None</code> </p> <code>setups</code> <p>the list of setup IDs</p> <p> TYPE: <code>list[str | int] | None</code> DEFAULT: <code>None</code> </p> <code>flows</code> <p>the list of flow IDs</p> <p> TYPE: <code>list[int, str]</code> DEFAULT: <code>None</code> </p> <code>runs</code> <p>the list of run IDs</p> <p> TYPE: <code>list[str | int] | None</code> DEFAULT: <code>None</code> </p> <code>uploaders</code> <p>the list of uploader IDs</p> <p> TYPE: <code>list[int, str]</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p>filter evaluation based on given tag</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>study</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>per_fold</code> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>sort_order</code> <p>order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>output_format</code> <p>The parameter decides the format of the output. - If 'object' the output is a dict of OpenMLEvaluation objects - If 'dataframe' the output is a pandas DataFrame</p> <p> TYPE: <code>Literal['object', 'dataframe']</code> DEFAULT: <code>'object'</code> </p> RETURNS DESCRIPTION <code>dict or dataframe</code> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_evaluations(\n    function: str,\n    offset: int | None = None,\n    size: int | None = None,\n    tasks: list[str | int] | None = None,\n    setups: list[str | int] | None = None,\n    flows: list[str | int] | None = None,\n    runs: list[str | int] | None = None,\n    uploaders: list[str | int] | None = None,\n    tag: str | None = None,\n    study: int | None = None,\n    per_fold: bool | None = None,\n    sort_order: str | None = None,\n    output_format: Literal[\"object\", \"dataframe\"] = \"object\",\n) -&gt; dict[int, OpenMLEvaluation] | pd.DataFrame:\n    \"\"\"List all run-evaluation pairs matching all of the given filters.\n\n    (Supports large amount of results)\n\n    Parameters\n    ----------\n    function : str\n        the evaluation function. e.g., predictive_accuracy\n    offset : int, optional\n        the number of runs to skip, starting from the first\n    size : int, default 10000\n        The maximum number of runs to show.\n        If set to ``None``, it returns all the results.\n\n    tasks : list[int,str], optional\n        the list of task IDs\n    setups: list[int,str], optional\n        the list of setup IDs\n    flows : list[int,str], optional\n        the list of flow IDs\n    runs :list[int,str], optional\n        the list of run IDs\n    uploaders : list[int,str], optional\n        the list of uploader IDs\n    tag : str, optional\n        filter evaluation based on given tag\n\n    study : int, optional\n\n    per_fold : bool, optional\n\n    sort_order : str, optional\n       order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")\n\n    output_format: str, optional (default='object')\n        The parameter decides the format of the output.\n        - If 'object' the output is a dict of OpenMLEvaluation objects\n        - If 'dataframe' the output is a pandas DataFrame\n\n    Returns\n    -------\n    dict or dataframe\n    \"\"\"\n    if output_format not in (\"dataframe\", \"object\"):\n        raise ValueError(\"Invalid output format. Only 'object', 'dataframe'.\")\n\n    per_fold_str = None\n    if per_fold is not None:\n        per_fold_str = str(per_fold).lower()\n\n    listing_call = partial(\n        _list_evaluations,\n        function=function,\n        tasks=tasks,\n        setups=setups,\n        flows=flows,\n        runs=runs,\n        uploaders=uploaders,\n        tag=tag,\n        study=study,\n        sort_order=sort_order,\n        per_fold=per_fold_str,\n    )\n    eval_collection = openml.utils._list_all(listing_call, offset=offset, limit=size)\n\n    flattened = list(chain.from_iterable(eval_collection))\n    if output_format == \"dataframe\":\n        records = [item._to_dict() for item in flattened]\n        return pd.DataFrame.from_records(records)  # No index...\n\n    return {e.run_id: e for e in flattened}\n</code></pre>"},{"location":"reference/evaluations/functions/#openml.evaluations.functions.list_evaluations_setups","title":"list_evaluations_setups","text":"<pre><code>list_evaluations_setups(function: str, offset: int | None = None, size: int | None = None, tasks: list | None = None, setups: list | None = None, flows: list | None = None, runs: list | None = None, uploaders: list | None = None, tag: str | None = None, per_fold: bool | None = None, sort_order: str | None = None, parameters_in_separate_columns: bool = False) -&gt; DataFrame\n</code></pre> <p>List all run-evaluation pairs matching all of the given filters and their hyperparameter settings.</p> PARAMETER DESCRIPTION <code>function</code> <p>the evaluation function. e.g., predictive_accuracy</p> <p> TYPE: <code>str</code> </p> <code>offset</code> <p>the number of runs to skip, starting from the first</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>the maximum number of runs to show</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>tasks</code> <p>the list of task IDs</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>None</code> </p> <code>setups</code> <p>the list of setup IDs</p> <p> TYPE: <code>list | None</code> DEFAULT: <code>None</code> </p> <code>flows</code> <p>the list of flow IDs</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>None</code> </p> <code>runs</code> <p>the list of run IDs</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>None</code> </p> <code>uploaders</code> <p>the list of uploader IDs</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p>filter evaluation based on given tag</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>per_fold</code> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <code>sort_order</code> <p>order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>parameters_in_separate_columns</code> <p>Returns hyperparameters in separate columns if set to True. Valid only for a single flow</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>dataframe with hyperparameter settings as a list of tuples.</code> Source code in <code>openml/evaluations/functions.py</code> <pre><code>def list_evaluations_setups(\n    function: str,\n    offset: int | None = None,\n    size: int | None = None,\n    tasks: list | None = None,\n    setups: list | None = None,\n    flows: list | None = None,\n    runs: list | None = None,\n    uploaders: list | None = None,\n    tag: str | None = None,\n    per_fold: bool | None = None,\n    sort_order: str | None = None,\n    parameters_in_separate_columns: bool = False,  # noqa: FBT001, FBT002\n) -&gt; pd.DataFrame:\n    \"\"\"List all run-evaluation pairs matching all of the given filters\n    and their hyperparameter settings.\n\n    Parameters\n    ----------\n    function : str\n        the evaluation function. e.g., predictive_accuracy\n    offset : int, optional\n        the number of runs to skip, starting from the first\n    size : int, optional\n        the maximum number of runs to show\n    tasks : list[int], optional\n        the list of task IDs\n    setups: list[int], optional\n        the list of setup IDs\n    flows : list[int], optional\n        the list of flow IDs\n    runs : list[int], optional\n        the list of run IDs\n    uploaders : list[int], optional\n        the list of uploader IDs\n    tag : str, optional\n        filter evaluation based on given tag\n    per_fold : bool, optional\n    sort_order : str, optional\n       order of sorting evaluations, ascending (\"asc\") or descending (\"desc\")\n    parameters_in_separate_columns: bool, optional (default= False)\n        Returns hyperparameters in separate columns if set to True.\n        Valid only for a single flow\n\n    Returns\n    -------\n    dataframe with hyperparameter settings as a list of tuples.\n    \"\"\"\n    if parameters_in_separate_columns and (flows is None or len(flows) != 1):\n        raise ValueError(\"Can set parameters_in_separate_columns to true only for single flow_id\")\n\n    # List evaluations\n    evals = list_evaluations(\n        function=function,\n        offset=offset,\n        size=size,\n        runs=runs,\n        tasks=tasks,\n        setups=setups,\n        flows=flows,\n        uploaders=uploaders,\n        tag=tag,\n        per_fold=per_fold,\n        sort_order=sort_order,\n        output_format=\"dataframe\",\n    )\n    # List setups\n    # list_setups by setup id does not support large sizes (exceeds URL length limit)\n    # Hence we split the list of unique setup ids returned by list_evaluations into chunks of size N\n    _df = pd.DataFrame()\n    if len(evals) != 0:\n        N = 100  # size of section\n        uniq = np.asarray(evals[\"setup_id\"].unique())\n        length = len(uniq)\n\n        # array_split - allows indices_or_sections to not equally divide the array\n        # array_split -length % N sub-arrays of size length//N + 1 and the rest of size length//N.\n        split_size = ((length - 1) // N) + 1\n        setup_chunks = np.array_split(uniq, split_size)\n\n        setup_data = pd.DataFrame()\n        for _setups in setup_chunks:\n            result = openml.setups.list_setups(setup=_setups, output_format=\"dataframe\")\n            assert isinstance(result, pd.DataFrame)\n            result = result.drop(\"flow_id\", axis=1)\n            # concat resulting setup chunks into single datframe\n            setup_data = pd.concat([setup_data, result])\n\n        parameters = []\n        # Convert parameters of setup into dict of (hyperparameter, value)\n        for parameter_dict in setup_data[\"parameters\"]:\n            if parameter_dict is not None:\n                parameters.append(\n                    {param[\"full_name\"]: param[\"value\"] for param in parameter_dict.values()},\n                )\n            else:\n                parameters.append({})\n        setup_data[\"parameters\"] = parameters\n        # Merge setups with evaluations\n        _df = evals.merge(setup_data, on=\"setup_id\", how=\"left\")\n\n    if parameters_in_separate_columns:\n        _df = pd.concat(\n            [_df.drop(\"parameters\", axis=1), _df[\"parameters\"].apply(pd.Series)],\n            axis=1,\n        )\n\n    return _df\n</code></pre>"},{"location":"reference/extensions/","title":"extensions","text":""},{"location":"reference/extensions/#openml.extensions","title":"openml.extensions","text":""},{"location":"reference/extensions/#openml.extensions.Extension","title":"Extension","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the interface to connect machine learning libraries to OpenML-Python.</p> <p>See <code>openml.extension.sklearn.extension</code> for an implementation to bootstrap from.</p>"},{"location":"reference/extensions/#openml.extensions.Extension.can_handle_flow","title":"can_handle_flow  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_handle_flow(flow: OpenMLFlow) -&gt; bool\n</code></pre> <p>Check whether a given flow can be handled by this extension.</p> <p>This is typically done by parsing the <code>external_version</code> field.</p> PARAMETER DESCRIPTION <code>flow</code> <p> TYPE: <code>OpenMLFlow</code> </p> RETURNS DESCRIPTION <code>bool</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@classmethod\n@abstractmethod\ndef can_handle_flow(cls, flow: OpenMLFlow) -&gt; bool:\n    \"\"\"Check whether a given flow can be handled by this extension.\n\n    This is typically done by parsing the ``external_version`` field.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.can_handle_model","title":"can_handle_model  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_handle_model(model: Any) -&gt; bool\n</code></pre> <p>Check whether a model flow can be handled by this extension.</p> <p>This is typically done by checking the type of the model, or the package it belongs to.</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>bool</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@classmethod\n@abstractmethod\ndef can_handle_model(cls, model: Any) -&gt; bool:\n    \"\"\"Check whether a model flow can be handled by this extension.\n\n    This is typically done by checking the type of the model, or the package it belongs to.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.check_if_model_fitted","title":"check_if_model_fitted  <code>abstractmethod</code>","text":"<pre><code>check_if_model_fitted(model: Any) -&gt; bool\n</code></pre> <p>Returns True/False denoting if the model has already been fitted/trained.</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>bool</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef check_if_model_fitted(self, model: Any) -&gt; bool:\n    \"\"\"Returns True/False denoting if the model has already been fitted/trained.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.create_setup_string","title":"create_setup_string  <code>abstractmethod</code>","text":"<pre><code>create_setup_string(model: Any) -&gt; str\n</code></pre> <p>Create a string which can be used to reinstantiate the given model.</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>str</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef create_setup_string(self, model: Any) -&gt; str:\n    \"\"\"Create a string which can be used to reinstantiate the given model.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    str\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.flow_to_model","title":"flow_to_model  <code>abstractmethod</code>","text":"<pre><code>flow_to_model(flow: OpenMLFlow, initialize_with_defaults: bool = False, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Instantiate a model from the flow representation.</p> PARAMETER DESCRIPTION <code>flow</code> <p> TYPE: <code>OpenMLFlow</code> </p> <code>initialize_with_defaults</code> <p>If this flag is set, the hyperparameter values of flows will be ignored and a flow with its defaults is returned.</p> <p> TYPE: <code>(bool, optional(default=False))</code> DEFAULT: <code>False</code> </p> <code>strict_version</code> <p>Whether to fail if version requirements are not fulfilled.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Any</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef flow_to_model(\n    self,\n    flow: OpenMLFlow,\n    initialize_with_defaults: bool = False,  # noqa: FBT001, FBT002\n    strict_version: bool = True,  # noqa: FBT002, FBT001\n) -&gt; Any:\n    \"\"\"Instantiate a model from the flow representation.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    initialize_with_defaults : bool, optional (default=False)\n        If this flag is set, the hyperparameter values of flows will be\n        ignored and a flow with its defaults is returned.\n\n    strict_version : bool, default=True\n        Whether to fail if version requirements are not fulfilled.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.get_version_information","title":"get_version_information  <code>abstractmethod</code>","text":"<pre><code>get_version_information() -&gt; list[str]\n</code></pre> <p>List versions of libraries required by the flow.</p> RETURNS DESCRIPTION <code>List</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef get_version_information(self) -&gt; list[str]:\n    \"\"\"List versions of libraries required by the flow.\n\n    Returns\n    -------\n    List\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.instantiate_model_from_hpo_class","title":"instantiate_model_from_hpo_class  <code>abstractmethod</code>","text":"<pre><code>instantiate_model_from_hpo_class(model: Any, trace_iteration: OpenMLTraceIteration) -&gt; Any\n</code></pre> <p>Instantiate a base model which can be searched over by the hyperparameter optimization model.</p> PARAMETER DESCRIPTION <code>model</code> <p>A hyperparameter optimization model which defines the model to be instantiated.</p> <p> TYPE: <code>Any</code> </p> <code>trace_iteration</code> <p>Describing the hyperparameter settings to instantiate.</p> <p> TYPE: <code>OpenMLTraceIteration</code> </p> RETURNS DESCRIPTION <code>Any</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef instantiate_model_from_hpo_class(\n    self,\n    model: Any,\n    trace_iteration: OpenMLTraceIteration,\n) -&gt; Any:\n    \"\"\"Instantiate a base model which can be searched over by the hyperparameter optimization\n    model.\n\n    Parameters\n    ----------\n    model : Any\n        A hyperparameter optimization model which defines the model to be instantiated.\n    trace_iteration : OpenMLTraceIteration\n        Describing the hyperparameter settings to instantiate.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.is_estimator","title":"is_estimator  <code>abstractmethod</code>","text":"<pre><code>is_estimator(model: Any) -&gt; bool\n</code></pre> <p>Check whether the given model is an estimator for the given extension.</p> <p>This function is only required for backwards compatibility and will be removed in the near future.</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>bool</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef is_estimator(self, model: Any) -&gt; bool:\n    \"\"\"Check whether the given model is an estimator for the given extension.\n\n    This function is only required for backwards compatibility and will be removed in the\n    near future.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.model_to_flow","title":"model_to_flow  <code>abstractmethod</code>","text":"<pre><code>model_to_flow(model: Any) -&gt; OpenMLFlow\n</code></pre> <p>Transform a model to a flow for uploading it to OpenML.</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>OpenMLFlow</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef model_to_flow(self, model: Any) -&gt; OpenMLFlow:\n    \"\"\"Transform a model to a flow for uploading it to OpenML.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    OpenMLFlow\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.obtain_parameter_values","title":"obtain_parameter_values  <code>abstractmethod</code>","text":"<pre><code>obtain_parameter_values(flow: OpenMLFlow, model: Any = None) -&gt; list[dict[str, Any]]\n</code></pre> <p>Extracts all parameter settings required for the flow from the model.</p> <p>If no explicit model is provided, the parameters will be extracted from <code>flow.model</code> instead.</p> PARAMETER DESCRIPTION <code>flow</code> <p>OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)</p> <p> TYPE: <code>OpenMLFlow</code> </p> <code>model</code> <p>The model from which to obtain the parameter values. Must match the flow signature. If None, use the model specified in <code>OpenMLFlow.model</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list</code> <p>A list of dicts, where each dict has the following entries: - <code>oml:name</code> : str: The OpenML parameter name - <code>oml:value</code> : mixed: A representation of the parameter value - <code>oml:component</code> : int: flow id to which the parameter belongs</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef obtain_parameter_values(\n    self,\n    flow: OpenMLFlow,\n    model: Any = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Extracts all parameter settings required for the flow from the model.\n\n    If no explicit model is provided, the parameters will be extracted from `flow.model`\n    instead.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)\n\n    model: Any, optional (default=None)\n        The model from which to obtain the parameter values. Must match the flow signature.\n        If None, use the model specified in ``OpenMLFlow.model``.\n\n    Returns\n    -------\n    list\n        A list of dicts, where each dict has the following entries:\n        - ``oml:name`` : str: The OpenML parameter name\n        - ``oml:value`` : mixed: A representation of the parameter value\n        - ``oml:component`` : int: flow id to which the parameter belongs\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.Extension.seed_model","title":"seed_model  <code>abstractmethod</code>","text":"<pre><code>seed_model(model: Any, seed: int | None) -&gt; Any\n</code></pre> <p>Set the seed of all the unseeded components of a model and return the seeded model.</p> <p>Required so that all seed information can be uploaded to OpenML for reproducible results.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to be seeded</p> <p> TYPE: <code>Any</code> </p> <code>seed</code> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>model</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef seed_model(self, model: Any, seed: int | None) -&gt; Any:\n    \"\"\"Set the seed of all the unseeded components of a model and return the seeded model.\n\n    Required so that all seed information can be uploaded to OpenML for reproducible results.\n\n    Parameters\n    ----------\n    model : Any\n        The model to be seeded\n    seed : int\n\n    Returns\n    -------\n    model\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.get_extension_by_flow","title":"get_extension_by_flow","text":"<pre><code>get_extension_by_flow(flow: OpenMLFlow, raise_if_no_extension: bool = False) -&gt; Extension | None\n</code></pre> <p>Get an extension which can handle the given flow.</p> <p>Iterates all registered extensions and checks whether they can handle the presented flow. Raises an exception if two extensions can handle a flow.</p> PARAMETER DESCRIPTION <code>flow</code> <p> TYPE: <code>OpenMLFlow</code> </p> <code>raise_if_no_extension</code> <p>Raise an exception if no registered extension can handle the presented flow.</p> <p> TYPE: <code>bool (optional</code> DEFAULT: <code>False)</code> </p> RETURNS DESCRIPTION <code>Extension or None</code> Source code in <code>openml/extensions/functions.py</code> <pre><code>def get_extension_by_flow(\n    flow: OpenMLFlow,\n    raise_if_no_extension: bool = False,  # noqa: FBT001, FBT002\n) -&gt; Extension | None:\n    \"\"\"Get an extension which can handle the given flow.\n\n    Iterates all registered extensions and checks whether they can handle the presented flow.\n    Raises an exception if two extensions can handle a flow.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    raise_if_no_extension : bool (optional, default=False)\n        Raise an exception if no registered extension can handle the presented flow.\n\n    Returns\n    -------\n    Extension or None\n    \"\"\"\n    # import openml_sklearn to register SklearnExtension\n    if importlib.util.find_spec(\"openml_sklearn\"):\n        import openml_sklearn  # noqa: F401\n\n    candidates = []\n    for extension_class in openml.extensions.extensions:\n        if extension_class.can_handle_flow(flow):\n            candidates.append(extension_class())\n    if len(candidates) == 0:\n        if raise_if_no_extension:\n            install_instruction = \"\"\n            if flow.name.startswith(\"sklearn\"):\n                install_instruction = SKLEARN_HINT\n            raise ValueError(\n                f\"No extension registered which can handle flow: {flow.flow_id} ({flow.name}). \"\n                f\"{install_instruction}\"\n            )\n\n        return None\n\n    if len(candidates) == 1:\n        return candidates[0]\n\n    raise ValueError(\n        f\"Multiple extensions registered which can handle flow: {flow}, but only one \"\n        f\"is allowed ({candidates}).\",\n    )\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.get_extension_by_model","title":"get_extension_by_model","text":"<pre><code>get_extension_by_model(model: Any, raise_if_no_extension: bool = False) -&gt; Extension | None\n</code></pre> <p>Get an extension which can handle the given flow.</p> <p>Iterates all registered extensions and checks whether they can handle the presented model. Raises an exception if two extensions can handle a model.</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Any</code> </p> <code>raise_if_no_extension</code> <p>Raise an exception if no registered extension can handle the presented model.</p> <p> TYPE: <code>bool (optional</code> DEFAULT: <code>False)</code> </p> RETURNS DESCRIPTION <code>Extension or None</code> Source code in <code>openml/extensions/functions.py</code> <pre><code>def get_extension_by_model(\n    model: Any,\n    raise_if_no_extension: bool = False,  # noqa: FBT001, FBT002\n) -&gt; Extension | None:\n    \"\"\"Get an extension which can handle the given flow.\n\n    Iterates all registered extensions and checks whether they can handle the presented model.\n    Raises an exception if two extensions can handle a model.\n\n    Parameters\n    ----------\n    model : Any\n\n    raise_if_no_extension : bool (optional, default=False)\n        Raise an exception if no registered extension can handle the presented model.\n\n    Returns\n    -------\n    Extension or None\n    \"\"\"\n    # import openml_sklearn to register SklearnExtension\n    if importlib.util.find_spec(\"openml_sklearn\"):\n        import openml_sklearn  # noqa: F401\n\n    candidates = []\n    for extension_class in openml.extensions.extensions:\n        if extension_class.can_handle_model(model):\n            candidates.append(extension_class())\n    if len(candidates) == 0:\n        if raise_if_no_extension:\n            install_instruction = \"\"\n            if type(model).__module__.startswith(\"sklearn\"):\n                install_instruction = SKLEARN_HINT\n            raise ValueError(\n                f\"No extension registered which can handle model: {model}. {install_instruction}\"\n            )\n\n        return None\n\n    if len(candidates) == 1:\n        return candidates[0]\n\n    raise ValueError(\n        f\"Multiple extensions registered which can handle model: {model}, but only one \"\n        f\"is allowed ({candidates}).\",\n    )\n</code></pre>"},{"location":"reference/extensions/#openml.extensions.register_extension","title":"register_extension","text":"<pre><code>register_extension(extension: type[Extension]) -&gt; None\n</code></pre> <p>Register an extension.</p> <p>Registered extensions are considered by <code>get_extension_by_flow</code> and <code>get_extension_by_model</code>, which are used by <code>openml.flow</code> and <code>openml.runs</code>.</p> PARAMETER DESCRIPTION <code>extension</code> <p> TYPE: <code>Type[Extension]</code> </p> RETURNS DESCRIPTION <code>None</code> Source code in <code>openml/extensions/functions.py</code> <pre><code>def register_extension(extension: type[Extension]) -&gt; None:\n    \"\"\"Register an extension.\n\n    Registered extensions are considered by ``get_extension_by_flow`` and\n    ``get_extension_by_model``, which are used by ``openml.flow`` and ``openml.runs``.\n\n    Parameters\n    ----------\n    extension : Type[Extension]\n\n    Returns\n    -------\n    None\n    \"\"\"\n    openml.extensions.extensions.append(extension)\n</code></pre>"},{"location":"reference/extensions/extension_interface/","title":"extension_interface","text":""},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface","title":"openml.extensions.extension_interface","text":""},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension","title":"Extension","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the interface to connect machine learning libraries to OpenML-Python.</p> <p>See <code>openml.extension.sklearn.extension</code> for an implementation to bootstrap from.</p>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.can_handle_flow","title":"can_handle_flow  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_handle_flow(flow: OpenMLFlow) -&gt; bool\n</code></pre> <p>Check whether a given flow can be handled by this extension.</p> <p>This is typically done by parsing the <code>external_version</code> field.</p> PARAMETER DESCRIPTION <code>flow</code> <p> TYPE: <code>OpenMLFlow</code> </p> RETURNS DESCRIPTION <code>bool</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@classmethod\n@abstractmethod\ndef can_handle_flow(cls, flow: OpenMLFlow) -&gt; bool:\n    \"\"\"Check whether a given flow can be handled by this extension.\n\n    This is typically done by parsing the ``external_version`` field.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.can_handle_model","title":"can_handle_model  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>can_handle_model(model: Any) -&gt; bool\n</code></pre> <p>Check whether a model flow can be handled by this extension.</p> <p>This is typically done by checking the type of the model, or the package it belongs to.</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>bool</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@classmethod\n@abstractmethod\ndef can_handle_model(cls, model: Any) -&gt; bool:\n    \"\"\"Check whether a model flow can be handled by this extension.\n\n    This is typically done by checking the type of the model, or the package it belongs to.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.check_if_model_fitted","title":"check_if_model_fitted  <code>abstractmethod</code>","text":"<pre><code>check_if_model_fitted(model: Any) -&gt; bool\n</code></pre> <p>Returns True/False denoting if the model has already been fitted/trained.</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>bool</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef check_if_model_fitted(self, model: Any) -&gt; bool:\n    \"\"\"Returns True/False denoting if the model has already been fitted/trained.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.create_setup_string","title":"create_setup_string  <code>abstractmethod</code>","text":"<pre><code>create_setup_string(model: Any) -&gt; str\n</code></pre> <p>Create a string which can be used to reinstantiate the given model.</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>str</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef create_setup_string(self, model: Any) -&gt; str:\n    \"\"\"Create a string which can be used to reinstantiate the given model.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    str\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.flow_to_model","title":"flow_to_model  <code>abstractmethod</code>","text":"<pre><code>flow_to_model(flow: OpenMLFlow, initialize_with_defaults: bool = False, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Instantiate a model from the flow representation.</p> PARAMETER DESCRIPTION <code>flow</code> <p> TYPE: <code>OpenMLFlow</code> </p> <code>initialize_with_defaults</code> <p>If this flag is set, the hyperparameter values of flows will be ignored and a flow with its defaults is returned.</p> <p> TYPE: <code>(bool, optional(default=False))</code> DEFAULT: <code>False</code> </p> <code>strict_version</code> <p>Whether to fail if version requirements are not fulfilled.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Any</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef flow_to_model(\n    self,\n    flow: OpenMLFlow,\n    initialize_with_defaults: bool = False,  # noqa: FBT001, FBT002\n    strict_version: bool = True,  # noqa: FBT002, FBT001\n) -&gt; Any:\n    \"\"\"Instantiate a model from the flow representation.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    initialize_with_defaults : bool, optional (default=False)\n        If this flag is set, the hyperparameter values of flows will be\n        ignored and a flow with its defaults is returned.\n\n    strict_version : bool, default=True\n        Whether to fail if version requirements are not fulfilled.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.get_version_information","title":"get_version_information  <code>abstractmethod</code>","text":"<pre><code>get_version_information() -&gt; list[str]\n</code></pre> <p>List versions of libraries required by the flow.</p> RETURNS DESCRIPTION <code>List</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef get_version_information(self) -&gt; list[str]:\n    \"\"\"List versions of libraries required by the flow.\n\n    Returns\n    -------\n    List\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.instantiate_model_from_hpo_class","title":"instantiate_model_from_hpo_class  <code>abstractmethod</code>","text":"<pre><code>instantiate_model_from_hpo_class(model: Any, trace_iteration: OpenMLTraceIteration) -&gt; Any\n</code></pre> <p>Instantiate a base model which can be searched over by the hyperparameter optimization model.</p> PARAMETER DESCRIPTION <code>model</code> <p>A hyperparameter optimization model which defines the model to be instantiated.</p> <p> TYPE: <code>Any</code> </p> <code>trace_iteration</code> <p>Describing the hyperparameter settings to instantiate.</p> <p> TYPE: <code>OpenMLTraceIteration</code> </p> RETURNS DESCRIPTION <code>Any</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef instantiate_model_from_hpo_class(\n    self,\n    model: Any,\n    trace_iteration: OpenMLTraceIteration,\n) -&gt; Any:\n    \"\"\"Instantiate a base model which can be searched over by the hyperparameter optimization\n    model.\n\n    Parameters\n    ----------\n    model : Any\n        A hyperparameter optimization model which defines the model to be instantiated.\n    trace_iteration : OpenMLTraceIteration\n        Describing the hyperparameter settings to instantiate.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.is_estimator","title":"is_estimator  <code>abstractmethod</code>","text":"<pre><code>is_estimator(model: Any) -&gt; bool\n</code></pre> <p>Check whether the given model is an estimator for the given extension.</p> <p>This function is only required for backwards compatibility and will be removed in the near future.</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>bool</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef is_estimator(self, model: Any) -&gt; bool:\n    \"\"\"Check whether the given model is an estimator for the given extension.\n\n    This function is only required for backwards compatibility and will be removed in the\n    near future.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.model_to_flow","title":"model_to_flow  <code>abstractmethod</code>","text":"<pre><code>model_to_flow(model: Any) -&gt; OpenMLFlow\n</code></pre> <p>Transform a model to a flow for uploading it to OpenML.</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>OpenMLFlow</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef model_to_flow(self, model: Any) -&gt; OpenMLFlow:\n    \"\"\"Transform a model to a flow for uploading it to OpenML.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    OpenMLFlow\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.obtain_parameter_values","title":"obtain_parameter_values  <code>abstractmethod</code>","text":"<pre><code>obtain_parameter_values(flow: OpenMLFlow, model: Any = None) -&gt; list[dict[str, Any]]\n</code></pre> <p>Extracts all parameter settings required for the flow from the model.</p> <p>If no explicit model is provided, the parameters will be extracted from <code>flow.model</code> instead.</p> PARAMETER DESCRIPTION <code>flow</code> <p>OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)</p> <p> TYPE: <code>OpenMLFlow</code> </p> <code>model</code> <p>The model from which to obtain the parameter values. Must match the flow signature. If None, use the model specified in <code>OpenMLFlow.model</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list</code> <p>A list of dicts, where each dict has the following entries: - <code>oml:name</code> : str: The OpenML parameter name - <code>oml:value</code> : mixed: A representation of the parameter value - <code>oml:component</code> : int: flow id to which the parameter belongs</p> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef obtain_parameter_values(\n    self,\n    flow: OpenMLFlow,\n    model: Any = None,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Extracts all parameter settings required for the flow from the model.\n\n    If no explicit model is provided, the parameters will be extracted from `flow.model`\n    instead.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)\n\n    model: Any, optional (default=None)\n        The model from which to obtain the parameter values. Must match the flow signature.\n        If None, use the model specified in ``OpenMLFlow.model``.\n\n    Returns\n    -------\n    list\n        A list of dicts, where each dict has the following entries:\n        - ``oml:name`` : str: The OpenML parameter name\n        - ``oml:value`` : mixed: A representation of the parameter value\n        - ``oml:component`` : int: flow id to which the parameter belongs\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/extension_interface/#openml.extensions.extension_interface.Extension.seed_model","title":"seed_model  <code>abstractmethod</code>","text":"<pre><code>seed_model(model: Any, seed: int | None) -&gt; Any\n</code></pre> <p>Set the seed of all the unseeded components of a model and return the seeded model.</p> <p>Required so that all seed information can be uploaded to OpenML for reproducible results.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to be seeded</p> <p> TYPE: <code>Any</code> </p> <code>seed</code> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>model</code> Source code in <code>openml/extensions/extension_interface.py</code> <pre><code>@abstractmethod\ndef seed_model(self, model: Any, seed: int | None) -&gt; Any:\n    \"\"\"Set the seed of all the unseeded components of a model and return the seeded model.\n\n    Required so that all seed information can be uploaded to OpenML for reproducible results.\n\n    Parameters\n    ----------\n    model : Any\n        The model to be seeded\n    seed : int\n\n    Returns\n    -------\n    model\n    \"\"\"\n</code></pre>"},{"location":"reference/extensions/functions/","title":"functions","text":""},{"location":"reference/extensions/functions/#openml.extensions.functions","title":"openml.extensions.functions","text":""},{"location":"reference/extensions/functions/#openml.extensions.functions.get_extension_by_flow","title":"get_extension_by_flow","text":"<pre><code>get_extension_by_flow(flow: OpenMLFlow, raise_if_no_extension: bool = False) -&gt; Extension | None\n</code></pre> <p>Get an extension which can handle the given flow.</p> <p>Iterates all registered extensions and checks whether they can handle the presented flow. Raises an exception if two extensions can handle a flow.</p> PARAMETER DESCRIPTION <code>flow</code> <p> TYPE: <code>OpenMLFlow</code> </p> <code>raise_if_no_extension</code> <p>Raise an exception if no registered extension can handle the presented flow.</p> <p> TYPE: <code>bool (optional</code> DEFAULT: <code>False)</code> </p> RETURNS DESCRIPTION <code>Extension or None</code> Source code in <code>openml/extensions/functions.py</code> <pre><code>def get_extension_by_flow(\n    flow: OpenMLFlow,\n    raise_if_no_extension: bool = False,  # noqa: FBT001, FBT002\n) -&gt; Extension | None:\n    \"\"\"Get an extension which can handle the given flow.\n\n    Iterates all registered extensions and checks whether they can handle the presented flow.\n    Raises an exception if two extensions can handle a flow.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    raise_if_no_extension : bool (optional, default=False)\n        Raise an exception if no registered extension can handle the presented flow.\n\n    Returns\n    -------\n    Extension or None\n    \"\"\"\n    # import openml_sklearn to register SklearnExtension\n    if importlib.util.find_spec(\"openml_sklearn\"):\n        import openml_sklearn  # noqa: F401\n\n    candidates = []\n    for extension_class in openml.extensions.extensions:\n        if extension_class.can_handle_flow(flow):\n            candidates.append(extension_class())\n    if len(candidates) == 0:\n        if raise_if_no_extension:\n            install_instruction = \"\"\n            if flow.name.startswith(\"sklearn\"):\n                install_instruction = SKLEARN_HINT\n            raise ValueError(\n                f\"No extension registered which can handle flow: {flow.flow_id} ({flow.name}). \"\n                f\"{install_instruction}\"\n            )\n\n        return None\n\n    if len(candidates) == 1:\n        return candidates[0]\n\n    raise ValueError(\n        f\"Multiple extensions registered which can handle flow: {flow}, but only one \"\n        f\"is allowed ({candidates}).\",\n    )\n</code></pre>"},{"location":"reference/extensions/functions/#openml.extensions.functions.get_extension_by_model","title":"get_extension_by_model","text":"<pre><code>get_extension_by_model(model: Any, raise_if_no_extension: bool = False) -&gt; Extension | None\n</code></pre> <p>Get an extension which can handle the given flow.</p> <p>Iterates all registered extensions and checks whether they can handle the presented model. Raises an exception if two extensions can handle a model.</p> PARAMETER DESCRIPTION <code>model</code> <p> TYPE: <code>Any</code> </p> <code>raise_if_no_extension</code> <p>Raise an exception if no registered extension can handle the presented model.</p> <p> TYPE: <code>bool (optional</code> DEFAULT: <code>False)</code> </p> RETURNS DESCRIPTION <code>Extension or None</code> Source code in <code>openml/extensions/functions.py</code> <pre><code>def get_extension_by_model(\n    model: Any,\n    raise_if_no_extension: bool = False,  # noqa: FBT001, FBT002\n) -&gt; Extension | None:\n    \"\"\"Get an extension which can handle the given flow.\n\n    Iterates all registered extensions and checks whether they can handle the presented model.\n    Raises an exception if two extensions can handle a model.\n\n    Parameters\n    ----------\n    model : Any\n\n    raise_if_no_extension : bool (optional, default=False)\n        Raise an exception if no registered extension can handle the presented model.\n\n    Returns\n    -------\n    Extension or None\n    \"\"\"\n    # import openml_sklearn to register SklearnExtension\n    if importlib.util.find_spec(\"openml_sklearn\"):\n        import openml_sklearn  # noqa: F401\n\n    candidates = []\n    for extension_class in openml.extensions.extensions:\n        if extension_class.can_handle_model(model):\n            candidates.append(extension_class())\n    if len(candidates) == 0:\n        if raise_if_no_extension:\n            install_instruction = \"\"\n            if type(model).__module__.startswith(\"sklearn\"):\n                install_instruction = SKLEARN_HINT\n            raise ValueError(\n                f\"No extension registered which can handle model: {model}. {install_instruction}\"\n            )\n\n        return None\n\n    if len(candidates) == 1:\n        return candidates[0]\n\n    raise ValueError(\n        f\"Multiple extensions registered which can handle model: {model}, but only one \"\n        f\"is allowed ({candidates}).\",\n    )\n</code></pre>"},{"location":"reference/extensions/functions/#openml.extensions.functions.register_extension","title":"register_extension","text":"<pre><code>register_extension(extension: type[Extension]) -&gt; None\n</code></pre> <p>Register an extension.</p> <p>Registered extensions are considered by <code>get_extension_by_flow</code> and <code>get_extension_by_model</code>, which are used by <code>openml.flow</code> and <code>openml.runs</code>.</p> PARAMETER DESCRIPTION <code>extension</code> <p> TYPE: <code>Type[Extension]</code> </p> RETURNS DESCRIPTION <code>None</code> Source code in <code>openml/extensions/functions.py</code> <pre><code>def register_extension(extension: type[Extension]) -&gt; None:\n    \"\"\"Register an extension.\n\n    Registered extensions are considered by ``get_extension_by_flow`` and\n    ``get_extension_by_model``, which are used by ``openml.flow`` and ``openml.runs``.\n\n    Parameters\n    ----------\n    extension : Type[Extension]\n\n    Returns\n    -------\n    None\n    \"\"\"\n    openml.extensions.extensions.append(extension)\n</code></pre>"},{"location":"reference/flows/","title":"flows","text":""},{"location":"reference/flows/#openml.flows","title":"openml.flows","text":""},{"location":"reference/flows/#openml.flows.OpenMLFlow","title":"OpenMLFlow","text":"<pre><code>OpenMLFlow(name: str, description: str, model: object, components: dict, parameters: dict, parameters_meta_info: dict, external_version: str, tags: list, language: str, dependencies: str, class_name: str | None = None, custom_name: str | None = None, binary_url: str | None = None, binary_format: str | None = None, binary_md5: str | None = None, uploader: str | None = None, upload_date: str | None = None, flow_id: int | None = None, extension: Extension | None = None, version: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Flow. Stores machine learning models.</p> <p>Flows should not be generated manually, but by the function :meth:<code>openml.flows.create_flow_from_model</code>. Using this helper function ensures that all relevant fields are filled in.</p> <p>Implements <code>openml.implementation.upload.xsd &lt;https://github.com/openml/openml/blob/master/openml_OS/views/pages/api_new/v1/xsd/ openml.implementation.upload.xsd&gt;</code>_.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the flow. Is used together with the attribute <code>external_version</code> as a unique identifier of the flow.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Human-readable description of the flow (free text).</p> <p> TYPE: <code>str</code> </p> <code>model</code> <p>ML model which is described by this flow.</p> <p> TYPE: <code>object</code> </p> <code>components</code> <p>Mapping from component identifier to an OpenMLFlow object. Components are usually subfunctions of an algorithm (e.g. kernels), base learners in ensemble algorithms (decision tree in adaboost) or building blocks of a machine learning pipeline. Components are modeled as independent flows and can be shared between flows (different pipelines can use the same components).</p> <p> TYPE: <code>OrderedDict</code> </p> <code>parameters</code> <p>Mapping from parameter name to the parameter default value. The parameter default value must be of type <code>str</code>, so that the respective toolbox plugin can take care of casting the parameter default value to the correct type.</p> <p> TYPE: <code>OrderedDict</code> </p> <code>parameters_meta_info</code> <p>Mapping from parameter name to <code>dict</code>. Stores additional information for each parameter. Required keys are <code>data_type</code> and <code>description</code>.</p> <p> TYPE: <code>OrderedDict</code> </p> <code>external_version</code> <p>Version number of the software the flow is implemented in. Is used together with the attribute <code>name</code> as a uniquer identifier of the flow.</p> <p> TYPE: <code>str</code> </p> <code>tags</code> <p>List of tags. Created on the server by other API calls.</p> <p> TYPE: <code>list</code> </p> <code>language</code> <p>Natural language the flow is described in (not the programming language).</p> <p> TYPE: <code>str</code> </p> <code>dependencies</code> <p>A list of dependencies necessary to run the flow. This field should contain all libraries the flow depends on. To allow reproducibility it should also specify the exact version numbers.</p> <p> TYPE: <code>str</code> </p> <code>class_name</code> <p>The development language name of the class which is described by this flow.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>custom_name</code> <p>Custom name of the flow given by the owner.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>binary_url</code> <p>Url from which the binary can be downloaded. Added by the server. Ignored when uploaded manually. Will not be used by the python API because binaries aren't compatible across machines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>binary_format</code> <p>Format in which the binary code was uploaded. Will not be used by the python API because binaries aren't compatible across machines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>binary_md5</code> <p>MD5 checksum to check if the binary code was correctly downloaded. Will not be used by the python API because binaries aren't compatible across machines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>uploader</code> <p>OpenML user ID of the uploader. Filled in by the server.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>upload_date</code> <p>Date the flow was uploaded. Filled in by the server.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>flow_id</code> <p>Flow ID. Assigned by the server.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>extension</code> <p>The extension for a flow (e.g., sklearn).</p> <p> TYPE: <code>Extension</code> DEFAULT: <code>None</code> </p> <code>version</code> <p>OpenML version of the flow. Assigned by the server.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/flows/flow.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    name: str,\n    description: str,\n    model: object,\n    components: dict,\n    parameters: dict,\n    parameters_meta_info: dict,\n    external_version: str,\n    tags: list,\n    language: str,\n    dependencies: str,\n    class_name: str | None = None,\n    custom_name: str | None = None,\n    binary_url: str | None = None,\n    binary_format: str | None = None,\n    binary_md5: str | None = None,\n    uploader: str | None = None,\n    upload_date: str | None = None,\n    flow_id: int | None = None,\n    extension: Extension | None = None,\n    version: str | None = None,\n):\n    self.name = name\n    self.description = description\n    self.model = model\n\n    for variable, variable_name in [\n        [components, \"components\"],\n        [parameters, \"parameters\"],\n        [parameters_meta_info, \"parameters_meta_info\"],\n    ]:\n        if not isinstance(variable, (OrderedDict, dict)):\n            raise TypeError(\n                f\"{variable_name} must be of type OrderedDict or dict, \"\n                f\"but is {type(variable)}.\",\n            )\n\n    self.components = components\n    self.parameters = parameters\n    self.parameters_meta_info = parameters_meta_info\n    self.class_name = class_name\n\n    keys_parameters = set(parameters.keys())\n    keys_parameters_meta_info = set(parameters_meta_info.keys())\n    if len(keys_parameters.difference(keys_parameters_meta_info)) &gt; 0:\n        raise ValueError(\n            f\"Parameter {keys_parameters.difference(keys_parameters_meta_info)!s} only in \"\n            \"parameters, but not in parameters_meta_info.\",\n        )\n    if len(keys_parameters_meta_info.difference(keys_parameters)) &gt; 0:\n        raise ValueError(\n            f\"Parameter {keys_parameters_meta_info.difference(keys_parameters)!s} only in \"\n            \" parameters_meta_info, but not in parameters.\",\n        )\n\n    self.external_version = external_version\n    self.uploader = uploader\n\n    self.custom_name = custom_name\n    self.tags = tags if tags is not None else []\n    self.binary_url = binary_url\n    self.binary_format = binary_format\n    self.binary_md5 = binary_md5\n    self.version = version\n    self.upload_date = upload_date\n    self.language = language\n    self.dependencies = dependencies\n    self.flow_id = flow_id\n    self._extension = extension\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.extension","title":"extension  <code>property</code>","text":"<pre><code>extension: Extension\n</code></pre> <p>The extension of the flow (e.g., sklearn).</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The ID of the flow.</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.from_filesystem","title":"from_filesystem  <code>classmethod</code>","text":"<pre><code>from_filesystem(input_directory: str | Path) -&gt; OpenMLFlow\n</code></pre> <p>Read a flow from an XML in input_directory on the filesystem.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>@classmethod\ndef from_filesystem(cls, input_directory: str | Path) -&gt; OpenMLFlow:\n    \"\"\"Read a flow from an XML in input_directory on the filesystem.\"\"\"\n    input_directory = Path(input_directory) / \"flow.xml\"\n    with input_directory.open() as f:\n        xml_string = f.read()\n    return OpenMLFlow._from_dict(xmltodict.parse(xml_string))\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.get_structure","title":"get_structure","text":"<pre><code>get_structure(key_item: str) -&gt; dict[str, list[str]]\n</code></pre> <p>Returns for each sub-component of the flow the path of identifiers that should be traversed to reach this component. The resulting dict maps a key (identifying a flow by either its id, name or fullname) to the parameter prefix.</p> PARAMETER DESCRIPTION <code>key_item</code> <p>The flow attribute that will be used to identify flows in the structure. Allowed values {flow_id, name}</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict[str, List[str]]</code> <p>The flow structure</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def get_structure(self, key_item: str) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Returns for each sub-component of the flow the path of identifiers\n    that should be traversed to reach this component. The resulting dict\n    maps a key (identifying a flow by either its id, name or fullname) to\n    the parameter prefix.\n\n    Parameters\n    ----------\n    key_item: str\n        The flow attribute that will be used to identify flows in the\n        structure. Allowed values {flow_id, name}\n\n    Returns\n    -------\n    dict[str, List[str]]\n        The flow structure\n    \"\"\"\n    if key_item not in [\"flow_id\", \"name\"]:\n        raise ValueError(\"key_item should be in {flow_id, name}\")\n    structure = {}\n    for key, sub_flow in self.components.items():\n        sub_structure = sub_flow.get_structure(key_item)\n        for flow_name, flow_sub_structure in sub_structure.items():\n            structure[flow_name] = [key, *flow_sub_structure]\n    structure[getattr(self, key_item)] = []\n    return structure\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.get_subflow","title":"get_subflow","text":"<pre><code>get_subflow(structure: list[str]) -&gt; OpenMLFlow\n</code></pre> <p>Returns a subflow from the tree of dependencies.</p> PARAMETER DESCRIPTION <code>structure</code> <p>A list of strings, indicating the location of the subflow</p> <p> TYPE: <code>list[str]</code> </p> RETURNS DESCRIPTION <code>OpenMLFlow</code> <p>The OpenMLFlow that corresponds to the structure</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def get_subflow(self, structure: list[str]) -&gt; OpenMLFlow:\n    \"\"\"\n    Returns a subflow from the tree of dependencies.\n\n    Parameters\n    ----------\n    structure: list[str]\n        A list of strings, indicating the location of the subflow\n\n    Returns\n    -------\n    OpenMLFlow\n        The OpenMLFlow that corresponds to the structure\n    \"\"\"\n    # make a copy of structure, as we don't want to change it in the\n    # outer scope\n    structure = list(structure)\n    if len(structure) &lt; 1:\n        raise ValueError(\"Please provide a structure list of size &gt;= 1\")\n    sub_identifier = structure[0]\n    if sub_identifier not in self.components:\n        raise ValueError(\n            f\"Flow {self.name} does not contain component with \" f\"identifier {sub_identifier}\",\n        )\n    if len(structure) == 1:\n        return self.components[sub_identifier]  # type: ignore\n\n    structure.pop(0)\n    return self.components[sub_identifier].get_subflow(structure)  # type: ignore\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.publish","title":"publish","text":"<pre><code>publish(raise_error_if_exists: bool = False) -&gt; OpenMLFlow\n</code></pre> <p>Publish this flow to OpenML server.</p> <p>Raises a PyOpenMLError if the flow exists on the server, but <code>self.flow_id</code> does not match the server known flow id.</p> PARAMETER DESCRIPTION <code>raise_error_if_exists</code> <p>If True, raise PyOpenMLError if the flow exists on the server. If False, update the local flow to match the server flow.</p> <p> TYPE: <code>(bool, optional(default=False))</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>self</code> <p> TYPE: <code>OpenMLFlow</code> </p> Source code in <code>openml/flows/flow.py</code> <pre><code>def publish(self, raise_error_if_exists: bool = False) -&gt; OpenMLFlow:  # noqa: FBT001, FBT002\n    \"\"\"Publish this flow to OpenML server.\n\n    Raises a PyOpenMLError if the flow exists on the server, but\n    `self.flow_id` does not match the server known flow id.\n\n    Parameters\n    ----------\n    raise_error_if_exists : bool, optional (default=False)\n        If True, raise PyOpenMLError if the flow exists on the server.\n        If False, update the local flow to match the server flow.\n\n    Returns\n    -------\n    self : OpenMLFlow\n\n    \"\"\"\n    # Import at top not possible because of cyclic dependencies. In\n    # particular, flow.py tries to import functions.py in order to call\n    # get_flow(), while functions.py tries to import flow.py in order to\n    # instantiate an OpenMLFlow.\n    import openml.flows.functions\n\n    flow_id = openml.flows.functions.flow_exists(self.name, self.external_version)\n    if not flow_id:\n        if self.flow_id:\n            raise openml.exceptions.PyOpenMLError(\n                \"Flow does not exist on the server, \" \"but 'flow.flow_id' is not None.\",\n            )\n        super().publish()\n        assert self.flow_id is not None  # for mypy\n        flow_id = self.flow_id\n    elif raise_error_if_exists:\n        error_message = f\"This OpenMLFlow already exists with id: {flow_id}.\"\n        raise openml.exceptions.PyOpenMLError(error_message)\n    elif self.flow_id is not None and self.flow_id != flow_id:\n        raise openml.exceptions.PyOpenMLError(\n            \"Local flow_id does not match server flow_id: \" f\"'{self.flow_id}' vs '{flow_id}'\",\n        )\n\n    flow = openml.flows.functions.get_flow(flow_id)\n    _copy_server_fields(flow, self)\n    try:\n        openml.flows.functions.assert_flows_equal(\n            self,\n            flow,\n            flow.upload_date,\n            ignore_parameter_values=True,\n            ignore_custom_name_if_none=True,\n        )\n    except ValueError as e:\n        message = e.args[0]\n        raise ValueError(\n            \"The flow on the server is inconsistent with the local flow. \"\n            f\"The server flow ID is {flow_id}. Please check manually and remove \"\n            f\"the flow if necessary! Error is:\\n'{message}'\",\n        ) from e\n    return self\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.to_filesystem","title":"to_filesystem","text":"<pre><code>to_filesystem(output_directory: str | Path) -&gt; None\n</code></pre> <p>Write a flow to the filesystem as XML to output_directory.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def to_filesystem(self, output_directory: str | Path) -&gt; None:\n    \"\"\"Write a flow to the filesystem as XML to output_directory.\"\"\"\n    output_directory = Path(output_directory)\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    output_path = output_directory / \"flow.xml\"\n    if output_path.exists():\n        raise ValueError(\"Output directory already contains a flow.xml file.\")\n\n    run_xml = self._to_xml()\n    with output_path.open(\"w\") as f:\n        f.write(run_xml)\n</code></pre>"},{"location":"reference/flows/#openml.flows.OpenMLFlow.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/flows/#openml.flows.assert_flows_equal","title":"assert_flows_equal","text":"<pre><code>assert_flows_equal(flow1: OpenMLFlow, flow2: OpenMLFlow, ignore_parameter_values_on_older_children: str | None = None, ignore_parameter_values: bool = False, ignore_custom_name_if_none: bool = False, check_description: bool = True) -&gt; None\n</code></pre> <p>Check equality of two flows.</p> <p>Two flows are equal if their all keys which are not set by the server are equal, as well as all their parameters and components.</p> PARAMETER DESCRIPTION <code>flow1</code> <p> TYPE: <code>OpenMLFlow</code> </p> <code>flow2</code> <p> TYPE: <code>OpenMLFlow</code> </p> <code>ignore_parameter_values_on_older_children</code> <p>If set to <code>OpenMLFlow.upload_date</code>, ignores parameters in a child flow if it's upload date predates the upload date of the parent flow.</p> <p> TYPE: <code>str(optional)</code> DEFAULT: <code>None</code> </p> <code>ignore_parameter_values</code> <p>Whether to ignore parameter values when comparing flows.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_custom_name_if_none</code> <p>Whether to ignore the custom name field if either flow has <code>custom_name</code> equal to <code>None</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>check_description</code> <p>Whether to ignore matching of flow descriptions.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>openml/flows/functions.py</code> <pre><code>def assert_flows_equal(  # noqa: C901, PLR0912, PLR0913, PLR0915\n    flow1: OpenMLFlow,\n    flow2: OpenMLFlow,\n    ignore_parameter_values_on_older_children: str | None = None,\n    ignore_parameter_values: bool = False,  # noqa: FBT001, FBT002\n    ignore_custom_name_if_none: bool = False,  # noqa:  FBT001, FBT002\n    check_description: bool = True,  # noqa:  FBT001, FBT002\n) -&gt; None:\n    \"\"\"Check equality of two flows.\n\n    Two flows are equal if their all keys which are not set by the server\n    are equal, as well as all their parameters and components.\n\n    Parameters\n    ----------\n    flow1 : OpenMLFlow\n\n    flow2 : OpenMLFlow\n\n    ignore_parameter_values_on_older_children : str (optional)\n        If set to ``OpenMLFlow.upload_date``, ignores parameters in a child\n        flow if it's upload date predates the upload date of the parent flow.\n\n    ignore_parameter_values : bool\n        Whether to ignore parameter values when comparing flows.\n\n    ignore_custom_name_if_none : bool\n        Whether to ignore the custom name field if either flow has `custom_name` equal to `None`.\n\n    check_description : bool\n        Whether to ignore matching of flow descriptions.\n    \"\"\"\n    if not isinstance(flow1, OpenMLFlow):\n        raise TypeError(f\"Argument 1 must be of type OpenMLFlow, but is {type(flow1)}\")\n\n    if not isinstance(flow2, OpenMLFlow):\n        raise TypeError(f\"Argument 2 must be of type OpenMLFlow, but is {type(flow2)}\")\n\n    # TODO as they are actually now saved during publish, it might be good to\n    # check for the equality of these as well.\n    generated_by_the_server = [\n        \"flow_id\",\n        \"uploader\",\n        \"version\",\n        \"upload_date\",\n        # Tags aren't directly created by the server,\n        # but the uploader has no control over them!\n        \"tags\",\n    ]\n    ignored_by_python_api = [\"binary_url\", \"binary_format\", \"binary_md5\", \"model\", \"_entity_id\"]\n\n    for key in set(flow1.__dict__.keys()).union(flow2.__dict__.keys()):\n        if key in generated_by_the_server + ignored_by_python_api:\n            continue\n        attr1 = getattr(flow1, key, None)\n        attr2 = getattr(flow2, key, None)\n        if key == \"components\":\n            if not (isinstance(attr1, Dict) and isinstance(attr2, Dict)):\n                raise TypeError(\"Cannot compare components because they are not dictionary.\")\n\n            for name in set(attr1.keys()).union(attr2.keys()):\n                if name not in attr1:\n                    raise ValueError(\n                        f\"Component {name} only available in argument2, but not in argument1.\",\n                    )\n                if name not in attr2:\n                    raise ValueError(\n                        f\"Component {name} only available in argument2, but not in argument1.\",\n                    )\n                assert_flows_equal(\n                    attr1[name],\n                    attr2[name],\n                    ignore_parameter_values_on_older_children,\n                    ignore_parameter_values,\n                    ignore_custom_name_if_none,\n                )\n        elif key == \"_extension\":\n            continue\n        elif check_description and key == \"description\":\n            # to ignore matching of descriptions since sklearn based flows may have\n            # altering docstrings and is not guaranteed to be consistent\n            continue\n        else:\n            if key == \"parameters\":\n                if ignore_parameter_values or ignore_parameter_values_on_older_children:\n                    params_flow_1 = set(flow1.parameters.keys())\n                    params_flow_2 = set(flow2.parameters.keys())\n                    symmetric_difference = params_flow_1 ^ params_flow_2\n                    if len(symmetric_difference) &gt; 0:\n                        raise ValueError(\n                            f\"Flow {flow1.name}: parameter set of flow \"\n                            \"differs from the parameters stored \"\n                            \"on the server.\",\n                        )\n\n                if ignore_parameter_values_on_older_children:\n                    assert (\n                        flow1.upload_date is not None\n                    ), \"Flow1 has no upload date that allows us to compare age of children.\"\n                    upload_date_current_flow = dateutil.parser.parse(flow1.upload_date)\n                    upload_date_parent_flow = dateutil.parser.parse(\n                        ignore_parameter_values_on_older_children,\n                    )\n                    if upload_date_current_flow &lt; upload_date_parent_flow:\n                        continue\n\n                if ignore_parameter_values:\n                    # Continue needs to be done here as the first if\n                    # statement triggers in both special cases\n                    continue\n            elif (\n                key == \"custom_name\"\n                and ignore_custom_name_if_none\n                and (attr1 is None or attr2 is None)\n            ):\n                # If specified, we allow `custom_name` inequality if one flow's name is None.\n                # Helps with backwards compatibility as `custom_name` is now auto-generated, but\n                # before it used to be `None`.\n                continue\n            elif key == \"parameters_meta_info\":\n                # this value is a dictionary where each key is a parameter name, containing another\n                # dictionary with keys specifying the parameter's 'description' and 'data_type'\n                # checking parameter descriptions can be ignored since that might change\n                # data type check can also be ignored if one of them is not defined, i.e., None\n                params1 = set(flow1.parameters_meta_info)\n                params2 = set(flow2.parameters_meta_info)\n                if params1 != params2:\n                    raise ValueError(\n                        \"Parameter list in meta info for parameters differ in the two flows.\",\n                    )\n                # iterating over the parameter's meta info list\n                for param in params1:\n                    if (\n                        isinstance(flow1.parameters_meta_info[param], Dict)\n                        and isinstance(flow2.parameters_meta_info[param], Dict)\n                        and \"data_type\" in flow1.parameters_meta_info[param]\n                        and \"data_type\" in flow2.parameters_meta_info[param]\n                    ):\n                        value1 = flow1.parameters_meta_info[param][\"data_type\"]\n                        value2 = flow2.parameters_meta_info[param][\"data_type\"]\n                    else:\n                        value1 = flow1.parameters_meta_info[param]\n                        value2 = flow2.parameters_meta_info[param]\n                    if value1 is None or value2 is None:\n                        continue\n\n                    if value1 != value2:\n                        raise ValueError(\n                            f\"Flow {flow1.name}: data type for parameter {param} in {key} differ \"\n                            f\"as {value1}\\nvs\\n{value2}\",\n                        )\n                # the continue is to avoid the 'attr != attr2' check at end of function\n                continue\n\n            if attr1 != attr2:\n                raise ValueError(\n                    f\"Flow {flow1.name!s}: values for attribute '{key!s}' differ: \"\n                    f\"'{attr1!s}'\\nvs\\n'{attr2!s}'.\",\n                )\n</code></pre>"},{"location":"reference/flows/#openml.flows.delete_flow","title":"delete_flow","text":"<pre><code>delete_flow(flow_id: int) -&gt; bool\n</code></pre> <p>Delete flow with id <code>flow_id</code> from the OpenML server.</p> <p>You can only delete flows which you uploaded and which which are not linked to runs.</p> PARAMETER DESCRIPTION <code>flow_id</code> <p>OpenML id of the flow</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the deletion was successful. False otherwise.</p> Source code in <code>openml/flows/functions.py</code> <pre><code>def delete_flow(flow_id: int) -&gt; bool:\n    \"\"\"Delete flow with id `flow_id` from the OpenML server.\n\n    You can only delete flows which you uploaded and which\n    which are not linked to runs.\n\n    Parameters\n    ----------\n    flow_id : int\n        OpenML id of the flow\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"flow\", flow_id)\n</code></pre>"},{"location":"reference/flows/#openml.flows.flow_exists","title":"flow_exists","text":"<pre><code>flow_exists(name: str, external_version: str) -&gt; int | bool\n</code></pre> <p>Retrieves the flow id.</p> <p>A flow is uniquely identified by name + external_version.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the flow</p> <p> TYPE: <code>string</code> </p> <code>external_version</code> <p>Version information associated with flow.</p> <p> TYPE: <code>string</code> </p> RETURNS DESCRIPTION <code>flow_exist</code> <p>flow id iff exists, False otherwise</p> <p> TYPE: <code>int or bool</code> </p> Notes <p>see www.openml.org/api_docs/#!/flow/get_flow_exists_name_version</p> Source code in <code>openml/flows/functions.py</code> <pre><code>def flow_exists(name: str, external_version: str) -&gt; int | bool:\n    \"\"\"Retrieves the flow id.\n\n    A flow is uniquely identified by name + external_version.\n\n    Parameters\n    ----------\n    name : string\n        Name of the flow\n    external_version : string\n        Version information associated with flow.\n\n    Returns\n    -------\n    flow_exist : int or bool\n        flow id iff exists, False otherwise\n\n    Notes\n    -----\n    see https://www.openml.org/api_docs/#!/flow/get_flow_exists_name_version\n    \"\"\"\n    if not (isinstance(name, str) and len(name) &gt; 0):\n        raise ValueError(\"Argument 'name' should be a non-empty string\")\n    if not (isinstance(name, str) and len(external_version) &gt; 0):\n        raise ValueError(\"Argument 'version' should be a non-empty string\")\n\n    xml_response = openml._api_calls._perform_api_call(\n        \"flow/exists\",\n        \"post\",\n        data={\"name\": name, \"external_version\": external_version},\n    )\n\n    result_dict = xmltodict.parse(xml_response)\n    flow_id = int(result_dict[\"oml:flow_exists\"][\"oml:id\"])\n    return flow_id if flow_id &gt; 0 else False\n</code></pre>"},{"location":"reference/flows/#openml.flows.get_flow","title":"get_flow","text":"<pre><code>get_flow(flow_id: int, reinstantiate: bool = False, strict_version: bool = True) -&gt; OpenMLFlow\n</code></pre> <p>Download the OpenML flow for a given flow ID.</p> PARAMETER DESCRIPTION <code>flow_id</code> <p>The OpenML flow id.</p> <p> TYPE: <code>int</code> </p> <code>reinstantiate</code> <p>Whether to reinstantiate the flow to a model instance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>strict_version</code> <p>Whether to fail if version requirements are not fulfilled.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>flow</code> <p>the flow</p> <p> TYPE: <code>OpenMLFlow</code> </p> Source code in <code>openml/flows/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_flow(flow_id: int, reinstantiate: bool = False, strict_version: bool = True) -&gt; OpenMLFlow:  # noqa: FBT001, FBT002\n    \"\"\"Download the OpenML flow for a given flow ID.\n\n    Parameters\n    ----------\n    flow_id : int\n        The OpenML flow id.\n\n    reinstantiate: bool\n        Whether to reinstantiate the flow to a model instance.\n\n    strict_version : bool, default=True\n        Whether to fail if version requirements are not fulfilled.\n\n    Returns\n    -------\n    flow : OpenMLFlow\n        the flow\n    \"\"\"\n    flow_id = int(flow_id)\n    flow = _get_flow_description(flow_id)\n\n    if reinstantiate:\n        flow.model = flow.extension.flow_to_model(flow, strict_version=strict_version)\n        if not strict_version:\n            # check if we need to return a new flow b/c of version mismatch\n            new_flow = flow.extension.model_to_flow(flow.model)\n            if new_flow.dependencies != flow.dependencies:\n                return new_flow\n    return flow\n</code></pre>"},{"location":"reference/flows/#openml.flows.get_flow_id","title":"get_flow_id","text":"<pre><code>get_flow_id(model: Any | None = None, name: str | None = None, exact_version: bool = True) -&gt; int | bool | list[int]\n</code></pre> <p>Retrieves the flow id for a model or a flow name.</p> <p>Provide either a model or a name to this function. Depending on the input, it does</p> <ul> <li><code>model</code> and <code>exact_version == True</code>: This helper function first queries for the necessary   extension. Second, it uses that extension to convert the model into a flow. Third, it   executes <code>flow_exists</code> to potentially obtain the flow id the flow is published to the   server.</li> <li><code>model</code> and <code>exact_version == False</code>: This helper function first queries for the   necessary extension. Second, it uses that extension to convert the model into a flow. Third   it calls <code>list_flows</code> and filters the returned values based on the flow name.</li> <li><code>name</code>: Ignores <code>exact_version</code> and calls <code>list_flows</code>, then filters the returned   values based on the flow name.</li> </ul> PARAMETER DESCRIPTION <code>model</code> <p>Any model. Must provide either <code>model</code> or <code>name</code>.</p> <p> TYPE: <code>object</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the flow. Must provide either <code>model</code> or <code>name</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>exact_version</code> <p>Whether to return the flow id of the exact version or all flow ids where the name of the flow matches. This is only taken into account for a model where a version number is available (requires <code>model</code> to be set).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>(int or bool, List)</code> <p>flow id iff exists, <code>False</code> otherwise, List if <code>exact_version is False</code></p> Source code in <code>openml/flows/functions.py</code> <pre><code>def get_flow_id(\n    model: Any | None = None,\n    name: str | None = None,\n    exact_version: bool = True,  # noqa: FBT001, FBT002\n) -&gt; int | bool | list[int]:\n    \"\"\"Retrieves the flow id for a model or a flow name.\n\n    Provide either a model or a name to this function. Depending on the input, it does\n\n    * ``model`` and ``exact_version == True``: This helper function first queries for the necessary\n      extension. Second, it uses that extension to convert the model into a flow. Third, it\n      executes ``flow_exists`` to potentially obtain the flow id the flow is published to the\n      server.\n    * ``model`` and ``exact_version == False``: This helper function first queries for the\n      necessary extension. Second, it uses that extension to convert the model into a flow. Third\n      it calls ``list_flows`` and filters the returned values based on the flow name.\n    * ``name``: Ignores ``exact_version`` and calls ``list_flows``, then filters the returned\n      values based on the flow name.\n\n    Parameters\n    ----------\n    model : object\n        Any model. Must provide either ``model`` or ``name``.\n    name : str\n        Name of the flow. Must provide either ``model`` or ``name``.\n    exact_version : bool\n        Whether to return the flow id of the exact version or all flow ids where the name\n        of the flow matches. This is only taken into account for a model where a version number\n        is available (requires ``model`` to be set).\n\n    Returns\n    -------\n    int or bool, List\n        flow id iff exists, ``False`` otherwise, List if ``exact_version is False``\n    \"\"\"\n    if model is not None and name is not None:\n        raise ValueError(\"Must provide either argument `model` or argument `name`, but not both.\")\n\n    if model is not None:\n        extension = openml.extensions.get_extension_by_model(model, raise_if_no_extension=True)\n        if extension is None:\n            # This should never happen and is only here to please mypy will be gone soon once the\n            # whole function is removed\n            raise TypeError(extension)\n        flow = extension.model_to_flow(model)\n        flow_name = flow.name\n        external_version = flow.external_version\n    elif name is not None:\n        flow_name = name\n        exact_version = False\n        external_version = None\n    else:\n        raise ValueError(\n            \"Need to provide either argument `model` or argument `name`, but both are `None`.\"\n        )\n\n    if exact_version:\n        if external_version is None:\n            raise ValueError(\"exact_version should be False if model is None!\")\n        return flow_exists(name=flow_name, external_version=external_version)\n\n    flows = list_flows()\n    flows = flows.query(f'name == \"{flow_name}\"')\n    return flows[\"id\"].to_list()  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/flows/#openml.flows.list_flows","title":"list_flows","text":"<pre><code>list_flows(offset: int | None = None, size: int | None = None, tag: str | None = None, uploader: str | None = None) -&gt; DataFrame\n</code></pre> <p>Return a list of all flows which are on OpenML. (Supports large amount of results)</p> PARAMETER DESCRIPTION <code>offset</code> <p>the number of flows to skip, starting from the first</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>the maximum number of flows to return</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p>the tag to include</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Legal filter operators: uploader.</p> <p> </p> RETURNS DESCRIPTION <code>flows</code> <p>Each row maps to a dataset Each column contains the following information: - flow id - full name - name - version - external version - uploader</p> <p> TYPE: <code>dataframe</code> </p> Source code in <code>openml/flows/functions.py</code> <pre><code>def list_flows(\n    offset: int | None = None,\n    size: int | None = None,\n    tag: str | None = None,\n    uploader: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a list of all flows which are on OpenML.\n    (Supports large amount of results)\n\n    Parameters\n    ----------\n    offset : int, optional\n        the number of flows to skip, starting from the first\n    size : int, optional\n        the maximum number of flows to return\n    tag : str, optional\n        the tag to include\n    kwargs: dict, optional\n        Legal filter operators: uploader.\n\n    Returns\n    -------\n    flows : dataframe\n            Each row maps to a dataset\n            Each column contains the following information:\n            - flow id\n            - full name\n            - name\n            - version\n            - external version\n            - uploader\n    \"\"\"\n    listing_call = partial(_list_flows, tag=tag, uploader=uploader)\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/flows/flow/","title":"flow","text":""},{"location":"reference/flows/flow/#openml.flows.flow","title":"openml.flows.flow","text":""},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow","title":"OpenMLFlow","text":"<pre><code>OpenMLFlow(name: str, description: str, model: object, components: dict, parameters: dict, parameters_meta_info: dict, external_version: str, tags: list, language: str, dependencies: str, class_name: str | None = None, custom_name: str | None = None, binary_url: str | None = None, binary_format: str | None = None, binary_md5: str | None = None, uploader: str | None = None, upload_date: str | None = None, flow_id: int | None = None, extension: Extension | None = None, version: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Flow. Stores machine learning models.</p> <p>Flows should not be generated manually, but by the function :meth:<code>openml.flows.create_flow_from_model</code>. Using this helper function ensures that all relevant fields are filled in.</p> <p>Implements <code>openml.implementation.upload.xsd &lt;https://github.com/openml/openml/blob/master/openml_OS/views/pages/api_new/v1/xsd/ openml.implementation.upload.xsd&gt;</code>_.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the flow. Is used together with the attribute <code>external_version</code> as a unique identifier of the flow.</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>Human-readable description of the flow (free text).</p> <p> TYPE: <code>str</code> </p> <code>model</code> <p>ML model which is described by this flow.</p> <p> TYPE: <code>object</code> </p> <code>components</code> <p>Mapping from component identifier to an OpenMLFlow object. Components are usually subfunctions of an algorithm (e.g. kernels), base learners in ensemble algorithms (decision tree in adaboost) or building blocks of a machine learning pipeline. Components are modeled as independent flows and can be shared between flows (different pipelines can use the same components).</p> <p> TYPE: <code>OrderedDict</code> </p> <code>parameters</code> <p>Mapping from parameter name to the parameter default value. The parameter default value must be of type <code>str</code>, so that the respective toolbox plugin can take care of casting the parameter default value to the correct type.</p> <p> TYPE: <code>OrderedDict</code> </p> <code>parameters_meta_info</code> <p>Mapping from parameter name to <code>dict</code>. Stores additional information for each parameter. Required keys are <code>data_type</code> and <code>description</code>.</p> <p> TYPE: <code>OrderedDict</code> </p> <code>external_version</code> <p>Version number of the software the flow is implemented in. Is used together with the attribute <code>name</code> as a uniquer identifier of the flow.</p> <p> TYPE: <code>str</code> </p> <code>tags</code> <p>List of tags. Created on the server by other API calls.</p> <p> TYPE: <code>list</code> </p> <code>language</code> <p>Natural language the flow is described in (not the programming language).</p> <p> TYPE: <code>str</code> </p> <code>dependencies</code> <p>A list of dependencies necessary to run the flow. This field should contain all libraries the flow depends on. To allow reproducibility it should also specify the exact version numbers.</p> <p> TYPE: <code>str</code> </p> <code>class_name</code> <p>The development language name of the class which is described by this flow.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>custom_name</code> <p>Custom name of the flow given by the owner.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>binary_url</code> <p>Url from which the binary can be downloaded. Added by the server. Ignored when uploaded manually. Will not be used by the python API because binaries aren't compatible across machines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>binary_format</code> <p>Format in which the binary code was uploaded. Will not be used by the python API because binaries aren't compatible across machines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>binary_md5</code> <p>MD5 checksum to check if the binary code was correctly downloaded. Will not be used by the python API because binaries aren't compatible across machines.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>uploader</code> <p>OpenML user ID of the uploader. Filled in by the server.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>upload_date</code> <p>Date the flow was uploaded. Filled in by the server.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>flow_id</code> <p>Flow ID. Assigned by the server.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>extension</code> <p>The extension for a flow (e.g., sklearn).</p> <p> TYPE: <code>Extension</code> DEFAULT: <code>None</code> </p> <code>version</code> <p>OpenML version of the flow. Assigned by the server.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/flows/flow.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    name: str,\n    description: str,\n    model: object,\n    components: dict,\n    parameters: dict,\n    parameters_meta_info: dict,\n    external_version: str,\n    tags: list,\n    language: str,\n    dependencies: str,\n    class_name: str | None = None,\n    custom_name: str | None = None,\n    binary_url: str | None = None,\n    binary_format: str | None = None,\n    binary_md5: str | None = None,\n    uploader: str | None = None,\n    upload_date: str | None = None,\n    flow_id: int | None = None,\n    extension: Extension | None = None,\n    version: str | None = None,\n):\n    self.name = name\n    self.description = description\n    self.model = model\n\n    for variable, variable_name in [\n        [components, \"components\"],\n        [parameters, \"parameters\"],\n        [parameters_meta_info, \"parameters_meta_info\"],\n    ]:\n        if not isinstance(variable, (OrderedDict, dict)):\n            raise TypeError(\n                f\"{variable_name} must be of type OrderedDict or dict, \"\n                f\"but is {type(variable)}.\",\n            )\n\n    self.components = components\n    self.parameters = parameters\n    self.parameters_meta_info = parameters_meta_info\n    self.class_name = class_name\n\n    keys_parameters = set(parameters.keys())\n    keys_parameters_meta_info = set(parameters_meta_info.keys())\n    if len(keys_parameters.difference(keys_parameters_meta_info)) &gt; 0:\n        raise ValueError(\n            f\"Parameter {keys_parameters.difference(keys_parameters_meta_info)!s} only in \"\n            \"parameters, but not in parameters_meta_info.\",\n        )\n    if len(keys_parameters_meta_info.difference(keys_parameters)) &gt; 0:\n        raise ValueError(\n            f\"Parameter {keys_parameters_meta_info.difference(keys_parameters)!s} only in \"\n            \" parameters_meta_info, but not in parameters.\",\n        )\n\n    self.external_version = external_version\n    self.uploader = uploader\n\n    self.custom_name = custom_name\n    self.tags = tags if tags is not None else []\n    self.binary_url = binary_url\n    self.binary_format = binary_format\n    self.binary_md5 = binary_md5\n    self.version = version\n    self.upload_date = upload_date\n    self.language = language\n    self.dependencies = dependencies\n    self.flow_id = flow_id\n    self._extension = extension\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.extension","title":"extension  <code>property</code>","text":"<pre><code>extension: Extension\n</code></pre> <p>The extension of the flow (e.g., sklearn).</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The ID of the flow.</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.from_filesystem","title":"from_filesystem  <code>classmethod</code>","text":"<pre><code>from_filesystem(input_directory: str | Path) -&gt; OpenMLFlow\n</code></pre> <p>Read a flow from an XML in input_directory on the filesystem.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>@classmethod\ndef from_filesystem(cls, input_directory: str | Path) -&gt; OpenMLFlow:\n    \"\"\"Read a flow from an XML in input_directory on the filesystem.\"\"\"\n    input_directory = Path(input_directory) / \"flow.xml\"\n    with input_directory.open() as f:\n        xml_string = f.read()\n    return OpenMLFlow._from_dict(xmltodict.parse(xml_string))\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.get_structure","title":"get_structure","text":"<pre><code>get_structure(key_item: str) -&gt; dict[str, list[str]]\n</code></pre> <p>Returns for each sub-component of the flow the path of identifiers that should be traversed to reach this component. The resulting dict maps a key (identifying a flow by either its id, name or fullname) to the parameter prefix.</p> PARAMETER DESCRIPTION <code>key_item</code> <p>The flow attribute that will be used to identify flows in the structure. Allowed values {flow_id, name}</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>dict[str, List[str]]</code> <p>The flow structure</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def get_structure(self, key_item: str) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Returns for each sub-component of the flow the path of identifiers\n    that should be traversed to reach this component. The resulting dict\n    maps a key (identifying a flow by either its id, name or fullname) to\n    the parameter prefix.\n\n    Parameters\n    ----------\n    key_item: str\n        The flow attribute that will be used to identify flows in the\n        structure. Allowed values {flow_id, name}\n\n    Returns\n    -------\n    dict[str, List[str]]\n        The flow structure\n    \"\"\"\n    if key_item not in [\"flow_id\", \"name\"]:\n        raise ValueError(\"key_item should be in {flow_id, name}\")\n    structure = {}\n    for key, sub_flow in self.components.items():\n        sub_structure = sub_flow.get_structure(key_item)\n        for flow_name, flow_sub_structure in sub_structure.items():\n            structure[flow_name] = [key, *flow_sub_structure]\n    structure[getattr(self, key_item)] = []\n    return structure\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.get_subflow","title":"get_subflow","text":"<pre><code>get_subflow(structure: list[str]) -&gt; OpenMLFlow\n</code></pre> <p>Returns a subflow from the tree of dependencies.</p> PARAMETER DESCRIPTION <code>structure</code> <p>A list of strings, indicating the location of the subflow</p> <p> TYPE: <code>list[str]</code> </p> RETURNS DESCRIPTION <code>OpenMLFlow</code> <p>The OpenMLFlow that corresponds to the structure</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def get_subflow(self, structure: list[str]) -&gt; OpenMLFlow:\n    \"\"\"\n    Returns a subflow from the tree of dependencies.\n\n    Parameters\n    ----------\n    structure: list[str]\n        A list of strings, indicating the location of the subflow\n\n    Returns\n    -------\n    OpenMLFlow\n        The OpenMLFlow that corresponds to the structure\n    \"\"\"\n    # make a copy of structure, as we don't want to change it in the\n    # outer scope\n    structure = list(structure)\n    if len(structure) &lt; 1:\n        raise ValueError(\"Please provide a structure list of size &gt;= 1\")\n    sub_identifier = structure[0]\n    if sub_identifier not in self.components:\n        raise ValueError(\n            f\"Flow {self.name} does not contain component with \" f\"identifier {sub_identifier}\",\n        )\n    if len(structure) == 1:\n        return self.components[sub_identifier]  # type: ignore\n\n    structure.pop(0)\n    return self.components[sub_identifier].get_subflow(structure)  # type: ignore\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.publish","title":"publish","text":"<pre><code>publish(raise_error_if_exists: bool = False) -&gt; OpenMLFlow\n</code></pre> <p>Publish this flow to OpenML server.</p> <p>Raises a PyOpenMLError if the flow exists on the server, but <code>self.flow_id</code> does not match the server known flow id.</p> PARAMETER DESCRIPTION <code>raise_error_if_exists</code> <p>If True, raise PyOpenMLError if the flow exists on the server. If False, update the local flow to match the server flow.</p> <p> TYPE: <code>(bool, optional(default=False))</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>self</code> <p> TYPE: <code>OpenMLFlow</code> </p> Source code in <code>openml/flows/flow.py</code> <pre><code>def publish(self, raise_error_if_exists: bool = False) -&gt; OpenMLFlow:  # noqa: FBT001, FBT002\n    \"\"\"Publish this flow to OpenML server.\n\n    Raises a PyOpenMLError if the flow exists on the server, but\n    `self.flow_id` does not match the server known flow id.\n\n    Parameters\n    ----------\n    raise_error_if_exists : bool, optional (default=False)\n        If True, raise PyOpenMLError if the flow exists on the server.\n        If False, update the local flow to match the server flow.\n\n    Returns\n    -------\n    self : OpenMLFlow\n\n    \"\"\"\n    # Import at top not possible because of cyclic dependencies. In\n    # particular, flow.py tries to import functions.py in order to call\n    # get_flow(), while functions.py tries to import flow.py in order to\n    # instantiate an OpenMLFlow.\n    import openml.flows.functions\n\n    flow_id = openml.flows.functions.flow_exists(self.name, self.external_version)\n    if not flow_id:\n        if self.flow_id:\n            raise openml.exceptions.PyOpenMLError(\n                \"Flow does not exist on the server, \" \"but 'flow.flow_id' is not None.\",\n            )\n        super().publish()\n        assert self.flow_id is not None  # for mypy\n        flow_id = self.flow_id\n    elif raise_error_if_exists:\n        error_message = f\"This OpenMLFlow already exists with id: {flow_id}.\"\n        raise openml.exceptions.PyOpenMLError(error_message)\n    elif self.flow_id is not None and self.flow_id != flow_id:\n        raise openml.exceptions.PyOpenMLError(\n            \"Local flow_id does not match server flow_id: \" f\"'{self.flow_id}' vs '{flow_id}'\",\n        )\n\n    flow = openml.flows.functions.get_flow(flow_id)\n    _copy_server_fields(flow, self)\n    try:\n        openml.flows.functions.assert_flows_equal(\n            self,\n            flow,\n            flow.upload_date,\n            ignore_parameter_values=True,\n            ignore_custom_name_if_none=True,\n        )\n    except ValueError as e:\n        message = e.args[0]\n        raise ValueError(\n            \"The flow on the server is inconsistent with the local flow. \"\n            f\"The server flow ID is {flow_id}. Please check manually and remove \"\n            f\"the flow if necessary! Error is:\\n'{message}'\",\n        ) from e\n    return self\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.to_filesystem","title":"to_filesystem","text":"<pre><code>to_filesystem(output_directory: str | Path) -&gt; None\n</code></pre> <p>Write a flow to the filesystem as XML to output_directory.</p> Source code in <code>openml/flows/flow.py</code> <pre><code>def to_filesystem(self, output_directory: str | Path) -&gt; None:\n    \"\"\"Write a flow to the filesystem as XML to output_directory.\"\"\"\n    output_directory = Path(output_directory)\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    output_path = output_directory / \"flow.xml\"\n    if output_path.exists():\n        raise ValueError(\"Output directory already contains a flow.xml file.\")\n\n    run_xml = self._to_xml()\n    with output_path.open(\"w\") as f:\n        f.write(run_xml)\n</code></pre>"},{"location":"reference/flows/flow/#openml.flows.flow.OpenMLFlow.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/flows/functions/","title":"functions","text":""},{"location":"reference/flows/functions/#openml.flows.functions","title":"openml.flows.functions","text":""},{"location":"reference/flows/functions/#openml.flows.functions.__list_flows","title":"__list_flows","text":"<pre><code>__list_flows(api_call: str) -&gt; DataFrame\n</code></pre> <p>Retrieve information about flows from OpenML API and parse it to a dictionary or a Pandas DataFrame.</p> PARAMETER DESCRIPTION <code>api_call</code> <p>Retrieves the information about flows.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>    The flows information in the specified output format.</code> Source code in <code>openml/flows/functions.py</code> <pre><code>def __list_flows(api_call: str) -&gt; pd.DataFrame:\n    \"\"\"Retrieve information about flows from OpenML API\n    and parse it to a dictionary or a Pandas DataFrame.\n\n    Parameters\n    ----------\n    api_call: str\n        Retrieves the information about flows.\n\n    Returns\n    -------\n        The flows information in the specified output format.\n    \"\"\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    flows_dict = xmltodict.parse(xml_string, force_list=(\"oml:flow\",))\n\n    # Minimalistic check if the XML is useful\n    assert isinstance(flows_dict[\"oml:flows\"][\"oml:flow\"], list), type(flows_dict[\"oml:flows\"])\n    assert flows_dict[\"oml:flows\"][\"@xmlns:oml\"] == \"http://openml.org/openml\", flows_dict[\n        \"oml:flows\"\n    ][\"@xmlns:oml\"]\n\n    flows = {}\n    for flow_ in flows_dict[\"oml:flows\"][\"oml:flow\"]:\n        fid = int(flow_[\"oml:id\"])\n        flow = {\n            \"id\": fid,\n            \"full_name\": flow_[\"oml:full_name\"],\n            \"name\": flow_[\"oml:name\"],\n            \"version\": flow_[\"oml:version\"],\n            \"external_version\": flow_[\"oml:external_version\"],\n            \"uploader\": flow_[\"oml:uploader\"],\n        }\n        flows[fid] = flow\n\n    return pd.DataFrame.from_dict(flows, orient=\"index\")\n</code></pre>"},{"location":"reference/flows/functions/#openml.flows.functions.assert_flows_equal","title":"assert_flows_equal","text":"<pre><code>assert_flows_equal(flow1: OpenMLFlow, flow2: OpenMLFlow, ignore_parameter_values_on_older_children: str | None = None, ignore_parameter_values: bool = False, ignore_custom_name_if_none: bool = False, check_description: bool = True) -&gt; None\n</code></pre> <p>Check equality of two flows.</p> <p>Two flows are equal if their all keys which are not set by the server are equal, as well as all their parameters and components.</p> PARAMETER DESCRIPTION <code>flow1</code> <p> TYPE: <code>OpenMLFlow</code> </p> <code>flow2</code> <p> TYPE: <code>OpenMLFlow</code> </p> <code>ignore_parameter_values_on_older_children</code> <p>If set to <code>OpenMLFlow.upload_date</code>, ignores parameters in a child flow if it's upload date predates the upload date of the parent flow.</p> <p> TYPE: <code>str(optional)</code> DEFAULT: <code>None</code> </p> <code>ignore_parameter_values</code> <p>Whether to ignore parameter values when comparing flows.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_custom_name_if_none</code> <p>Whether to ignore the custom name field if either flow has <code>custom_name</code> equal to <code>None</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>check_description</code> <p>Whether to ignore matching of flow descriptions.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>openml/flows/functions.py</code> <pre><code>def assert_flows_equal(  # noqa: C901, PLR0912, PLR0913, PLR0915\n    flow1: OpenMLFlow,\n    flow2: OpenMLFlow,\n    ignore_parameter_values_on_older_children: str | None = None,\n    ignore_parameter_values: bool = False,  # noqa: FBT001, FBT002\n    ignore_custom_name_if_none: bool = False,  # noqa:  FBT001, FBT002\n    check_description: bool = True,  # noqa:  FBT001, FBT002\n) -&gt; None:\n    \"\"\"Check equality of two flows.\n\n    Two flows are equal if their all keys which are not set by the server\n    are equal, as well as all their parameters and components.\n\n    Parameters\n    ----------\n    flow1 : OpenMLFlow\n\n    flow2 : OpenMLFlow\n\n    ignore_parameter_values_on_older_children : str (optional)\n        If set to ``OpenMLFlow.upload_date``, ignores parameters in a child\n        flow if it's upload date predates the upload date of the parent flow.\n\n    ignore_parameter_values : bool\n        Whether to ignore parameter values when comparing flows.\n\n    ignore_custom_name_if_none : bool\n        Whether to ignore the custom name field if either flow has `custom_name` equal to `None`.\n\n    check_description : bool\n        Whether to ignore matching of flow descriptions.\n    \"\"\"\n    if not isinstance(flow1, OpenMLFlow):\n        raise TypeError(f\"Argument 1 must be of type OpenMLFlow, but is {type(flow1)}\")\n\n    if not isinstance(flow2, OpenMLFlow):\n        raise TypeError(f\"Argument 2 must be of type OpenMLFlow, but is {type(flow2)}\")\n\n    # TODO as they are actually now saved during publish, it might be good to\n    # check for the equality of these as well.\n    generated_by_the_server = [\n        \"flow_id\",\n        \"uploader\",\n        \"version\",\n        \"upload_date\",\n        # Tags aren't directly created by the server,\n        # but the uploader has no control over them!\n        \"tags\",\n    ]\n    ignored_by_python_api = [\"binary_url\", \"binary_format\", \"binary_md5\", \"model\", \"_entity_id\"]\n\n    for key in set(flow1.__dict__.keys()).union(flow2.__dict__.keys()):\n        if key in generated_by_the_server + ignored_by_python_api:\n            continue\n        attr1 = getattr(flow1, key, None)\n        attr2 = getattr(flow2, key, None)\n        if key == \"components\":\n            if not (isinstance(attr1, Dict) and isinstance(attr2, Dict)):\n                raise TypeError(\"Cannot compare components because they are not dictionary.\")\n\n            for name in set(attr1.keys()).union(attr2.keys()):\n                if name not in attr1:\n                    raise ValueError(\n                        f\"Component {name} only available in argument2, but not in argument1.\",\n                    )\n                if name not in attr2:\n                    raise ValueError(\n                        f\"Component {name} only available in argument2, but not in argument1.\",\n                    )\n                assert_flows_equal(\n                    attr1[name],\n                    attr2[name],\n                    ignore_parameter_values_on_older_children,\n                    ignore_parameter_values,\n                    ignore_custom_name_if_none,\n                )\n        elif key == \"_extension\":\n            continue\n        elif check_description and key == \"description\":\n            # to ignore matching of descriptions since sklearn based flows may have\n            # altering docstrings and is not guaranteed to be consistent\n            continue\n        else:\n            if key == \"parameters\":\n                if ignore_parameter_values or ignore_parameter_values_on_older_children:\n                    params_flow_1 = set(flow1.parameters.keys())\n                    params_flow_2 = set(flow2.parameters.keys())\n                    symmetric_difference = params_flow_1 ^ params_flow_2\n                    if len(symmetric_difference) &gt; 0:\n                        raise ValueError(\n                            f\"Flow {flow1.name}: parameter set of flow \"\n                            \"differs from the parameters stored \"\n                            \"on the server.\",\n                        )\n\n                if ignore_parameter_values_on_older_children:\n                    assert (\n                        flow1.upload_date is not None\n                    ), \"Flow1 has no upload date that allows us to compare age of children.\"\n                    upload_date_current_flow = dateutil.parser.parse(flow1.upload_date)\n                    upload_date_parent_flow = dateutil.parser.parse(\n                        ignore_parameter_values_on_older_children,\n                    )\n                    if upload_date_current_flow &lt; upload_date_parent_flow:\n                        continue\n\n                if ignore_parameter_values:\n                    # Continue needs to be done here as the first if\n                    # statement triggers in both special cases\n                    continue\n            elif (\n                key == \"custom_name\"\n                and ignore_custom_name_if_none\n                and (attr1 is None or attr2 is None)\n            ):\n                # If specified, we allow `custom_name` inequality if one flow's name is None.\n                # Helps with backwards compatibility as `custom_name` is now auto-generated, but\n                # before it used to be `None`.\n                continue\n            elif key == \"parameters_meta_info\":\n                # this value is a dictionary where each key is a parameter name, containing another\n                # dictionary with keys specifying the parameter's 'description' and 'data_type'\n                # checking parameter descriptions can be ignored since that might change\n                # data type check can also be ignored if one of them is not defined, i.e., None\n                params1 = set(flow1.parameters_meta_info)\n                params2 = set(flow2.parameters_meta_info)\n                if params1 != params2:\n                    raise ValueError(\n                        \"Parameter list in meta info for parameters differ in the two flows.\",\n                    )\n                # iterating over the parameter's meta info list\n                for param in params1:\n                    if (\n                        isinstance(flow1.parameters_meta_info[param], Dict)\n                        and isinstance(flow2.parameters_meta_info[param], Dict)\n                        and \"data_type\" in flow1.parameters_meta_info[param]\n                        and \"data_type\" in flow2.parameters_meta_info[param]\n                    ):\n                        value1 = flow1.parameters_meta_info[param][\"data_type\"]\n                        value2 = flow2.parameters_meta_info[param][\"data_type\"]\n                    else:\n                        value1 = flow1.parameters_meta_info[param]\n                        value2 = flow2.parameters_meta_info[param]\n                    if value1 is None or value2 is None:\n                        continue\n\n                    if value1 != value2:\n                        raise ValueError(\n                            f\"Flow {flow1.name}: data type for parameter {param} in {key} differ \"\n                            f\"as {value1}\\nvs\\n{value2}\",\n                        )\n                # the continue is to avoid the 'attr != attr2' check at end of function\n                continue\n\n            if attr1 != attr2:\n                raise ValueError(\n                    f\"Flow {flow1.name!s}: values for attribute '{key!s}' differ: \"\n                    f\"'{attr1!s}'\\nvs\\n'{attr2!s}'.\",\n                )\n</code></pre>"},{"location":"reference/flows/functions/#openml.flows.functions.delete_flow","title":"delete_flow","text":"<pre><code>delete_flow(flow_id: int) -&gt; bool\n</code></pre> <p>Delete flow with id <code>flow_id</code> from the OpenML server.</p> <p>You can only delete flows which you uploaded and which which are not linked to runs.</p> PARAMETER DESCRIPTION <code>flow_id</code> <p>OpenML id of the flow</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the deletion was successful. False otherwise.</p> Source code in <code>openml/flows/functions.py</code> <pre><code>def delete_flow(flow_id: int) -&gt; bool:\n    \"\"\"Delete flow with id `flow_id` from the OpenML server.\n\n    You can only delete flows which you uploaded and which\n    which are not linked to runs.\n\n    Parameters\n    ----------\n    flow_id : int\n        OpenML id of the flow\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"flow\", flow_id)\n</code></pre>"},{"location":"reference/flows/functions/#openml.flows.functions.flow_exists","title":"flow_exists","text":"<pre><code>flow_exists(name: str, external_version: str) -&gt; int | bool\n</code></pre> <p>Retrieves the flow id.</p> <p>A flow is uniquely identified by name + external_version.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the flow</p> <p> TYPE: <code>string</code> </p> <code>external_version</code> <p>Version information associated with flow.</p> <p> TYPE: <code>string</code> </p> RETURNS DESCRIPTION <code>flow_exist</code> <p>flow id iff exists, False otherwise</p> <p> TYPE: <code>int or bool</code> </p> Notes <p>see www.openml.org/api_docs/#!/flow/get_flow_exists_name_version</p> Source code in <code>openml/flows/functions.py</code> <pre><code>def flow_exists(name: str, external_version: str) -&gt; int | bool:\n    \"\"\"Retrieves the flow id.\n\n    A flow is uniquely identified by name + external_version.\n\n    Parameters\n    ----------\n    name : string\n        Name of the flow\n    external_version : string\n        Version information associated with flow.\n\n    Returns\n    -------\n    flow_exist : int or bool\n        flow id iff exists, False otherwise\n\n    Notes\n    -----\n    see https://www.openml.org/api_docs/#!/flow/get_flow_exists_name_version\n    \"\"\"\n    if not (isinstance(name, str) and len(name) &gt; 0):\n        raise ValueError(\"Argument 'name' should be a non-empty string\")\n    if not (isinstance(name, str) and len(external_version) &gt; 0):\n        raise ValueError(\"Argument 'version' should be a non-empty string\")\n\n    xml_response = openml._api_calls._perform_api_call(\n        \"flow/exists\",\n        \"post\",\n        data={\"name\": name, \"external_version\": external_version},\n    )\n\n    result_dict = xmltodict.parse(xml_response)\n    flow_id = int(result_dict[\"oml:flow_exists\"][\"oml:id\"])\n    return flow_id if flow_id &gt; 0 else False\n</code></pre>"},{"location":"reference/flows/functions/#openml.flows.functions.get_flow","title":"get_flow","text":"<pre><code>get_flow(flow_id: int, reinstantiate: bool = False, strict_version: bool = True) -&gt; OpenMLFlow\n</code></pre> <p>Download the OpenML flow for a given flow ID.</p> PARAMETER DESCRIPTION <code>flow_id</code> <p>The OpenML flow id.</p> <p> TYPE: <code>int</code> </p> <code>reinstantiate</code> <p>Whether to reinstantiate the flow to a model instance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>strict_version</code> <p>Whether to fail if version requirements are not fulfilled.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>flow</code> <p>the flow</p> <p> TYPE: <code>OpenMLFlow</code> </p> Source code in <code>openml/flows/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_flow(flow_id: int, reinstantiate: bool = False, strict_version: bool = True) -&gt; OpenMLFlow:  # noqa: FBT001, FBT002\n    \"\"\"Download the OpenML flow for a given flow ID.\n\n    Parameters\n    ----------\n    flow_id : int\n        The OpenML flow id.\n\n    reinstantiate: bool\n        Whether to reinstantiate the flow to a model instance.\n\n    strict_version : bool, default=True\n        Whether to fail if version requirements are not fulfilled.\n\n    Returns\n    -------\n    flow : OpenMLFlow\n        the flow\n    \"\"\"\n    flow_id = int(flow_id)\n    flow = _get_flow_description(flow_id)\n\n    if reinstantiate:\n        flow.model = flow.extension.flow_to_model(flow, strict_version=strict_version)\n        if not strict_version:\n            # check if we need to return a new flow b/c of version mismatch\n            new_flow = flow.extension.model_to_flow(flow.model)\n            if new_flow.dependencies != flow.dependencies:\n                return new_flow\n    return flow\n</code></pre>"},{"location":"reference/flows/functions/#openml.flows.functions.get_flow_id","title":"get_flow_id","text":"<pre><code>get_flow_id(model: Any | None = None, name: str | None = None, exact_version: bool = True) -&gt; int | bool | list[int]\n</code></pre> <p>Retrieves the flow id for a model or a flow name.</p> <p>Provide either a model or a name to this function. Depending on the input, it does</p> <ul> <li><code>model</code> and <code>exact_version == True</code>: This helper function first queries for the necessary   extension. Second, it uses that extension to convert the model into a flow. Third, it   executes <code>flow_exists</code> to potentially obtain the flow id the flow is published to the   server.</li> <li><code>model</code> and <code>exact_version == False</code>: This helper function first queries for the   necessary extension. Second, it uses that extension to convert the model into a flow. Third   it calls <code>list_flows</code> and filters the returned values based on the flow name.</li> <li><code>name</code>: Ignores <code>exact_version</code> and calls <code>list_flows</code>, then filters the returned   values based on the flow name.</li> </ul> PARAMETER DESCRIPTION <code>model</code> <p>Any model. Must provide either <code>model</code> or <code>name</code>.</p> <p> TYPE: <code>object</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the flow. Must provide either <code>model</code> or <code>name</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>exact_version</code> <p>Whether to return the flow id of the exact version or all flow ids where the name of the flow matches. This is only taken into account for a model where a version number is available (requires <code>model</code> to be set).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>(int or bool, List)</code> <p>flow id iff exists, <code>False</code> otherwise, List if <code>exact_version is False</code></p> Source code in <code>openml/flows/functions.py</code> <pre><code>def get_flow_id(\n    model: Any | None = None,\n    name: str | None = None,\n    exact_version: bool = True,  # noqa: FBT001, FBT002\n) -&gt; int | bool | list[int]:\n    \"\"\"Retrieves the flow id for a model or a flow name.\n\n    Provide either a model or a name to this function. Depending on the input, it does\n\n    * ``model`` and ``exact_version == True``: This helper function first queries for the necessary\n      extension. Second, it uses that extension to convert the model into a flow. Third, it\n      executes ``flow_exists`` to potentially obtain the flow id the flow is published to the\n      server.\n    * ``model`` and ``exact_version == False``: This helper function first queries for the\n      necessary extension. Second, it uses that extension to convert the model into a flow. Third\n      it calls ``list_flows`` and filters the returned values based on the flow name.\n    * ``name``: Ignores ``exact_version`` and calls ``list_flows``, then filters the returned\n      values based on the flow name.\n\n    Parameters\n    ----------\n    model : object\n        Any model. Must provide either ``model`` or ``name``.\n    name : str\n        Name of the flow. Must provide either ``model`` or ``name``.\n    exact_version : bool\n        Whether to return the flow id of the exact version or all flow ids where the name\n        of the flow matches. This is only taken into account for a model where a version number\n        is available (requires ``model`` to be set).\n\n    Returns\n    -------\n    int or bool, List\n        flow id iff exists, ``False`` otherwise, List if ``exact_version is False``\n    \"\"\"\n    if model is not None and name is not None:\n        raise ValueError(\"Must provide either argument `model` or argument `name`, but not both.\")\n\n    if model is not None:\n        extension = openml.extensions.get_extension_by_model(model, raise_if_no_extension=True)\n        if extension is None:\n            # This should never happen and is only here to please mypy will be gone soon once the\n            # whole function is removed\n            raise TypeError(extension)\n        flow = extension.model_to_flow(model)\n        flow_name = flow.name\n        external_version = flow.external_version\n    elif name is not None:\n        flow_name = name\n        exact_version = False\n        external_version = None\n    else:\n        raise ValueError(\n            \"Need to provide either argument `model` or argument `name`, but both are `None`.\"\n        )\n\n    if exact_version:\n        if external_version is None:\n            raise ValueError(\"exact_version should be False if model is None!\")\n        return flow_exists(name=flow_name, external_version=external_version)\n\n    flows = list_flows()\n    flows = flows.query(f'name == \"{flow_name}\"')\n    return flows[\"id\"].to_list()  # type: ignore[no-any-return]\n</code></pre>"},{"location":"reference/flows/functions/#openml.flows.functions.list_flows","title":"list_flows","text":"<pre><code>list_flows(offset: int | None = None, size: int | None = None, tag: str | None = None, uploader: str | None = None) -&gt; DataFrame\n</code></pre> <p>Return a list of all flows which are on OpenML. (Supports large amount of results)</p> PARAMETER DESCRIPTION <code>offset</code> <p>the number of flows to skip, starting from the first</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>the maximum number of flows to return</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p>the tag to include</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Legal filter operators: uploader.</p> <p> </p> RETURNS DESCRIPTION <code>flows</code> <p>Each row maps to a dataset Each column contains the following information: - flow id - full name - name - version - external version - uploader</p> <p> TYPE: <code>dataframe</code> </p> Source code in <code>openml/flows/functions.py</code> <pre><code>def list_flows(\n    offset: int | None = None,\n    size: int | None = None,\n    tag: str | None = None,\n    uploader: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a list of all flows which are on OpenML.\n    (Supports large amount of results)\n\n    Parameters\n    ----------\n    offset : int, optional\n        the number of flows to skip, starting from the first\n    size : int, optional\n        the maximum number of flows to return\n    tag : str, optional\n        the tag to include\n    kwargs: dict, optional\n        Legal filter operators: uploader.\n\n    Returns\n    -------\n    flows : dataframe\n            Each row maps to a dataset\n            Each column contains the following information:\n            - flow id\n            - full name\n            - name\n            - version\n            - external version\n            - uploader\n    \"\"\"\n    listing_call = partial(_list_flows, tag=tag, uploader=uploader)\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/runs/","title":"runs","text":""},{"location":"reference/runs/#openml.runs","title":"openml.runs","text":""},{"location":"reference/runs/#openml.runs.OpenMLRun","title":"OpenMLRun","text":"<pre><code>OpenMLRun(task_id: int, flow_id: int | None, dataset_id: int | None, setup_string: str | None = None, output_files: dict[str, int] | None = None, setup_id: int | None = None, tags: list[str] | None = None, uploader: int | None = None, uploader_name: str | None = None, evaluations: dict | None = None, fold_evaluations: dict | None = None, sample_evaluations: dict | None = None, data_content: list[list] | None = None, trace: OpenMLRunTrace | None = None, model: object | None = None, task_type: str | None = None, task_evaluation_measure: str | None = None, flow_name: str | None = None, parameter_settings: list[dict[str, Any]] | None = None, predictions_url: str | None = None, task: OpenMLTask | None = None, flow: OpenMLFlow | None = None, run_id: int | None = None, description_text: str | None = None, run_details: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Run: result of running a model on an OpenML dataset.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>The ID of the OpenML task associated with the run.</p> <p> TYPE: <code>int</code> </p> <code>flow_id</code> <p>The ID of the OpenML flow associated with the run.</p> <p> TYPE: <code>int | None</code> </p> <code>dataset_id</code> <p>The ID of the OpenML dataset used for the run.</p> <p> TYPE: <code>int | None</code> </p> <code>setup_string</code> <p>The setup string of the run.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>output_files</code> <p>Specifies where each related file can be found.</p> <p> TYPE: <code>dict[str, int] | None</code> DEFAULT: <code>None</code> </p> <code>setup_id</code> <p>An integer representing the ID of the setup used for the run.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Representing the tags associated with the run.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>uploader</code> <p>User ID of the uploader.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>uploader_name</code> <p>The name of the person who uploaded the run.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>evaluations</code> <p>Representing the evaluations of the run.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>fold_evaluations</code> <p>The evaluations of the run for each fold.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>sample_evaluations</code> <p>The evaluations of the run for each sample.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>data_content</code> <p>The predictions generated from executing this run.</p> <p> TYPE: <code>list[list] | None</code> DEFAULT: <code>None</code> </p> <code>trace</code> <p>The trace containing information on internal model evaluations of this run.</p> <p> TYPE: <code>OpenMLRunTrace | None</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>The untrained model that was evaluated in the run.</p> <p> TYPE: <code>object | None</code> DEFAULT: <code>None</code> </p> <code>task_type</code> <p>The type of the OpenML task associated with the run.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>task_evaluation_measure</code> <p>The evaluation measure used for the task.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>flow_name</code> <p>The name of the OpenML flow associated with the run.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>parameter_settings</code> <p>Representing the parameter settings used for the run.</p> <p> TYPE: <code>list[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>predictions_url</code> <p>The URL of the predictions file.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>task</code> <p>An instance of the OpenMLTask class, representing the OpenML task associated with the run.</p> <p> TYPE: <code>OpenMLTask | None</code> DEFAULT: <code>None</code> </p> <code>flow</code> <p>An instance of the OpenMLFlow class, representing the OpenML flow associated with the run.</p> <p> TYPE: <code>OpenMLFlow | None</code> DEFAULT: <code>None</code> </p> <code>run_id</code> <p>The ID of the run.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>description_text</code> <p>Description text to add to the predictions file. If left None, is set to the time the arff file is generated.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>run_details</code> <p>Description of the run stored in the run meta-data.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/runs/run.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_id: int,\n    flow_id: int | None,\n    dataset_id: int | None,\n    setup_string: str | None = None,\n    output_files: dict[str, int] | None = None,\n    setup_id: int | None = None,\n    tags: list[str] | None = None,\n    uploader: int | None = None,\n    uploader_name: str | None = None,\n    evaluations: dict | None = None,\n    fold_evaluations: dict | None = None,\n    sample_evaluations: dict | None = None,\n    data_content: list[list] | None = None,\n    trace: OpenMLRunTrace | None = None,\n    model: object | None = None,\n    task_type: str | None = None,\n    task_evaluation_measure: str | None = None,\n    flow_name: str | None = None,\n    parameter_settings: list[dict[str, Any]] | None = None,\n    predictions_url: str | None = None,\n    task: OpenMLTask | None = None,\n    flow: OpenMLFlow | None = None,\n    run_id: int | None = None,\n    description_text: str | None = None,\n    run_details: str | None = None,\n):\n    self.uploader = uploader\n    self.uploader_name = uploader_name\n    self.task_id = task_id\n    self.task_type = task_type\n    self.task_evaluation_measure = task_evaluation_measure\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.setup_id = setup_id\n    self.setup_string = setup_string\n    self.parameter_settings = parameter_settings\n    self.dataset_id = dataset_id\n    self.evaluations = evaluations\n    self.fold_evaluations = fold_evaluations\n    self.sample_evaluations = sample_evaluations\n    self.data_content = data_content\n    self.output_files = output_files\n    self.trace = trace\n    self.error_message = None\n    self.task = task\n    self.flow = flow\n    self.run_id = run_id\n    self.model = model\n    self.tags = tags\n    self.predictions_url = predictions_url\n    self.description_text = description_text\n    self.run_details = run_details\n    self._predictions = None\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The ID of the run, None if not uploaded to the server yet.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.predictions","title":"predictions  <code>property</code>","text":"<pre><code>predictions: DataFrame\n</code></pre> <p>Return a DataFrame with predictions for this run</p>"},{"location":"reference/runs/#openml.runs.OpenMLRun.from_filesystem","title":"from_filesystem  <code>classmethod</code>","text":"<pre><code>from_filesystem(directory: str | Path, expect_model: bool = True) -&gt; OpenMLRun\n</code></pre> <p>The inverse of the to_filesystem method. Instantiates an OpenMLRun object based on files stored on the file system.</p> PARAMETER DESCRIPTION <code>directory</code> <p>a path leading to the folder where the results are stored</p> <p> TYPE: <code>str</code> </p> <code>expect_model</code> <p>if True, it requires the model pickle to be present, and an error will be thrown if not. Otherwise, the model might or might not be present.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>run</code> <p>the re-instantiated run object</p> <p> TYPE: <code>OpenMLRun</code> </p> Source code in <code>openml/runs/run.py</code> <pre><code>@classmethod\ndef from_filesystem(cls, directory: str | Path, expect_model: bool = True) -&gt; OpenMLRun:  # noqa: FBT001, FBT002\n    \"\"\"\n    The inverse of the to_filesystem method. Instantiates an OpenMLRun\n    object based on files stored on the file system.\n\n    Parameters\n    ----------\n    directory : str\n        a path leading to the folder where the results\n        are stored\n\n    expect_model : bool\n        if True, it requires the model pickle to be present, and an error\n        will be thrown if not. Otherwise, the model might or might not\n        be present.\n\n    Returns\n    -------\n    run : OpenMLRun\n        the re-instantiated run object\n    \"\"\"\n    # Avoiding cyclic imports\n    import openml.runs.functions\n\n    directory = Path(directory)\n    if not directory.is_dir():\n        raise ValueError(\"Could not find folder\")\n\n    description_path = directory / \"description.xml\"\n    predictions_path = directory / \"predictions.arff\"\n    trace_path = directory / \"trace.arff\"\n    model_path = directory / \"model.pkl\"\n\n    if not description_path.is_file():\n        raise ValueError(\"Could not find description.xml\")\n    if not predictions_path.is_file():\n        raise ValueError(\"Could not find predictions.arff\")\n    if (not model_path.is_file()) and expect_model:\n        raise ValueError(\"Could not find model.pkl\")\n\n    with description_path.open() as fht:\n        xml_string = fht.read()\n    run = openml.runs.functions._create_run_from_xml(xml_string, from_server=False)\n\n    if run.flow_id is None:\n        flow = openml.flows.OpenMLFlow.from_filesystem(directory)\n        run.flow = flow\n        run.flow_name = flow.name\n\n    with predictions_path.open() as fht:\n        predictions = arff.load(fht)\n        run.data_content = predictions[\"data\"]\n\n    if model_path.is_file():\n        # note that it will load the model if the file exists, even if\n        # expect_model is False\n        with model_path.open(\"rb\") as fhb:\n            run.model = pickle.load(fhb)  # noqa: S301\n\n    if trace_path.is_file():\n        run.trace = openml.runs.OpenMLRunTrace._from_filesystem(trace_path)\n\n    return run\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.get_metric_fn","title":"get_metric_fn","text":"<pre><code>get_metric_fn(sklearn_fn: Callable, kwargs: dict | None = None) -&gt; ndarray\n</code></pre> <p>Calculates metric scores based on predicted values. Assumes the run has been executed locally (and contains run_data). Furthermore, it assumes that the 'correct' or 'truth' attribute is specified in the arff (which is an optional field, but always the case for openml-python runs)</p> PARAMETER DESCRIPTION <code>sklearn_fn</code> <p>a function pointer to a sklearn function that accepts <code>y_true</code>, <code>y_pred</code> and <code>**kwargs</code></p> <p> TYPE: <code>function</code> </p> <code>kwargs</code> <p>kwargs for the function</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>scores</code> <p>metric results</p> <p> TYPE: <code>ndarray of scores of length num_folds * num_repeats</code> </p> Source code in <code>openml/runs/run.py</code> <pre><code>def get_metric_fn(self, sklearn_fn: Callable, kwargs: dict | None = None) -&gt; np.ndarray:  # noqa: PLR0915, PLR0912, C901\n    \"\"\"Calculates metric scores based on predicted values. Assumes the\n    run has been executed locally (and contains run_data). Furthermore,\n    it assumes that the 'correct' or 'truth' attribute is specified in\n    the arff (which is an optional field, but always the case for\n    openml-python runs)\n\n    Parameters\n    ----------\n    sklearn_fn : function\n        a function pointer to a sklearn function that\n        accepts ``y_true``, ``y_pred`` and ``**kwargs``\n    kwargs : dict\n        kwargs for the function\n\n    Returns\n    -------\n    scores : ndarray of scores of length num_folds * num_repeats\n        metric results\n    \"\"\"\n    kwargs = kwargs if kwargs else {}\n    if self.data_content is not None and self.task_id is not None:\n        predictions_arff = self._generate_arff_dict()\n    elif (self.output_files is not None) and (\"predictions\" in self.output_files):\n        predictions_file_url = openml._api_calls._file_id_to_url(\n            self.output_files[\"predictions\"],\n            \"predictions.arff\",\n        )\n        response = openml._api_calls._download_text_file(predictions_file_url)\n        predictions_arff = arff.loads(response)\n        # TODO: make this a stream reader\n    else:\n        raise ValueError(\n            \"Run should have been locally executed or \" \"contain outputfile reference.\",\n        )\n\n    # Need to know more about the task to compute scores correctly\n    task = get_task(self.task_id)\n\n    attribute_names = [att[0] for att in predictions_arff[\"attributes\"]]\n    if (\n        task.task_type_id in [TaskType.SUPERVISED_CLASSIFICATION, TaskType.LEARNING_CURVE]\n        and \"correct\" not in attribute_names\n    ):\n        raise ValueError('Attribute \"correct\" should be set for ' \"classification task runs\")\n    if task.task_type_id == TaskType.SUPERVISED_REGRESSION and \"truth\" not in attribute_names:\n        raise ValueError('Attribute \"truth\" should be set for ' \"regression task runs\")\n    if task.task_type_id != TaskType.CLUSTERING and \"prediction\" not in attribute_names:\n        raise ValueError('Attribute \"predict\" should be set for ' \"supervised task runs\")\n\n    def _attribute_list_to_dict(attribute_list):  # type: ignore\n        # convenience function: Creates a mapping to map from the name of\n        # attributes present in the arff prediction file to their index.\n        # This is necessary because the number of classes can be different\n        # for different tasks.\n        res = OrderedDict()\n        for idx in range(len(attribute_list)):\n            res[attribute_list[idx][0]] = idx\n        return res\n\n    attribute_dict = _attribute_list_to_dict(predictions_arff[\"attributes\"])\n\n    repeat_idx = attribute_dict[\"repeat\"]\n    fold_idx = attribute_dict[\"fold\"]\n    predicted_idx = attribute_dict[\"prediction\"]  # Assume supervised task\n\n    if task.task_type_id in (TaskType.SUPERVISED_CLASSIFICATION, TaskType.LEARNING_CURVE):\n        correct_idx = attribute_dict[\"correct\"]\n    elif task.task_type_id == TaskType.SUPERVISED_REGRESSION:\n        correct_idx = attribute_dict[\"truth\"]\n    has_samples = False\n    if \"sample\" in attribute_dict:\n        sample_idx = attribute_dict[\"sample\"]\n        has_samples = True\n\n    if (\n        predictions_arff[\"attributes\"][predicted_idx][1]\n        != predictions_arff[\"attributes\"][correct_idx][1]\n    ):\n        pred = predictions_arff[\"attributes\"][predicted_idx][1]\n        corr = predictions_arff[\"attributes\"][correct_idx][1]\n        raise ValueError(\n            \"Predicted and Correct do not have equal values:\" f\" {pred!s} Vs. {corr!s}\",\n        )\n\n    # TODO: these could be cached\n    values_predict: dict[int, dict[int, dict[int, list[float]]]] = {}\n    values_correct: dict[int, dict[int, dict[int, list[float]]]] = {}\n    for _line_idx, line in enumerate(predictions_arff[\"data\"]):\n        rep = line[repeat_idx]\n        fold = line[fold_idx]\n        samp = line[sample_idx] if has_samples else 0\n\n        if task.task_type_id in [\n            TaskType.SUPERVISED_CLASSIFICATION,\n            TaskType.LEARNING_CURVE,\n        ]:\n            prediction = predictions_arff[\"attributes\"][predicted_idx][1].index(\n                line[predicted_idx],\n            )\n            correct = predictions_arff[\"attributes\"][predicted_idx][1].index(line[correct_idx])\n        elif task.task_type_id == TaskType.SUPERVISED_REGRESSION:\n            prediction = line[predicted_idx]\n            correct = line[correct_idx]\n        if rep not in values_predict:\n            values_predict[rep] = OrderedDict()\n            values_correct[rep] = OrderedDict()\n        if fold not in values_predict[rep]:\n            values_predict[rep][fold] = OrderedDict()\n            values_correct[rep][fold] = OrderedDict()\n        if samp not in values_predict[rep][fold]:\n            values_predict[rep][fold][samp] = []\n            values_correct[rep][fold][samp] = []\n\n        values_predict[rep][fold][samp].append(prediction)\n        values_correct[rep][fold][samp].append(correct)\n\n    scores = []\n    for rep in values_predict:\n        for fold in values_predict[rep]:\n            last_sample = len(values_predict[rep][fold]) - 1\n            y_pred = values_predict[rep][fold][last_sample]\n            y_true = values_correct[rep][fold][last_sample]\n            scores.append(sklearn_fn(y_true, y_pred, **kwargs))\n    return np.array(scores)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.to_filesystem","title":"to_filesystem","text":"<pre><code>to_filesystem(directory: str | Path, store_model: bool = True) -&gt; None\n</code></pre> <p>The inverse of the from_filesystem method. Serializes a run on the filesystem, to be uploaded later.</p> PARAMETER DESCRIPTION <code>directory</code> <p>a path leading to the folder where the results will be stored. Should be empty</p> <p> TYPE: <code>str</code> </p> <code>store_model</code> <p>if True, a model will be pickled as well. As this is the most storage expensive part, it is often desirable to not store the model.</p> <p> TYPE: <code>(bool, optional(default=True))</code> DEFAULT: <code>True</code> </p> Source code in <code>openml/runs/run.py</code> <pre><code>def to_filesystem(\n    self,\n    directory: str | Path,\n    store_model: bool = True,  # noqa: FBT001, FBT002\n) -&gt; None:\n    \"\"\"\n    The inverse of the from_filesystem method. Serializes a run\n    on the filesystem, to be uploaded later.\n\n    Parameters\n    ----------\n    directory : str\n        a path leading to the folder where the results\n        will be stored. Should be empty\n\n    store_model : bool, optional (default=True)\n        if True, a model will be pickled as well. As this is the most\n        storage expensive part, it is often desirable to not store the\n        model.\n    \"\"\"\n    if self.data_content is None or self.model is None:\n        raise ValueError(\"Run should have been executed (and contain \" \"model / predictions)\")\n    directory = Path(directory)\n    directory.mkdir(exist_ok=True, parents=True)\n\n    if any(directory.iterdir()):\n        raise ValueError(f\"Output directory {directory.expanduser().resolve()} should be empty\")\n\n    run_xml = self._to_xml()\n    predictions_arff = arff.dumps(self._generate_arff_dict())\n\n    # It seems like typing does not allow to define the same variable multiple times\n    with (directory / \"description.xml\").open(\"w\") as fh:\n        fh.write(run_xml)\n    with (directory / \"predictions.arff\").open(\"w\") as fh:\n        fh.write(predictions_arff)\n    if store_model:\n        with (directory / \"model.pkl\").open(\"wb\") as fh_b:\n            pickle.dump(self.model, fh_b)\n\n    if self.flow_id is None and self.flow is not None:\n        self.flow.to_filesystem(directory)\n\n    if self.trace is not None:\n        self.trace._to_filesystem(directory)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRun.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace","title":"OpenMLRunTrace","text":"<pre><code>OpenMLRunTrace(run_id: int | None, trace_iterations: dict[tuple[int, int, int], OpenMLTraceIteration])\n</code></pre> <p>OpenML Run Trace: parsed output from Run Trace call</p> PARAMETER DESCRIPTION <code>run_id</code> <p>OpenML run id.</p> <p> TYPE: <code>int</code> </p> <code>trace_iterations</code> <p>Mapping from key <code>(repeat, fold, iteration)</code> to an object of OpenMLTraceIteration.</p> <p> TYPE: <code>dict</code> </p> PARAMETER DESCRIPTION <code>run_id</code> <p>Id for which the trace content is to be stored.</p> <p> TYPE: <code>int</code> </p> <code>trace_iterations</code> <p>The trace content obtained by running a flow on a task.</p> <p> TYPE: <code>List[List]</code> </p> Source code in <code>openml/runs/trace.py</code> <pre><code>def __init__(\n    self,\n    run_id: int | None,\n    trace_iterations: dict[tuple[int, int, int], OpenMLTraceIteration],\n):\n    \"\"\"Object to hold the trace content of a run.\n\n    Parameters\n    ----------\n    run_id : int\n        Id for which the trace content is to be stored.\n    trace_iterations : List[List]\n        The trace content obtained by running a flow on a task.\n    \"\"\"\n    self.run_id = run_id\n    self.trace_iterations = trace_iterations\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.generate","title":"generate  <code>classmethod</code>","text":"<pre><code>generate(attributes: list[tuple[str, str]], content: list[list[int | float | str]]) -&gt; OpenMLRunTrace\n</code></pre> <p>Generates an OpenMLRunTrace.</p> <p>Generates the trace object from the attributes and content extracted while running the underlying flow.</p> PARAMETER DESCRIPTION <code>attributes</code> <p>List of tuples describing the arff attributes.</p> <p> TYPE: <code>list</code> </p> <code>content</code> <p>List of lists containing information about the individual tuning runs.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>OpenMLRunTrace</code> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef generate(\n    cls,\n    attributes: list[tuple[str, str]],\n    content: list[list[int | float | str]],\n) -&gt; OpenMLRunTrace:\n    \"\"\"Generates an OpenMLRunTrace.\n\n    Generates the trace object from the attributes and content extracted\n    while running the underlying flow.\n\n    Parameters\n    ----------\n    attributes : list\n        List of tuples describing the arff attributes.\n\n    content : list\n        List of lists containing information about the individual tuning\n        runs.\n\n    Returns\n    -------\n    OpenMLRunTrace\n    \"\"\"\n    if content is None:\n        raise ValueError(\"Trace content not available.\")\n    if attributes is None:\n        raise ValueError(\"Trace attributes not available.\")\n    if len(content) == 0:\n        raise ValueError(\"Trace content is empty.\")\n    if len(attributes) != len(content[0]):\n        raise ValueError(\n            \"Trace_attributes and trace_content not compatible:\"\n            f\" {attributes} vs {content[0]}\",\n        )\n\n    return cls._trace_from_arff_struct(\n        attributes=attributes,\n        content=content,\n        error_message=\"setup_string not allowed when constructing a \"\n        \"trace object from run results.\",\n    )\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.get_selected_iteration","title":"get_selected_iteration","text":"<pre><code>get_selected_iteration(fold: int, repeat: int) -&gt; int\n</code></pre> <p>Returns the trace iteration that was marked as selected. In case multiple are marked as selected (should not happen) the first of these is returned</p> PARAMETER DESCRIPTION <code>fold</code> <p> TYPE: <code>int</code> </p> <code>repeat</code> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The trace iteration from the given fold and repeat that was selected as the best iteration by the search procedure</p> Source code in <code>openml/runs/trace.py</code> <pre><code>def get_selected_iteration(self, fold: int, repeat: int) -&gt; int:\n    \"\"\"\n    Returns the trace iteration that was marked as selected. In\n    case multiple are marked as selected (should not happen) the\n    first of these is returned\n\n    Parameters\n    ----------\n    fold: int\n\n    repeat: int\n\n    Returns\n    -------\n    int\n        The trace iteration from the given fold and repeat that was\n        selected as the best iteration by the search procedure\n    \"\"\"\n    for r, f, i in self.trace_iterations:\n        if r == repeat and f == fold and self.trace_iterations[(r, f, i)].selected is True:\n            return i\n    raise ValueError(\n        \"Could not find the selected iteration for rep/fold %d/%d\" % (repeat, fold),\n    )\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.merge_traces","title":"merge_traces  <code>classmethod</code>","text":"<pre><code>merge_traces(traces: list[OpenMLRunTrace]) -&gt; OpenMLRunTrace\n</code></pre> <p>Merge multiple traces into a single trace.</p> PARAMETER DESCRIPTION <code>cls</code> <p>Type of the trace object to be created.</p> <p> TYPE: <code>type</code> </p> <code>traces</code> <p>List of traces to merge.</p> <p> TYPE: <code>List[OpenMLRunTrace]</code> </p> RETURNS DESCRIPTION <code>OpenMLRunTrace</code> <p>A trace object representing the merged traces.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the parameters in the iterations of the traces being merged are not equal. If a key (repeat, fold, iteration) is encountered twice while merging the traces.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef merge_traces(cls, traces: list[OpenMLRunTrace]) -&gt; OpenMLRunTrace:\n    \"\"\"Merge multiple traces into a single trace.\n\n    Parameters\n    ----------\n    cls : type\n        Type of the trace object to be created.\n    traces : List[OpenMLRunTrace]\n        List of traces to merge.\n\n    Returns\n    -------\n    OpenMLRunTrace\n        A trace object representing the merged traces.\n\n    Raises\n    ------\n    ValueError\n        If the parameters in the iterations of the traces being merged are not equal.\n        If a key (repeat, fold, iteration) is encountered twice while merging the traces.\n    \"\"\"\n    merged_trace: dict[tuple[int, int, int], OpenMLTraceIteration] = {}\n\n    previous_iteration = None\n    for trace in traces:\n        for iteration in trace:\n            key = (iteration.repeat, iteration.fold, iteration.iteration)\n\n            assert iteration.parameters is not None\n            param_keys = iteration.parameters.keys()\n\n            if previous_iteration is not None:\n                trace_itr = merged_trace[previous_iteration]\n\n                assert trace_itr.parameters is not None\n                trace_itr_keys = trace_itr.parameters.keys()\n\n                if list(param_keys) != list(trace_itr_keys):\n                    raise ValueError(\n                        \"Cannot merge traces because the parameters are not equal: \"\n                        f\"{list(trace_itr.parameters.keys())} vs \"\n                        f\"{list(iteration.parameters.keys())}\",\n                    )\n\n            if key in merged_trace:\n                raise ValueError(\n                    f\"Cannot merge traces because key '{key}' was encountered twice\",\n                )\n\n            merged_trace[key] = iteration\n            previous_iteration = key\n\n    return cls(None, merged_trace)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.trace_from_arff","title":"trace_from_arff  <code>classmethod</code>","text":"<pre><code>trace_from_arff(arff_obj: dict[str, Any]) -&gt; OpenMLRunTrace\n</code></pre> <p>Generate trace from arff trace.</p> <p>Creates a trace file from arff object (for example, generated by a local run).</p> PARAMETER DESCRIPTION <code>arff_obj</code> <p>LIAC arff obj, dict containing attributes, relation, data.</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>OpenMLRunTrace</code> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef trace_from_arff(cls, arff_obj: dict[str, Any]) -&gt; OpenMLRunTrace:\n    \"\"\"Generate trace from arff trace.\n\n    Creates a trace file from arff object (for example, generated by a\n    local run).\n\n    Parameters\n    ----------\n    arff_obj : dict\n        LIAC arff obj, dict containing attributes, relation, data.\n\n    Returns\n    -------\n    OpenMLRunTrace\n    \"\"\"\n    attributes = arff_obj[\"attributes\"]\n    content = arff_obj[\"data\"]\n    return cls._trace_from_arff_struct(\n        attributes=attributes,\n        content=content,\n        error_message=\"setup_string not supported for arff serialization\",\n    )\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.trace_from_xml","title":"trace_from_xml  <code>classmethod</code>","text":"<pre><code>trace_from_xml(xml: str | Path | IO) -&gt; OpenMLRunTrace\n</code></pre> <p>Generate trace from xml.</p> <p>Creates a trace file from the xml description.</p> PARAMETER DESCRIPTION <code>xml</code> <p>An xml description that can be either a <code>string</code> or a file-like object.</p> <p> TYPE: <code>string | file-like object</code> </p> RETURNS DESCRIPTION <code>run</code> <p>Object containing the run id and a dict containing the trace iterations.</p> <p> TYPE: <code>OpenMLRunTrace</code> </p> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef trace_from_xml(cls, xml: str | Path | IO) -&gt; OpenMLRunTrace:\n    \"\"\"Generate trace from xml.\n\n    Creates a trace file from the xml description.\n\n    Parameters\n    ----------\n    xml : string | file-like object\n        An xml description that can be either a `string` or a file-like\n        object.\n\n    Returns\n    -------\n    run : OpenMLRunTrace\n        Object containing the run id and a dict containing the trace\n        iterations.\n    \"\"\"\n    if isinstance(xml, Path):\n        xml = str(xml.absolute())\n\n    result_dict = xmltodict.parse(xml, force_list=(\"oml:trace_iteration\",))[\"oml:trace\"]\n\n    run_id = result_dict[\"oml:run_id\"]\n    trace = OrderedDict()\n\n    if \"oml:trace_iteration\" not in result_dict:\n        raise ValueError(\"Run does not contain valid trace. \")\n    if not isinstance(result_dict[\"oml:trace_iteration\"], list):\n        raise TypeError(type(result_dict[\"oml:trace_iteration\"]))\n\n    for itt in result_dict[\"oml:trace_iteration\"]:\n        repeat = int(itt[\"oml:repeat\"])\n        fold = int(itt[\"oml:fold\"])\n        iteration = int(itt[\"oml:iteration\"])\n        setup_string = json.loads(itt[\"oml:setup_string\"])\n        evaluation = float(itt[\"oml:evaluation\"])\n        selected_value = itt[\"oml:selected\"]\n        if selected_value == \"true\":\n            selected = True\n        elif selected_value == \"false\":\n            selected = False\n        else:\n            raise ValueError(\n                'expected {\"true\", \"false\"} value for '\n                f\"selected field, received: {selected_value}\",\n            )\n\n        current = OpenMLTraceIteration(\n            repeat=repeat,\n            fold=fold,\n            iteration=iteration,\n            setup_string=setup_string,\n            evaluation=evaluation,\n            selected=selected,\n        )\n        trace[(repeat, fold, iteration)] = current\n\n    return cls(run_id, trace)\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLRunTrace.trace_to_arff","title":"trace_to_arff","text":"<pre><code>trace_to_arff() -&gt; dict[str, Any]\n</code></pre> <p>Generate the arff dictionary for uploading predictions to the server.</p> <p>Uses the trace object to generate an arff dictionary representation.</p> RETURNS DESCRIPTION <code>arff_dict</code> <p>Dictionary representation of the ARFF file that will be uploaded. Contains information about the optimization trace.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>openml/runs/trace.py</code> <pre><code>def trace_to_arff(self) -&gt; dict[str, Any]:\n    \"\"\"Generate the arff dictionary for uploading predictions to the server.\n\n    Uses the trace object to generate an arff dictionary representation.\n\n    Returns\n    -------\n    arff_dict : dict\n        Dictionary representation of the ARFF file that will be uploaded.\n        Contains information about the optimization trace.\n    \"\"\"\n    if self.trace_iterations is None:\n        raise ValueError(\"trace_iterations missing from the trace object\")\n\n    # attributes that will be in trace arff\n    trace_attributes = [\n        (\"repeat\", \"NUMERIC\"),\n        (\"fold\", \"NUMERIC\"),\n        (\"iteration\", \"NUMERIC\"),\n        (\"evaluation\", \"NUMERIC\"),\n        (\"selected\", [\"true\", \"false\"]),\n    ]\n    trace_attributes.extend(\n        [\n            (PREFIX + parameter, \"STRING\")\n            for parameter in next(iter(self.trace_iterations.values())).get_parameters()\n        ],\n    )\n\n    arff_dict: dict[str, Any] = {}\n    data = []\n    for trace_iteration in self.trace_iterations.values():\n        tmp_list = []\n        for _attr, _ in trace_attributes:\n            if _attr.startswith(PREFIX):\n                attr = _attr[len(PREFIX) :]\n                value = trace_iteration.get_parameters()[attr]\n            else:\n                attr = _attr\n                value = getattr(trace_iteration, attr)\n\n            if attr == \"selected\":\n                tmp_list.append(\"true\" if value else \"false\")\n            else:\n                tmp_list.append(value)\n        data.append(tmp_list)\n\n    arff_dict[\"attributes\"] = trace_attributes\n    arff_dict[\"data\"] = data\n    # TODO allow to pass a trace description when running a flow\n    arff_dict[\"relation\"] = \"Trace\"\n    return arff_dict\n</code></pre>"},{"location":"reference/runs/#openml.runs.OpenMLTraceIteration","title":"OpenMLTraceIteration  <code>dataclass</code>","text":"<pre><code>OpenMLTraceIteration(repeat: int, fold: int, iteration: int, evaluation: float, selected: bool, setup_string: dict[str, str] | None = None, parameters: dict[str, str | int | float] | None = None)\n</code></pre> <p>OpenML Trace Iteration: parsed output from Run Trace call Exactly one of <code>setup_string</code> or <code>parameters</code> must be provided.</p> PARAMETER DESCRIPTION <code>repeat</code> <p>repeat number (in case of no repeats: 0)</p> <p> TYPE: <code>int</code> </p> <code>fold</code> <p>fold number (in case of no folds: 0)</p> <p> TYPE: <code>int</code> </p> <code>iteration</code> <p>iteration number of optimization procedure</p> <p> TYPE: <code>int</code> </p> <code>setup_string</code> <p>json string representing the parameters If not provided, <code>parameters</code> should be set.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>evaluation</code> <p>The evaluation that was awarded to this trace iteration. Measure is defined by the task</p> <p> TYPE: <code>double</code> </p> <code>selected</code> <p>Whether this was the best of all iterations, and hence selected for making predictions. Per fold/repeat there should be only one iteration selected</p> <p> TYPE: <code>bool</code> </p> <code>parameters</code> <p>Dictionary specifying parameter names and their values. If not provided, <code>setup_string</code> should be set.</p> <p> TYPE: <code>OrderedDict</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/runs/#openml.runs.OpenMLTraceIteration.get_parameters","title":"get_parameters","text":"<pre><code>get_parameters() -&gt; dict[str, Any]\n</code></pre> <p>Get the parameters of this trace iteration.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>def get_parameters(self) -&gt; dict[str, Any]:\n    \"\"\"Get the parameters of this trace iteration.\"\"\"\n    # parameters have prefix 'parameter_'\n    if self.setup_string:\n        return {\n            param[len(PREFIX) :]: json.loads(value)\n            for param, value in self.setup_string.items()\n        }\n\n    assert self.parameters is not None\n    return {param[len(PREFIX) :]: value for param, value in self.parameters.items()}\n</code></pre>"},{"location":"reference/runs/#openml.runs.delete_run","title":"delete_run","text":"<pre><code>delete_run(run_id: int) -&gt; bool\n</code></pre> <p>Delete run with id <code>run_id</code> from the OpenML server.</p> <p>You can only delete runs which you uploaded.</p> PARAMETER DESCRIPTION <code>run_id</code> <p>OpenML id of the run</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the deletion was successful. False otherwise.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def delete_run(run_id: int) -&gt; bool:\n    \"\"\"Delete run with id `run_id` from the OpenML server.\n\n    You can only delete runs which you uploaded.\n\n    Parameters\n    ----------\n    run_id : int\n        OpenML id of the run\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"run\", run_id)\n</code></pre>"},{"location":"reference/runs/#openml.runs.get_run","title":"get_run","text":"<pre><code>get_run(run_id: int, ignore_cache: bool = False) -&gt; OpenMLRun\n</code></pre> <p>Gets run corresponding to run_id.</p> PARAMETER DESCRIPTION <code>run_id</code> <p> TYPE: <code>int</code> </p> <code>ignore_cache</code> <p>Whether to ignore the cache. If <code>true</code> this will download and overwrite the run xml even if the requested run is already cached.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_cache</code> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>run</code> <p>Run corresponding to ID, fetched from the server.</p> <p> TYPE: <code>OpenMLRun</code> </p> Source code in <code>openml/runs/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_run(run_id: int, ignore_cache: bool = False) -&gt; OpenMLRun:  # noqa: FBT002, FBT001\n    \"\"\"Gets run corresponding to run_id.\n\n    Parameters\n    ----------\n    run_id : int\n\n    ignore_cache : bool\n        Whether to ignore the cache. If ``true`` this will download and overwrite the run xml\n        even if the requested run is already cached.\n\n    ignore_cache\n\n    Returns\n    -------\n    run : OpenMLRun\n        Run corresponding to ID, fetched from the server.\n    \"\"\"\n    run_dir = Path(openml.utils._create_cache_directory_for_id(RUNS_CACHE_DIR_NAME, run_id))\n    run_file = run_dir / \"description.xml\"\n\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    try:\n        if not ignore_cache:\n            return _get_cached_run(run_id)\n\n        raise OpenMLCacheException(message=\"dummy\")\n\n    except OpenMLCacheException:\n        run_xml = openml._api_calls._perform_api_call(\"run/%d\" % run_id, \"get\")\n        with run_file.open(\"w\", encoding=\"utf8\") as fh:\n            fh.write(run_xml)\n\n    return _create_run_from_xml(run_xml)\n</code></pre>"},{"location":"reference/runs/#openml.runs.get_run_trace","title":"get_run_trace","text":"<pre><code>get_run_trace(run_id: int) -&gt; OpenMLRunTrace\n</code></pre> <p>Get the optimization trace object for a given run id.</p> PARAMETER DESCRIPTION <code>run_id</code> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>OpenMLTrace</code> Source code in <code>openml/runs/functions.py</code> <pre><code>def get_run_trace(run_id: int) -&gt; OpenMLRunTrace:\n    \"\"\"\n    Get the optimization trace object for a given run id.\n\n    Parameters\n    ----------\n    run_id : int\n\n    Returns\n    -------\n    openml.runs.OpenMLTrace\n    \"\"\"\n    trace_xml = openml._api_calls._perform_api_call(\"run/trace/%d\" % run_id, \"get\")\n    return OpenMLRunTrace.trace_from_xml(trace_xml)\n</code></pre>"},{"location":"reference/runs/#openml.runs.get_runs","title":"get_runs","text":"<pre><code>get_runs(run_ids: list[int]) -&gt; list[OpenMLRun]\n</code></pre> <p>Gets all runs in run_ids list.</p> PARAMETER DESCRIPTION <code>run_ids</code> <p> TYPE: <code>list of ints</code> </p> RETURNS DESCRIPTION <code>runs</code> <p>List of runs corresponding to IDs, fetched from the server.</p> <p> TYPE: <code>list of OpenMLRun</code> </p> Source code in <code>openml/runs/functions.py</code> <pre><code>def get_runs(run_ids: list[int]) -&gt; list[OpenMLRun]:\n    \"\"\"Gets all runs in run_ids list.\n\n    Parameters\n    ----------\n    run_ids : list of ints\n\n    Returns\n    -------\n    runs : list of OpenMLRun\n        List of runs corresponding to IDs, fetched from the server.\n    \"\"\"\n    runs = []\n    for run_id in run_ids:\n        runs.append(get_run(run_id))\n    return runs\n</code></pre>"},{"location":"reference/runs/#openml.runs.initialize_model_from_run","title":"initialize_model_from_run","text":"<pre><code>initialize_model_from_run(run_id: int, *, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Initialized a model based on a run_id (i.e., using the exact same parameter settings)</p> PARAMETER DESCRIPTION <code>run_id</code> <p>The Openml run_id</p> <p> TYPE: <code>int</code> </p> <code>strict_version</code> <p>See <code>flow_to_model</code> strict_version.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>model</code> Source code in <code>openml/runs/functions.py</code> <pre><code>def initialize_model_from_run(run_id: int, *, strict_version: bool = True) -&gt; Any:\n    \"\"\"\n    Initialized a model based on a run_id (i.e., using the exact\n    same parameter settings)\n\n    Parameters\n    ----------\n    run_id : int\n        The Openml run_id\n    strict_version: bool (default=True)\n        See `flow_to_model` strict_version.\n\n    Returns\n    -------\n    model\n    \"\"\"\n    run = get_run(run_id)\n    # TODO(eddiebergman): I imagine this is None if it's not published,\n    # might need to raise an explicit error for that\n    assert run.setup_id is not None\n    return initialize_model(setup_id=run.setup_id, strict_version=strict_version)\n</code></pre>"},{"location":"reference/runs/#openml.runs.initialize_model_from_trace","title":"initialize_model_from_trace","text":"<pre><code>initialize_model_from_trace(run_id: int, repeat: int, fold: int, iteration: int | None = None) -&gt; Any\n</code></pre> <p>Initialize a model based on the parameters that were set by an optimization procedure (i.e., using the exact same parameter settings)</p> PARAMETER DESCRIPTION <code>run_id</code> <p>The Openml run_id. Should contain a trace file, otherwise a OpenMLServerException is raised</p> <p> TYPE: <code>int</code> </p> <code>repeat</code> <p>The repeat nr (column in trace file)</p> <p> TYPE: <code>int</code> </p> <code>fold</code> <p>The fold nr (column in trace file)</p> <p> TYPE: <code>int</code> </p> <code>iteration</code> <p>The iteration nr (column in trace file). If None, the best (selected) iteration will be searched (slow), according to the selection criteria implemented in OpenMLRunTrace.get_selected_iteration</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>model</code> Source code in <code>openml/runs/functions.py</code> <pre><code>def initialize_model_from_trace(\n    run_id: int,\n    repeat: int,\n    fold: int,\n    iteration: int | None = None,\n) -&gt; Any:\n    \"\"\"\n    Initialize a model based on the parameters that were set\n    by an optimization procedure (i.e., using the exact same\n    parameter settings)\n\n    Parameters\n    ----------\n    run_id : int\n        The Openml run_id. Should contain a trace file,\n        otherwise a OpenMLServerException is raised\n\n    repeat : int\n        The repeat nr (column in trace file)\n\n    fold : int\n        The fold nr (column in trace file)\n\n    iteration : int\n        The iteration nr (column in trace file). If None, the\n        best (selected) iteration will be searched (slow),\n        according to the selection criteria implemented in\n        OpenMLRunTrace.get_selected_iteration\n\n    Returns\n    -------\n    model\n    \"\"\"\n    run = get_run(run_id)\n    # TODO(eddiebergman): I imagine this is None if it's not published,\n    # might need to raise an explicit error for that\n    assert run.flow_id is not None\n\n    flow = get_flow(run.flow_id)\n    run_trace = get_run_trace(run_id)\n\n    if iteration is None:\n        iteration = run_trace.get_selected_iteration(repeat, fold)\n\n    request = (repeat, fold, iteration)\n    if request not in run_trace.trace_iterations:\n        raise ValueError(\"Combination repeat, fold, iteration not available\")\n    current = run_trace.trace_iterations[(repeat, fold, iteration)]\n\n    search_model = initialize_model_from_run(run_id)\n    return flow.extension.instantiate_model_from_hpo_class(search_model, current)\n</code></pre>"},{"location":"reference/runs/#openml.runs.list_runs","title":"list_runs","text":"<pre><code>list_runs(offset: int | None = None, size: int | None = None, id: list | None = None, task: list[int] | None = None, setup: list | None = None, flow: list | None = None, uploader: list | None = None, tag: str | None = None, study: int | None = None, display_errors: bool = False, task_type: TaskType | int | None = None) -&gt; DataFrame\n</code></pre> <p>List all runs matching all of the given filters. (Supports large amount of results)</p> PARAMETER DESCRIPTION <code>offset</code> <p>the number of runs to skip, starting from the first</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>the maximum number of runs to show</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>id</code> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>task</code> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>setup</code> <p> TYPE: <code>list | None</code> DEFAULT: <code>None</code> </p> <code>flow</code> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>uploader</code> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>study</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>display_errors</code> <p>Whether to list runs which have an error (for example a missing prediction file).</p> <p> TYPE: <code>(bool, optional(default=None))</code> DEFAULT: <code>False</code> </p> <code>task_type</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dataframe</code> Source code in <code>openml/runs/functions.py</code> <pre><code>def list_runs(  # noqa: PLR0913\n    offset: int | None = None,\n    size: int | None = None,\n    id: list | None = None,  # noqa: A002\n    task: list[int] | None = None,\n    setup: list | None = None,\n    flow: list | None = None,\n    uploader: list | None = None,\n    tag: str | None = None,\n    study: int | None = None,\n    display_errors: bool = False,  # noqa: FBT001, FBT002\n    task_type: TaskType | int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    List all runs matching all of the given filters.\n    (Supports large amount of results)\n\n    Parameters\n    ----------\n    offset : int, optional\n        the number of runs to skip, starting from the first\n    size : int, optional\n        the maximum number of runs to show\n\n    id : list, optional\n\n    task : list, optional\n\n    setup: list, optional\n\n    flow : list, optional\n\n    uploader : list, optional\n\n    tag : str, optional\n\n    study : int, optional\n\n    display_errors : bool, optional (default=None)\n        Whether to list runs which have an error (for example a missing\n        prediction file).\n\n    task_type : str, optional\n\n    Returns\n    -------\n    dataframe\n    \"\"\"\n    if id is not None and (not isinstance(id, list)):\n        raise TypeError(\"id must be of type list.\")\n    if task is not None and (not isinstance(task, list)):\n        raise TypeError(\"task must be of type list.\")\n    if setup is not None and (not isinstance(setup, list)):\n        raise TypeError(\"setup must be of type list.\")\n    if flow is not None and (not isinstance(flow, list)):\n        raise TypeError(\"flow must be of type list.\")\n    if uploader is not None and (not isinstance(uploader, list)):\n        raise TypeError(\"uploader must be of type list.\")\n\n    listing_call = partial(\n        _list_runs,\n        id=id,\n        task=task,\n        setup=setup,\n        flow=flow,\n        uploader=uploader,\n        tag=tag,\n        study=study,\n        display_errors=display_errors,\n        task_type=task_type,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/runs/#openml.runs.run_exists","title":"run_exists","text":"<pre><code>run_exists(task_id: int, setup_id: int) -&gt; set[int]\n</code></pre> <p>Checks whether a task/setup combination is already present on the server.</p> PARAMETER DESCRIPTION <code>task_id</code> <p> TYPE: <code>int</code> </p> <code>setup_id</code> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>    Set run ids for runs where flow setup_id was run on task_id. Empty</code> <p>set if it wasn't run yet.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def run_exists(task_id: int, setup_id: int) -&gt; set[int]:\n    \"\"\"Checks whether a task/setup combination is already present on the\n    server.\n\n    Parameters\n    ----------\n    task_id : int\n\n    setup_id : int\n\n    Returns\n    -------\n        Set run ids for runs where flow setup_id was run on task_id. Empty\n        set if it wasn't run yet.\n    \"\"\"\n    if setup_id &lt;= 0:\n        # openml setups are in range 1-inf\n        return set()\n\n    try:\n        result = list_runs(task=[task_id], setup=[setup_id])\n        return set() if result.empty else set(result[\"run_id\"])\n    except OpenMLServerException as exception:\n        # error code implies no results. The run does not exist yet\n        if exception.code != ERROR_CODE:\n            raise exception\n        return set()\n</code></pre>"},{"location":"reference/runs/#openml.runs.run_flow_on_task","title":"run_flow_on_task","text":"<pre><code>run_flow_on_task(flow: OpenMLFlow, task: OpenMLTask, avoid_duplicate_runs: bool | None = None, flow_tags: list[str] | None = None, seed: int | None = None, add_local_measures: bool = True, upload_flow: bool = False, n_jobs: int | None = None) -&gt; OpenMLRun\n</code></pre> <p>Run the model provided by the flow on the dataset defined by task.</p> <p>Takes the flow and repeat information into account. The Flow may optionally be published.</p> PARAMETER DESCRIPTION <code>flow</code> <p>A flow wraps a machine learning model together with relevant information. The model has a function fit(X,Y) and predict(X), all supervised estimators of scikit learn follow this definition of a model.</p> <p> TYPE: <code>OpenMLFlow</code> </p> <code>task</code> <p>Task to perform. This may be an OpenMLFlow instead if the first argument is an OpenMLTask.</p> <p> TYPE: <code>OpenMLTask</code> </p> <code>avoid_duplicate_runs</code> <p>If True, the run will throw an error if the setup/task combination is already present on the server. This feature requires an internet connection. If not set, it will use the default from your openml configuration (False if unset).</p> <p> TYPE: <code>(bool, optional(default=None))</code> DEFAULT: <code>None</code> </p> <code>flow_tags</code> <p>A list of tags that the flow should have at creation.</p> <p> TYPE: <code>(List[str], optional(default=None))</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Models that are not seeded will get this seed.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>add_local_measures</code> <p>Determines whether to calculate a set of evaluation measures locally, to later verify server behaviour.</p> <p> TYPE: <code>(bool, optional(default=True))</code> DEFAULT: <code>True</code> </p> <code>upload_flow</code> <p>If True, upload the flow to OpenML if it does not exist yet. If False, do not upload the flow to OpenML.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>n_jobs</code> <p>The number of processes/threads to distribute the evaluation asynchronously. If <code>None</code> or <code>1</code>, then the evaluation is treated as synchronous and processed sequentially. If <code>-1</code>, then the job uses as many cores available.</p> <p> TYPE: <code>int(default=None)</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>run</code> <p>Result of the run.</p> <p> TYPE: <code>OpenMLRun</code> </p> Source code in <code>openml/runs/functions.py</code> <pre><code>def run_flow_on_task(  # noqa: C901, PLR0912, PLR0915, PLR0913\n    flow: OpenMLFlow,\n    task: OpenMLTask,\n    avoid_duplicate_runs: bool | None = None,\n    flow_tags: list[str] | None = None,\n    seed: int | None = None,\n    add_local_measures: bool = True,  # noqa: FBT001, FBT002\n    upload_flow: bool = False,  # noqa: FBT001, FBT002\n    n_jobs: int | None = None,\n) -&gt; OpenMLRun:\n    \"\"\"Run the model provided by the flow on the dataset defined by task.\n\n    Takes the flow and repeat information into account.\n    The Flow may optionally be published.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        A flow wraps a machine learning model together with relevant information.\n        The model has a function fit(X,Y) and predict(X),\n        all supervised estimators of scikit learn follow this definition of a model.\n    task : OpenMLTask\n        Task to perform. This may be an OpenMLFlow instead if the first argument is an OpenMLTask.\n    avoid_duplicate_runs : bool, optional (default=None)\n        If True, the run will throw an error if the setup/task combination is already present on\n        the server. This feature requires an internet connection.\n        If not set, it will use the default from your openml configuration (False if unset).\n    flow_tags : List[str], optional (default=None)\n        A list of tags that the flow should have at creation.\n    seed: int, optional (default=None)\n        Models that are not seeded will get this seed.\n    add_local_measures : bool, optional (default=True)\n        Determines whether to calculate a set of evaluation measures locally,\n        to later verify server behaviour.\n    upload_flow : bool (default=False)\n        If True, upload the flow to OpenML if it does not exist yet.\n        If False, do not upload the flow to OpenML.\n    n_jobs : int (default=None)\n        The number of processes/threads to distribute the evaluation asynchronously.\n        If `None` or `1`, then the evaluation is treated as synchronous and processed sequentially.\n        If `-1`, then the job uses as many cores available.\n\n    Returns\n    -------\n    run : OpenMLRun\n        Result of the run.\n    \"\"\"\n    if flow_tags is not None and not isinstance(flow_tags, list):\n        raise ValueError(\"flow_tags should be a list\")\n\n    if avoid_duplicate_runs is None:\n        avoid_duplicate_runs = openml.config.avoid_duplicate_runs\n\n    # TODO: At some point in the future do not allow for arguments in old order (changed 6-2018).\n    # Flexibility currently still allowed due to code-snippet in OpenML100 paper (3-2019).\n    if isinstance(flow, OpenMLTask) and isinstance(task, OpenMLFlow):\n        # We want to allow either order of argument (to avoid confusion).\n        warnings.warn(\n            \"The old argument order (Flow, model) is deprecated and \"\n            \"will not be supported in the future. Please use the \"\n            \"order (model, Flow).\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        task, flow = flow, task\n\n    if task.task_id is None:\n        raise ValueError(\"The task should be published at OpenML\")\n\n    if flow.model is None:\n        flow.model = flow.extension.flow_to_model(flow)\n\n    flow.model = flow.extension.seed_model(flow.model, seed=seed)\n\n    # We only need to sync with the server right now if we want to upload the flow,\n    # or ensure no duplicate runs exist. Otherwise it can be synced at upload time.\n    flow_id = None\n    if upload_flow or avoid_duplicate_runs:\n        flow_id = flow_exists(flow.name, flow.external_version)\n        if isinstance(flow.flow_id, int) and flow_id != flow.flow_id:\n            if flow_id is not False:\n                raise PyOpenMLError(\n                    f\"Local flow_id does not match server flow_id: '{flow.flow_id}' vs '{flow_id}'\",\n                )\n            raise PyOpenMLError(\n                \"Flow does not exist on the server, but 'flow.flow_id' is not None.\"\n            )\n        if upload_flow and flow_id is False:\n            flow.publish()\n            flow_id = flow.flow_id\n        elif flow_id:\n            flow_from_server = get_flow(flow_id)\n            _copy_server_fields(flow_from_server, flow)\n            if avoid_duplicate_runs:\n                flow_from_server.model = flow.model\n                setup_id = setup_exists(flow_from_server)\n                ids = run_exists(task.task_id, setup_id)\n                if ids:\n                    error_message = (\n                        \"One or more runs of this setup were already performed on the task.\"\n                    )\n                    raise OpenMLRunsExistError(ids, error_message)\n        else:\n            # Flow does not exist on server and we do not want to upload it.\n            # No sync with the server happens.\n            flow_id = None\n\n    dataset = task.get_dataset()\n\n    run_environment = flow.extension.get_version_information()\n    tags = [\"openml-python\", run_environment[1]]\n\n    if flow.extension.check_if_model_fitted(flow.model):\n        warnings.warn(\n            \"The model is already fitted! This might cause inconsistency in comparison of results.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n\n    # execute the run\n    res = _run_task_get_arffcontent(\n        model=flow.model,\n        task=task,\n        extension=flow.extension,\n        add_local_measures=add_local_measures,\n        n_jobs=n_jobs,\n    )\n\n    data_content, trace, fold_evaluations, sample_evaluations = res\n    fields = [*run_environment, time.strftime(\"%c\"), \"Created by run_flow_on_task\"]\n    generated_description = \"\\n\".join(fields)\n    run = OpenMLRun(\n        task_id=task.task_id,\n        flow_id=flow_id,\n        dataset_id=dataset.dataset_id,\n        model=flow.model,\n        flow_name=flow.name,\n        tags=tags,\n        trace=trace,\n        data_content=data_content,\n        flow=flow,\n        setup_string=flow.extension.create_setup_string(flow.model),\n        description_text=generated_description,\n    )\n\n    if (upload_flow or avoid_duplicate_runs) and flow.flow_id is not None:\n        # We only extract the parameter settings if a sync happened with the server.\n        # I.e. when the flow was uploaded or we found it in the avoid_duplicate check.\n        # Otherwise, we will do this at upload time.\n        run.parameter_settings = flow.extension.obtain_parameter_values(flow)\n\n    # now we need to attach the detailed evaluations\n    if task.task_type_id == TaskType.LEARNING_CURVE:\n        run.sample_evaluations = sample_evaluations\n    else:\n        run.fold_evaluations = fold_evaluations\n\n    if flow_id:\n        message = f\"Executed Task {task.task_id} with Flow id:{run.flow_id}\"\n    else:\n        message = f\"Executed Task {task.task_id} on local Flow with name {flow.name}.\"\n    config.logger.info(message)\n\n    return run\n</code></pre>"},{"location":"reference/runs/#openml.runs.run_model_on_task","title":"run_model_on_task","text":"<pre><code>run_model_on_task(model: Any, task: int | str | OpenMLTask, avoid_duplicate_runs: bool | None = None, flow_tags: list[str] | None = None, seed: int | None = None, add_local_measures: bool = True, upload_flow: bool = False, return_flow: bool = False, n_jobs: int | None = None) -&gt; OpenMLRun | tuple[OpenMLRun, OpenMLFlow]\n</code></pre> <p>Run the model on the dataset defined by the task.</p> PARAMETER DESCRIPTION <code>model</code> <p>A model which has a function fit(X,Y) and predict(X), all supervised estimators of scikit learn follow this definition of a model.</p> <p> TYPE: <code>sklearn model</code> </p> <code>task</code> <p>Task to perform or Task id. This may be a model instead if the first argument is an OpenMLTask.</p> <p> TYPE: <code>OpenMLTask or int or str</code> </p> <code>avoid_duplicate_runs</code> <p>If True, the run will throw an error if the setup/task combination is already present on the server. This feature requires an internet connection. If not set, it will use the default from your openml configuration (False if unset).</p> <p> TYPE: <code>(bool, optional(default=None))</code> DEFAULT: <code>None</code> </p> <code>flow_tags</code> <p>A list of tags that the flow should have at creation.</p> <p> TYPE: <code>(List[str], optional(default=None))</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Models that are not seeded will get this seed.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>add_local_measures</code> <p>Determines whether to calculate a set of evaluation measures locally, to later verify server behaviour.</p> <p> TYPE: <code>(bool, optional(default=True))</code> DEFAULT: <code>True</code> </p> <code>upload_flow</code> <p>If True, upload the flow to OpenML if it does not exist yet. If False, do not upload the flow to OpenML.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>return_flow</code> <p>If True, returns the OpenMLFlow generated from the model in addition to the OpenMLRun.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>n_jobs</code> <p>The number of processes/threads to distribute the evaluation asynchronously. If <code>None</code> or <code>1</code>, then the evaluation is treated as synchronous and processed sequentially. If <code>-1</code>, then the job uses as many cores available.</p> <p> TYPE: <code>int(default=None)</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>run</code> <p>Result of the run.</p> <p> TYPE: <code>OpenMLRun</code> </p> <code>flow</code> <p>Flow generated from the model.</p> <p> TYPE: <code>OpenMLFlow (optional, only if `return_flow` is True).</code> </p> Source code in <code>openml/runs/functions.py</code> <pre><code>def run_model_on_task(  # noqa: PLR0913\n    model: Any,\n    task: int | str | OpenMLTask,\n    avoid_duplicate_runs: bool | None = None,\n    flow_tags: list[str] | None = None,\n    seed: int | None = None,\n    add_local_measures: bool = True,  # noqa: FBT001, FBT002\n    upload_flow: bool = False,  # noqa: FBT001, FBT002\n    return_flow: bool = False,  # noqa: FBT001, FBT002\n    n_jobs: int | None = None,\n) -&gt; OpenMLRun | tuple[OpenMLRun, OpenMLFlow]:\n    \"\"\"Run the model on the dataset defined by the task.\n\n    Parameters\n    ----------\n    model : sklearn model\n        A model which has a function fit(X,Y) and predict(X),\n        all supervised estimators of scikit learn follow this definition of a model.\n    task : OpenMLTask or int or str\n        Task to perform or Task id.\n        This may be a model instead if the first argument is an OpenMLTask.\n    avoid_duplicate_runs : bool, optional (default=None)\n        If True, the run will throw an error if the setup/task combination is already present on\n        the server. This feature requires an internet connection.\n        If not set, it will use the default from your openml configuration (False if unset).\n    flow_tags : List[str], optional (default=None)\n        A list of tags that the flow should have at creation.\n    seed: int, optional (default=None)\n        Models that are not seeded will get this seed.\n    add_local_measures : bool, optional (default=True)\n        Determines whether to calculate a set of evaluation measures locally,\n        to later verify server behaviour.\n    upload_flow : bool (default=False)\n        If True, upload the flow to OpenML if it does not exist yet.\n        If False, do not upload the flow to OpenML.\n    return_flow : bool (default=False)\n        If True, returns the OpenMLFlow generated from the model in addition to the OpenMLRun.\n    n_jobs : int (default=None)\n        The number of processes/threads to distribute the evaluation asynchronously.\n        If `None` or `1`, then the evaluation is treated as synchronous and processed sequentially.\n        If `-1`, then the job uses as many cores available.\n\n    Returns\n    -------\n    run : OpenMLRun\n        Result of the run.\n    flow : OpenMLFlow (optional, only if `return_flow` is True).\n        Flow generated from the model.\n    \"\"\"\n    if avoid_duplicate_runs is None:\n        avoid_duplicate_runs = openml.config.avoid_duplicate_runs\n    if avoid_duplicate_runs and not config.apikey:\n        warnings.warn(\n            \"avoid_duplicate_runs is set to True, but no API key is set. \"\n            \"Please set your API key in the OpenML configuration file, see\"\n            \"https://openml.github.io/openml-python/main/examples/20_basic/introduction_tutorial\"\n            \".html#authentication for more information on authentication.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n\n    # TODO: At some point in the future do not allow for arguments in old order (6-2018).\n    # Flexibility currently still allowed due to code-snippet in OpenML100 paper (3-2019).\n    # When removing this please also remove the method `is_estimator` from the extension\n    # interface as it is only used here (MF, 3-2019)\n    if isinstance(model, (int, str, OpenMLTask)):\n        warnings.warn(\n            \"The old argument order (task, model) is deprecated and \"\n            \"will not be supported in the future. Please use the \"\n            \"order (model, task).\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        task, model = model, task\n\n    extension = get_extension_by_model(model, raise_if_no_extension=True)\n    if extension is None:\n        # This should never happen and is only here to please mypy will be gone soon once the\n        # whole function is removed\n        raise TypeError(extension)\n\n    flow = extension.model_to_flow(model)\n\n    def get_task_and_type_conversion(_task: int | str | OpenMLTask) -&gt; OpenMLTask:\n        \"\"\"Retrieve an OpenMLTask object from either an integer or string ID,\n        or directly from an OpenMLTask object.\n\n        Parameters\n        ----------\n        _task : Union[int, str, OpenMLTask]\n            The task ID or the OpenMLTask object.\n\n        Returns\n        -------\n        OpenMLTask\n            The OpenMLTask object.\n        \"\"\"\n        if isinstance(_task, (int, str)):\n            return get_task(int(_task))  # type: ignore\n\n        return _task\n\n    task = get_task_and_type_conversion(task)\n\n    run = run_flow_on_task(\n        task=task,\n        flow=flow,\n        avoid_duplicate_runs=avoid_duplicate_runs,\n        flow_tags=flow_tags,\n        seed=seed,\n        add_local_measures=add_local_measures,\n        upload_flow=upload_flow,\n        n_jobs=n_jobs,\n    )\n    if return_flow:\n        return run, flow\n    return run\n</code></pre>"},{"location":"reference/runs/functions/","title":"functions","text":""},{"location":"reference/runs/functions/#openml.runs.functions","title":"openml.runs.functions","text":""},{"location":"reference/runs/functions/#openml.runs.functions.__list_runs","title":"__list_runs","text":"<pre><code>__list_runs(api_call: str) -&gt; DataFrame\n</code></pre> <p>Helper function to parse API calls which are lists of runs</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def __list_runs(api_call: str) -&gt; pd.DataFrame:\n    \"\"\"Helper function to parse API calls which are lists of runs\"\"\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    runs_dict = xmltodict.parse(xml_string, force_list=(\"oml:run\",))\n    # Minimalistic check if the XML is useful\n    if \"oml:runs\" not in runs_dict:\n        raise ValueError(f'Error in return XML, does not contain \"oml:runs\": {runs_dict}')\n\n    if \"@xmlns:oml\" not in runs_dict[\"oml:runs\"]:\n        raise ValueError(\n            f'Error in return XML, does not contain \"oml:runs\"/@xmlns:oml: {runs_dict}'\n        )\n\n    if runs_dict[\"oml:runs\"][\"@xmlns:oml\"] != \"http://openml.org/openml\":\n        raise ValueError(\n            \"Error in return XML, value of  \"\n            '\"oml:runs\"/@xmlns:oml is not '\n            f'\"http://openml.org/openml\": {runs_dict}',\n        )\n\n    assert isinstance(runs_dict[\"oml:runs\"][\"oml:run\"], list), type(runs_dict[\"oml:runs\"])\n\n    runs = {\n        int(r[\"oml:run_id\"]): {\n            \"run_id\": int(r[\"oml:run_id\"]),\n            \"task_id\": int(r[\"oml:task_id\"]),\n            \"setup_id\": int(r[\"oml:setup_id\"]),\n            \"flow_id\": int(r[\"oml:flow_id\"]),\n            \"uploader\": int(r[\"oml:uploader\"]),\n            \"task_type\": TaskType(int(r[\"oml:task_type_id\"])),\n            \"upload_time\": str(r[\"oml:upload_time\"]),\n            \"error_message\": str((r[\"oml:error_message\"]) or \"\"),\n        }\n        for r in runs_dict[\"oml:runs\"][\"oml:run\"]\n    }\n    return pd.DataFrame.from_dict(runs, orient=\"index\")\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.delete_run","title":"delete_run","text":"<pre><code>delete_run(run_id: int) -&gt; bool\n</code></pre> <p>Delete run with id <code>run_id</code> from the OpenML server.</p> <p>You can only delete runs which you uploaded.</p> PARAMETER DESCRIPTION <code>run_id</code> <p>OpenML id of the run</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the deletion was successful. False otherwise.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def delete_run(run_id: int) -&gt; bool:\n    \"\"\"Delete run with id `run_id` from the OpenML server.\n\n    You can only delete runs which you uploaded.\n\n    Parameters\n    ----------\n    run_id : int\n        OpenML id of the run\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"run\", run_id)\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.format_prediction","title":"format_prediction","text":"<pre><code>format_prediction(task: OpenMLSupervisedTask, repeat: int, fold: int, index: int, prediction: str | int | float, truth: str | int | float, sample: int | None = None, proba: dict[str, float] | None = None) -&gt; list[str | int | float]\n</code></pre> <p>Format the predictions in the specific order as required for the run results.</p> PARAMETER DESCRIPTION <code>task</code> <p>Task for which to format the predictions.</p> <p> TYPE: <code>OpenMLSupervisedTask</code> </p> <code>repeat</code> <p>From which repeat this predictions is made.</p> <p> TYPE: <code>int</code> </p> <code>fold</code> <p>From which fold this prediction is made.</p> <p> TYPE: <code>int</code> </p> <code>index</code> <p>For which index this prediction is made.</p> <p> TYPE: <code>int</code> </p> <code>prediction</code> <p>The predicted class label or value.</p> <p> TYPE: <code>str | int | float</code> </p> <code>truth</code> <p>The true class label or value.</p> <p> TYPE: <code>str | int | float</code> </p> <code>sample</code> <p>From which sample set this prediction is made. Required only for LearningCurve tasks.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>proba</code> <p>For classification tasks only. A mapping from each class label to their predicted probability. The dictionary should contain an entry for each of the <code>task.class_labels</code>. E.g.: {\"Iris-Setosa\": 0.2, \"Iris-Versicolor\": 0.7, \"Iris-Virginica\": 0.1}</p> <p> TYPE: <code>dict[str, float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>A list with elements for the prediction results of a run.</code> <code>The returned order of the elements is (if available):</code> <p>[repeat, fold, sample, index, prediction, truth, *probabilities]</p> <code>This order follows the R Client API.</code> Source code in <code>openml/runs/functions.py</code> <pre><code>def format_prediction(  # noqa: PLR0913\n    task: OpenMLSupervisedTask,\n    repeat: int,\n    fold: int,\n    index: int,\n    prediction: str | int | float,\n    truth: str | int | float,\n    sample: int | None = None,\n    proba: dict[str, float] | None = None,\n) -&gt; list[str | int | float]:\n    \"\"\"Format the predictions in the specific order as required for the run results.\n\n    Parameters\n    ----------\n    task: OpenMLSupervisedTask\n        Task for which to format the predictions.\n    repeat: int\n        From which repeat this predictions is made.\n    fold: int\n        From which fold this prediction is made.\n    index: int\n        For which index this prediction is made.\n    prediction: str, int or float\n        The predicted class label or value.\n    truth: str, int or float\n        The true class label or value.\n    sample: int, optional (default=None)\n        From which sample set this prediction is made.\n        Required only for LearningCurve tasks.\n    proba: Dict[str, float], optional (default=None)\n        For classification tasks only.\n        A mapping from each class label to their predicted probability.\n        The dictionary should contain an entry for each of the `task.class_labels`.\n        E.g.: {\"Iris-Setosa\": 0.2, \"Iris-Versicolor\": 0.7, \"Iris-Virginica\": 0.1}\n\n    Returns\n    -------\n    A list with elements for the prediction results of a run.\n\n    The returned order of the elements is (if available):\n        [repeat, fold, sample, index, prediction, truth, *probabilities]\n\n    This order follows the R Client API.\n    \"\"\"\n    if isinstance(task, OpenMLClassificationTask):\n        if proba is None:\n            raise ValueError(\"`proba` is required for classification task\")\n        if task.class_labels is None:\n            raise ValueError(\"The classification task must have class labels set\")\n        if not set(task.class_labels) == set(proba):\n            raise ValueError(\"Each class should have a predicted probability\")\n        if sample is None:\n            if isinstance(task, OpenMLLearningCurveTask):\n                raise ValueError(\"`sample` can not be none for LearningCurveTask\")\n\n            sample = 0\n        probabilities = [proba[c] for c in task.class_labels]\n        return [repeat, fold, sample, index, prediction, truth, *probabilities]\n\n    if isinstance(task, OpenMLRegressionTask):\n        return [repeat, fold, index, prediction, truth]\n\n    raise NotImplementedError(f\"Formatting for {type(task)} is not supported.\")\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.get_run","title":"get_run","text":"<pre><code>get_run(run_id: int, ignore_cache: bool = False) -&gt; OpenMLRun\n</code></pre> <p>Gets run corresponding to run_id.</p> PARAMETER DESCRIPTION <code>run_id</code> <p> TYPE: <code>int</code> </p> <code>ignore_cache</code> <p>Whether to ignore the cache. If <code>true</code> this will download and overwrite the run xml even if the requested run is already cached.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>ignore_cache</code> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>run</code> <p>Run corresponding to ID, fetched from the server.</p> <p> TYPE: <code>OpenMLRun</code> </p> Source code in <code>openml/runs/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_run(run_id: int, ignore_cache: bool = False) -&gt; OpenMLRun:  # noqa: FBT002, FBT001\n    \"\"\"Gets run corresponding to run_id.\n\n    Parameters\n    ----------\n    run_id : int\n\n    ignore_cache : bool\n        Whether to ignore the cache. If ``true`` this will download and overwrite the run xml\n        even if the requested run is already cached.\n\n    ignore_cache\n\n    Returns\n    -------\n    run : OpenMLRun\n        Run corresponding to ID, fetched from the server.\n    \"\"\"\n    run_dir = Path(openml.utils._create_cache_directory_for_id(RUNS_CACHE_DIR_NAME, run_id))\n    run_file = run_dir / \"description.xml\"\n\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    try:\n        if not ignore_cache:\n            return _get_cached_run(run_id)\n\n        raise OpenMLCacheException(message=\"dummy\")\n\n    except OpenMLCacheException:\n        run_xml = openml._api_calls._perform_api_call(\"run/%d\" % run_id, \"get\")\n        with run_file.open(\"w\", encoding=\"utf8\") as fh:\n            fh.write(run_xml)\n\n    return _create_run_from_xml(run_xml)\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.get_run_trace","title":"get_run_trace","text":"<pre><code>get_run_trace(run_id: int) -&gt; OpenMLRunTrace\n</code></pre> <p>Get the optimization trace object for a given run id.</p> PARAMETER DESCRIPTION <code>run_id</code> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>OpenMLTrace</code> Source code in <code>openml/runs/functions.py</code> <pre><code>def get_run_trace(run_id: int) -&gt; OpenMLRunTrace:\n    \"\"\"\n    Get the optimization trace object for a given run id.\n\n    Parameters\n    ----------\n    run_id : int\n\n    Returns\n    -------\n    openml.runs.OpenMLTrace\n    \"\"\"\n    trace_xml = openml._api_calls._perform_api_call(\"run/trace/%d\" % run_id, \"get\")\n    return OpenMLRunTrace.trace_from_xml(trace_xml)\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.get_runs","title":"get_runs","text":"<pre><code>get_runs(run_ids: list[int]) -&gt; list[OpenMLRun]\n</code></pre> <p>Gets all runs in run_ids list.</p> PARAMETER DESCRIPTION <code>run_ids</code> <p> TYPE: <code>list of ints</code> </p> RETURNS DESCRIPTION <code>runs</code> <p>List of runs corresponding to IDs, fetched from the server.</p> <p> TYPE: <code>list of OpenMLRun</code> </p> Source code in <code>openml/runs/functions.py</code> <pre><code>def get_runs(run_ids: list[int]) -&gt; list[OpenMLRun]:\n    \"\"\"Gets all runs in run_ids list.\n\n    Parameters\n    ----------\n    run_ids : list of ints\n\n    Returns\n    -------\n    runs : list of OpenMLRun\n        List of runs corresponding to IDs, fetched from the server.\n    \"\"\"\n    runs = []\n    for run_id in run_ids:\n        runs.append(get_run(run_id))\n    return runs\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.initialize_model_from_run","title":"initialize_model_from_run","text":"<pre><code>initialize_model_from_run(run_id: int, *, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Initialized a model based on a run_id (i.e., using the exact same parameter settings)</p> PARAMETER DESCRIPTION <code>run_id</code> <p>The Openml run_id</p> <p> TYPE: <code>int</code> </p> <code>strict_version</code> <p>See <code>flow_to_model</code> strict_version.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>model</code> Source code in <code>openml/runs/functions.py</code> <pre><code>def initialize_model_from_run(run_id: int, *, strict_version: bool = True) -&gt; Any:\n    \"\"\"\n    Initialized a model based on a run_id (i.e., using the exact\n    same parameter settings)\n\n    Parameters\n    ----------\n    run_id : int\n        The Openml run_id\n    strict_version: bool (default=True)\n        See `flow_to_model` strict_version.\n\n    Returns\n    -------\n    model\n    \"\"\"\n    run = get_run(run_id)\n    # TODO(eddiebergman): I imagine this is None if it's not published,\n    # might need to raise an explicit error for that\n    assert run.setup_id is not None\n    return initialize_model(setup_id=run.setup_id, strict_version=strict_version)\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.initialize_model_from_trace","title":"initialize_model_from_trace","text":"<pre><code>initialize_model_from_trace(run_id: int, repeat: int, fold: int, iteration: int | None = None) -&gt; Any\n</code></pre> <p>Initialize a model based on the parameters that were set by an optimization procedure (i.e., using the exact same parameter settings)</p> PARAMETER DESCRIPTION <code>run_id</code> <p>The Openml run_id. Should contain a trace file, otherwise a OpenMLServerException is raised</p> <p> TYPE: <code>int</code> </p> <code>repeat</code> <p>The repeat nr (column in trace file)</p> <p> TYPE: <code>int</code> </p> <code>fold</code> <p>The fold nr (column in trace file)</p> <p> TYPE: <code>int</code> </p> <code>iteration</code> <p>The iteration nr (column in trace file). If None, the best (selected) iteration will be searched (slow), according to the selection criteria implemented in OpenMLRunTrace.get_selected_iteration</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>model</code> Source code in <code>openml/runs/functions.py</code> <pre><code>def initialize_model_from_trace(\n    run_id: int,\n    repeat: int,\n    fold: int,\n    iteration: int | None = None,\n) -&gt; Any:\n    \"\"\"\n    Initialize a model based on the parameters that were set\n    by an optimization procedure (i.e., using the exact same\n    parameter settings)\n\n    Parameters\n    ----------\n    run_id : int\n        The Openml run_id. Should contain a trace file,\n        otherwise a OpenMLServerException is raised\n\n    repeat : int\n        The repeat nr (column in trace file)\n\n    fold : int\n        The fold nr (column in trace file)\n\n    iteration : int\n        The iteration nr (column in trace file). If None, the\n        best (selected) iteration will be searched (slow),\n        according to the selection criteria implemented in\n        OpenMLRunTrace.get_selected_iteration\n\n    Returns\n    -------\n    model\n    \"\"\"\n    run = get_run(run_id)\n    # TODO(eddiebergman): I imagine this is None if it's not published,\n    # might need to raise an explicit error for that\n    assert run.flow_id is not None\n\n    flow = get_flow(run.flow_id)\n    run_trace = get_run_trace(run_id)\n\n    if iteration is None:\n        iteration = run_trace.get_selected_iteration(repeat, fold)\n\n    request = (repeat, fold, iteration)\n    if request not in run_trace.trace_iterations:\n        raise ValueError(\"Combination repeat, fold, iteration not available\")\n    current = run_trace.trace_iterations[(repeat, fold, iteration)]\n\n    search_model = initialize_model_from_run(run_id)\n    return flow.extension.instantiate_model_from_hpo_class(search_model, current)\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.list_runs","title":"list_runs","text":"<pre><code>list_runs(offset: int | None = None, size: int | None = None, id: list | None = None, task: list[int] | None = None, setup: list | None = None, flow: list | None = None, uploader: list | None = None, tag: str | None = None, study: int | None = None, display_errors: bool = False, task_type: TaskType | int | None = None) -&gt; DataFrame\n</code></pre> <p>List all runs matching all of the given filters. (Supports large amount of results)</p> PARAMETER DESCRIPTION <code>offset</code> <p>the number of runs to skip, starting from the first</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>the maximum number of runs to show</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>id</code> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>task</code> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>setup</code> <p> TYPE: <code>list | None</code> DEFAULT: <code>None</code> </p> <code>flow</code> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>uploader</code> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>study</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>display_errors</code> <p>Whether to list runs which have an error (for example a missing prediction file).</p> <p> TYPE: <code>(bool, optional(default=None))</code> DEFAULT: <code>False</code> </p> <code>task_type</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dataframe</code> Source code in <code>openml/runs/functions.py</code> <pre><code>def list_runs(  # noqa: PLR0913\n    offset: int | None = None,\n    size: int | None = None,\n    id: list | None = None,  # noqa: A002\n    task: list[int] | None = None,\n    setup: list | None = None,\n    flow: list | None = None,\n    uploader: list | None = None,\n    tag: str | None = None,\n    study: int | None = None,\n    display_errors: bool = False,  # noqa: FBT001, FBT002\n    task_type: TaskType | int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    List all runs matching all of the given filters.\n    (Supports large amount of results)\n\n    Parameters\n    ----------\n    offset : int, optional\n        the number of runs to skip, starting from the first\n    size : int, optional\n        the maximum number of runs to show\n\n    id : list, optional\n\n    task : list, optional\n\n    setup: list, optional\n\n    flow : list, optional\n\n    uploader : list, optional\n\n    tag : str, optional\n\n    study : int, optional\n\n    display_errors : bool, optional (default=None)\n        Whether to list runs which have an error (for example a missing\n        prediction file).\n\n    task_type : str, optional\n\n    Returns\n    -------\n    dataframe\n    \"\"\"\n    if id is not None and (not isinstance(id, list)):\n        raise TypeError(\"id must be of type list.\")\n    if task is not None and (not isinstance(task, list)):\n        raise TypeError(\"task must be of type list.\")\n    if setup is not None and (not isinstance(setup, list)):\n        raise TypeError(\"setup must be of type list.\")\n    if flow is not None and (not isinstance(flow, list)):\n        raise TypeError(\"flow must be of type list.\")\n    if uploader is not None and (not isinstance(uploader, list)):\n        raise TypeError(\"uploader must be of type list.\")\n\n    listing_call = partial(\n        _list_runs,\n        id=id,\n        task=task,\n        setup=setup,\n        flow=flow,\n        uploader=uploader,\n        tag=tag,\n        study=study,\n        display_errors=display_errors,\n        task_type=task_type,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.run_exists","title":"run_exists","text":"<pre><code>run_exists(task_id: int, setup_id: int) -&gt; set[int]\n</code></pre> <p>Checks whether a task/setup combination is already present on the server.</p> PARAMETER DESCRIPTION <code>task_id</code> <p> TYPE: <code>int</code> </p> <code>setup_id</code> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>    Set run ids for runs where flow setup_id was run on task_id. Empty</code> <p>set if it wasn't run yet.</p> Source code in <code>openml/runs/functions.py</code> <pre><code>def run_exists(task_id: int, setup_id: int) -&gt; set[int]:\n    \"\"\"Checks whether a task/setup combination is already present on the\n    server.\n\n    Parameters\n    ----------\n    task_id : int\n\n    setup_id : int\n\n    Returns\n    -------\n        Set run ids for runs where flow setup_id was run on task_id. Empty\n        set if it wasn't run yet.\n    \"\"\"\n    if setup_id &lt;= 0:\n        # openml setups are in range 1-inf\n        return set()\n\n    try:\n        result = list_runs(task=[task_id], setup=[setup_id])\n        return set() if result.empty else set(result[\"run_id\"])\n    except OpenMLServerException as exception:\n        # error code implies no results. The run does not exist yet\n        if exception.code != ERROR_CODE:\n            raise exception\n        return set()\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.run_flow_on_task","title":"run_flow_on_task","text":"<pre><code>run_flow_on_task(flow: OpenMLFlow, task: OpenMLTask, avoid_duplicate_runs: bool | None = None, flow_tags: list[str] | None = None, seed: int | None = None, add_local_measures: bool = True, upload_flow: bool = False, n_jobs: int | None = None) -&gt; OpenMLRun\n</code></pre> <p>Run the model provided by the flow on the dataset defined by task.</p> <p>Takes the flow and repeat information into account. The Flow may optionally be published.</p> PARAMETER DESCRIPTION <code>flow</code> <p>A flow wraps a machine learning model together with relevant information. The model has a function fit(X,Y) and predict(X), all supervised estimators of scikit learn follow this definition of a model.</p> <p> TYPE: <code>OpenMLFlow</code> </p> <code>task</code> <p>Task to perform. This may be an OpenMLFlow instead if the first argument is an OpenMLTask.</p> <p> TYPE: <code>OpenMLTask</code> </p> <code>avoid_duplicate_runs</code> <p>If True, the run will throw an error if the setup/task combination is already present on the server. This feature requires an internet connection. If not set, it will use the default from your openml configuration (False if unset).</p> <p> TYPE: <code>(bool, optional(default=None))</code> DEFAULT: <code>None</code> </p> <code>flow_tags</code> <p>A list of tags that the flow should have at creation.</p> <p> TYPE: <code>(List[str], optional(default=None))</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Models that are not seeded will get this seed.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>add_local_measures</code> <p>Determines whether to calculate a set of evaluation measures locally, to later verify server behaviour.</p> <p> TYPE: <code>(bool, optional(default=True))</code> DEFAULT: <code>True</code> </p> <code>upload_flow</code> <p>If True, upload the flow to OpenML if it does not exist yet. If False, do not upload the flow to OpenML.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>n_jobs</code> <p>The number of processes/threads to distribute the evaluation asynchronously. If <code>None</code> or <code>1</code>, then the evaluation is treated as synchronous and processed sequentially. If <code>-1</code>, then the job uses as many cores available.</p> <p> TYPE: <code>int(default=None)</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>run</code> <p>Result of the run.</p> <p> TYPE: <code>OpenMLRun</code> </p> Source code in <code>openml/runs/functions.py</code> <pre><code>def run_flow_on_task(  # noqa: C901, PLR0912, PLR0915, PLR0913\n    flow: OpenMLFlow,\n    task: OpenMLTask,\n    avoid_duplicate_runs: bool | None = None,\n    flow_tags: list[str] | None = None,\n    seed: int | None = None,\n    add_local_measures: bool = True,  # noqa: FBT001, FBT002\n    upload_flow: bool = False,  # noqa: FBT001, FBT002\n    n_jobs: int | None = None,\n) -&gt; OpenMLRun:\n    \"\"\"Run the model provided by the flow on the dataset defined by task.\n\n    Takes the flow and repeat information into account.\n    The Flow may optionally be published.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        A flow wraps a machine learning model together with relevant information.\n        The model has a function fit(X,Y) and predict(X),\n        all supervised estimators of scikit learn follow this definition of a model.\n    task : OpenMLTask\n        Task to perform. This may be an OpenMLFlow instead if the first argument is an OpenMLTask.\n    avoid_duplicate_runs : bool, optional (default=None)\n        If True, the run will throw an error if the setup/task combination is already present on\n        the server. This feature requires an internet connection.\n        If not set, it will use the default from your openml configuration (False if unset).\n    flow_tags : List[str], optional (default=None)\n        A list of tags that the flow should have at creation.\n    seed: int, optional (default=None)\n        Models that are not seeded will get this seed.\n    add_local_measures : bool, optional (default=True)\n        Determines whether to calculate a set of evaluation measures locally,\n        to later verify server behaviour.\n    upload_flow : bool (default=False)\n        If True, upload the flow to OpenML if it does not exist yet.\n        If False, do not upload the flow to OpenML.\n    n_jobs : int (default=None)\n        The number of processes/threads to distribute the evaluation asynchronously.\n        If `None` or `1`, then the evaluation is treated as synchronous and processed sequentially.\n        If `-1`, then the job uses as many cores available.\n\n    Returns\n    -------\n    run : OpenMLRun\n        Result of the run.\n    \"\"\"\n    if flow_tags is not None and not isinstance(flow_tags, list):\n        raise ValueError(\"flow_tags should be a list\")\n\n    if avoid_duplicate_runs is None:\n        avoid_duplicate_runs = openml.config.avoid_duplicate_runs\n\n    # TODO: At some point in the future do not allow for arguments in old order (changed 6-2018).\n    # Flexibility currently still allowed due to code-snippet in OpenML100 paper (3-2019).\n    if isinstance(flow, OpenMLTask) and isinstance(task, OpenMLFlow):\n        # We want to allow either order of argument (to avoid confusion).\n        warnings.warn(\n            \"The old argument order (Flow, model) is deprecated and \"\n            \"will not be supported in the future. Please use the \"\n            \"order (model, Flow).\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        task, flow = flow, task\n\n    if task.task_id is None:\n        raise ValueError(\"The task should be published at OpenML\")\n\n    if flow.model is None:\n        flow.model = flow.extension.flow_to_model(flow)\n\n    flow.model = flow.extension.seed_model(flow.model, seed=seed)\n\n    # We only need to sync with the server right now if we want to upload the flow,\n    # or ensure no duplicate runs exist. Otherwise it can be synced at upload time.\n    flow_id = None\n    if upload_flow or avoid_duplicate_runs:\n        flow_id = flow_exists(flow.name, flow.external_version)\n        if isinstance(flow.flow_id, int) and flow_id != flow.flow_id:\n            if flow_id is not False:\n                raise PyOpenMLError(\n                    f\"Local flow_id does not match server flow_id: '{flow.flow_id}' vs '{flow_id}'\",\n                )\n            raise PyOpenMLError(\n                \"Flow does not exist on the server, but 'flow.flow_id' is not None.\"\n            )\n        if upload_flow and flow_id is False:\n            flow.publish()\n            flow_id = flow.flow_id\n        elif flow_id:\n            flow_from_server = get_flow(flow_id)\n            _copy_server_fields(flow_from_server, flow)\n            if avoid_duplicate_runs:\n                flow_from_server.model = flow.model\n                setup_id = setup_exists(flow_from_server)\n                ids = run_exists(task.task_id, setup_id)\n                if ids:\n                    error_message = (\n                        \"One or more runs of this setup were already performed on the task.\"\n                    )\n                    raise OpenMLRunsExistError(ids, error_message)\n        else:\n            # Flow does not exist on server and we do not want to upload it.\n            # No sync with the server happens.\n            flow_id = None\n\n    dataset = task.get_dataset()\n\n    run_environment = flow.extension.get_version_information()\n    tags = [\"openml-python\", run_environment[1]]\n\n    if flow.extension.check_if_model_fitted(flow.model):\n        warnings.warn(\n            \"The model is already fitted! This might cause inconsistency in comparison of results.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n\n    # execute the run\n    res = _run_task_get_arffcontent(\n        model=flow.model,\n        task=task,\n        extension=flow.extension,\n        add_local_measures=add_local_measures,\n        n_jobs=n_jobs,\n    )\n\n    data_content, trace, fold_evaluations, sample_evaluations = res\n    fields = [*run_environment, time.strftime(\"%c\"), \"Created by run_flow_on_task\"]\n    generated_description = \"\\n\".join(fields)\n    run = OpenMLRun(\n        task_id=task.task_id,\n        flow_id=flow_id,\n        dataset_id=dataset.dataset_id,\n        model=flow.model,\n        flow_name=flow.name,\n        tags=tags,\n        trace=trace,\n        data_content=data_content,\n        flow=flow,\n        setup_string=flow.extension.create_setup_string(flow.model),\n        description_text=generated_description,\n    )\n\n    if (upload_flow or avoid_duplicate_runs) and flow.flow_id is not None:\n        # We only extract the parameter settings if a sync happened with the server.\n        # I.e. when the flow was uploaded or we found it in the avoid_duplicate check.\n        # Otherwise, we will do this at upload time.\n        run.parameter_settings = flow.extension.obtain_parameter_values(flow)\n\n    # now we need to attach the detailed evaluations\n    if task.task_type_id == TaskType.LEARNING_CURVE:\n        run.sample_evaluations = sample_evaluations\n    else:\n        run.fold_evaluations = fold_evaluations\n\n    if flow_id:\n        message = f\"Executed Task {task.task_id} with Flow id:{run.flow_id}\"\n    else:\n        message = f\"Executed Task {task.task_id} on local Flow with name {flow.name}.\"\n    config.logger.info(message)\n\n    return run\n</code></pre>"},{"location":"reference/runs/functions/#openml.runs.functions.run_model_on_task","title":"run_model_on_task","text":"<pre><code>run_model_on_task(model: Any, task: int | str | OpenMLTask, avoid_duplicate_runs: bool | None = None, flow_tags: list[str] | None = None, seed: int | None = None, add_local_measures: bool = True, upload_flow: bool = False, return_flow: bool = False, n_jobs: int | None = None) -&gt; OpenMLRun | tuple[OpenMLRun, OpenMLFlow]\n</code></pre> <p>Run the model on the dataset defined by the task.</p> PARAMETER DESCRIPTION <code>model</code> <p>A model which has a function fit(X,Y) and predict(X), all supervised estimators of scikit learn follow this definition of a model.</p> <p> TYPE: <code>sklearn model</code> </p> <code>task</code> <p>Task to perform or Task id. This may be a model instead if the first argument is an OpenMLTask.</p> <p> TYPE: <code>OpenMLTask or int or str</code> </p> <code>avoid_duplicate_runs</code> <p>If True, the run will throw an error if the setup/task combination is already present on the server. This feature requires an internet connection. If not set, it will use the default from your openml configuration (False if unset).</p> <p> TYPE: <code>(bool, optional(default=None))</code> DEFAULT: <code>None</code> </p> <code>flow_tags</code> <p>A list of tags that the flow should have at creation.</p> <p> TYPE: <code>(List[str], optional(default=None))</code> DEFAULT: <code>None</code> </p> <code>seed</code> <p>Models that are not seeded will get this seed.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>add_local_measures</code> <p>Determines whether to calculate a set of evaluation measures locally, to later verify server behaviour.</p> <p> TYPE: <code>(bool, optional(default=True))</code> DEFAULT: <code>True</code> </p> <code>upload_flow</code> <p>If True, upload the flow to OpenML if it does not exist yet. If False, do not upload the flow to OpenML.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>return_flow</code> <p>If True, returns the OpenMLFlow generated from the model in addition to the OpenMLRun.</p> <p> TYPE: <code>bool(default=False)</code> DEFAULT: <code>False</code> </p> <code>n_jobs</code> <p>The number of processes/threads to distribute the evaluation asynchronously. If <code>None</code> or <code>1</code>, then the evaluation is treated as synchronous and processed sequentially. If <code>-1</code>, then the job uses as many cores available.</p> <p> TYPE: <code>int(default=None)</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>run</code> <p>Result of the run.</p> <p> TYPE: <code>OpenMLRun</code> </p> <code>flow</code> <p>Flow generated from the model.</p> <p> TYPE: <code>OpenMLFlow (optional, only if `return_flow` is True).</code> </p> Source code in <code>openml/runs/functions.py</code> <pre><code>def run_model_on_task(  # noqa: PLR0913\n    model: Any,\n    task: int | str | OpenMLTask,\n    avoid_duplicate_runs: bool | None = None,\n    flow_tags: list[str] | None = None,\n    seed: int | None = None,\n    add_local_measures: bool = True,  # noqa: FBT001, FBT002\n    upload_flow: bool = False,  # noqa: FBT001, FBT002\n    return_flow: bool = False,  # noqa: FBT001, FBT002\n    n_jobs: int | None = None,\n) -&gt; OpenMLRun | tuple[OpenMLRun, OpenMLFlow]:\n    \"\"\"Run the model on the dataset defined by the task.\n\n    Parameters\n    ----------\n    model : sklearn model\n        A model which has a function fit(X,Y) and predict(X),\n        all supervised estimators of scikit learn follow this definition of a model.\n    task : OpenMLTask or int or str\n        Task to perform or Task id.\n        This may be a model instead if the first argument is an OpenMLTask.\n    avoid_duplicate_runs : bool, optional (default=None)\n        If True, the run will throw an error if the setup/task combination is already present on\n        the server. This feature requires an internet connection.\n        If not set, it will use the default from your openml configuration (False if unset).\n    flow_tags : List[str], optional (default=None)\n        A list of tags that the flow should have at creation.\n    seed: int, optional (default=None)\n        Models that are not seeded will get this seed.\n    add_local_measures : bool, optional (default=True)\n        Determines whether to calculate a set of evaluation measures locally,\n        to later verify server behaviour.\n    upload_flow : bool (default=False)\n        If True, upload the flow to OpenML if it does not exist yet.\n        If False, do not upload the flow to OpenML.\n    return_flow : bool (default=False)\n        If True, returns the OpenMLFlow generated from the model in addition to the OpenMLRun.\n    n_jobs : int (default=None)\n        The number of processes/threads to distribute the evaluation asynchronously.\n        If `None` or `1`, then the evaluation is treated as synchronous and processed sequentially.\n        If `-1`, then the job uses as many cores available.\n\n    Returns\n    -------\n    run : OpenMLRun\n        Result of the run.\n    flow : OpenMLFlow (optional, only if `return_flow` is True).\n        Flow generated from the model.\n    \"\"\"\n    if avoid_duplicate_runs is None:\n        avoid_duplicate_runs = openml.config.avoid_duplicate_runs\n    if avoid_duplicate_runs and not config.apikey:\n        warnings.warn(\n            \"avoid_duplicate_runs is set to True, but no API key is set. \"\n            \"Please set your API key in the OpenML configuration file, see\"\n            \"https://openml.github.io/openml-python/main/examples/20_basic/introduction_tutorial\"\n            \".html#authentication for more information on authentication.\",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n\n    # TODO: At some point in the future do not allow for arguments in old order (6-2018).\n    # Flexibility currently still allowed due to code-snippet in OpenML100 paper (3-2019).\n    # When removing this please also remove the method `is_estimator` from the extension\n    # interface as it is only used here (MF, 3-2019)\n    if isinstance(model, (int, str, OpenMLTask)):\n        warnings.warn(\n            \"The old argument order (task, model) is deprecated and \"\n            \"will not be supported in the future. Please use the \"\n            \"order (model, task).\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        task, model = model, task\n\n    extension = get_extension_by_model(model, raise_if_no_extension=True)\n    if extension is None:\n        # This should never happen and is only here to please mypy will be gone soon once the\n        # whole function is removed\n        raise TypeError(extension)\n\n    flow = extension.model_to_flow(model)\n\n    def get_task_and_type_conversion(_task: int | str | OpenMLTask) -&gt; OpenMLTask:\n        \"\"\"Retrieve an OpenMLTask object from either an integer or string ID,\n        or directly from an OpenMLTask object.\n\n        Parameters\n        ----------\n        _task : Union[int, str, OpenMLTask]\n            The task ID or the OpenMLTask object.\n\n        Returns\n        -------\n        OpenMLTask\n            The OpenMLTask object.\n        \"\"\"\n        if isinstance(_task, (int, str)):\n            return get_task(int(_task))  # type: ignore\n\n        return _task\n\n    task = get_task_and_type_conversion(task)\n\n    run = run_flow_on_task(\n        task=task,\n        flow=flow,\n        avoid_duplicate_runs=avoid_duplicate_runs,\n        flow_tags=flow_tags,\n        seed=seed,\n        add_local_measures=add_local_measures,\n        upload_flow=upload_flow,\n        n_jobs=n_jobs,\n    )\n    if return_flow:\n        return run, flow\n    return run\n</code></pre>"},{"location":"reference/runs/run/","title":"run","text":""},{"location":"reference/runs/run/#openml.runs.run","title":"openml.runs.run","text":""},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun","title":"OpenMLRun","text":"<pre><code>OpenMLRun(task_id: int, flow_id: int | None, dataset_id: int | None, setup_string: str | None = None, output_files: dict[str, int] | None = None, setup_id: int | None = None, tags: list[str] | None = None, uploader: int | None = None, uploader_name: str | None = None, evaluations: dict | None = None, fold_evaluations: dict | None = None, sample_evaluations: dict | None = None, data_content: list[list] | None = None, trace: OpenMLRunTrace | None = None, model: object | None = None, task_type: str | None = None, task_evaluation_measure: str | None = None, flow_name: str | None = None, parameter_settings: list[dict[str, Any]] | None = None, predictions_url: str | None = None, task: OpenMLTask | None = None, flow: OpenMLFlow | None = None, run_id: int | None = None, description_text: str | None = None, run_details: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Run: result of running a model on an OpenML dataset.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>The ID of the OpenML task associated with the run.</p> <p> TYPE: <code>int</code> </p> <code>flow_id</code> <p>The ID of the OpenML flow associated with the run.</p> <p> TYPE: <code>int | None</code> </p> <code>dataset_id</code> <p>The ID of the OpenML dataset used for the run.</p> <p> TYPE: <code>int | None</code> </p> <code>setup_string</code> <p>The setup string of the run.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>output_files</code> <p>Specifies where each related file can be found.</p> <p> TYPE: <code>dict[str, int] | None</code> DEFAULT: <code>None</code> </p> <code>setup_id</code> <p>An integer representing the ID of the setup used for the run.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Representing the tags associated with the run.</p> <p> TYPE: <code>list[str] | None</code> DEFAULT: <code>None</code> </p> <code>uploader</code> <p>User ID of the uploader.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>uploader_name</code> <p>The name of the person who uploaded the run.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>evaluations</code> <p>Representing the evaluations of the run.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>fold_evaluations</code> <p>The evaluations of the run for each fold.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>sample_evaluations</code> <p>The evaluations of the run for each sample.</p> <p> TYPE: <code>dict | None</code> DEFAULT: <code>None</code> </p> <code>data_content</code> <p>The predictions generated from executing this run.</p> <p> TYPE: <code>list[list] | None</code> DEFAULT: <code>None</code> </p> <code>trace</code> <p>The trace containing information on internal model evaluations of this run.</p> <p> TYPE: <code>OpenMLRunTrace | None</code> DEFAULT: <code>None</code> </p> <code>model</code> <p>The untrained model that was evaluated in the run.</p> <p> TYPE: <code>object | None</code> DEFAULT: <code>None</code> </p> <code>task_type</code> <p>The type of the OpenML task associated with the run.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>task_evaluation_measure</code> <p>The evaluation measure used for the task.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>flow_name</code> <p>The name of the OpenML flow associated with the run.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>parameter_settings</code> <p>Representing the parameter settings used for the run.</p> <p> TYPE: <code>list[dict[str, Any]] | None</code> DEFAULT: <code>None</code> </p> <code>predictions_url</code> <p>The URL of the predictions file.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>task</code> <p>An instance of the OpenMLTask class, representing the OpenML task associated with the run.</p> <p> TYPE: <code>OpenMLTask | None</code> DEFAULT: <code>None</code> </p> <code>flow</code> <p>An instance of the OpenMLFlow class, representing the OpenML flow associated with the run.</p> <p> TYPE: <code>OpenMLFlow | None</code> DEFAULT: <code>None</code> </p> <code>run_id</code> <p>The ID of the run.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>description_text</code> <p>Description text to add to the predictions file. If left None, is set to the time the arff file is generated.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>run_details</code> <p>Description of the run stored in the run meta-data.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/runs/run.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_id: int,\n    flow_id: int | None,\n    dataset_id: int | None,\n    setup_string: str | None = None,\n    output_files: dict[str, int] | None = None,\n    setup_id: int | None = None,\n    tags: list[str] | None = None,\n    uploader: int | None = None,\n    uploader_name: str | None = None,\n    evaluations: dict | None = None,\n    fold_evaluations: dict | None = None,\n    sample_evaluations: dict | None = None,\n    data_content: list[list] | None = None,\n    trace: OpenMLRunTrace | None = None,\n    model: object | None = None,\n    task_type: str | None = None,\n    task_evaluation_measure: str | None = None,\n    flow_name: str | None = None,\n    parameter_settings: list[dict[str, Any]] | None = None,\n    predictions_url: str | None = None,\n    task: OpenMLTask | None = None,\n    flow: OpenMLFlow | None = None,\n    run_id: int | None = None,\n    description_text: str | None = None,\n    run_details: str | None = None,\n):\n    self.uploader = uploader\n    self.uploader_name = uploader_name\n    self.task_id = task_id\n    self.task_type = task_type\n    self.task_evaluation_measure = task_evaluation_measure\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.setup_id = setup_id\n    self.setup_string = setup_string\n    self.parameter_settings = parameter_settings\n    self.dataset_id = dataset_id\n    self.evaluations = evaluations\n    self.fold_evaluations = fold_evaluations\n    self.sample_evaluations = sample_evaluations\n    self.data_content = data_content\n    self.output_files = output_files\n    self.trace = trace\n    self.error_message = None\n    self.task = task\n    self.flow = flow\n    self.run_id = run_id\n    self.model = model\n    self.tags = tags\n    self.predictions_url = predictions_url\n    self.description_text = description_text\n    self.run_details = run_details\n    self._predictions = None\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>The ID of the run, None if not uploaded to the server yet.</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.predictions","title":"predictions  <code>property</code>","text":"<pre><code>predictions: DataFrame\n</code></pre> <p>Return a DataFrame with predictions for this run</p>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.from_filesystem","title":"from_filesystem  <code>classmethod</code>","text":"<pre><code>from_filesystem(directory: str | Path, expect_model: bool = True) -&gt; OpenMLRun\n</code></pre> <p>The inverse of the to_filesystem method. Instantiates an OpenMLRun object based on files stored on the file system.</p> PARAMETER DESCRIPTION <code>directory</code> <p>a path leading to the folder where the results are stored</p> <p> TYPE: <code>str</code> </p> <code>expect_model</code> <p>if True, it requires the model pickle to be present, and an error will be thrown if not. Otherwise, the model might or might not be present.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>run</code> <p>the re-instantiated run object</p> <p> TYPE: <code>OpenMLRun</code> </p> Source code in <code>openml/runs/run.py</code> <pre><code>@classmethod\ndef from_filesystem(cls, directory: str | Path, expect_model: bool = True) -&gt; OpenMLRun:  # noqa: FBT001, FBT002\n    \"\"\"\n    The inverse of the to_filesystem method. Instantiates an OpenMLRun\n    object based on files stored on the file system.\n\n    Parameters\n    ----------\n    directory : str\n        a path leading to the folder where the results\n        are stored\n\n    expect_model : bool\n        if True, it requires the model pickle to be present, and an error\n        will be thrown if not. Otherwise, the model might or might not\n        be present.\n\n    Returns\n    -------\n    run : OpenMLRun\n        the re-instantiated run object\n    \"\"\"\n    # Avoiding cyclic imports\n    import openml.runs.functions\n\n    directory = Path(directory)\n    if not directory.is_dir():\n        raise ValueError(\"Could not find folder\")\n\n    description_path = directory / \"description.xml\"\n    predictions_path = directory / \"predictions.arff\"\n    trace_path = directory / \"trace.arff\"\n    model_path = directory / \"model.pkl\"\n\n    if not description_path.is_file():\n        raise ValueError(\"Could not find description.xml\")\n    if not predictions_path.is_file():\n        raise ValueError(\"Could not find predictions.arff\")\n    if (not model_path.is_file()) and expect_model:\n        raise ValueError(\"Could not find model.pkl\")\n\n    with description_path.open() as fht:\n        xml_string = fht.read()\n    run = openml.runs.functions._create_run_from_xml(xml_string, from_server=False)\n\n    if run.flow_id is None:\n        flow = openml.flows.OpenMLFlow.from_filesystem(directory)\n        run.flow = flow\n        run.flow_name = flow.name\n\n    with predictions_path.open() as fht:\n        predictions = arff.load(fht)\n        run.data_content = predictions[\"data\"]\n\n    if model_path.is_file():\n        # note that it will load the model if the file exists, even if\n        # expect_model is False\n        with model_path.open(\"rb\") as fhb:\n            run.model = pickle.load(fhb)  # noqa: S301\n\n    if trace_path.is_file():\n        run.trace = openml.runs.OpenMLRunTrace._from_filesystem(trace_path)\n\n    return run\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.get_metric_fn","title":"get_metric_fn","text":"<pre><code>get_metric_fn(sklearn_fn: Callable, kwargs: dict | None = None) -&gt; ndarray\n</code></pre> <p>Calculates metric scores based on predicted values. Assumes the run has been executed locally (and contains run_data). Furthermore, it assumes that the 'correct' or 'truth' attribute is specified in the arff (which is an optional field, but always the case for openml-python runs)</p> PARAMETER DESCRIPTION <code>sklearn_fn</code> <p>a function pointer to a sklearn function that accepts <code>y_true</code>, <code>y_pred</code> and <code>**kwargs</code></p> <p> TYPE: <code>function</code> </p> <code>kwargs</code> <p>kwargs for the function</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>scores</code> <p>metric results</p> <p> TYPE: <code>ndarray of scores of length num_folds * num_repeats</code> </p> Source code in <code>openml/runs/run.py</code> <pre><code>def get_metric_fn(self, sklearn_fn: Callable, kwargs: dict | None = None) -&gt; np.ndarray:  # noqa: PLR0915, PLR0912, C901\n    \"\"\"Calculates metric scores based on predicted values. Assumes the\n    run has been executed locally (and contains run_data). Furthermore,\n    it assumes that the 'correct' or 'truth' attribute is specified in\n    the arff (which is an optional field, but always the case for\n    openml-python runs)\n\n    Parameters\n    ----------\n    sklearn_fn : function\n        a function pointer to a sklearn function that\n        accepts ``y_true``, ``y_pred`` and ``**kwargs``\n    kwargs : dict\n        kwargs for the function\n\n    Returns\n    -------\n    scores : ndarray of scores of length num_folds * num_repeats\n        metric results\n    \"\"\"\n    kwargs = kwargs if kwargs else {}\n    if self.data_content is not None and self.task_id is not None:\n        predictions_arff = self._generate_arff_dict()\n    elif (self.output_files is not None) and (\"predictions\" in self.output_files):\n        predictions_file_url = openml._api_calls._file_id_to_url(\n            self.output_files[\"predictions\"],\n            \"predictions.arff\",\n        )\n        response = openml._api_calls._download_text_file(predictions_file_url)\n        predictions_arff = arff.loads(response)\n        # TODO: make this a stream reader\n    else:\n        raise ValueError(\n            \"Run should have been locally executed or \" \"contain outputfile reference.\",\n        )\n\n    # Need to know more about the task to compute scores correctly\n    task = get_task(self.task_id)\n\n    attribute_names = [att[0] for att in predictions_arff[\"attributes\"]]\n    if (\n        task.task_type_id in [TaskType.SUPERVISED_CLASSIFICATION, TaskType.LEARNING_CURVE]\n        and \"correct\" not in attribute_names\n    ):\n        raise ValueError('Attribute \"correct\" should be set for ' \"classification task runs\")\n    if task.task_type_id == TaskType.SUPERVISED_REGRESSION and \"truth\" not in attribute_names:\n        raise ValueError('Attribute \"truth\" should be set for ' \"regression task runs\")\n    if task.task_type_id != TaskType.CLUSTERING and \"prediction\" not in attribute_names:\n        raise ValueError('Attribute \"predict\" should be set for ' \"supervised task runs\")\n\n    def _attribute_list_to_dict(attribute_list):  # type: ignore\n        # convenience function: Creates a mapping to map from the name of\n        # attributes present in the arff prediction file to their index.\n        # This is necessary because the number of classes can be different\n        # for different tasks.\n        res = OrderedDict()\n        for idx in range(len(attribute_list)):\n            res[attribute_list[idx][0]] = idx\n        return res\n\n    attribute_dict = _attribute_list_to_dict(predictions_arff[\"attributes\"])\n\n    repeat_idx = attribute_dict[\"repeat\"]\n    fold_idx = attribute_dict[\"fold\"]\n    predicted_idx = attribute_dict[\"prediction\"]  # Assume supervised task\n\n    if task.task_type_id in (TaskType.SUPERVISED_CLASSIFICATION, TaskType.LEARNING_CURVE):\n        correct_idx = attribute_dict[\"correct\"]\n    elif task.task_type_id == TaskType.SUPERVISED_REGRESSION:\n        correct_idx = attribute_dict[\"truth\"]\n    has_samples = False\n    if \"sample\" in attribute_dict:\n        sample_idx = attribute_dict[\"sample\"]\n        has_samples = True\n\n    if (\n        predictions_arff[\"attributes\"][predicted_idx][1]\n        != predictions_arff[\"attributes\"][correct_idx][1]\n    ):\n        pred = predictions_arff[\"attributes\"][predicted_idx][1]\n        corr = predictions_arff[\"attributes\"][correct_idx][1]\n        raise ValueError(\n            \"Predicted and Correct do not have equal values:\" f\" {pred!s} Vs. {corr!s}\",\n        )\n\n    # TODO: these could be cached\n    values_predict: dict[int, dict[int, dict[int, list[float]]]] = {}\n    values_correct: dict[int, dict[int, dict[int, list[float]]]] = {}\n    for _line_idx, line in enumerate(predictions_arff[\"data\"]):\n        rep = line[repeat_idx]\n        fold = line[fold_idx]\n        samp = line[sample_idx] if has_samples else 0\n\n        if task.task_type_id in [\n            TaskType.SUPERVISED_CLASSIFICATION,\n            TaskType.LEARNING_CURVE,\n        ]:\n            prediction = predictions_arff[\"attributes\"][predicted_idx][1].index(\n                line[predicted_idx],\n            )\n            correct = predictions_arff[\"attributes\"][predicted_idx][1].index(line[correct_idx])\n        elif task.task_type_id == TaskType.SUPERVISED_REGRESSION:\n            prediction = line[predicted_idx]\n            correct = line[correct_idx]\n        if rep not in values_predict:\n            values_predict[rep] = OrderedDict()\n            values_correct[rep] = OrderedDict()\n        if fold not in values_predict[rep]:\n            values_predict[rep][fold] = OrderedDict()\n            values_correct[rep][fold] = OrderedDict()\n        if samp not in values_predict[rep][fold]:\n            values_predict[rep][fold][samp] = []\n            values_correct[rep][fold][samp] = []\n\n        values_predict[rep][fold][samp].append(prediction)\n        values_correct[rep][fold][samp].append(correct)\n\n    scores = []\n    for rep in values_predict:\n        for fold in values_predict[rep]:\n            last_sample = len(values_predict[rep][fold]) - 1\n            y_pred = values_predict[rep][fold][last_sample]\n            y_true = values_correct[rep][fold][last_sample]\n            scores.append(sklearn_fn(y_true, y_pred, **kwargs))\n    return np.array(scores)\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.to_filesystem","title":"to_filesystem","text":"<pre><code>to_filesystem(directory: str | Path, store_model: bool = True) -&gt; None\n</code></pre> <p>The inverse of the from_filesystem method. Serializes a run on the filesystem, to be uploaded later.</p> PARAMETER DESCRIPTION <code>directory</code> <p>a path leading to the folder where the results will be stored. Should be empty</p> <p> TYPE: <code>str</code> </p> <code>store_model</code> <p>if True, a model will be pickled as well. As this is the most storage expensive part, it is often desirable to not store the model.</p> <p> TYPE: <code>(bool, optional(default=True))</code> DEFAULT: <code>True</code> </p> Source code in <code>openml/runs/run.py</code> <pre><code>def to_filesystem(\n    self,\n    directory: str | Path,\n    store_model: bool = True,  # noqa: FBT001, FBT002\n) -&gt; None:\n    \"\"\"\n    The inverse of the from_filesystem method. Serializes a run\n    on the filesystem, to be uploaded later.\n\n    Parameters\n    ----------\n    directory : str\n        a path leading to the folder where the results\n        will be stored. Should be empty\n\n    store_model : bool, optional (default=True)\n        if True, a model will be pickled as well. As this is the most\n        storage expensive part, it is often desirable to not store the\n        model.\n    \"\"\"\n    if self.data_content is None or self.model is None:\n        raise ValueError(\"Run should have been executed (and contain \" \"model / predictions)\")\n    directory = Path(directory)\n    directory.mkdir(exist_ok=True, parents=True)\n\n    if any(directory.iterdir()):\n        raise ValueError(f\"Output directory {directory.expanduser().resolve()} should be empty\")\n\n    run_xml = self._to_xml()\n    predictions_arff = arff.dumps(self._generate_arff_dict())\n\n    # It seems like typing does not allow to define the same variable multiple times\n    with (directory / \"description.xml\").open(\"w\") as fh:\n        fh.write(run_xml)\n    with (directory / \"predictions.arff\").open(\"w\") as fh:\n        fh.write(predictions_arff)\n    if store_model:\n        with (directory / \"model.pkl\").open(\"wb\") as fh_b:\n            pickle.dump(self.model, fh_b)\n\n    if self.flow_id is None and self.flow is not None:\n        self.flow.to_filesystem(directory)\n\n    if self.trace is not None:\n        self.trace._to_filesystem(directory)\n</code></pre>"},{"location":"reference/runs/run/#openml.runs.run.OpenMLRun.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/runs/trace/","title":"trace","text":""},{"location":"reference/runs/trace/#openml.runs.trace","title":"openml.runs.trace","text":""},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace","title":"OpenMLRunTrace","text":"<pre><code>OpenMLRunTrace(run_id: int | None, trace_iterations: dict[tuple[int, int, int], OpenMLTraceIteration])\n</code></pre> <p>OpenML Run Trace: parsed output from Run Trace call</p> PARAMETER DESCRIPTION <code>run_id</code> <p>OpenML run id.</p> <p> TYPE: <code>int</code> </p> <code>trace_iterations</code> <p>Mapping from key <code>(repeat, fold, iteration)</code> to an object of OpenMLTraceIteration.</p> <p> TYPE: <code>dict</code> </p> PARAMETER DESCRIPTION <code>run_id</code> <p>Id for which the trace content is to be stored.</p> <p> TYPE: <code>int</code> </p> <code>trace_iterations</code> <p>The trace content obtained by running a flow on a task.</p> <p> TYPE: <code>List[List]</code> </p> Source code in <code>openml/runs/trace.py</code> <pre><code>def __init__(\n    self,\n    run_id: int | None,\n    trace_iterations: dict[tuple[int, int, int], OpenMLTraceIteration],\n):\n    \"\"\"Object to hold the trace content of a run.\n\n    Parameters\n    ----------\n    run_id : int\n        Id for which the trace content is to be stored.\n    trace_iterations : List[List]\n        The trace content obtained by running a flow on a task.\n    \"\"\"\n    self.run_id = run_id\n    self.trace_iterations = trace_iterations\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.generate","title":"generate  <code>classmethod</code>","text":"<pre><code>generate(attributes: list[tuple[str, str]], content: list[list[int | float | str]]) -&gt; OpenMLRunTrace\n</code></pre> <p>Generates an OpenMLRunTrace.</p> <p>Generates the trace object from the attributes and content extracted while running the underlying flow.</p> PARAMETER DESCRIPTION <code>attributes</code> <p>List of tuples describing the arff attributes.</p> <p> TYPE: <code>list</code> </p> <code>content</code> <p>List of lists containing information about the individual tuning runs.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>OpenMLRunTrace</code> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef generate(\n    cls,\n    attributes: list[tuple[str, str]],\n    content: list[list[int | float | str]],\n) -&gt; OpenMLRunTrace:\n    \"\"\"Generates an OpenMLRunTrace.\n\n    Generates the trace object from the attributes and content extracted\n    while running the underlying flow.\n\n    Parameters\n    ----------\n    attributes : list\n        List of tuples describing the arff attributes.\n\n    content : list\n        List of lists containing information about the individual tuning\n        runs.\n\n    Returns\n    -------\n    OpenMLRunTrace\n    \"\"\"\n    if content is None:\n        raise ValueError(\"Trace content not available.\")\n    if attributes is None:\n        raise ValueError(\"Trace attributes not available.\")\n    if len(content) == 0:\n        raise ValueError(\"Trace content is empty.\")\n    if len(attributes) != len(content[0]):\n        raise ValueError(\n            \"Trace_attributes and trace_content not compatible:\"\n            f\" {attributes} vs {content[0]}\",\n        )\n\n    return cls._trace_from_arff_struct(\n        attributes=attributes,\n        content=content,\n        error_message=\"setup_string not allowed when constructing a \"\n        \"trace object from run results.\",\n    )\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.get_selected_iteration","title":"get_selected_iteration","text":"<pre><code>get_selected_iteration(fold: int, repeat: int) -&gt; int\n</code></pre> <p>Returns the trace iteration that was marked as selected. In case multiple are marked as selected (should not happen) the first of these is returned</p> PARAMETER DESCRIPTION <code>fold</code> <p> TYPE: <code>int</code> </p> <code>repeat</code> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>int</code> <p>The trace iteration from the given fold and repeat that was selected as the best iteration by the search procedure</p> Source code in <code>openml/runs/trace.py</code> <pre><code>def get_selected_iteration(self, fold: int, repeat: int) -&gt; int:\n    \"\"\"\n    Returns the trace iteration that was marked as selected. In\n    case multiple are marked as selected (should not happen) the\n    first of these is returned\n\n    Parameters\n    ----------\n    fold: int\n\n    repeat: int\n\n    Returns\n    -------\n    int\n        The trace iteration from the given fold and repeat that was\n        selected as the best iteration by the search procedure\n    \"\"\"\n    for r, f, i in self.trace_iterations:\n        if r == repeat and f == fold and self.trace_iterations[(r, f, i)].selected is True:\n            return i\n    raise ValueError(\n        \"Could not find the selected iteration for rep/fold %d/%d\" % (repeat, fold),\n    )\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.merge_traces","title":"merge_traces  <code>classmethod</code>","text":"<pre><code>merge_traces(traces: list[OpenMLRunTrace]) -&gt; OpenMLRunTrace\n</code></pre> <p>Merge multiple traces into a single trace.</p> PARAMETER DESCRIPTION <code>cls</code> <p>Type of the trace object to be created.</p> <p> TYPE: <code>type</code> </p> <code>traces</code> <p>List of traces to merge.</p> <p> TYPE: <code>List[OpenMLRunTrace]</code> </p> RETURNS DESCRIPTION <code>OpenMLRunTrace</code> <p>A trace object representing the merged traces.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the parameters in the iterations of the traces being merged are not equal. If a key (repeat, fold, iteration) is encountered twice while merging the traces.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef merge_traces(cls, traces: list[OpenMLRunTrace]) -&gt; OpenMLRunTrace:\n    \"\"\"Merge multiple traces into a single trace.\n\n    Parameters\n    ----------\n    cls : type\n        Type of the trace object to be created.\n    traces : List[OpenMLRunTrace]\n        List of traces to merge.\n\n    Returns\n    -------\n    OpenMLRunTrace\n        A trace object representing the merged traces.\n\n    Raises\n    ------\n    ValueError\n        If the parameters in the iterations of the traces being merged are not equal.\n        If a key (repeat, fold, iteration) is encountered twice while merging the traces.\n    \"\"\"\n    merged_trace: dict[tuple[int, int, int], OpenMLTraceIteration] = {}\n\n    previous_iteration = None\n    for trace in traces:\n        for iteration in trace:\n            key = (iteration.repeat, iteration.fold, iteration.iteration)\n\n            assert iteration.parameters is not None\n            param_keys = iteration.parameters.keys()\n\n            if previous_iteration is not None:\n                trace_itr = merged_trace[previous_iteration]\n\n                assert trace_itr.parameters is not None\n                trace_itr_keys = trace_itr.parameters.keys()\n\n                if list(param_keys) != list(trace_itr_keys):\n                    raise ValueError(\n                        \"Cannot merge traces because the parameters are not equal: \"\n                        f\"{list(trace_itr.parameters.keys())} vs \"\n                        f\"{list(iteration.parameters.keys())}\",\n                    )\n\n            if key in merged_trace:\n                raise ValueError(\n                    f\"Cannot merge traces because key '{key}' was encountered twice\",\n                )\n\n            merged_trace[key] = iteration\n            previous_iteration = key\n\n    return cls(None, merged_trace)\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.trace_from_arff","title":"trace_from_arff  <code>classmethod</code>","text":"<pre><code>trace_from_arff(arff_obj: dict[str, Any]) -&gt; OpenMLRunTrace\n</code></pre> <p>Generate trace from arff trace.</p> <p>Creates a trace file from arff object (for example, generated by a local run).</p> PARAMETER DESCRIPTION <code>arff_obj</code> <p>LIAC arff obj, dict containing attributes, relation, data.</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>OpenMLRunTrace</code> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef trace_from_arff(cls, arff_obj: dict[str, Any]) -&gt; OpenMLRunTrace:\n    \"\"\"Generate trace from arff trace.\n\n    Creates a trace file from arff object (for example, generated by a\n    local run).\n\n    Parameters\n    ----------\n    arff_obj : dict\n        LIAC arff obj, dict containing attributes, relation, data.\n\n    Returns\n    -------\n    OpenMLRunTrace\n    \"\"\"\n    attributes = arff_obj[\"attributes\"]\n    content = arff_obj[\"data\"]\n    return cls._trace_from_arff_struct(\n        attributes=attributes,\n        content=content,\n        error_message=\"setup_string not supported for arff serialization\",\n    )\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.trace_from_xml","title":"trace_from_xml  <code>classmethod</code>","text":"<pre><code>trace_from_xml(xml: str | Path | IO) -&gt; OpenMLRunTrace\n</code></pre> <p>Generate trace from xml.</p> <p>Creates a trace file from the xml description.</p> PARAMETER DESCRIPTION <code>xml</code> <p>An xml description that can be either a <code>string</code> or a file-like object.</p> <p> TYPE: <code>string | file-like object</code> </p> RETURNS DESCRIPTION <code>run</code> <p>Object containing the run id and a dict containing the trace iterations.</p> <p> TYPE: <code>OpenMLRunTrace</code> </p> Source code in <code>openml/runs/trace.py</code> <pre><code>@classmethod\ndef trace_from_xml(cls, xml: str | Path | IO) -&gt; OpenMLRunTrace:\n    \"\"\"Generate trace from xml.\n\n    Creates a trace file from the xml description.\n\n    Parameters\n    ----------\n    xml : string | file-like object\n        An xml description that can be either a `string` or a file-like\n        object.\n\n    Returns\n    -------\n    run : OpenMLRunTrace\n        Object containing the run id and a dict containing the trace\n        iterations.\n    \"\"\"\n    if isinstance(xml, Path):\n        xml = str(xml.absolute())\n\n    result_dict = xmltodict.parse(xml, force_list=(\"oml:trace_iteration\",))[\"oml:trace\"]\n\n    run_id = result_dict[\"oml:run_id\"]\n    trace = OrderedDict()\n\n    if \"oml:trace_iteration\" not in result_dict:\n        raise ValueError(\"Run does not contain valid trace. \")\n    if not isinstance(result_dict[\"oml:trace_iteration\"], list):\n        raise TypeError(type(result_dict[\"oml:trace_iteration\"]))\n\n    for itt in result_dict[\"oml:trace_iteration\"]:\n        repeat = int(itt[\"oml:repeat\"])\n        fold = int(itt[\"oml:fold\"])\n        iteration = int(itt[\"oml:iteration\"])\n        setup_string = json.loads(itt[\"oml:setup_string\"])\n        evaluation = float(itt[\"oml:evaluation\"])\n        selected_value = itt[\"oml:selected\"]\n        if selected_value == \"true\":\n            selected = True\n        elif selected_value == \"false\":\n            selected = False\n        else:\n            raise ValueError(\n                'expected {\"true\", \"false\"} value for '\n                f\"selected field, received: {selected_value}\",\n            )\n\n        current = OpenMLTraceIteration(\n            repeat=repeat,\n            fold=fold,\n            iteration=iteration,\n            setup_string=setup_string,\n            evaluation=evaluation,\n            selected=selected,\n        )\n        trace[(repeat, fold, iteration)] = current\n\n    return cls(run_id, trace)\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLRunTrace.trace_to_arff","title":"trace_to_arff","text":"<pre><code>trace_to_arff() -&gt; dict[str, Any]\n</code></pre> <p>Generate the arff dictionary for uploading predictions to the server.</p> <p>Uses the trace object to generate an arff dictionary representation.</p> RETURNS DESCRIPTION <code>arff_dict</code> <p>Dictionary representation of the ARFF file that will be uploaded. Contains information about the optimization trace.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>openml/runs/trace.py</code> <pre><code>def trace_to_arff(self) -&gt; dict[str, Any]:\n    \"\"\"Generate the arff dictionary for uploading predictions to the server.\n\n    Uses the trace object to generate an arff dictionary representation.\n\n    Returns\n    -------\n    arff_dict : dict\n        Dictionary representation of the ARFF file that will be uploaded.\n        Contains information about the optimization trace.\n    \"\"\"\n    if self.trace_iterations is None:\n        raise ValueError(\"trace_iterations missing from the trace object\")\n\n    # attributes that will be in trace arff\n    trace_attributes = [\n        (\"repeat\", \"NUMERIC\"),\n        (\"fold\", \"NUMERIC\"),\n        (\"iteration\", \"NUMERIC\"),\n        (\"evaluation\", \"NUMERIC\"),\n        (\"selected\", [\"true\", \"false\"]),\n    ]\n    trace_attributes.extend(\n        [\n            (PREFIX + parameter, \"STRING\")\n            for parameter in next(iter(self.trace_iterations.values())).get_parameters()\n        ],\n    )\n\n    arff_dict: dict[str, Any] = {}\n    data = []\n    for trace_iteration in self.trace_iterations.values():\n        tmp_list = []\n        for _attr, _ in trace_attributes:\n            if _attr.startswith(PREFIX):\n                attr = _attr[len(PREFIX) :]\n                value = trace_iteration.get_parameters()[attr]\n            else:\n                attr = _attr\n                value = getattr(trace_iteration, attr)\n\n            if attr == \"selected\":\n                tmp_list.append(\"true\" if value else \"false\")\n            else:\n                tmp_list.append(value)\n        data.append(tmp_list)\n\n    arff_dict[\"attributes\"] = trace_attributes\n    arff_dict[\"data\"] = data\n    # TODO allow to pass a trace description when running a flow\n    arff_dict[\"relation\"] = \"Trace\"\n    return arff_dict\n</code></pre>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLTraceIteration","title":"OpenMLTraceIteration  <code>dataclass</code>","text":"<pre><code>OpenMLTraceIteration(repeat: int, fold: int, iteration: int, evaluation: float, selected: bool, setup_string: dict[str, str] | None = None, parameters: dict[str, str | int | float] | None = None)\n</code></pre> <p>OpenML Trace Iteration: parsed output from Run Trace call Exactly one of <code>setup_string</code> or <code>parameters</code> must be provided.</p> PARAMETER DESCRIPTION <code>repeat</code> <p>repeat number (in case of no repeats: 0)</p> <p> TYPE: <code>int</code> </p> <code>fold</code> <p>fold number (in case of no folds: 0)</p> <p> TYPE: <code>int</code> </p> <code>iteration</code> <p>iteration number of optimization procedure</p> <p> TYPE: <code>int</code> </p> <code>setup_string</code> <p>json string representing the parameters If not provided, <code>parameters</code> should be set.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>evaluation</code> <p>The evaluation that was awarded to this trace iteration. Measure is defined by the task</p> <p> TYPE: <code>double</code> </p> <code>selected</code> <p>Whether this was the best of all iterations, and hence selected for making predictions. Per fold/repeat there should be only one iteration selected</p> <p> TYPE: <code>bool</code> </p> <code>parameters</code> <p>Dictionary specifying parameter names and their values. If not provided, <code>setup_string</code> should be set.</p> <p> TYPE: <code>OrderedDict</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/runs/trace/#openml.runs.trace.OpenMLTraceIteration.get_parameters","title":"get_parameters","text":"<pre><code>get_parameters() -&gt; dict[str, Any]\n</code></pre> <p>Get the parameters of this trace iteration.</p> Source code in <code>openml/runs/trace.py</code> <pre><code>def get_parameters(self) -&gt; dict[str, Any]:\n    \"\"\"Get the parameters of this trace iteration.\"\"\"\n    # parameters have prefix 'parameter_'\n    if self.setup_string:\n        return {\n            param[len(PREFIX) :]: json.loads(value)\n            for param, value in self.setup_string.items()\n        }\n\n    assert self.parameters is not None\n    return {param[len(PREFIX) :]: value for param, value in self.parameters.items()}\n</code></pre>"},{"location":"reference/setups/","title":"setups","text":""},{"location":"reference/setups/#openml.setups","title":"openml.setups","text":""},{"location":"reference/setups/#openml.setups.OpenMLParameter","title":"OpenMLParameter","text":"<pre><code>OpenMLParameter(input_id: int, flow_id: int, flow_name: str, full_name: str, parameter_name: str, data_type: str, default_value: str, value: str)\n</code></pre> <p>Parameter object (used in setup).</p> PARAMETER DESCRIPTION <code>input_id</code> <p>The input id from the openml database</p> <p> TYPE: <code>int</code> </p> <code>flow</code> <p>The flow to which this parameter is associated</p> <p> </p> <code>flow</code> <p>The name of the flow (no version number) to which this parameter is associated</p> <p> </p> <code>full_name</code> <p>The name of the flow and parameter combined</p> <p> TYPE: <code>str</code> </p> <code>parameter_name</code> <p>The name of the parameter</p> <p> TYPE: <code>str</code> </p> <code>data_type</code> <p>The datatype of the parameter. generally unused for sklearn flows</p> <p> TYPE: <code>str</code> </p> <code>default_value</code> <p>The default value. For sklearn parameters, this is unknown and a default value is selected arbitrarily</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>If the parameter was set, the value that it was set to.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/setups/setup.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    input_id: int,\n    flow_id: int,\n    flow_name: str,\n    full_name: str,\n    parameter_name: str,\n    data_type: str,\n    default_value: str,\n    value: str,\n):\n    self.id = input_id\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.full_name = full_name\n    self.parameter_name = parameter_name\n    self.data_type = data_type\n    self.default_value = default_value\n    self.value = value\n</code></pre>"},{"location":"reference/setups/#openml.setups.OpenMLSetup","title":"OpenMLSetup","text":"<pre><code>OpenMLSetup(setup_id: int, flow_id: int, parameters: dict[int, Any] | None)\n</code></pre> <p>Setup object (a.k.a. Configuration).</p> PARAMETER DESCRIPTION <code>setup_id</code> <p>The OpenML setup id</p> <p> TYPE: <code>int</code> </p> <code>flow_id</code> <p>The flow that it is build upon</p> <p> TYPE: <code>int</code> </p> <code>parameters</code> <p>The setting of the parameters</p> <p> TYPE: <code>dict</code> </p> Source code in <code>openml/setups/setup.py</code> <pre><code>def __init__(self, setup_id: int, flow_id: int, parameters: dict[int, Any] | None):\n    if not isinstance(setup_id, int):\n        raise ValueError(\"setup id should be int\")\n\n    if not isinstance(flow_id, int):\n        raise ValueError(\"flow id should be int\")\n\n    if parameters is not None and not isinstance(parameters, dict):\n        raise ValueError(\"parameters should be dict\")\n\n    self.setup_id = setup_id\n    self.flow_id = flow_id\n    self.parameters = parameters\n</code></pre>"},{"location":"reference/setups/#openml.setups.get_setup","title":"get_setup","text":"<pre><code>get_setup(setup_id: int) -&gt; OpenMLSetup\n</code></pre> <p>Downloads the setup (configuration) description from OpenML  and returns a structured object</p> PARAMETER DESCRIPTION <code>setup_id</code> <p>The Openml setup_id</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>OpenMLSetup (an initialized openml setup object)</code> Source code in <code>openml/setups/functions.py</code> <pre><code>def get_setup(setup_id: int) -&gt; OpenMLSetup:\n    \"\"\"\n     Downloads the setup (configuration) description from OpenML\n     and returns a structured object\n\n    Parameters\n    ----------\n    setup_id : int\n        The Openml setup_id\n\n    Returns\n    -------\n    OpenMLSetup (an initialized openml setup object)\n    \"\"\"\n    setup_dir = Path(config.get_cache_directory()) / \"setups\" / str(setup_id)\n    setup_dir.mkdir(exist_ok=True, parents=True)\n\n    setup_file = setup_dir / \"description.xml\"\n\n    try:\n        return _get_cached_setup(setup_id)\n    except openml.exceptions.OpenMLCacheException:\n        url_suffix = f\"/setup/{setup_id}\"\n        setup_xml = openml._api_calls._perform_api_call(url_suffix, \"get\")\n        with setup_file.open(\"w\", encoding=\"utf8\") as fh:\n            fh.write(setup_xml)\n\n    result_dict = xmltodict.parse(setup_xml)\n    return _create_setup_from_xml(result_dict)\n</code></pre>"},{"location":"reference/setups/#openml.setups.initialize_model","title":"initialize_model","text":"<pre><code>initialize_model(setup_id: int, *, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Initialized a model based on a setup_id (i.e., using the exact same parameter settings)</p> PARAMETER DESCRIPTION <code>setup_id</code> <p>The Openml setup_id</p> <p> TYPE: <code>int</code> </p> <code>strict_version</code> <p>See <code>flow_to_model</code> strict_version.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>model</code> Source code in <code>openml/setups/functions.py</code> <pre><code>def initialize_model(setup_id: int, *, strict_version: bool = True) -&gt; Any:\n    \"\"\"\n    Initialized a model based on a setup_id (i.e., using the exact\n    same parameter settings)\n\n    Parameters\n    ----------\n    setup_id : int\n        The Openml setup_id\n    strict_version: bool (default=True)\n        See `flow_to_model` strict_version.\n\n    Returns\n    -------\n    model\n    \"\"\"\n    setup = get_setup(setup_id)\n    flow = openml.flows.get_flow(setup.flow_id)\n\n    # instead of using scikit-learns or any other library's \"set_params\" function, we override the\n    # OpenMLFlow objects default parameter value so we can utilize the\n    # Extension.flow_to_model() function to reinitialize the flow with the set defaults.\n    if setup.parameters is not None:\n        for hyperparameter in setup.parameters.values():\n            structure = flow.get_structure(\"flow_id\")\n            if len(structure[hyperparameter.flow_id]) &gt; 0:\n                subflow = flow.get_subflow(structure[hyperparameter.flow_id])\n            else:\n                subflow = flow\n            subflow.parameters[hyperparameter.parameter_name] = hyperparameter.value\n\n    return flow.extension.flow_to_model(flow, strict_version=strict_version)\n</code></pre>"},{"location":"reference/setups/#openml.setups.list_setups","title":"list_setups","text":"<pre><code>list_setups(offset: int | None = None, size: int | None = None, flow: int | None = None, tag: str | None = None, setup: Iterable[int] | None = None, output_format: Literal['object', 'dataframe'] = 'object') -&gt; dict[int, OpenMLSetup] | DataFrame\n</code></pre> <p>List all setups matching all of the given filters.</p> PARAMETER DESCRIPTION <code>offset</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>flow</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>setup</code> <p> TYPE: <code>Iterable[int]</code> DEFAULT: <code>None</code> </p> <code>output_format</code> <p>The parameter decides the format of the output. - If 'dataframe' the output is a pandas DataFrame - If 'object' the output is a dictionary of OpenMLSetup objects</p> <p> TYPE: <code>Literal['object', 'dataframe']</code> DEFAULT: <code>'object'</code> </p> RETURNS DESCRIPTION <code>dict or dataframe</code> Source code in <code>openml/setups/functions.py</code> <pre><code>def list_setups(  # noqa: PLR0913\n    offset: int | None = None,\n    size: int | None = None,\n    flow: int | None = None,\n    tag: str | None = None,\n    setup: Iterable[int] | None = None,\n    output_format: Literal[\"object\", \"dataframe\"] = \"object\",\n) -&gt; dict[int, OpenMLSetup] | pd.DataFrame:\n    \"\"\"\n    List all setups matching all of the given filters.\n\n    Parameters\n    ----------\n    offset : int, optional\n    size : int, optional\n    flow : int, optional\n    tag : str, optional\n    setup : Iterable[int], optional\n    output_format: str, optional (default='object')\n        The parameter decides the format of the output.\n        - If 'dataframe' the output is a pandas DataFrame\n        - If 'object' the output is a dictionary of OpenMLSetup objects\n\n    Returns\n    -------\n    dict or dataframe\n    \"\"\"\n    if output_format not in [\"dataframe\", \"object\"]:\n        raise ValueError(\n            \"Invalid output format selected. Only 'object', or 'dataframe' applicable.\",\n        )\n\n    listing_call = partial(_list_setups, flow=flow, tag=tag, setup=setup)\n    batches = openml.utils._list_all(\n        listing_call,\n        batch_size=1_000,  # batch size for setups is lower\n        offset=offset,\n        limit=size,\n    )\n    flattened = list(chain.from_iterable(batches))\n    if output_format == \"object\":\n        return {setup.setup_id: setup for setup in flattened}\n\n    records = [setup._to_dict() for setup in flattened]\n    return pd.DataFrame.from_records(records, index=\"setup_id\")\n</code></pre>"},{"location":"reference/setups/#openml.setups.setup_exists","title":"setup_exists","text":"<pre><code>setup_exists(flow: OpenMLFlow) -&gt; int\n</code></pre> <p>Checks whether a hyperparameter configuration already exists on the server.</p> PARAMETER DESCRIPTION <code>flow</code> <p>The openml flow object. Should have flow id present for the main flow and all subflows (i.e., it should be downloaded from the server by means of flow.get, and not instantiated locally)</p> <p> TYPE: <code>OpenMLFlow</code> </p> RETURNS DESCRIPTION <code>setup_id</code> <p>setup id iff exists, False otherwise</p> <p> TYPE: <code>int</code> </p> Source code in <code>openml/setups/functions.py</code> <pre><code>def setup_exists(flow: OpenMLFlow) -&gt; int:\n    \"\"\"\n    Checks whether a hyperparameter configuration already exists on the server.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        The openml flow object. Should have flow id present for the main flow\n        and all subflows (i.e., it should be downloaded from the server by\n        means of flow.get, and not instantiated locally)\n\n    Returns\n    -------\n    setup_id : int\n        setup id iff exists, False otherwise\n    \"\"\"\n    # sadly, this api call relies on a run object\n    openml.flows.functions._check_flow_for_server_id(flow)\n    if flow.model is None:\n        raise ValueError(\"Flow should have model field set with the actual model.\")\n    if flow.extension is None:\n        raise ValueError(\"Flow should have model field set with the correct extension.\")\n\n    # checks whether the flow exists on the server and flow ids align\n    exists = flow_exists(flow.name, flow.external_version)\n    if exists != flow.flow_id:\n        raise ValueError(\n            f\"Local flow id ({flow.id}) differs from server id ({exists}). \"\n            \"If this issue persists, please contact the developers.\",\n        )\n\n    openml_param_settings = flow.extension.obtain_parameter_values(flow)\n    description = xmltodict.unparse(_to_dict(flow.flow_id, openml_param_settings), pretty=True)\n    file_elements = {\n        \"description\": (\"description.arff\", description),\n    }  # type: openml._api_calls.FILE_ELEMENTS_TYPE\n    result = openml._api_calls._perform_api_call(\n        \"/setup/exists/\",\n        \"post\",\n        file_elements=file_elements,\n    )\n    result_dict = xmltodict.parse(result)\n    setup_id = int(result_dict[\"oml:setup_exists\"][\"oml:id\"])\n    return setup_id if setup_id &gt; 0 else False\n</code></pre>"},{"location":"reference/setups/functions/","title":"functions","text":""},{"location":"reference/setups/functions/#openml.setups.functions","title":"openml.setups.functions","text":""},{"location":"reference/setups/functions/#openml.setups.functions.__list_setups","title":"__list_setups","text":"<pre><code>__list_setups(api_call: str) -&gt; list[OpenMLSetup]\n</code></pre> <p>Helper function to parse API calls which are lists of setups</p> Source code in <code>openml/setups/functions.py</code> <pre><code>def __list_setups(api_call: str) -&gt; list[OpenMLSetup]:\n    \"\"\"Helper function to parse API calls which are lists of setups\"\"\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    setups_dict = xmltodict.parse(xml_string, force_list=(\"oml:setup\",))\n    openml_uri = \"http://openml.org/openml\"\n    # Minimalistic check if the XML is useful\n    if \"oml:setups\" not in setups_dict:\n        raise ValueError(\n            f'Error in return XML, does not contain \"oml:setups\": {setups_dict!s}',\n        )\n\n    if \"@xmlns:oml\" not in setups_dict[\"oml:setups\"]:\n        raise ValueError(\n            f'Error in return XML, does not contain \"oml:setups\"/@xmlns:oml: {setups_dict!s}',\n        )\n\n    if setups_dict[\"oml:setups\"][\"@xmlns:oml\"] != openml_uri:\n        raise ValueError(\n            \"Error in return XML, value of  \"\n            '\"oml:seyups\"/@xmlns:oml is not '\n            f'\"{openml_uri}\": {setups_dict!s}',\n        )\n\n    assert isinstance(setups_dict[\"oml:setups\"][\"oml:setup\"], list), type(setups_dict[\"oml:setups\"])\n\n    return [\n        _create_setup_from_xml({\"oml:setup_parameters\": setup_})\n        for setup_ in setups_dict[\"oml:setups\"][\"oml:setup\"]\n    ]\n</code></pre>"},{"location":"reference/setups/functions/#openml.setups.functions.get_setup","title":"get_setup","text":"<pre><code>get_setup(setup_id: int) -&gt; OpenMLSetup\n</code></pre> <p>Downloads the setup (configuration) description from OpenML  and returns a structured object</p> PARAMETER DESCRIPTION <code>setup_id</code> <p>The Openml setup_id</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>OpenMLSetup (an initialized openml setup object)</code> Source code in <code>openml/setups/functions.py</code> <pre><code>def get_setup(setup_id: int) -&gt; OpenMLSetup:\n    \"\"\"\n     Downloads the setup (configuration) description from OpenML\n     and returns a structured object\n\n    Parameters\n    ----------\n    setup_id : int\n        The Openml setup_id\n\n    Returns\n    -------\n    OpenMLSetup (an initialized openml setup object)\n    \"\"\"\n    setup_dir = Path(config.get_cache_directory()) / \"setups\" / str(setup_id)\n    setup_dir.mkdir(exist_ok=True, parents=True)\n\n    setup_file = setup_dir / \"description.xml\"\n\n    try:\n        return _get_cached_setup(setup_id)\n    except openml.exceptions.OpenMLCacheException:\n        url_suffix = f\"/setup/{setup_id}\"\n        setup_xml = openml._api_calls._perform_api_call(url_suffix, \"get\")\n        with setup_file.open(\"w\", encoding=\"utf8\") as fh:\n            fh.write(setup_xml)\n\n    result_dict = xmltodict.parse(setup_xml)\n    return _create_setup_from_xml(result_dict)\n</code></pre>"},{"location":"reference/setups/functions/#openml.setups.functions.initialize_model","title":"initialize_model","text":"<pre><code>initialize_model(setup_id: int, *, strict_version: bool = True) -&gt; Any\n</code></pre> <p>Initialized a model based on a setup_id (i.e., using the exact same parameter settings)</p> PARAMETER DESCRIPTION <code>setup_id</code> <p>The Openml setup_id</p> <p> TYPE: <code>int</code> </p> <code>strict_version</code> <p>See <code>flow_to_model</code> strict_version.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>model</code> Source code in <code>openml/setups/functions.py</code> <pre><code>def initialize_model(setup_id: int, *, strict_version: bool = True) -&gt; Any:\n    \"\"\"\n    Initialized a model based on a setup_id (i.e., using the exact\n    same parameter settings)\n\n    Parameters\n    ----------\n    setup_id : int\n        The Openml setup_id\n    strict_version: bool (default=True)\n        See `flow_to_model` strict_version.\n\n    Returns\n    -------\n    model\n    \"\"\"\n    setup = get_setup(setup_id)\n    flow = openml.flows.get_flow(setup.flow_id)\n\n    # instead of using scikit-learns or any other library's \"set_params\" function, we override the\n    # OpenMLFlow objects default parameter value so we can utilize the\n    # Extension.flow_to_model() function to reinitialize the flow with the set defaults.\n    if setup.parameters is not None:\n        for hyperparameter in setup.parameters.values():\n            structure = flow.get_structure(\"flow_id\")\n            if len(structure[hyperparameter.flow_id]) &gt; 0:\n                subflow = flow.get_subflow(structure[hyperparameter.flow_id])\n            else:\n                subflow = flow\n            subflow.parameters[hyperparameter.parameter_name] = hyperparameter.value\n\n    return flow.extension.flow_to_model(flow, strict_version=strict_version)\n</code></pre>"},{"location":"reference/setups/functions/#openml.setups.functions.list_setups","title":"list_setups","text":"<pre><code>list_setups(offset: int | None = None, size: int | None = None, flow: int | None = None, tag: str | None = None, setup: Iterable[int] | None = None, output_format: Literal['object', 'dataframe'] = 'object') -&gt; dict[int, OpenMLSetup] | DataFrame\n</code></pre> <p>List all setups matching all of the given filters.</p> PARAMETER DESCRIPTION <code>offset</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>flow</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>setup</code> <p> TYPE: <code>Iterable[int]</code> DEFAULT: <code>None</code> </p> <code>output_format</code> <p>The parameter decides the format of the output. - If 'dataframe' the output is a pandas DataFrame - If 'object' the output is a dictionary of OpenMLSetup objects</p> <p> TYPE: <code>Literal['object', 'dataframe']</code> DEFAULT: <code>'object'</code> </p> RETURNS DESCRIPTION <code>dict or dataframe</code> Source code in <code>openml/setups/functions.py</code> <pre><code>def list_setups(  # noqa: PLR0913\n    offset: int | None = None,\n    size: int | None = None,\n    flow: int | None = None,\n    tag: str | None = None,\n    setup: Iterable[int] | None = None,\n    output_format: Literal[\"object\", \"dataframe\"] = \"object\",\n) -&gt; dict[int, OpenMLSetup] | pd.DataFrame:\n    \"\"\"\n    List all setups matching all of the given filters.\n\n    Parameters\n    ----------\n    offset : int, optional\n    size : int, optional\n    flow : int, optional\n    tag : str, optional\n    setup : Iterable[int], optional\n    output_format: str, optional (default='object')\n        The parameter decides the format of the output.\n        - If 'dataframe' the output is a pandas DataFrame\n        - If 'object' the output is a dictionary of OpenMLSetup objects\n\n    Returns\n    -------\n    dict or dataframe\n    \"\"\"\n    if output_format not in [\"dataframe\", \"object\"]:\n        raise ValueError(\n            \"Invalid output format selected. Only 'object', or 'dataframe' applicable.\",\n        )\n\n    listing_call = partial(_list_setups, flow=flow, tag=tag, setup=setup)\n    batches = openml.utils._list_all(\n        listing_call,\n        batch_size=1_000,  # batch size for setups is lower\n        offset=offset,\n        limit=size,\n    )\n    flattened = list(chain.from_iterable(batches))\n    if output_format == \"object\":\n        return {setup.setup_id: setup for setup in flattened}\n\n    records = [setup._to_dict() for setup in flattened]\n    return pd.DataFrame.from_records(records, index=\"setup_id\")\n</code></pre>"},{"location":"reference/setups/functions/#openml.setups.functions.setup_exists","title":"setup_exists","text":"<pre><code>setup_exists(flow: OpenMLFlow) -&gt; int\n</code></pre> <p>Checks whether a hyperparameter configuration already exists on the server.</p> PARAMETER DESCRIPTION <code>flow</code> <p>The openml flow object. Should have flow id present for the main flow and all subflows (i.e., it should be downloaded from the server by means of flow.get, and not instantiated locally)</p> <p> TYPE: <code>OpenMLFlow</code> </p> RETURNS DESCRIPTION <code>setup_id</code> <p>setup id iff exists, False otherwise</p> <p> TYPE: <code>int</code> </p> Source code in <code>openml/setups/functions.py</code> <pre><code>def setup_exists(flow: OpenMLFlow) -&gt; int:\n    \"\"\"\n    Checks whether a hyperparameter configuration already exists on the server.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        The openml flow object. Should have flow id present for the main flow\n        and all subflows (i.e., it should be downloaded from the server by\n        means of flow.get, and not instantiated locally)\n\n    Returns\n    -------\n    setup_id : int\n        setup id iff exists, False otherwise\n    \"\"\"\n    # sadly, this api call relies on a run object\n    openml.flows.functions._check_flow_for_server_id(flow)\n    if flow.model is None:\n        raise ValueError(\"Flow should have model field set with the actual model.\")\n    if flow.extension is None:\n        raise ValueError(\"Flow should have model field set with the correct extension.\")\n\n    # checks whether the flow exists on the server and flow ids align\n    exists = flow_exists(flow.name, flow.external_version)\n    if exists != flow.flow_id:\n        raise ValueError(\n            f\"Local flow id ({flow.id}) differs from server id ({exists}). \"\n            \"If this issue persists, please contact the developers.\",\n        )\n\n    openml_param_settings = flow.extension.obtain_parameter_values(flow)\n    description = xmltodict.unparse(_to_dict(flow.flow_id, openml_param_settings), pretty=True)\n    file_elements = {\n        \"description\": (\"description.arff\", description),\n    }  # type: openml._api_calls.FILE_ELEMENTS_TYPE\n    result = openml._api_calls._perform_api_call(\n        \"/setup/exists/\",\n        \"post\",\n        file_elements=file_elements,\n    )\n    result_dict = xmltodict.parse(result)\n    setup_id = int(result_dict[\"oml:setup_exists\"][\"oml:id\"])\n    return setup_id if setup_id &gt; 0 else False\n</code></pre>"},{"location":"reference/setups/setup/","title":"setup","text":""},{"location":"reference/setups/setup/#openml.setups.setup","title":"openml.setups.setup","text":""},{"location":"reference/setups/setup/#openml.setups.setup.OpenMLParameter","title":"OpenMLParameter","text":"<pre><code>OpenMLParameter(input_id: int, flow_id: int, flow_name: str, full_name: str, parameter_name: str, data_type: str, default_value: str, value: str)\n</code></pre> <p>Parameter object (used in setup).</p> PARAMETER DESCRIPTION <code>input_id</code> <p>The input id from the openml database</p> <p> TYPE: <code>int</code> </p> <code>flow</code> <p>The flow to which this parameter is associated</p> <p> </p> <code>flow</code> <p>The name of the flow (no version number) to which this parameter is associated</p> <p> </p> <code>full_name</code> <p>The name of the flow and parameter combined</p> <p> TYPE: <code>str</code> </p> <code>parameter_name</code> <p>The name of the parameter</p> <p> TYPE: <code>str</code> </p> <code>data_type</code> <p>The datatype of the parameter. generally unused for sklearn flows</p> <p> TYPE: <code>str</code> </p> <code>default_value</code> <p>The default value. For sklearn parameters, this is unknown and a default value is selected arbitrarily</p> <p> TYPE: <code>str</code> </p> <code>value</code> <p>If the parameter was set, the value that it was set to.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/setups/setup.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    input_id: int,\n    flow_id: int,\n    flow_name: str,\n    full_name: str,\n    parameter_name: str,\n    data_type: str,\n    default_value: str,\n    value: str,\n):\n    self.id = input_id\n    self.flow_id = flow_id\n    self.flow_name = flow_name\n    self.full_name = full_name\n    self.parameter_name = parameter_name\n    self.data_type = data_type\n    self.default_value = default_value\n    self.value = value\n</code></pre>"},{"location":"reference/setups/setup/#openml.setups.setup.OpenMLSetup","title":"OpenMLSetup","text":"<pre><code>OpenMLSetup(setup_id: int, flow_id: int, parameters: dict[int, Any] | None)\n</code></pre> <p>Setup object (a.k.a. Configuration).</p> PARAMETER DESCRIPTION <code>setup_id</code> <p>The OpenML setup id</p> <p> TYPE: <code>int</code> </p> <code>flow_id</code> <p>The flow that it is build upon</p> <p> TYPE: <code>int</code> </p> <code>parameters</code> <p>The setting of the parameters</p> <p> TYPE: <code>dict</code> </p> Source code in <code>openml/setups/setup.py</code> <pre><code>def __init__(self, setup_id: int, flow_id: int, parameters: dict[int, Any] | None):\n    if not isinstance(setup_id, int):\n        raise ValueError(\"setup id should be int\")\n\n    if not isinstance(flow_id, int):\n        raise ValueError(\"flow id should be int\")\n\n    if parameters is not None and not isinstance(parameters, dict):\n        raise ValueError(\"parameters should be dict\")\n\n    self.setup_id = setup_id\n    self.flow_id = flow_id\n    self.parameters = parameters\n</code></pre>"},{"location":"reference/study/","title":"study","text":""},{"location":"reference/study/#openml.study","title":"openml.study","text":""},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite","title":"OpenMLBenchmarkSuite","text":"<pre><code>OpenMLBenchmarkSuite(suite_id: int | None, alias: str | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None)\n</code></pre> <p>               Bases: <code>BaseStudy</code></p> <p>An OpenMLBenchmarkSuite represents the OpenML concept of a suite (a collection of tasks).</p> <p>It contains the following information: name, id, description, creation date, creator id and the task ids.</p> <p>According to this list of task ids, the suite object receives a list of OpenML object ids (datasets).</p> PARAMETER DESCRIPTION <code>suite_id</code> <p>the study id</p> <p> TYPE: <code>int</code> </p> <code>alias</code> <p>a string ID, unique on server (url-friendly)</p> <p> TYPE: <code>str(optional)</code> </p> <code>main_entity_type</code> <p>the entity type (e.g., task, run) that is core in this study. only entities of this type can be added explicitly</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>the name of the study (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>brief description (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>Whether the study is in preparation, active or deactivated</p> <p> TYPE: <code>str</code> </p> <code>creation_date</code> <p>date of creation (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>creator</code> <p>openml user id of the owner / creator</p> <p> TYPE: <code>int</code> </p> <code>tags</code> <p>The list of tags shows which tags are associated with the study. Each tag is a dict of (tag) name, window_start and write_access.</p> <p> TYPE: <code>list(dict)</code> </p> <code>data</code> <p>a list of data ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>tasks</code> <p>a list of task ids associated with this study</p> <p> TYPE: <code>list</code> </p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    suite_id: int | None,\n    alias: str | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n):\n    super().__init__(\n        study_id=suite_id,\n        alias=alias,\n        main_entity_type=\"task\",\n        benchmark_suite=None,\n        name=name,\n        description=description,\n        status=status,\n        creation_date=creation_date,\n        creator=creator,\n        tags=tags,\n        data=data,\n        tasks=tasks,\n        flows=None,\n        runs=None,\n        setups=None,\n    )\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLBenchmarkSuite.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLStudy","title":"OpenMLStudy","text":"<pre><code>OpenMLStudy(study_id: int | None, alias: str | None, benchmark_suite: int | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None, flows: list[int] | None, runs: list[int] | None, setups: list[int] | None)\n</code></pre> <p>               Bases: <code>BaseStudy</code></p> <p>An OpenMLStudy represents the OpenML concept of a study (a collection of runs).</p> <p>It contains the following information: name, id, description, creation date, creator id and a list of run ids.</p> <p>According to this list of run ids, the study object receives a list of OpenML object ids (datasets, flows, tasks and setups).</p> PARAMETER DESCRIPTION <code>study_id</code> <p>the study id</p> <p> TYPE: <code>int</code> </p> <code>alias</code> <p>a string ID, unique on server (url-friendly)</p> <p> TYPE: <code>str(optional)</code> </p> <code>benchmark_suite</code> <p>the benchmark suite (another study) upon which this study is ran. can only be active if main entity type is runs.</p> <p> TYPE: <code>int(optional)</code> </p> <code>name</code> <p>the name of the study (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>brief description (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>Whether the study is in preparation, active or deactivated</p> <p> TYPE: <code>str</code> </p> <code>creation_date</code> <p>date of creation (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>creator</code> <p>openml user id of the owner / creator</p> <p> TYPE: <code>int</code> </p> <code>tags</code> <p>The list of tags shows which tags are associated with the study. Each tag is a dict of (tag) name, window_start and write_access.</p> <p> TYPE: <code>list(dict)</code> </p> <code>data</code> <p>a list of data ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>tasks</code> <p>a list of task ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>flows</code> <p>a list of flow ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>runs</code> <p>a list of run ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>setups</code> <p>a list of setup ids associated with this study</p> <p> TYPE: <code>list</code> </p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    study_id: int | None,\n    alias: str | None,\n    benchmark_suite: int | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n    flows: list[int] | None,\n    runs: list[int] | None,\n    setups: list[int] | None,\n):\n    super().__init__(\n        study_id=study_id,\n        alias=alias,\n        main_entity_type=\"run\",\n        benchmark_suite=benchmark_suite,\n        name=name,\n        description=description,\n        status=status,\n        creation_date=creation_date,\n        creator=creator,\n        tags=tags,\n        data=data,\n        tasks=tasks,\n        flows=flows,\n        runs=runs,\n        setups=setups,\n    )\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLStudy.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/study/#openml.study.OpenMLStudy.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/study/#openml.study.OpenMLStudy.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLStudy.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLStudy.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLStudy.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/#openml.study.OpenMLStudy.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/study/#openml.study.attach_to_study","title":"attach_to_study","text":"<pre><code>attach_to_study(study_id: int, run_ids: list[int]) -&gt; int\n</code></pre> <p>Attaches a set of runs to a study.</p> PARAMETER DESCRIPTION <code>study_id</code> <p>OpenML id of the study</p> <p> TYPE: <code>int</code> </p> <code>run_ids</code> <p>List of entities to link to the collection</p> <p> TYPE: <code>list(int)</code> </p> RETURNS DESCRIPTION <code>int</code> <p>new size of the study (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def attach_to_study(study_id: int, run_ids: list[int]) -&gt; int:\n    \"\"\"Attaches a set of runs to a study.\n\n    Parameters\n    ----------\n    study_id : int\n        OpenML id of the study\n\n    run_ids : list (int)\n        List of entities to link to the collection\n\n    Returns\n    -------\n    int\n        new size of the study (in terms of explicitly linked entities)\n    \"\"\"\n    # Interestingly, there's no need to tell the server about the entity type, it knows by itself\n    result_xml = openml._api_calls._perform_api_call(\n        call=f\"study/{study_id}/attach\",\n        request_method=\"post\",\n        data={\"ids\": \",\".join(str(x) for x in run_ids)},\n    )\n    result = xmltodict.parse(result_xml)[\"oml:study_attach\"]\n    return int(result[\"oml:linked_entities\"])\n</code></pre>"},{"location":"reference/study/#openml.study.attach_to_suite","title":"attach_to_suite","text":"<pre><code>attach_to_suite(suite_id: int, task_ids: list[int]) -&gt; int\n</code></pre> <p>Attaches a set of tasks to a benchmarking suite.</p> PARAMETER DESCRIPTION <code>suite_id</code> <p>OpenML id of the study</p> <p> TYPE: <code>int</code> </p> <code>task_ids</code> <p>List of entities to link to the collection</p> <p> TYPE: <code>list(int)</code> </p> RETURNS DESCRIPTION <code>int</code> <p>new size of the suite (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def attach_to_suite(suite_id: int, task_ids: list[int]) -&gt; int:\n    \"\"\"Attaches a set of tasks to a benchmarking suite.\n\n    Parameters\n    ----------\n    suite_id : int\n        OpenML id of the study\n\n    task_ids : list (int)\n        List of entities to link to the collection\n\n    Returns\n    -------\n    int\n        new size of the suite (in terms of explicitly linked entities)\n    \"\"\"\n    return attach_to_study(suite_id, task_ids)\n</code></pre>"},{"location":"reference/study/#openml.study.create_benchmark_suite","title":"create_benchmark_suite","text":"<pre><code>create_benchmark_suite(name: str, description: str, task_ids: list[int], alias: str | None = None) -&gt; OpenMLBenchmarkSuite\n</code></pre> <p>Creates an OpenML benchmark suite (collection of entity types, where the tasks are the linked entity)</p> PARAMETER DESCRIPTION <code>name</code> <p>the name of the study (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>brief description (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>task_ids</code> <p>a list of task ids associated with this study more can be added later with <code>attach_to_suite</code>.</p> <p> TYPE: <code>list</code> </p> <code>alias</code> <p>a string ID, unique on server (url-friendly)</p> <p> TYPE: <code>str(optional)</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>OpenMLStudy</code> <p>A local OpenML study object (call publish method to upload to server)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def create_benchmark_suite(\n    name: str,\n    description: str,\n    task_ids: list[int],\n    alias: str | None = None,\n) -&gt; OpenMLBenchmarkSuite:\n    \"\"\"\n    Creates an OpenML benchmark suite (collection of entity types, where\n    the tasks are the linked entity)\n\n    Parameters\n    ----------\n    name : str\n        the name of the study (meta-info)\n    description : str\n        brief description (meta-info)\n    task_ids : list\n        a list of task ids associated with this study\n        more can be added later with ``attach_to_suite``.\n    alias : str (optional)\n        a string ID, unique on server (url-friendly)\n\n    Returns\n    -------\n    OpenMLStudy\n        A local OpenML study object (call publish method to upload to server)\n    \"\"\"\n    return OpenMLBenchmarkSuite(\n        suite_id=None,\n        alias=alias,\n        name=name,\n        description=description,\n        status=None,\n        creation_date=None,\n        creator=None,\n        tags=None,\n        data=None,\n        tasks=task_ids,\n    )\n</code></pre>"},{"location":"reference/study/#openml.study.create_study","title":"create_study","text":"<pre><code>create_study(name: str, description: str, run_ids: list[int] | None = None, alias: str | None = None, benchmark_suite: int | None = None) -&gt; OpenMLStudy\n</code></pre> <p>Creates an OpenML study (collection of data, tasks, flows, setups and run), where the runs are the main entity (collection consists of runs and all entities (flows, tasks, etc) that are related to these runs)</p> PARAMETER DESCRIPTION <code>benchmark_suite</code> <p>the benchmark suite (another study) upon which this study is ran.</p> <p> TYPE: <code>int(optional)</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>the name of the study (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>brief description (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>run_ids</code> <p>a list of run ids associated with this study, these can also be added later with <code>attach_to_study</code>.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>alias</code> <p>a string ID, unique on server (url-friendly)</p> <p> TYPE: <code>str(optional)</code> DEFAULT: <code>None</code> </p> <code>benchmark_suite</code> <p>the ID of the suite for which this study contains run results</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>OpenMLStudy</code> <p>A local OpenML study object (call publish method to upload to server)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def create_study(\n    name: str,\n    description: str,\n    run_ids: list[int] | None = None,\n    alias: str | None = None,\n    benchmark_suite: int | None = None,\n) -&gt; OpenMLStudy:\n    \"\"\"\n    Creates an OpenML study (collection of data, tasks, flows, setups and run),\n    where the runs are the main entity (collection consists of runs and all\n    entities (flows, tasks, etc) that are related to these runs)\n\n    Parameters\n    ----------\n    benchmark_suite : int (optional)\n        the benchmark suite (another study) upon which this study is ran.\n    name : str\n        the name of the study (meta-info)\n    description : str\n        brief description (meta-info)\n    run_ids : list, optional\n        a list of run ids associated with this study,\n        these can also be added later with ``attach_to_study``.\n    alias : str (optional)\n        a string ID, unique on server (url-friendly)\n    benchmark_suite: int (optional)\n        the ID of the suite for which this study contains run results\n\n    Returns\n    -------\n    OpenMLStudy\n        A local OpenML study object (call publish method to upload to server)\n    \"\"\"\n    return OpenMLStudy(\n        study_id=None,\n        alias=alias,\n        benchmark_suite=benchmark_suite,\n        name=name,\n        description=description,\n        status=None,\n        creation_date=None,\n        creator=None,\n        tags=None,\n        data=None,\n        tasks=None,\n        flows=None,\n        runs=run_ids if run_ids != [] else None,\n        setups=None,\n    )\n</code></pre>"},{"location":"reference/study/#openml.study.delete_study","title":"delete_study","text":"<pre><code>delete_study(study_id: int) -&gt; bool\n</code></pre> <p>Deletes a study from the OpenML server.</p> PARAMETER DESCRIPTION <code>study_id</code> <p>OpenML id of the study</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True iff the deletion was successful. False otherwise</p> Source code in <code>openml/study/functions.py</code> <pre><code>def delete_study(study_id: int) -&gt; bool:\n    \"\"\"Deletes a study from the OpenML server.\n\n    Parameters\n    ----------\n    study_id : int\n        OpenML id of the study\n\n    Returns\n    -------\n    bool\n        True iff the deletion was successful. False otherwise\n    \"\"\"\n    return openml.utils._delete_entity(\"study\", study_id)\n</code></pre>"},{"location":"reference/study/#openml.study.delete_suite","title":"delete_suite","text":"<pre><code>delete_suite(suite_id: int) -&gt; bool\n</code></pre> <p>Deletes a study from the OpenML server.</p> PARAMETER DESCRIPTION <code>suite_id</code> <p>OpenML id of the study</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True iff the deletion was successful. False otherwise</p> Source code in <code>openml/study/functions.py</code> <pre><code>def delete_suite(suite_id: int) -&gt; bool:\n    \"\"\"Deletes a study from the OpenML server.\n\n    Parameters\n    ----------\n    suite_id : int\n        OpenML id of the study\n\n    Returns\n    -------\n    bool\n        True iff the deletion was successful. False otherwise\n    \"\"\"\n    return delete_study(suite_id)\n</code></pre>"},{"location":"reference/study/#openml.study.detach_from_study","title":"detach_from_study","text":"<pre><code>detach_from_study(study_id: int, run_ids: list[int]) -&gt; int\n</code></pre> <p>Detaches a set of run ids from a study.</p> PARAMETER DESCRIPTION <code>study_id</code> <p>OpenML id of the study</p> <p> TYPE: <code>int</code> </p> <code>run_ids</code> <p>List of entities to unlink from the collection</p> <p> TYPE: <code>list(int)</code> </p> RETURNS DESCRIPTION <code>int</code> <p>new size of the study (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def detach_from_study(study_id: int, run_ids: list[int]) -&gt; int:\n    \"\"\"Detaches a set of run ids from a study.\n\n    Parameters\n    ----------\n    study_id : int\n        OpenML id of the study\n\n    run_ids : list (int)\n        List of entities to unlink from the collection\n\n    Returns\n    -------\n    int\n        new size of the study (in terms of explicitly linked entities)\n    \"\"\"\n    # Interestingly, there's no need to tell the server about the entity type, it knows by itself\n    uri = \"study/%d/detach\" % study_id\n    post_variables = {\"ids\": \",\".join(str(x) for x in run_ids)}  # type: openml._api_calls.DATA_TYPE\n    result_xml = openml._api_calls._perform_api_call(\n        call=uri,\n        request_method=\"post\",\n        data=post_variables,\n    )\n    result = xmltodict.parse(result_xml)[\"oml:study_detach\"]\n    return int(result[\"oml:linked_entities\"])\n</code></pre>"},{"location":"reference/study/#openml.study.detach_from_suite","title":"detach_from_suite","text":"<pre><code>detach_from_suite(suite_id: int, task_ids: list[int]) -&gt; int\n</code></pre> <p>Detaches a set of task ids from a suite.</p> PARAMETER DESCRIPTION <code>suite_id</code> <p>OpenML id of the study</p> <p> TYPE: <code>int</code> </p> <code>task_ids</code> <p>List of entities to unlink from the collection</p> <p> TYPE: <code>list(int)</code> </p> RETURNS DESCRIPTION <code>int</code> <code>new size of the study (in terms of explicitly linked entities)</code> Source code in <code>openml/study/functions.py</code> <pre><code>def detach_from_suite(suite_id: int, task_ids: list[int]) -&gt; int:\n    \"\"\"Detaches a set of task ids from a suite.\n\n    Parameters\n    ----------\n    suite_id : int\n        OpenML id of the study\n\n    task_ids : list (int)\n        List of entities to unlink from the collection\n\n    Returns\n    -------\n    int\n    new size of the study (in terms of explicitly linked entities)\n    \"\"\"\n    return detach_from_study(suite_id, task_ids)\n</code></pre>"},{"location":"reference/study/#openml.study.get_study","title":"get_study","text":"<pre><code>get_study(study_id: int | str, arg_for_backwards_compat: str | None = None) -&gt; OpenMLStudy\n</code></pre> <p>Retrieves all relevant information of an OpenML study from the server.</p> PARAMETER DESCRIPTION <code>study</code> <p>study id (numeric or alias)</p> <p> </p> <code>arg_for_backwards_compat</code> <p>The example given in arxiv.org/pdf/1708.03731.pdf uses an older version of the API which required specifying the type of study, i.e. tasks. We changed the implementation of studies since then and split them up into suites (collections of tasks) and studies (collections of runs) so this argument is no longer needed.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>OpenMLStudy</code> <p>The OpenML study object</p> Source code in <code>openml/study/functions.py</code> <pre><code>def get_study(\n    study_id: int | str,\n    arg_for_backwards_compat: str | None = None,  # noqa: ARG001\n) -&gt; OpenMLStudy:  # F401\n    \"\"\"\n    Retrieves all relevant information of an OpenML study from the server.\n\n    Parameters\n    ----------\n    study id : int, str\n        study id (numeric or alias)\n\n    arg_for_backwards_compat : str, optional\n        The example given in https://arxiv.org/pdf/1708.03731.pdf uses an older version of the\n        API which required specifying the type of study, i.e. tasks. We changed the\n        implementation of studies since then and split them up into suites (collections of tasks)\n        and studies (collections of runs) so this argument is no longer needed.\n\n    Returns\n    -------\n    OpenMLStudy\n        The OpenML study object\n    \"\"\"\n    if study_id == \"OpenML100\":\n        message = (\n            \"It looks like you are running code from the OpenML100 paper. It still works, but lots \"\n            \"of things have changed since then. Please use `get_suite('OpenML100')` instead.\"\n        )\n        warnings.warn(message, DeprecationWarning, stacklevel=2)\n        openml.config.logger.warning(message)\n        study = _get_study(study_id, entity_type=\"task\")\n        assert isinstance(study, OpenMLBenchmarkSuite)\n\n        return study  # type: ignore\n\n    study = _get_study(study_id, entity_type=\"run\")\n    assert isinstance(study, OpenMLStudy)\n    return study\n</code></pre>"},{"location":"reference/study/#openml.study.get_suite","title":"get_suite","text":"<pre><code>get_suite(suite_id: int | str) -&gt; OpenMLBenchmarkSuite\n</code></pre> <p>Retrieves all relevant information of an OpenML benchmarking suite from the server.</p> PARAMETER DESCRIPTION <code>study</code> <p>study id (numeric or alias)</p> <p> </p> RETURNS DESCRIPTION <code>OpenMLSuite</code> <p>The OpenML suite object</p> Source code in <code>openml/study/functions.py</code> <pre><code>def get_suite(suite_id: int | str) -&gt; OpenMLBenchmarkSuite:\n    \"\"\"\n    Retrieves all relevant information of an OpenML benchmarking suite from the server.\n\n    Parameters\n    ----------\n    study id : int, str\n        study id (numeric or alias)\n\n    Returns\n    -------\n    OpenMLSuite\n        The OpenML suite object\n    \"\"\"\n    study = _get_study(suite_id, entity_type=\"task\")\n    assert isinstance(study, OpenMLBenchmarkSuite)\n\n    return study\n</code></pre>"},{"location":"reference/study/#openml.study.list_studies","title":"list_studies","text":"<pre><code>list_studies(offset: int | None = None, size: int | None = None, status: str | None = None, uploader: list[str] | None = None, benchmark_suite: int | None = None) -&gt; DataFrame\n</code></pre> <p>Return a list of all studies which are on OpenML.</p> PARAMETER DESCRIPTION <code>offset</code> <p>The number of studies to skip, starting from the first.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>The maximum number of studies to show.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>status</code> <p>Should be {active, in_preparation, deactivated, all}. By default active studies are returned.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>uploader</code> <p>Result filter. Will only return studies created by these users.</p> <p> TYPE: <code>list(int)</code> DEFAULT: <code>None</code> </p> <code>benchmark_suite</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>datasets</code> <p>Every dataset is represented by a dictionary containing the following information: - id - alias (optional) - name - benchmark_suite (optional) - status - creator - creation_date If qualities are calculated for the dataset, some of these are also returned.</p> <p> TYPE: <code>dataframe</code> </p> Source code in <code>openml/study/functions.py</code> <pre><code>def list_studies(\n    offset: int | None = None,\n    size: int | None = None,\n    status: str | None = None,\n    uploader: list[str] | None = None,\n    benchmark_suite: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a list of all studies which are on OpenML.\n\n    Parameters\n    ----------\n    offset : int, optional\n        The number of studies to skip, starting from the first.\n    size : int, optional\n        The maximum number of studies to show.\n    status : str, optional\n        Should be {active, in_preparation, deactivated, all}. By default active\n        studies are returned.\n    uploader : list (int), optional\n        Result filter. Will only return studies created by these users.\n    benchmark_suite : int, optional\n\n    Returns\n    -------\n    datasets : dataframe\n        Every dataset is represented by a dictionary containing\n        the following information:\n        - id\n        - alias (optional)\n        - name\n        - benchmark_suite (optional)\n        - status\n        - creator\n        - creation_date\n        If qualities are calculated for the dataset, some of\n        these are also returned.\n    \"\"\"\n    listing_call = partial(\n        _list_studies,\n        main_entity_type=\"run\",\n        status=status,\n        uploader=uploader,\n        benchmark_suite=benchmark_suite,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/study/#openml.study.list_suites","title":"list_suites","text":"<pre><code>list_suites(offset: int | None = None, size: int | None = None, status: str | None = None, uploader: list[int] | None = None) -&gt; DataFrame\n</code></pre> <p>Return a list of all suites which are on OpenML.</p> PARAMETER DESCRIPTION <code>offset</code> <p>The number of suites to skip, starting from the first.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>The maximum number of suites to show.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>status</code> <p>Should be {active, in_preparation, deactivated, all}. By default active suites are returned.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>uploader</code> <p>Result filter. Will only return suites created by these users.</p> <p> TYPE: <code>list(int)</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>datasets</code> <p>Every row is represented by a dictionary containing the following information: - id - alias (optional) - name - main_entity_type - status - creator - creation_date</p> <p> TYPE: <code>dataframe</code> </p> Source code in <code>openml/study/functions.py</code> <pre><code>def list_suites(\n    offset: int | None = None,\n    size: int | None = None,\n    status: str | None = None,\n    uploader: list[int] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a list of all suites which are on OpenML.\n\n    Parameters\n    ----------\n    offset : int, optional\n        The number of suites to skip, starting from the first.\n    size : int, optional\n        The maximum number of suites to show.\n    status : str, optional\n        Should be {active, in_preparation, deactivated, all}. By default active\n        suites are returned.\n    uploader : list (int), optional\n        Result filter. Will only return suites created by these users.\n\n    Returns\n    -------\n    datasets : dataframe\n        Every row is represented by a dictionary containing the following information:\n        - id\n        - alias (optional)\n        - name\n        - main_entity_type\n        - status\n        - creator\n        - creation_date\n    \"\"\"\n    listing_call = partial(\n        _list_studies,\n        main_entity_type=\"task\",\n        status=status,\n        uploader=uploader,\n    )\n    batches = openml.utils._list_all(listing_call, limit=size, offset=offset)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/study/#openml.study.update_study_status","title":"update_study_status","text":"<pre><code>update_study_status(study_id: int, status: str) -&gt; None\n</code></pre> <p>Updates the status of a study to either 'active' or 'deactivated'.</p> PARAMETER DESCRIPTION <code>study_id</code> <p>The data id of the dataset</p> <p> TYPE: <code>int</code> </p> <code>status</code> <p>'active' or 'deactivated'</p> <p> TYPE: <code>(str)</code> </p> Source code in <code>openml/study/functions.py</code> <pre><code>def update_study_status(study_id: int, status: str) -&gt; None:\n    \"\"\"\n    Updates the status of a study to either 'active' or 'deactivated'.\n\n    Parameters\n    ----------\n    study_id : int\n        The data id of the dataset\n    status : str,\n        'active' or 'deactivated'\n    \"\"\"\n    legal_status = {\"active\", \"deactivated\"}\n    if status not in legal_status:\n        raise ValueError(f\"Illegal status value. Legal values: {legal_status}\")\n    data = {\"study_id\": study_id, \"status\": status}  # type: openml._api_calls.DATA_TYPE\n    result_xml = openml._api_calls._perform_api_call(\"study/status/update\", \"post\", data=data)\n    result = xmltodict.parse(result_xml)\n    server_study_id = result[\"oml:study_status_update\"][\"oml:id\"]\n    server_status = result[\"oml:study_status_update\"][\"oml:status\"]\n    if status != server_status or int(study_id) != int(server_study_id):\n        # This should never happen\n        raise ValueError(\"Study id/status does not collide\")\n</code></pre>"},{"location":"reference/study/#openml.study.update_suite_status","title":"update_suite_status","text":"<pre><code>update_suite_status(suite_id: int, status: str) -&gt; None\n</code></pre> <p>Updates the status of a study to either 'active' or 'deactivated'.</p> PARAMETER DESCRIPTION <code>suite_id</code> <p>The data id of the dataset</p> <p> TYPE: <code>int</code> </p> <code>status</code> <p>'active' or 'deactivated'</p> <p> TYPE: <code>(str)</code> </p> Source code in <code>openml/study/functions.py</code> <pre><code>def update_suite_status(suite_id: int, status: str) -&gt; None:\n    \"\"\"\n    Updates the status of a study to either 'active' or 'deactivated'.\n\n    Parameters\n    ----------\n    suite_id : int\n        The data id of the dataset\n    status : str,\n        'active' or 'deactivated'\n    \"\"\"\n    return update_study_status(suite_id, status)\n</code></pre>"},{"location":"reference/study/functions/","title":"functions","text":""},{"location":"reference/study/functions/#openml.study.functions","title":"openml.study.functions","text":""},{"location":"reference/study/functions/#openml.study.functions.__list_studies","title":"__list_studies","text":"<pre><code>__list_studies(api_call: str) -&gt; DataFrame\n</code></pre> <p>Retrieves the list of OpenML studies and returns it in a dictionary or a Pandas DataFrame.</p> PARAMETER DESCRIPTION <code>api_call</code> <p>The API call for retrieving the list of OpenML studies.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A Pandas DataFrame of OpenML studies</p> Source code in <code>openml/study/functions.py</code> <pre><code>def __list_studies(api_call: str) -&gt; pd.DataFrame:\n    \"\"\"Retrieves the list of OpenML studies and\n    returns it in a dictionary or a Pandas DataFrame.\n\n    Parameters\n    ----------\n    api_call : str\n        The API call for retrieving the list of OpenML studies.\n\n    Returns\n    -------\n    pd.DataFrame\n        A Pandas DataFrame of OpenML studies\n    \"\"\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    study_dict = xmltodict.parse(xml_string, force_list=(\"oml:study\",))\n\n    # Minimalistic check if the XML is useful\n    assert isinstance(study_dict[\"oml:study_list\"][\"oml:study\"], list), type(\n        study_dict[\"oml:study_list\"],\n    )\n    assert study_dict[\"oml:study_list\"][\"@xmlns:oml\"] == \"http://openml.org/openml\", study_dict[\n        \"oml:study_list\"\n    ][\"@xmlns:oml\"]\n\n    studies = {}\n    for study_ in study_dict[\"oml:study_list\"][\"oml:study\"]:\n        # maps from xml name to a tuple of (dict name, casting fn)\n        expected_fields = {\n            \"oml:id\": (\"id\", int),\n            \"oml:alias\": (\"alias\", str),\n            \"oml:main_entity_type\": (\"main_entity_type\", str),\n            \"oml:benchmark_suite\": (\"benchmark_suite\", int),\n            \"oml:name\": (\"name\", str),\n            \"oml:status\": (\"status\", str),\n            \"oml:creation_date\": (\"creation_date\", str),\n            \"oml:creator\": (\"creator\", int),\n        }\n        study_id = int(study_[\"oml:id\"])\n        current_study = {}\n        for oml_field_name, (real_field_name, cast_fn) in expected_fields.items():\n            if oml_field_name in study_:\n                current_study[real_field_name] = cast_fn(study_[oml_field_name])\n        current_study[\"id\"] = int(current_study[\"id\"])\n        studies[study_id] = current_study\n\n    return pd.DataFrame.from_dict(studies, orient=\"index\")\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.attach_to_study","title":"attach_to_study","text":"<pre><code>attach_to_study(study_id: int, run_ids: list[int]) -&gt; int\n</code></pre> <p>Attaches a set of runs to a study.</p> PARAMETER DESCRIPTION <code>study_id</code> <p>OpenML id of the study</p> <p> TYPE: <code>int</code> </p> <code>run_ids</code> <p>List of entities to link to the collection</p> <p> TYPE: <code>list(int)</code> </p> RETURNS DESCRIPTION <code>int</code> <p>new size of the study (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def attach_to_study(study_id: int, run_ids: list[int]) -&gt; int:\n    \"\"\"Attaches a set of runs to a study.\n\n    Parameters\n    ----------\n    study_id : int\n        OpenML id of the study\n\n    run_ids : list (int)\n        List of entities to link to the collection\n\n    Returns\n    -------\n    int\n        new size of the study (in terms of explicitly linked entities)\n    \"\"\"\n    # Interestingly, there's no need to tell the server about the entity type, it knows by itself\n    result_xml = openml._api_calls._perform_api_call(\n        call=f\"study/{study_id}/attach\",\n        request_method=\"post\",\n        data={\"ids\": \",\".join(str(x) for x in run_ids)},\n    )\n    result = xmltodict.parse(result_xml)[\"oml:study_attach\"]\n    return int(result[\"oml:linked_entities\"])\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.attach_to_suite","title":"attach_to_suite","text":"<pre><code>attach_to_suite(suite_id: int, task_ids: list[int]) -&gt; int\n</code></pre> <p>Attaches a set of tasks to a benchmarking suite.</p> PARAMETER DESCRIPTION <code>suite_id</code> <p>OpenML id of the study</p> <p> TYPE: <code>int</code> </p> <code>task_ids</code> <p>List of entities to link to the collection</p> <p> TYPE: <code>list(int)</code> </p> RETURNS DESCRIPTION <code>int</code> <p>new size of the suite (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def attach_to_suite(suite_id: int, task_ids: list[int]) -&gt; int:\n    \"\"\"Attaches a set of tasks to a benchmarking suite.\n\n    Parameters\n    ----------\n    suite_id : int\n        OpenML id of the study\n\n    task_ids : list (int)\n        List of entities to link to the collection\n\n    Returns\n    -------\n    int\n        new size of the suite (in terms of explicitly linked entities)\n    \"\"\"\n    return attach_to_study(suite_id, task_ids)\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.create_benchmark_suite","title":"create_benchmark_suite","text":"<pre><code>create_benchmark_suite(name: str, description: str, task_ids: list[int], alias: str | None = None) -&gt; OpenMLBenchmarkSuite\n</code></pre> <p>Creates an OpenML benchmark suite (collection of entity types, where the tasks are the linked entity)</p> PARAMETER DESCRIPTION <code>name</code> <p>the name of the study (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>brief description (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>task_ids</code> <p>a list of task ids associated with this study more can be added later with <code>attach_to_suite</code>.</p> <p> TYPE: <code>list</code> </p> <code>alias</code> <p>a string ID, unique on server (url-friendly)</p> <p> TYPE: <code>str(optional)</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>OpenMLStudy</code> <p>A local OpenML study object (call publish method to upload to server)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def create_benchmark_suite(\n    name: str,\n    description: str,\n    task_ids: list[int],\n    alias: str | None = None,\n) -&gt; OpenMLBenchmarkSuite:\n    \"\"\"\n    Creates an OpenML benchmark suite (collection of entity types, where\n    the tasks are the linked entity)\n\n    Parameters\n    ----------\n    name : str\n        the name of the study (meta-info)\n    description : str\n        brief description (meta-info)\n    task_ids : list\n        a list of task ids associated with this study\n        more can be added later with ``attach_to_suite``.\n    alias : str (optional)\n        a string ID, unique on server (url-friendly)\n\n    Returns\n    -------\n    OpenMLStudy\n        A local OpenML study object (call publish method to upload to server)\n    \"\"\"\n    return OpenMLBenchmarkSuite(\n        suite_id=None,\n        alias=alias,\n        name=name,\n        description=description,\n        status=None,\n        creation_date=None,\n        creator=None,\n        tags=None,\n        data=None,\n        tasks=task_ids,\n    )\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.create_study","title":"create_study","text":"<pre><code>create_study(name: str, description: str, run_ids: list[int] | None = None, alias: str | None = None, benchmark_suite: int | None = None) -&gt; OpenMLStudy\n</code></pre> <p>Creates an OpenML study (collection of data, tasks, flows, setups and run), where the runs are the main entity (collection consists of runs and all entities (flows, tasks, etc) that are related to these runs)</p> PARAMETER DESCRIPTION <code>benchmark_suite</code> <p>the benchmark suite (another study) upon which this study is ran.</p> <p> TYPE: <code>int(optional)</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>the name of the study (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>brief description (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>run_ids</code> <p>a list of run ids associated with this study, these can also be added later with <code>attach_to_study</code>.</p> <p> TYPE: <code>list</code> DEFAULT: <code>None</code> </p> <code>alias</code> <p>a string ID, unique on server (url-friendly)</p> <p> TYPE: <code>str(optional)</code> DEFAULT: <code>None</code> </p> <code>benchmark_suite</code> <p>the ID of the suite for which this study contains run results</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>OpenMLStudy</code> <p>A local OpenML study object (call publish method to upload to server)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def create_study(\n    name: str,\n    description: str,\n    run_ids: list[int] | None = None,\n    alias: str | None = None,\n    benchmark_suite: int | None = None,\n) -&gt; OpenMLStudy:\n    \"\"\"\n    Creates an OpenML study (collection of data, tasks, flows, setups and run),\n    where the runs are the main entity (collection consists of runs and all\n    entities (flows, tasks, etc) that are related to these runs)\n\n    Parameters\n    ----------\n    benchmark_suite : int (optional)\n        the benchmark suite (another study) upon which this study is ran.\n    name : str\n        the name of the study (meta-info)\n    description : str\n        brief description (meta-info)\n    run_ids : list, optional\n        a list of run ids associated with this study,\n        these can also be added later with ``attach_to_study``.\n    alias : str (optional)\n        a string ID, unique on server (url-friendly)\n    benchmark_suite: int (optional)\n        the ID of the suite for which this study contains run results\n\n    Returns\n    -------\n    OpenMLStudy\n        A local OpenML study object (call publish method to upload to server)\n    \"\"\"\n    return OpenMLStudy(\n        study_id=None,\n        alias=alias,\n        benchmark_suite=benchmark_suite,\n        name=name,\n        description=description,\n        status=None,\n        creation_date=None,\n        creator=None,\n        tags=None,\n        data=None,\n        tasks=None,\n        flows=None,\n        runs=run_ids if run_ids != [] else None,\n        setups=None,\n    )\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.delete_study","title":"delete_study","text":"<pre><code>delete_study(study_id: int) -&gt; bool\n</code></pre> <p>Deletes a study from the OpenML server.</p> PARAMETER DESCRIPTION <code>study_id</code> <p>OpenML id of the study</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True iff the deletion was successful. False otherwise</p> Source code in <code>openml/study/functions.py</code> <pre><code>def delete_study(study_id: int) -&gt; bool:\n    \"\"\"Deletes a study from the OpenML server.\n\n    Parameters\n    ----------\n    study_id : int\n        OpenML id of the study\n\n    Returns\n    -------\n    bool\n        True iff the deletion was successful. False otherwise\n    \"\"\"\n    return openml.utils._delete_entity(\"study\", study_id)\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.delete_suite","title":"delete_suite","text":"<pre><code>delete_suite(suite_id: int) -&gt; bool\n</code></pre> <p>Deletes a study from the OpenML server.</p> PARAMETER DESCRIPTION <code>suite_id</code> <p>OpenML id of the study</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True iff the deletion was successful. False otherwise</p> Source code in <code>openml/study/functions.py</code> <pre><code>def delete_suite(suite_id: int) -&gt; bool:\n    \"\"\"Deletes a study from the OpenML server.\n\n    Parameters\n    ----------\n    suite_id : int\n        OpenML id of the study\n\n    Returns\n    -------\n    bool\n        True iff the deletion was successful. False otherwise\n    \"\"\"\n    return delete_study(suite_id)\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.detach_from_study","title":"detach_from_study","text":"<pre><code>detach_from_study(study_id: int, run_ids: list[int]) -&gt; int\n</code></pre> <p>Detaches a set of run ids from a study.</p> PARAMETER DESCRIPTION <code>study_id</code> <p>OpenML id of the study</p> <p> TYPE: <code>int</code> </p> <code>run_ids</code> <p>List of entities to unlink from the collection</p> <p> TYPE: <code>list(int)</code> </p> RETURNS DESCRIPTION <code>int</code> <p>new size of the study (in terms of explicitly linked entities)</p> Source code in <code>openml/study/functions.py</code> <pre><code>def detach_from_study(study_id: int, run_ids: list[int]) -&gt; int:\n    \"\"\"Detaches a set of run ids from a study.\n\n    Parameters\n    ----------\n    study_id : int\n        OpenML id of the study\n\n    run_ids : list (int)\n        List of entities to unlink from the collection\n\n    Returns\n    -------\n    int\n        new size of the study (in terms of explicitly linked entities)\n    \"\"\"\n    # Interestingly, there's no need to tell the server about the entity type, it knows by itself\n    uri = \"study/%d/detach\" % study_id\n    post_variables = {\"ids\": \",\".join(str(x) for x in run_ids)}  # type: openml._api_calls.DATA_TYPE\n    result_xml = openml._api_calls._perform_api_call(\n        call=uri,\n        request_method=\"post\",\n        data=post_variables,\n    )\n    result = xmltodict.parse(result_xml)[\"oml:study_detach\"]\n    return int(result[\"oml:linked_entities\"])\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.detach_from_suite","title":"detach_from_suite","text":"<pre><code>detach_from_suite(suite_id: int, task_ids: list[int]) -&gt; int\n</code></pre> <p>Detaches a set of task ids from a suite.</p> PARAMETER DESCRIPTION <code>suite_id</code> <p>OpenML id of the study</p> <p> TYPE: <code>int</code> </p> <code>task_ids</code> <p>List of entities to unlink from the collection</p> <p> TYPE: <code>list(int)</code> </p> RETURNS DESCRIPTION <code>int</code> <code>new size of the study (in terms of explicitly linked entities)</code> Source code in <code>openml/study/functions.py</code> <pre><code>def detach_from_suite(suite_id: int, task_ids: list[int]) -&gt; int:\n    \"\"\"Detaches a set of task ids from a suite.\n\n    Parameters\n    ----------\n    suite_id : int\n        OpenML id of the study\n\n    task_ids : list (int)\n        List of entities to unlink from the collection\n\n    Returns\n    -------\n    int\n    new size of the study (in terms of explicitly linked entities)\n    \"\"\"\n    return detach_from_study(suite_id, task_ids)\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.get_study","title":"get_study","text":"<pre><code>get_study(study_id: int | str, arg_for_backwards_compat: str | None = None) -&gt; OpenMLStudy\n</code></pre> <p>Retrieves all relevant information of an OpenML study from the server.</p> PARAMETER DESCRIPTION <code>study</code> <p>study id (numeric or alias)</p> <p> </p> <code>arg_for_backwards_compat</code> <p>The example given in arxiv.org/pdf/1708.03731.pdf uses an older version of the API which required specifying the type of study, i.e. tasks. We changed the implementation of studies since then and split them up into suites (collections of tasks) and studies (collections of runs) so this argument is no longer needed.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>OpenMLStudy</code> <p>The OpenML study object</p> Source code in <code>openml/study/functions.py</code> <pre><code>def get_study(\n    study_id: int | str,\n    arg_for_backwards_compat: str | None = None,  # noqa: ARG001\n) -&gt; OpenMLStudy:  # F401\n    \"\"\"\n    Retrieves all relevant information of an OpenML study from the server.\n\n    Parameters\n    ----------\n    study id : int, str\n        study id (numeric or alias)\n\n    arg_for_backwards_compat : str, optional\n        The example given in https://arxiv.org/pdf/1708.03731.pdf uses an older version of the\n        API which required specifying the type of study, i.e. tasks. We changed the\n        implementation of studies since then and split them up into suites (collections of tasks)\n        and studies (collections of runs) so this argument is no longer needed.\n\n    Returns\n    -------\n    OpenMLStudy\n        The OpenML study object\n    \"\"\"\n    if study_id == \"OpenML100\":\n        message = (\n            \"It looks like you are running code from the OpenML100 paper. It still works, but lots \"\n            \"of things have changed since then. Please use `get_suite('OpenML100')` instead.\"\n        )\n        warnings.warn(message, DeprecationWarning, stacklevel=2)\n        openml.config.logger.warning(message)\n        study = _get_study(study_id, entity_type=\"task\")\n        assert isinstance(study, OpenMLBenchmarkSuite)\n\n        return study  # type: ignore\n\n    study = _get_study(study_id, entity_type=\"run\")\n    assert isinstance(study, OpenMLStudy)\n    return study\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.get_suite","title":"get_suite","text":"<pre><code>get_suite(suite_id: int | str) -&gt; OpenMLBenchmarkSuite\n</code></pre> <p>Retrieves all relevant information of an OpenML benchmarking suite from the server.</p> PARAMETER DESCRIPTION <code>study</code> <p>study id (numeric or alias)</p> <p> </p> RETURNS DESCRIPTION <code>OpenMLSuite</code> <p>The OpenML suite object</p> Source code in <code>openml/study/functions.py</code> <pre><code>def get_suite(suite_id: int | str) -&gt; OpenMLBenchmarkSuite:\n    \"\"\"\n    Retrieves all relevant information of an OpenML benchmarking suite from the server.\n\n    Parameters\n    ----------\n    study id : int, str\n        study id (numeric or alias)\n\n    Returns\n    -------\n    OpenMLSuite\n        The OpenML suite object\n    \"\"\"\n    study = _get_study(suite_id, entity_type=\"task\")\n    assert isinstance(study, OpenMLBenchmarkSuite)\n\n    return study\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.list_studies","title":"list_studies","text":"<pre><code>list_studies(offset: int | None = None, size: int | None = None, status: str | None = None, uploader: list[str] | None = None, benchmark_suite: int | None = None) -&gt; DataFrame\n</code></pre> <p>Return a list of all studies which are on OpenML.</p> PARAMETER DESCRIPTION <code>offset</code> <p>The number of studies to skip, starting from the first.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>The maximum number of studies to show.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>status</code> <p>Should be {active, in_preparation, deactivated, all}. By default active studies are returned.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>uploader</code> <p>Result filter. Will only return studies created by these users.</p> <p> TYPE: <code>list(int)</code> DEFAULT: <code>None</code> </p> <code>benchmark_suite</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>datasets</code> <p>Every dataset is represented by a dictionary containing the following information: - id - alias (optional) - name - benchmark_suite (optional) - status - creator - creation_date If qualities are calculated for the dataset, some of these are also returned.</p> <p> TYPE: <code>dataframe</code> </p> Source code in <code>openml/study/functions.py</code> <pre><code>def list_studies(\n    offset: int | None = None,\n    size: int | None = None,\n    status: str | None = None,\n    uploader: list[str] | None = None,\n    benchmark_suite: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a list of all studies which are on OpenML.\n\n    Parameters\n    ----------\n    offset : int, optional\n        The number of studies to skip, starting from the first.\n    size : int, optional\n        The maximum number of studies to show.\n    status : str, optional\n        Should be {active, in_preparation, deactivated, all}. By default active\n        studies are returned.\n    uploader : list (int), optional\n        Result filter. Will only return studies created by these users.\n    benchmark_suite : int, optional\n\n    Returns\n    -------\n    datasets : dataframe\n        Every dataset is represented by a dictionary containing\n        the following information:\n        - id\n        - alias (optional)\n        - name\n        - benchmark_suite (optional)\n        - status\n        - creator\n        - creation_date\n        If qualities are calculated for the dataset, some of\n        these are also returned.\n    \"\"\"\n    listing_call = partial(\n        _list_studies,\n        main_entity_type=\"run\",\n        status=status,\n        uploader=uploader,\n        benchmark_suite=benchmark_suite,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.list_suites","title":"list_suites","text":"<pre><code>list_suites(offset: int | None = None, size: int | None = None, status: str | None = None, uploader: list[int] | None = None) -&gt; DataFrame\n</code></pre> <p>Return a list of all suites which are on OpenML.</p> PARAMETER DESCRIPTION <code>offset</code> <p>The number of suites to skip, starting from the first.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>The maximum number of suites to show.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>status</code> <p>Should be {active, in_preparation, deactivated, all}. By default active suites are returned.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>uploader</code> <p>Result filter. Will only return suites created by these users.</p> <p> TYPE: <code>list(int)</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>datasets</code> <p>Every row is represented by a dictionary containing the following information: - id - alias (optional) - name - main_entity_type - status - creator - creation_date</p> <p> TYPE: <code>dataframe</code> </p> Source code in <code>openml/study/functions.py</code> <pre><code>def list_suites(\n    offset: int | None = None,\n    size: int | None = None,\n    status: str | None = None,\n    uploader: list[int] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a list of all suites which are on OpenML.\n\n    Parameters\n    ----------\n    offset : int, optional\n        The number of suites to skip, starting from the first.\n    size : int, optional\n        The maximum number of suites to show.\n    status : str, optional\n        Should be {active, in_preparation, deactivated, all}. By default active\n        suites are returned.\n    uploader : list (int), optional\n        Result filter. Will only return suites created by these users.\n\n    Returns\n    -------\n    datasets : dataframe\n        Every row is represented by a dictionary containing the following information:\n        - id\n        - alias (optional)\n        - name\n        - main_entity_type\n        - status\n        - creator\n        - creation_date\n    \"\"\"\n    listing_call = partial(\n        _list_studies,\n        main_entity_type=\"task\",\n        status=status,\n        uploader=uploader,\n    )\n    batches = openml.utils._list_all(listing_call, limit=size, offset=offset)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.update_study_status","title":"update_study_status","text":"<pre><code>update_study_status(study_id: int, status: str) -&gt; None\n</code></pre> <p>Updates the status of a study to either 'active' or 'deactivated'.</p> PARAMETER DESCRIPTION <code>study_id</code> <p>The data id of the dataset</p> <p> TYPE: <code>int</code> </p> <code>status</code> <p>'active' or 'deactivated'</p> <p> TYPE: <code>(str)</code> </p> Source code in <code>openml/study/functions.py</code> <pre><code>def update_study_status(study_id: int, status: str) -&gt; None:\n    \"\"\"\n    Updates the status of a study to either 'active' or 'deactivated'.\n\n    Parameters\n    ----------\n    study_id : int\n        The data id of the dataset\n    status : str,\n        'active' or 'deactivated'\n    \"\"\"\n    legal_status = {\"active\", \"deactivated\"}\n    if status not in legal_status:\n        raise ValueError(f\"Illegal status value. Legal values: {legal_status}\")\n    data = {\"study_id\": study_id, \"status\": status}  # type: openml._api_calls.DATA_TYPE\n    result_xml = openml._api_calls._perform_api_call(\"study/status/update\", \"post\", data=data)\n    result = xmltodict.parse(result_xml)\n    server_study_id = result[\"oml:study_status_update\"][\"oml:id\"]\n    server_status = result[\"oml:study_status_update\"][\"oml:status\"]\n    if status != server_status or int(study_id) != int(server_study_id):\n        # This should never happen\n        raise ValueError(\"Study id/status does not collide\")\n</code></pre>"},{"location":"reference/study/functions/#openml.study.functions.update_suite_status","title":"update_suite_status","text":"<pre><code>update_suite_status(suite_id: int, status: str) -&gt; None\n</code></pre> <p>Updates the status of a study to either 'active' or 'deactivated'.</p> PARAMETER DESCRIPTION <code>suite_id</code> <p>The data id of the dataset</p> <p> TYPE: <code>int</code> </p> <code>status</code> <p>'active' or 'deactivated'</p> <p> TYPE: <code>(str)</code> </p> Source code in <code>openml/study/functions.py</code> <pre><code>def update_suite_status(suite_id: int, status: str) -&gt; None:\n    \"\"\"\n    Updates the status of a study to either 'active' or 'deactivated'.\n\n    Parameters\n    ----------\n    suite_id : int\n        The data id of the dataset\n    status : str,\n        'active' or 'deactivated'\n    \"\"\"\n    return update_study_status(suite_id, status)\n</code></pre>"},{"location":"reference/study/study/","title":"study","text":""},{"location":"reference/study/study/#openml.study.study","title":"openml.study.study","text":""},{"location":"reference/study/study/#openml.study.study.BaseStudy","title":"BaseStudy","text":"<pre><code>BaseStudy(study_id: int | None, alias: str | None, main_entity_type: str, benchmark_suite: int | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None, flows: list[int] | None, runs: list[int] | None, setups: list[int] | None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>An OpenMLStudy represents the OpenML concept of a study. It contains the following information: name, id, description, creation date, creator id and a set of tags.</p> <p>According to this list of tags, the study object receives a list of OpenML object ids (datasets, flows, tasks and setups).</p> <p>Can be used to obtain all relevant information from a study at once.</p> PARAMETER DESCRIPTION <code>study_id</code> <p>the study id</p> <p> TYPE: <code>int</code> </p> <code>alias</code> <p>a string ID, unique on server (url-friendly)</p> <p> TYPE: <code>str(optional)</code> </p> <code>main_entity_type</code> <p>the entity type (e.g., task, run) that is core in this study. only entities of this type can be added explicitly</p> <p> TYPE: <code>str</code> </p> <code>benchmark_suite</code> <p>the benchmark suite (another study) upon which this study is ran. can only be active if main entity type is runs.</p> <p> TYPE: <code>int(optional)</code> </p> <code>name</code> <p>the name of the study (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>brief description (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>Whether the study is in preparation, active or deactivated</p> <p> TYPE: <code>str</code> </p> <code>creation_date</code> <p>date of creation (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>creator</code> <p>openml user id of the owner / creator</p> <p> TYPE: <code>int</code> </p> <code>tags</code> <p>The list of tags shows which tags are associated with the study. Each tag is a dict of (tag) name, window_start and write_access.</p> <p> TYPE: <code>list(dict)</code> </p> <code>data</code> <p>a list of data ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>tasks</code> <p>a list of task ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>flows</code> <p>a list of flow ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>runs</code> <p>a list of run ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>setups</code> <p>a list of setup ids associated with this study</p> <p> TYPE: <code>list</code> </p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    study_id: int | None,\n    alias: str | None,\n    main_entity_type: str,\n    benchmark_suite: int | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n    flows: list[int] | None,\n    runs: list[int] | None,\n    setups: list[int] | None,\n):\n    self.study_id = study_id\n    self.alias = alias\n    self.main_entity_type = main_entity_type\n    self.benchmark_suite = benchmark_suite\n    self.name = name\n    self.description = description\n    self.status = status\n    self.creation_date = creation_date\n    self.creator = creator\n    self.tags = tags  # LEGACY. Can be removed soon\n    self.data = data\n    self.tasks = tasks\n    self.flows = flows\n    self.setups = setups\n    self.runs = runs\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.BaseStudy.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite","title":"OpenMLBenchmarkSuite","text":"<pre><code>OpenMLBenchmarkSuite(suite_id: int | None, alias: str | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None)\n</code></pre> <p>               Bases: <code>BaseStudy</code></p> <p>An OpenMLBenchmarkSuite represents the OpenML concept of a suite (a collection of tasks).</p> <p>It contains the following information: name, id, description, creation date, creator id and the task ids.</p> <p>According to this list of task ids, the suite object receives a list of OpenML object ids (datasets).</p> PARAMETER DESCRIPTION <code>suite_id</code> <p>the study id</p> <p> TYPE: <code>int</code> </p> <code>alias</code> <p>a string ID, unique on server (url-friendly)</p> <p> TYPE: <code>str(optional)</code> </p> <code>main_entity_type</code> <p>the entity type (e.g., task, run) that is core in this study. only entities of this type can be added explicitly</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>the name of the study (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>brief description (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>Whether the study is in preparation, active or deactivated</p> <p> TYPE: <code>str</code> </p> <code>creation_date</code> <p>date of creation (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>creator</code> <p>openml user id of the owner / creator</p> <p> TYPE: <code>int</code> </p> <code>tags</code> <p>The list of tags shows which tags are associated with the study. Each tag is a dict of (tag) name, window_start and write_access.</p> <p> TYPE: <code>list(dict)</code> </p> <code>data</code> <p>a list of data ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>tasks</code> <p>a list of task ids associated with this study</p> <p> TYPE: <code>list</code> </p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    suite_id: int | None,\n    alias: str | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n):\n    super().__init__(\n        study_id=suite_id,\n        alias=alias,\n        main_entity_type=\"task\",\n        benchmark_suite=None,\n        name=name,\n        description=description,\n        status=status,\n        creation_date=creation_date,\n        creator=creator,\n        tags=tags,\n        data=data,\n        tasks=tasks,\n        flows=None,\n        runs=None,\n        setups=None,\n    )\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLBenchmarkSuite.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy","title":"OpenMLStudy","text":"<pre><code>OpenMLStudy(study_id: int | None, alias: str | None, benchmark_suite: int | None, name: str, description: str, status: str | None, creation_date: str | None, creator: int | None, tags: list[dict] | None, data: list[int] | None, tasks: list[int] | None, flows: list[int] | None, runs: list[int] | None, setups: list[int] | None)\n</code></pre> <p>               Bases: <code>BaseStudy</code></p> <p>An OpenMLStudy represents the OpenML concept of a study (a collection of runs).</p> <p>It contains the following information: name, id, description, creation date, creator id and a list of run ids.</p> <p>According to this list of run ids, the study object receives a list of OpenML object ids (datasets, flows, tasks and setups).</p> PARAMETER DESCRIPTION <code>study_id</code> <p>the study id</p> <p> TYPE: <code>int</code> </p> <code>alias</code> <p>a string ID, unique on server (url-friendly)</p> <p> TYPE: <code>str(optional)</code> </p> <code>benchmark_suite</code> <p>the benchmark suite (another study) upon which this study is ran. can only be active if main entity type is runs.</p> <p> TYPE: <code>int(optional)</code> </p> <code>name</code> <p>the name of the study (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>description</code> <p>brief description (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>status</code> <p>Whether the study is in preparation, active or deactivated</p> <p> TYPE: <code>str</code> </p> <code>creation_date</code> <p>date of creation (meta-info)</p> <p> TYPE: <code>str</code> </p> <code>creator</code> <p>openml user id of the owner / creator</p> <p> TYPE: <code>int</code> </p> <code>tags</code> <p>The list of tags shows which tags are associated with the study. Each tag is a dict of (tag) name, window_start and write_access.</p> <p> TYPE: <code>list(dict)</code> </p> <code>data</code> <p>a list of data ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>tasks</code> <p>a list of task ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>flows</code> <p>a list of flow ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>runs</code> <p>a list of run ids associated with this study</p> <p> TYPE: <code>list</code> </p> <code>setups</code> <p>a list of setup ids associated with this study</p> <p> TYPE: <code>list</code> </p> Source code in <code>openml/study/study.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    study_id: int | None,\n    alias: str | None,\n    benchmark_suite: int | None,\n    name: str,\n    description: str,\n    status: str | None,\n    creation_date: str | None,\n    creator: int | None,\n    tags: list[dict] | None,\n    data: list[int] | None,\n    tasks: list[int] | None,\n    flows: list[int] | None,\n    runs: list[int] | None,\n    setups: list[int] | None,\n):\n    super().__init__(\n        study_id=study_id,\n        alias=alias,\n        main_entity_type=\"run\",\n        benchmark_suite=benchmark_suite,\n        name=name,\n        description=description,\n        status=status,\n        creation_date=creation_date,\n        creator=creator,\n        tags=tags,\n        data=data,\n        tasks=tasks,\n        flows=flows,\n        runs=runs,\n        setups=setups,\n    )\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the id of the study.</p>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Add a tag to the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Add a tag to the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Remove a tag from the study.</p> Source code in <code>openml/study/study.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Remove a tag from the study.\"\"\"\n    raise NotImplementedError(\"Tags for studies is not (yet) supported.\")\n</code></pre>"},{"location":"reference/study/study/#openml.study.study.OpenMLStudy.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/","title":"tasks","text":""},{"location":"reference/tasks/#openml.tasks","title":"openml.tasks","text":""},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask","title":"OpenMLClassificationTask","text":"<pre><code>OpenMLClassificationTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None, task_id: int | None = None, class_labels: list[str] | None = None, cost_matrix: ndarray | None = None)\n</code></pre> <p>               Bases: <code>OpenMLSupervisedTask</code></p> <p>OpenML Classification object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>ID of the Classification task type.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Name of the Classification task type.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the OpenML dataset associated with the Classification task.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>Name of the target variable.</p> <p> TYPE: <code>str</code> </p> <code>estimation_procedure_id</code> <p>ID of the estimation procedure for the Classification task.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the estimation procedure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Estimation parameters for the Classification task.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Name of the evaluation measure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the data splits for the Classification task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID of the Classification task (if it already exists on OpenML).</p> <p> TYPE: <code>Union[int, None]</code> DEFAULT: <code>None</code> </p> <code>class_labels</code> <p>A list of class labels (for classification tasks).</p> <p> TYPE: <code>List of str</code> DEFAULT: <code>None</code> </p> <code>cost_matrix</code> <p>A cost matrix (for classification tasks).</p> <p> TYPE: <code>array</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    class_labels: list[str] | None = None,\n    cost_matrix: np.ndarray | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n    )\n    self.class_labels = class_labels\n    self.cost_matrix = cost_matrix\n\n    if cost_matrix is not None:\n        raise NotImplementedError(\"Costmatrix\")\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>tuple - X and y</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClassificationTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask","title":"OpenMLClusteringTask","text":"<pre><code>OpenMLClusteringTask(task_type_id: TaskType, task_type: str, data_set_id: int, estimation_procedure_id: int = 17, task_id: int | None = None, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, evaluation_measure: str | None = None, target_name: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLTask</code></p> <p>OpenML Clustering object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>Task type ID of the OpenML clustering task.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Task type of the OpenML clustering task.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the OpenML dataset used in clustering the task.</p> <p> TYPE: <code>int</code> </p> <code>estimation_procedure_id</code> <p>ID of the OpenML estimation procedure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID of the OpenML clustering task.</p> <p> TYPE: <code>Union[int, None]</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the OpenML estimation procedure used in the clustering task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Parameters used by the OpenML estimation procedure.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the OpenML data splits for the clustering task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Evaluation measure used in the clustering task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>target_name</code> <p>Name of the target feature (class) that is not part of the feature set for the clustering task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    estimation_procedure_id: int = 17,\n    task_id: int | None = None,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    evaluation_measure: str | None = None,\n    target_name: str | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        evaluation_measure=evaluation_measure,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        data_splits_url=data_splits_url,\n    )\n\n    self.target_name = target_name\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.get_X","title":"get_X","text":"<pre><code>get_X() -&gt; DataFrame\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>The X data as a dataframe</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X(self) -&gt; pd.DataFrame:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    The X data as a dataframe\n    \"\"\"\n    dataset = self.get_dataset()\n    data, *_ = dataset.get_data(target=None)\n    return data\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLClusteringTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask","title":"OpenMLLearningCurveTask","text":"<pre><code>OpenMLLearningCurveTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 13, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, task_id: int | None = None, evaluation_measure: str | None = None, class_labels: list[str] | None = None, cost_matrix: ndarray | None = None)\n</code></pre> <p>               Bases: <code>OpenMLClassificationTask</code></p> <p>OpenML Learning Curve object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>ID of the Learning Curve task.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Name of the Learning Curve task.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the dataset that this task is associated with.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>Name of the target feature in the dataset.</p> <p> TYPE: <code>str</code> </p> <code>estimation_procedure_id</code> <p>ID of the estimation procedure to use for evaluating models.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the estimation procedure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Additional parameters for the estimation procedure.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the file containing the data splits for Learning Curve task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID of the Learning Curve task.</p> <p> TYPE: <code>Union[int, None]</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Name of the evaluation measure to use for evaluating models.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>class_labels</code> <p>Class labels for Learning Curve tasks.</p> <p> TYPE: <code>list of str</code> DEFAULT: <code>None</code> </p> <code>cost_matrix</code> <p>Cost matrix for Learning Curve tasks.</p> <p> TYPE: <code>numpy array</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 13,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    evaluation_measure: str | None = None,\n    class_labels: list[str] | None = None,\n    cost_matrix: np.ndarray | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n        class_labels=class_labels,\n        cost_matrix=cost_matrix,\n    )\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>tuple - X and y</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLLearningCurveTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask","title":"OpenMLRegressionTask","text":"<pre><code>OpenMLRegressionTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 7, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, task_id: int | None = None, evaluation_measure: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLSupervisedTask</code></p> <p>OpenML Regression object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>Task type ID of the OpenML Regression task.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Task type of the OpenML Regression task.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the OpenML dataset.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>Name of the target feature used in the Regression task.</p> <p> TYPE: <code>str</code> </p> <code>estimation_procedure_id</code> <p>ID of the OpenML estimation procedure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the OpenML estimation procedure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Parameters used by the OpenML estimation procedure.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the OpenML data splits for the Regression task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID of the OpenML Regression task.</p> <p> TYPE: <code>Union[int, None]</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Evaluation measure used in the Regression task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 7,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    evaluation_measure: str | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n    )\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>tuple - X and y</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLRegressionTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSplit","title":"OpenMLSplit","text":"<pre><code>OpenMLSplit(name: int | str, description: str, split: dict[int, dict[int, dict[int, tuple[ndarray, ndarray]]]])\n</code></pre> <p>OpenML Split object.</p> <p>This class manages train-test splits for a dataset across multiple repetitions, folds, and samples.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name or ID of the split.</p> <p> TYPE: <code>int or str</code> </p> <code>description</code> <p>A description of the split.</p> <p> TYPE: <code>str</code> </p> <code>split</code> <p>A dictionary containing the splits organized by repetition, fold, and sample.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>openml/tasks/split.py</code> <pre><code>def __init__(\n    self,\n    name: int | str,\n    description: str,\n    split: dict[int, dict[int, dict[int, tuple[np.ndarray, np.ndarray]]]],\n):\n    self.description = description\n    self.name = name\n    self.split: dict[int, dict[int, dict[int, tuple[np.ndarray, np.ndarray]]]] = {}\n\n    # Add splits according to repetition\n    for repetition in split:\n        _rep = int(repetition)\n        self.split[_rep] = OrderedDict()\n        for fold in split[_rep]:\n            self.split[_rep][fold] = OrderedDict()\n            for sample in split[_rep][fold]:\n                self.split[_rep][fold][sample] = split[_rep][fold][sample]\n\n    self.repeats = len(self.split)\n\n    # TODO(eddiebergman): Better error message\n    if any(len(self.split[0]) != len(self.split[i]) for i in range(self.repeats)):\n        raise ValueError(\"\")\n\n    self.folds = len(self.split[0])\n    self.samples = len(self.split[0][0])\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSplit.get","title":"get","text":"<pre><code>get(repeat: int = 0, fold: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns the specified data split from the CrossValidationSplit object.</p> PARAMETER DESCRIPTION <code>repeat</code> <p>Index of the repeat to retrieve.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>fold</code> <p>Index of the fold to retrieve.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sample</code> <p>Index of the sample to retrieve.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The data split for the specified repeat, fold, and sample.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the specified repeat, fold, or sample is not known.</p> Source code in <code>openml/tasks/split.py</code> <pre><code>def get(self, repeat: int = 0, fold: int = 0, sample: int = 0) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns the specified data split from the CrossValidationSplit object.\n\n    Parameters\n    ----------\n    repeat : int\n        Index of the repeat to retrieve.\n    fold : int\n        Index of the fold to retrieve.\n    sample : int\n        Index of the sample to retrieve.\n\n    Returns\n    -------\n    numpy.ndarray\n        The data split for the specified repeat, fold, and sample.\n\n    Raises\n    ------\n    ValueError\n        If the specified repeat, fold, or sample is not known.\n    \"\"\"\n    if repeat not in self.split:\n        raise ValueError(f\"Repeat {repeat!s} not known\")\n    if fold not in self.split[repeat]:\n        raise ValueError(f\"Fold {fold!s} not known\")\n    if sample not in self.split[repeat][fold]:\n        raise ValueError(f\"Sample {sample!s} not known\")\n    return self.split[repeat][fold][sample]\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask","title":"OpenMLSupervisedTask","text":"<pre><code>OpenMLSupervisedTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None, task_id: int | None = None)\n</code></pre> <p>               Bases: <code>OpenMLTask</code>, <code>ABC</code></p> <p>OpenML Supervised Classification object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>ID of the task type.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Name of the task type.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the OpenML dataset associated with the task.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>Name of the target feature (the class variable).</p> <p> TYPE: <code>str</code> </p> <code>estimation_procedure_id</code> <p>ID of the estimation procedure for the task.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the estimation procedure for the task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Estimation parameters for the task.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Name of the evaluation measure for the task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the data splits for the task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>Refers to the unique identifier of task.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        data_splits_url=data_splits_url,\n    )\n\n    self.target_name = target_name\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>tuple - X and y</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLSupervisedTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask","title":"OpenMLTask","text":"<pre><code>OpenMLTask(task_id: int | None, task_type_id: TaskType, task_type: str, data_set_id: int, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Task object.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>Refers to the unique identifier of OpenML task.</p> <p> TYPE: <code>int | None</code> </p> <code>task_type_id</code> <p>Refers to the type of OpenML task.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Refers to the OpenML task.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>Refers to the data.</p> <p> TYPE: <code>int</code> </p> <code>estimation_procedure_id</code> <p>Refers to the type of estimates used.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>estimation_procedure_type</code> <p>Refers to the type of estimation procedure used for the OpenML task.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Estimation parameters used for the OpenML task.</p> <p> TYPE: <code>dict[str, str] | None</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Refers to the evaluation measure.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>Refers to the URL of the data splits used for the OpenML task.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_id: int | None,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n):\n    self.task_id = int(task_id) if task_id is not None else None\n    self.task_type_id = task_type_id\n    self.task_type = task_type\n    self.dataset_id = int(data_set_id)\n    self.evaluation_measure = evaluation_measure\n    self.estimation_procedure: _EstimationProcedure = {\n        \"type\": estimation_procedure_type,\n        \"parameters\": estimation_parameters,\n        \"data_splits_url\": data_splits_url,\n    }\n    self.estimation_procedure_id = estimation_procedure_id\n    self.split: OpenMLSplit | None = None\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.OpenMLTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.TaskType","title":"TaskType","text":"<p>               Bases: <code>Enum</code></p> <p>Possible task types as defined in OpenML.</p>"},{"location":"reference/tasks/#openml.tasks.create_task","title":"create_task","text":"<pre><code>create_task(task_type: TaskType, dataset_id: int, estimation_procedure_id: int, target_name: str | None = None, evaluation_measure: str | None = None, **kwargs: Any) -&gt; OpenMLClassificationTask | OpenMLRegressionTask | OpenMLLearningCurveTask | OpenMLClusteringTask\n</code></pre> <p>Create a task based on different given attributes.</p> <p>Builds a task object with the function arguments as attributes. The type of the task object built is determined from the task type id. More information on how the arguments (task attributes), relate to the different possible tasks can be found in the individual task objects at the openml.tasks.task module.</p> PARAMETER DESCRIPTION <code>task_type</code> <p>Id of the task type.</p> <p> TYPE: <code>TaskType</code> </p> <code>dataset_id</code> <p>The id of the dataset for the task.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>The name of the feature used as a target. At the moment, only optional for the clustering tasks.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_id</code> <p>The id of the estimation procedure.</p> <p> TYPE: <code>int</code> </p> <code>evaluation_measure</code> <p>The name of the evaluation measure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Other task attributes that are not mandatory for task upload.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>(OpenMLClassificationTask, OpenMLRegressionTask)</code> <code>(OpenMLLearningCurveTask, OpenMLClusteringTask)</code> Source code in <code>openml/tasks/functions.py</code> <pre><code>def create_task(\n    task_type: TaskType,\n    dataset_id: int,\n    estimation_procedure_id: int,\n    target_name: str | None = None,\n    evaluation_measure: str | None = None,\n    **kwargs: Any,\n) -&gt; (\n    OpenMLClassificationTask | OpenMLRegressionTask | OpenMLLearningCurveTask | OpenMLClusteringTask\n):\n    \"\"\"Create a task based on different given attributes.\n\n    Builds a task object with the function arguments as\n    attributes. The type of the task object built is\n    determined from the task type id.\n    More information on how the arguments (task attributes),\n    relate to the different possible tasks can be found in\n    the individual task objects at the openml.tasks.task\n    module.\n\n    Parameters\n    ----------\n    task_type : TaskType\n        Id of the task type.\n    dataset_id : int\n        The id of the dataset for the task.\n    target_name : str, optional\n        The name of the feature used as a target.\n        At the moment, only optional for the clustering tasks.\n    estimation_procedure_id : int\n        The id of the estimation procedure.\n    evaluation_measure : str, optional\n        The name of the evaluation measure.\n    kwargs : dict, optional\n        Other task attributes that are not mandatory\n        for task upload.\n\n    Returns\n    -------\n    OpenMLClassificationTask, OpenMLRegressionTask,\n    OpenMLLearningCurveTask, OpenMLClusteringTask\n    \"\"\"\n    if task_type == TaskType.CLUSTERING:\n        task_cls = OpenMLClusteringTask\n    elif task_type == TaskType.LEARNING_CURVE:\n        task_cls = OpenMLLearningCurveTask  # type: ignore\n    elif task_type == TaskType.SUPERVISED_CLASSIFICATION:\n        task_cls = OpenMLClassificationTask  # type: ignore\n    elif task_type == TaskType.SUPERVISED_REGRESSION:\n        task_cls = OpenMLRegressionTask  # type: ignore\n    else:\n        raise NotImplementedError(f\"Task type {task_type:d} not supported.\")\n\n    return task_cls(\n        task_type_id=task_type,\n        task_type=\"None\",  # TODO: refactor to get task type string from ID.\n        data_set_id=dataset_id,\n        target_name=target_name,  # type: ignore\n        estimation_procedure_id=estimation_procedure_id,\n        evaluation_measure=evaluation_measure,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.delete_task","title":"delete_task","text":"<pre><code>delete_task(task_id: int) -&gt; bool\n</code></pre> <p>Delete task with id <code>task_id</code> from the OpenML server.</p> <p>You can only delete tasks which you created and have no runs associated with them.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>OpenML id of the task</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the deletion was successful. False otherwise.</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def delete_task(task_id: int) -&gt; bool:\n    \"\"\"Delete task with id `task_id` from the OpenML server.\n\n    You can only delete tasks which you created and have\n    no runs associated with them.\n\n    Parameters\n    ----------\n    task_id : int\n        OpenML id of the task\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"task\", task_id)\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.get_task","title":"get_task","text":"<pre><code>get_task(task_id: int, download_splits: bool = False, **get_dataset_kwargs: Any) -&gt; OpenMLTask\n</code></pre> <p>Download OpenML task for a given task ID.</p> <p>Downloads the task representation.</p> <p>Use the <code>download_splits</code> parameter to control whether the splits are downloaded. Moreover, you may pass additional parameter (args or kwargs) that are passed to :meth:<code>openml.datasets.get_dataset</code>.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>The OpenML task id of the task to download.</p> <p> TYPE: <code>int</code> </p> <code>download_splits</code> <p>Whether to download the splits as well.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>get_dataset_kwargs</code> <p>Args and kwargs can be used pass optional parameters to :meth:<code>openml.datasets.get_dataset</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>task</code> <p> TYPE: <code>OpenMLTask</code> </p> Source code in <code>openml/tasks/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_task(\n    task_id: int,\n    download_splits: bool = False,  # noqa: FBT001, FBT002\n    **get_dataset_kwargs: Any,\n) -&gt; OpenMLTask:\n    \"\"\"Download OpenML task for a given task ID.\n\n    Downloads the task representation.\n\n    Use the `download_splits` parameter to control whether the splits are downloaded.\n    Moreover, you may pass additional parameter (args or kwargs) that are passed to\n    :meth:`openml.datasets.get_dataset`.\n\n    Parameters\n    ----------\n    task_id : int\n        The OpenML task id of the task to download.\n    download_splits: bool (default=False)\n        Whether to download the splits as well.\n    get_dataset_kwargs :\n        Args and kwargs can be used pass optional parameters to :meth:`openml.datasets.get_dataset`.\n\n    Returns\n    -------\n    task: OpenMLTask\n    \"\"\"\n    if not isinstance(task_id, int):\n        raise TypeError(f\"Task id should be integer, is {type(task_id)}\")\n\n    cache_key_dir = openml.utils._create_cache_directory_for_id(TASKS_CACHE_DIR_NAME, task_id)\n    tid_cache_dir = cache_key_dir / str(task_id)\n    tid_cache_dir_existed = tid_cache_dir.exists()\n    try:\n        task = _get_task_description(task_id)\n        dataset = get_dataset(task.dataset_id, **get_dataset_kwargs)\n        # List of class labels available in dataset description\n        # Including class labels as part of task meta data handles\n        #   the case where data download was initially disabled\n        if isinstance(task, (OpenMLClassificationTask, OpenMLLearningCurveTask)):\n            task.class_labels = dataset.retrieve_class_labels(task.target_name)\n        # Clustering tasks do not have class labels\n        # and do not offer download_split\n        if download_splits and isinstance(task, OpenMLSupervisedTask):\n            task.download_split()\n    except Exception as e:\n        if not tid_cache_dir_existed:\n            openml.utils._remove_cache_dir_for_id(TASKS_CACHE_DIR_NAME, tid_cache_dir)\n        raise e\n\n    return task\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.get_tasks","title":"get_tasks","text":"<pre><code>get_tasks(task_ids: list[int], download_data: bool | None = None, download_qualities: bool | None = None) -&gt; list[OpenMLTask]\n</code></pre> <p>Download tasks.</p> <p>This function iterates :meth:<code>openml.tasks.get_task</code>.</p> PARAMETER DESCRIPTION <code>task_ids</code> <p>A list of task ids to download.</p> <p> TYPE: <code>List[int]</code> </p> <code>download_data</code> <p>Option to trigger download of data along with the meta data.</p> <p> TYPE: <code>bool(default=True)</code> DEFAULT: <code>None</code> </p> <code>download_qualities</code> <p>Option to download 'qualities' meta-data in addition to the minimal dataset description.</p> <p> TYPE: <code>bool(default=True)</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list</code> Source code in <code>openml/tasks/functions.py</code> <pre><code>def get_tasks(\n    task_ids: list[int],\n    download_data: bool | None = None,\n    download_qualities: bool | None = None,\n) -&gt; list[OpenMLTask]:\n    \"\"\"Download tasks.\n\n    This function iterates :meth:`openml.tasks.get_task`.\n\n    Parameters\n    ----------\n    task_ids : List[int]\n        A list of task ids to download.\n    download_data : bool (default = True)\n        Option to trigger download of data along with the meta data.\n    download_qualities : bool (default=True)\n        Option to download 'qualities' meta-data in addition to the minimal dataset description.\n\n    Returns\n    -------\n    list\n    \"\"\"\n    if download_data is None:\n        warnings.warn(\n            \"`download_data` will default to False starting in 0.16. \"\n            \"Please set `download_data` explicitly to suppress this warning.\",\n            stacklevel=1,\n        )\n        download_data = True\n\n    if download_qualities is None:\n        warnings.warn(\n            \"`download_qualities` will default to False starting in 0.16. \"\n            \"Please set `download_qualities` explicitly to suppress this warning.\",\n            stacklevel=1,\n        )\n        download_qualities = True\n\n    tasks = []\n    for task_id in task_ids:\n        tasks.append(\n            get_task(task_id, download_data=download_data, download_qualities=download_qualities)\n        )\n    return tasks\n</code></pre>"},{"location":"reference/tasks/#openml.tasks.list_tasks","title":"list_tasks","text":"<pre><code>list_tasks(task_type: TaskType | None = None, offset: int | None = None, size: int | None = None, tag: str | None = None, data_tag: str | None = None, status: str | None = None, data_name: str | None = None, data_id: int | None = None, number_instances: int | None = None, number_features: int | None = None, number_classes: int | None = None, number_missing_values: int | None = None) -&gt; DataFrame\n</code></pre> <p>Return a number of tasks having the given tag and task_type</p> PARAMETER DESCRIPTION <code>Filter</code> <p> </p> <code>it</code> <p> </p> <code>type</code> <p> </p> <code>offset</code> <p>the number of tasks to skip, starting from the first</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>task_type</code> <p>Refers to the type of task.</p> <p> TYPE: <code>TaskType</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>the maximum number of tasks to show</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p>the tag to include</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_tag</code> <p>the tag of the dataset</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_id</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>status</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_name</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>number_instances</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>number_features</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>number_classes</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>number_missing_values</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dataframe</code> <p>All tasks having the given task_type and the give tag. Every task is represented by a row in the data frame containing the following information as columns: task id, dataset id, task_type and status. If qualities are calculated for the associated dataset, some of these are also returned.</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def list_tasks(  # noqa: PLR0913\n    task_type: TaskType | None = None,\n    offset: int | None = None,\n    size: int | None = None,\n    tag: str | None = None,\n    data_tag: str | None = None,\n    status: str | None = None,\n    data_name: str | None = None,\n    data_id: int | None = None,\n    number_instances: int | None = None,\n    number_features: int | None = None,\n    number_classes: int | None = None,\n    number_missing_values: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a number of tasks having the given tag and task_type\n\n    Parameters\n    ----------\n    Filter task_type is separated from the other filters because\n    it is used as task_type in the task description, but it is named\n    type when used as a filter in list tasks call.\n    offset : int, optional\n        the number of tasks to skip, starting from the first\n    task_type : TaskType, optional\n        Refers to the type of task.\n    size : int, optional\n        the maximum number of tasks to show\n    tag : str, optional\n        the tag to include\n    data_tag : str, optional\n        the tag of the dataset\n    data_id : int, optional\n    status : str, optional\n    data_name : str, optional\n    number_instances : int, optional\n    number_features : int, optional\n    number_classes : int, optional\n    number_missing_values : int, optional\n\n    Returns\n    -------\n    dataframe\n        All tasks having the given task_type and the give tag. Every task is\n        represented by a row in the data frame containing the following information\n        as columns: task id, dataset id, task_type and status. If qualities are\n        calculated for the associated dataset, some of these are also returned.\n    \"\"\"\n    listing_call = partial(\n        _list_tasks,\n        task_type=task_type,\n        tag=tag,\n        data_tag=data_tag,\n        status=status,\n        data_id=data_id,\n        data_name=data_name,\n        number_instances=number_instances,\n        number_features=number_features,\n        number_classes=number_classes,\n        number_missing_values=number_missing_values,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/tasks/functions/","title":"functions","text":""},{"location":"reference/tasks/functions/#openml.tasks.functions","title":"openml.tasks.functions","text":""},{"location":"reference/tasks/functions/#openml.tasks.functions.__list_tasks","title":"__list_tasks","text":"<pre><code>__list_tasks(api_call: str) -&gt; DataFrame\n</code></pre> <p>Returns a Pandas DataFrame with information about OpenML tasks.</p> PARAMETER DESCRIPTION <code>api_call</code> <p>The API call specifying which tasks to return.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>    A Pandas DataFrame with information about OpenML tasks.</code> RAISES DESCRIPTION <code>ValueError</code> <p>If the XML returned by the OpenML API does not contain 'oml:tasks', '@xmlns:oml', or has an incorrect value for '@xmlns:oml'.</p> <code>KeyError</code> <p>If an invalid key is found in the XML for a task.</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def __list_tasks(api_call: str) -&gt; pd.DataFrame:  # noqa: C901, PLR0912\n    \"\"\"Returns a Pandas DataFrame with information about OpenML tasks.\n\n    Parameters\n    ----------\n    api_call : str\n        The API call specifying which tasks to return.\n\n    Returns\n    -------\n        A Pandas DataFrame with information about OpenML tasks.\n\n    Raises\n    ------\n    ValueError\n        If the XML returned by the OpenML API does not contain 'oml:tasks', '@xmlns:oml',\n        or has an incorrect value for '@xmlns:oml'.\n    KeyError\n        If an invalid key is found in the XML for a task.\n    \"\"\"\n    xml_string = openml._api_calls._perform_api_call(api_call, \"get\")\n    tasks_dict = xmltodict.parse(xml_string, force_list=(\"oml:task\", \"oml:input\"))\n    # Minimalistic check if the XML is useful\n    if \"oml:tasks\" not in tasks_dict:\n        raise ValueError(f'Error in return XML, does not contain \"oml:runs\": {tasks_dict}')\n\n    if \"@xmlns:oml\" not in tasks_dict[\"oml:tasks\"]:\n        raise ValueError(\n            f'Error in return XML, does not contain \"oml:runs\"/@xmlns:oml: {tasks_dict}'\n        )\n\n    if tasks_dict[\"oml:tasks\"][\"@xmlns:oml\"] != \"http://openml.org/openml\":\n        raise ValueError(\n            \"Error in return XML, value of  \"\n            '\"oml:runs\"/@xmlns:oml is not '\n            f'\"http://openml.org/openml\": {tasks_dict!s}',\n        )\n\n    assert isinstance(tasks_dict[\"oml:tasks\"][\"oml:task\"], list), type(tasks_dict[\"oml:tasks\"])\n\n    tasks = {}\n    procs = _get_estimation_procedure_list()\n    proc_dict = {x[\"id\"]: x for x in procs}\n\n    for task_ in tasks_dict[\"oml:tasks\"][\"oml:task\"]:\n        tid = None\n        try:\n            tid = int(task_[\"oml:task_id\"])\n            task_type_int = int(task_[\"oml:task_type_id\"])\n            try:\n                task_type_id = TaskType(task_type_int)\n            except ValueError as e:\n                warnings.warn(\n                    f\"Could not create task type id for {task_type_int} due to error {e}\",\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n                continue\n\n            task = {\n                \"tid\": tid,\n                \"ttid\": task_type_id,\n                \"did\": int(task_[\"oml:did\"]),\n                \"name\": task_[\"oml:name\"],\n                \"task_type\": task_[\"oml:task_type\"],\n                \"status\": task_[\"oml:status\"],\n            }\n\n            # Other task inputs\n            for _input in task_.get(\"oml:input\", []):\n                if _input[\"@name\"] == \"estimation_procedure\":\n                    task[_input[\"@name\"]] = proc_dict[int(_input[\"#text\"])][\"name\"]\n                else:\n                    value = _input.get(\"#text\")\n                    task[_input[\"@name\"]] = value\n\n            # The number of qualities can range from 0 to infinity\n            for quality in task_.get(\"oml:quality\", []):\n                if \"#text\" not in quality:\n                    quality_value = 0.0\n                else:\n                    quality[\"#text\"] = float(quality[\"#text\"])\n                    if abs(int(quality[\"#text\"]) - quality[\"#text\"]) &lt; 0.0000001:\n                        quality[\"#text\"] = int(quality[\"#text\"])\n                    quality_value = quality[\"#text\"]\n                task[quality[\"@name\"]] = quality_value\n            tasks[tid] = task\n        except KeyError as e:\n            if tid is not None:\n                warnings.warn(\n                    \"Invalid xml for task %d: %s\\nFrom %s\" % (tid, e, task_),\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n            else:\n                warnings.warn(f\"Could not find key {e} in {task_}!\", RuntimeWarning, stacklevel=2)\n\n    return pd.DataFrame.from_dict(tasks, orient=\"index\")\n</code></pre>"},{"location":"reference/tasks/functions/#openml.tasks.functions.create_task","title":"create_task","text":"<pre><code>create_task(task_type: TaskType, dataset_id: int, estimation_procedure_id: int, target_name: str | None = None, evaluation_measure: str | None = None, **kwargs: Any) -&gt; OpenMLClassificationTask | OpenMLRegressionTask | OpenMLLearningCurveTask | OpenMLClusteringTask\n</code></pre> <p>Create a task based on different given attributes.</p> <p>Builds a task object with the function arguments as attributes. The type of the task object built is determined from the task type id. More information on how the arguments (task attributes), relate to the different possible tasks can be found in the individual task objects at the openml.tasks.task module.</p> PARAMETER DESCRIPTION <code>task_type</code> <p>Id of the task type.</p> <p> TYPE: <code>TaskType</code> </p> <code>dataset_id</code> <p>The id of the dataset for the task.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>The name of the feature used as a target. At the moment, only optional for the clustering tasks.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_id</code> <p>The id of the estimation procedure.</p> <p> TYPE: <code>int</code> </p> <code>evaluation_measure</code> <p>The name of the evaluation measure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Other task attributes that are not mandatory for task upload.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>(OpenMLClassificationTask, OpenMLRegressionTask)</code> <code>(OpenMLLearningCurveTask, OpenMLClusteringTask)</code> Source code in <code>openml/tasks/functions.py</code> <pre><code>def create_task(\n    task_type: TaskType,\n    dataset_id: int,\n    estimation_procedure_id: int,\n    target_name: str | None = None,\n    evaluation_measure: str | None = None,\n    **kwargs: Any,\n) -&gt; (\n    OpenMLClassificationTask | OpenMLRegressionTask | OpenMLLearningCurveTask | OpenMLClusteringTask\n):\n    \"\"\"Create a task based on different given attributes.\n\n    Builds a task object with the function arguments as\n    attributes. The type of the task object built is\n    determined from the task type id.\n    More information on how the arguments (task attributes),\n    relate to the different possible tasks can be found in\n    the individual task objects at the openml.tasks.task\n    module.\n\n    Parameters\n    ----------\n    task_type : TaskType\n        Id of the task type.\n    dataset_id : int\n        The id of the dataset for the task.\n    target_name : str, optional\n        The name of the feature used as a target.\n        At the moment, only optional for the clustering tasks.\n    estimation_procedure_id : int\n        The id of the estimation procedure.\n    evaluation_measure : str, optional\n        The name of the evaluation measure.\n    kwargs : dict, optional\n        Other task attributes that are not mandatory\n        for task upload.\n\n    Returns\n    -------\n    OpenMLClassificationTask, OpenMLRegressionTask,\n    OpenMLLearningCurveTask, OpenMLClusteringTask\n    \"\"\"\n    if task_type == TaskType.CLUSTERING:\n        task_cls = OpenMLClusteringTask\n    elif task_type == TaskType.LEARNING_CURVE:\n        task_cls = OpenMLLearningCurveTask  # type: ignore\n    elif task_type == TaskType.SUPERVISED_CLASSIFICATION:\n        task_cls = OpenMLClassificationTask  # type: ignore\n    elif task_type == TaskType.SUPERVISED_REGRESSION:\n        task_cls = OpenMLRegressionTask  # type: ignore\n    else:\n        raise NotImplementedError(f\"Task type {task_type:d} not supported.\")\n\n    return task_cls(\n        task_type_id=task_type,\n        task_type=\"None\",  # TODO: refactor to get task type string from ID.\n        data_set_id=dataset_id,\n        target_name=target_name,  # type: ignore\n        estimation_procedure_id=estimation_procedure_id,\n        evaluation_measure=evaluation_measure,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/tasks/functions/#openml.tasks.functions.delete_task","title":"delete_task","text":"<pre><code>delete_task(task_id: int) -&gt; bool\n</code></pre> <p>Delete task with id <code>task_id</code> from the OpenML server.</p> <p>You can only delete tasks which you created and have no runs associated with them.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>OpenML id of the task</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the deletion was successful. False otherwise.</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def delete_task(task_id: int) -&gt; bool:\n    \"\"\"Delete task with id `task_id` from the OpenML server.\n\n    You can only delete tasks which you created and have\n    no runs associated with them.\n\n    Parameters\n    ----------\n    task_id : int\n        OpenML id of the task\n\n    Returns\n    -------\n    bool\n        True if the deletion was successful. False otherwise.\n    \"\"\"\n    return openml.utils._delete_entity(\"task\", task_id)\n</code></pre>"},{"location":"reference/tasks/functions/#openml.tasks.functions.get_task","title":"get_task","text":"<pre><code>get_task(task_id: int, download_splits: bool = False, **get_dataset_kwargs: Any) -&gt; OpenMLTask\n</code></pre> <p>Download OpenML task for a given task ID.</p> <p>Downloads the task representation.</p> <p>Use the <code>download_splits</code> parameter to control whether the splits are downloaded. Moreover, you may pass additional parameter (args or kwargs) that are passed to :meth:<code>openml.datasets.get_dataset</code>.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>The OpenML task id of the task to download.</p> <p> TYPE: <code>int</code> </p> <code>download_splits</code> <p>Whether to download the splits as well.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>get_dataset_kwargs</code> <p>Args and kwargs can be used pass optional parameters to :meth:<code>openml.datasets.get_dataset</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>task</code> <p> TYPE: <code>OpenMLTask</code> </p> Source code in <code>openml/tasks/functions.py</code> <pre><code>@openml.utils.thread_safe_if_oslo_installed\ndef get_task(\n    task_id: int,\n    download_splits: bool = False,  # noqa: FBT001, FBT002\n    **get_dataset_kwargs: Any,\n) -&gt; OpenMLTask:\n    \"\"\"Download OpenML task for a given task ID.\n\n    Downloads the task representation.\n\n    Use the `download_splits` parameter to control whether the splits are downloaded.\n    Moreover, you may pass additional parameter (args or kwargs) that are passed to\n    :meth:`openml.datasets.get_dataset`.\n\n    Parameters\n    ----------\n    task_id : int\n        The OpenML task id of the task to download.\n    download_splits: bool (default=False)\n        Whether to download the splits as well.\n    get_dataset_kwargs :\n        Args and kwargs can be used pass optional parameters to :meth:`openml.datasets.get_dataset`.\n\n    Returns\n    -------\n    task: OpenMLTask\n    \"\"\"\n    if not isinstance(task_id, int):\n        raise TypeError(f\"Task id should be integer, is {type(task_id)}\")\n\n    cache_key_dir = openml.utils._create_cache_directory_for_id(TASKS_CACHE_DIR_NAME, task_id)\n    tid_cache_dir = cache_key_dir / str(task_id)\n    tid_cache_dir_existed = tid_cache_dir.exists()\n    try:\n        task = _get_task_description(task_id)\n        dataset = get_dataset(task.dataset_id, **get_dataset_kwargs)\n        # List of class labels available in dataset description\n        # Including class labels as part of task meta data handles\n        #   the case where data download was initially disabled\n        if isinstance(task, (OpenMLClassificationTask, OpenMLLearningCurveTask)):\n            task.class_labels = dataset.retrieve_class_labels(task.target_name)\n        # Clustering tasks do not have class labels\n        # and do not offer download_split\n        if download_splits and isinstance(task, OpenMLSupervisedTask):\n            task.download_split()\n    except Exception as e:\n        if not tid_cache_dir_existed:\n            openml.utils._remove_cache_dir_for_id(TASKS_CACHE_DIR_NAME, tid_cache_dir)\n        raise e\n\n    return task\n</code></pre>"},{"location":"reference/tasks/functions/#openml.tasks.functions.get_tasks","title":"get_tasks","text":"<pre><code>get_tasks(task_ids: list[int], download_data: bool | None = None, download_qualities: bool | None = None) -&gt; list[OpenMLTask]\n</code></pre> <p>Download tasks.</p> <p>This function iterates :meth:<code>openml.tasks.get_task</code>.</p> PARAMETER DESCRIPTION <code>task_ids</code> <p>A list of task ids to download.</p> <p> TYPE: <code>List[int]</code> </p> <code>download_data</code> <p>Option to trigger download of data along with the meta data.</p> <p> TYPE: <code>bool(default=True)</code> DEFAULT: <code>None</code> </p> <code>download_qualities</code> <p>Option to download 'qualities' meta-data in addition to the minimal dataset description.</p> <p> TYPE: <code>bool(default=True)</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list</code> Source code in <code>openml/tasks/functions.py</code> <pre><code>def get_tasks(\n    task_ids: list[int],\n    download_data: bool | None = None,\n    download_qualities: bool | None = None,\n) -&gt; list[OpenMLTask]:\n    \"\"\"Download tasks.\n\n    This function iterates :meth:`openml.tasks.get_task`.\n\n    Parameters\n    ----------\n    task_ids : List[int]\n        A list of task ids to download.\n    download_data : bool (default = True)\n        Option to trigger download of data along with the meta data.\n    download_qualities : bool (default=True)\n        Option to download 'qualities' meta-data in addition to the minimal dataset description.\n\n    Returns\n    -------\n    list\n    \"\"\"\n    if download_data is None:\n        warnings.warn(\n            \"`download_data` will default to False starting in 0.16. \"\n            \"Please set `download_data` explicitly to suppress this warning.\",\n            stacklevel=1,\n        )\n        download_data = True\n\n    if download_qualities is None:\n        warnings.warn(\n            \"`download_qualities` will default to False starting in 0.16. \"\n            \"Please set `download_qualities` explicitly to suppress this warning.\",\n            stacklevel=1,\n        )\n        download_qualities = True\n\n    tasks = []\n    for task_id in task_ids:\n        tasks.append(\n            get_task(task_id, download_data=download_data, download_qualities=download_qualities)\n        )\n    return tasks\n</code></pre>"},{"location":"reference/tasks/functions/#openml.tasks.functions.list_tasks","title":"list_tasks","text":"<pre><code>list_tasks(task_type: TaskType | None = None, offset: int | None = None, size: int | None = None, tag: str | None = None, data_tag: str | None = None, status: str | None = None, data_name: str | None = None, data_id: int | None = None, number_instances: int | None = None, number_features: int | None = None, number_classes: int | None = None, number_missing_values: int | None = None) -&gt; DataFrame\n</code></pre> <p>Return a number of tasks having the given tag and task_type</p> PARAMETER DESCRIPTION <code>Filter</code> <p> </p> <code>it</code> <p> </p> <code>type</code> <p> </p> <code>offset</code> <p>the number of tasks to skip, starting from the first</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>task_type</code> <p>Refers to the type of task.</p> <p> TYPE: <code>TaskType</code> DEFAULT: <code>None</code> </p> <code>size</code> <p>the maximum number of tasks to show</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>tag</code> <p>the tag to include</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_tag</code> <p>the tag of the dataset</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_id</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>status</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_name</code> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>number_instances</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>number_features</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>number_classes</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>number_missing_values</code> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>dataframe</code> <p>All tasks having the given task_type and the give tag. Every task is represented by a row in the data frame containing the following information as columns: task id, dataset id, task_type and status. If qualities are calculated for the associated dataset, some of these are also returned.</p> Source code in <code>openml/tasks/functions.py</code> <pre><code>def list_tasks(  # noqa: PLR0913\n    task_type: TaskType | None = None,\n    offset: int | None = None,\n    size: int | None = None,\n    tag: str | None = None,\n    data_tag: str | None = None,\n    status: str | None = None,\n    data_name: str | None = None,\n    data_id: int | None = None,\n    number_instances: int | None = None,\n    number_features: int | None = None,\n    number_classes: int | None = None,\n    number_missing_values: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a number of tasks having the given tag and task_type\n\n    Parameters\n    ----------\n    Filter task_type is separated from the other filters because\n    it is used as task_type in the task description, but it is named\n    type when used as a filter in list tasks call.\n    offset : int, optional\n        the number of tasks to skip, starting from the first\n    task_type : TaskType, optional\n        Refers to the type of task.\n    size : int, optional\n        the maximum number of tasks to show\n    tag : str, optional\n        the tag to include\n    data_tag : str, optional\n        the tag of the dataset\n    data_id : int, optional\n    status : str, optional\n    data_name : str, optional\n    number_instances : int, optional\n    number_features : int, optional\n    number_classes : int, optional\n    number_missing_values : int, optional\n\n    Returns\n    -------\n    dataframe\n        All tasks having the given task_type and the give tag. Every task is\n        represented by a row in the data frame containing the following information\n        as columns: task id, dataset id, task_type and status. If qualities are\n        calculated for the associated dataset, some of these are also returned.\n    \"\"\"\n    listing_call = partial(\n        _list_tasks,\n        task_type=task_type,\n        tag=tag,\n        data_tag=data_tag,\n        status=status,\n        data_id=data_id,\n        data_name=data_name,\n        number_instances=number_instances,\n        number_features=number_features,\n        number_classes=number_classes,\n        number_missing_values=number_missing_values,\n    )\n    batches = openml.utils._list_all(listing_call, offset=offset, limit=size)\n    if len(batches) == 0:\n        return pd.DataFrame()\n\n    return pd.concat(batches)\n</code></pre>"},{"location":"reference/tasks/split/","title":"split","text":""},{"location":"reference/tasks/split/#openml.tasks.split","title":"openml.tasks.split","text":""},{"location":"reference/tasks/split/#openml.tasks.split.OpenMLSplit","title":"OpenMLSplit","text":"<pre><code>OpenMLSplit(name: int | str, description: str, split: dict[int, dict[int, dict[int, tuple[ndarray, ndarray]]]])\n</code></pre> <p>OpenML Split object.</p> <p>This class manages train-test splits for a dataset across multiple repetitions, folds, and samples.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name or ID of the split.</p> <p> TYPE: <code>int or str</code> </p> <code>description</code> <p>A description of the split.</p> <p> TYPE: <code>str</code> </p> <code>split</code> <p>A dictionary containing the splits organized by repetition, fold, and sample.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>openml/tasks/split.py</code> <pre><code>def __init__(\n    self,\n    name: int | str,\n    description: str,\n    split: dict[int, dict[int, dict[int, tuple[np.ndarray, np.ndarray]]]],\n):\n    self.description = description\n    self.name = name\n    self.split: dict[int, dict[int, dict[int, tuple[np.ndarray, np.ndarray]]]] = {}\n\n    # Add splits according to repetition\n    for repetition in split:\n        _rep = int(repetition)\n        self.split[_rep] = OrderedDict()\n        for fold in split[_rep]:\n            self.split[_rep][fold] = OrderedDict()\n            for sample in split[_rep][fold]:\n                self.split[_rep][fold][sample] = split[_rep][fold][sample]\n\n    self.repeats = len(self.split)\n\n    # TODO(eddiebergman): Better error message\n    if any(len(self.split[0]) != len(self.split[i]) for i in range(self.repeats)):\n        raise ValueError(\"\")\n\n    self.folds = len(self.split[0])\n    self.samples = len(self.split[0][0])\n</code></pre>"},{"location":"reference/tasks/split/#openml.tasks.split.OpenMLSplit.get","title":"get","text":"<pre><code>get(repeat: int = 0, fold: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns the specified data split from the CrossValidationSplit object.</p> PARAMETER DESCRIPTION <code>repeat</code> <p>Index of the repeat to retrieve.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>fold</code> <p>Index of the fold to retrieve.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sample</code> <p>Index of the sample to retrieve.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>The data split for the specified repeat, fold, and sample.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the specified repeat, fold, or sample is not known.</p> Source code in <code>openml/tasks/split.py</code> <pre><code>def get(self, repeat: int = 0, fold: int = 0, sample: int = 0) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns the specified data split from the CrossValidationSplit object.\n\n    Parameters\n    ----------\n    repeat : int\n        Index of the repeat to retrieve.\n    fold : int\n        Index of the fold to retrieve.\n    sample : int\n        Index of the sample to retrieve.\n\n    Returns\n    -------\n    numpy.ndarray\n        The data split for the specified repeat, fold, and sample.\n\n    Raises\n    ------\n    ValueError\n        If the specified repeat, fold, or sample is not known.\n    \"\"\"\n    if repeat not in self.split:\n        raise ValueError(f\"Repeat {repeat!s} not known\")\n    if fold not in self.split[repeat]:\n        raise ValueError(f\"Fold {fold!s} not known\")\n    if sample not in self.split[repeat][fold]:\n        raise ValueError(f\"Sample {sample!s} not known\")\n    return self.split[repeat][fold][sample]\n</code></pre>"},{"location":"reference/tasks/split/#openml.tasks.split.Split","title":"Split","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A single split of a dataset.</p>"},{"location":"reference/tasks/task/","title":"task","text":""},{"location":"reference/tasks/task/#openml.tasks.task","title":"openml.tasks.task","text":""},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask","title":"OpenMLClassificationTask","text":"<pre><code>OpenMLClassificationTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None, task_id: int | None = None, class_labels: list[str] | None = None, cost_matrix: ndarray | None = None)\n</code></pre> <p>               Bases: <code>OpenMLSupervisedTask</code></p> <p>OpenML Classification object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>ID of the Classification task type.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Name of the Classification task type.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the OpenML dataset associated with the Classification task.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>Name of the target variable.</p> <p> TYPE: <code>str</code> </p> <code>estimation_procedure_id</code> <p>ID of the estimation procedure for the Classification task.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the estimation procedure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Estimation parameters for the Classification task.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Name of the evaluation measure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the data splits for the Classification task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID of the Classification task (if it already exists on OpenML).</p> <p> TYPE: <code>Union[int, None]</code> DEFAULT: <code>None</code> </p> <code>class_labels</code> <p>A list of class labels (for classification tasks).</p> <p> TYPE: <code>List of str</code> DEFAULT: <code>None</code> </p> <code>cost_matrix</code> <p>A cost matrix (for classification tasks).</p> <p> TYPE: <code>array</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    class_labels: list[str] | None = None,\n    cost_matrix: np.ndarray | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n    )\n    self.class_labels = class_labels\n    self.cost_matrix = cost_matrix\n\n    if cost_matrix is not None:\n        raise NotImplementedError(\"Costmatrix\")\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>tuple - X and y</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClassificationTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask","title":"OpenMLClusteringTask","text":"<pre><code>OpenMLClusteringTask(task_type_id: TaskType, task_type: str, data_set_id: int, estimation_procedure_id: int = 17, task_id: int | None = None, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, evaluation_measure: str | None = None, target_name: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLTask</code></p> <p>OpenML Clustering object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>Task type ID of the OpenML clustering task.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Task type of the OpenML clustering task.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the OpenML dataset used in clustering the task.</p> <p> TYPE: <code>int</code> </p> <code>estimation_procedure_id</code> <p>ID of the OpenML estimation procedure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID of the OpenML clustering task.</p> <p> TYPE: <code>Union[int, None]</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the OpenML estimation procedure used in the clustering task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Parameters used by the OpenML estimation procedure.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the OpenML data splits for the clustering task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Evaluation measure used in the clustering task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>target_name</code> <p>Name of the target feature (class) that is not part of the feature set for the clustering task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    estimation_procedure_id: int = 17,\n    task_id: int | None = None,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    evaluation_measure: str | None = None,\n    target_name: str | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        evaluation_measure=evaluation_measure,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        data_splits_url=data_splits_url,\n    )\n\n    self.target_name = target_name\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.get_X","title":"get_X","text":"<pre><code>get_X() -&gt; DataFrame\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>The X data as a dataframe</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X(self) -&gt; pd.DataFrame:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    The X data as a dataframe\n    \"\"\"\n    dataset = self.get_dataset()\n    data, *_ = dataset.get_data(target=None)\n    return data\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLClusteringTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask","title":"OpenMLLearningCurveTask","text":"<pre><code>OpenMLLearningCurveTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 13, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, task_id: int | None = None, evaluation_measure: str | None = None, class_labels: list[str] | None = None, cost_matrix: ndarray | None = None)\n</code></pre> <p>               Bases: <code>OpenMLClassificationTask</code></p> <p>OpenML Learning Curve object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>ID of the Learning Curve task.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Name of the Learning Curve task.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the dataset that this task is associated with.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>Name of the target feature in the dataset.</p> <p> TYPE: <code>str</code> </p> <code>estimation_procedure_id</code> <p>ID of the estimation procedure to use for evaluating models.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the estimation procedure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Additional parameters for the estimation procedure.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the file containing the data splits for Learning Curve task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID of the Learning Curve task.</p> <p> TYPE: <code>Union[int, None]</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Name of the evaluation measure to use for evaluating models.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>class_labels</code> <p>Class labels for Learning Curve tasks.</p> <p> TYPE: <code>list of str</code> DEFAULT: <code>None</code> </p> <code>cost_matrix</code> <p>Cost matrix for Learning Curve tasks.</p> <p> TYPE: <code>numpy array</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 13,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    evaluation_measure: str | None = None,\n    class_labels: list[str] | None = None,\n    cost_matrix: np.ndarray | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n        class_labels=class_labels,\n        cost_matrix=cost_matrix,\n    )\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>tuple - X and y</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLLearningCurveTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask","title":"OpenMLRegressionTask","text":"<pre><code>OpenMLRegressionTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 7, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, data_splits_url: str | None = None, task_id: int | None = None, evaluation_measure: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLSupervisedTask</code></p> <p>OpenML Regression object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>Task type ID of the OpenML Regression task.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Task type of the OpenML Regression task.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the OpenML dataset.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>Name of the target feature used in the Regression task.</p> <p> TYPE: <code>str</code> </p> <code>estimation_procedure_id</code> <p>ID of the OpenML estimation procedure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the OpenML estimation procedure.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Parameters used by the OpenML estimation procedure.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the OpenML data splits for the Regression task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>ID of the OpenML Regression task.</p> <p> TYPE: <code>Union[int, None]</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Evaluation measure used in the Regression task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 7,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n    evaluation_measure: str | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        data_splits_url=data_splits_url,\n    )\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>tuple - X and y</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLRegressionTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask","title":"OpenMLSupervisedTask","text":"<pre><code>OpenMLSupervisedTask(task_type_id: TaskType, task_type: str, data_set_id: int, target_name: str, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None, task_id: int | None = None)\n</code></pre> <p>               Bases: <code>OpenMLTask</code>, <code>ABC</code></p> <p>OpenML Supervised Classification object.</p> PARAMETER DESCRIPTION <code>task_type_id</code> <p>ID of the task type.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Name of the task type.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>ID of the OpenML dataset associated with the task.</p> <p> TYPE: <code>int</code> </p> <code>target_name</code> <p>Name of the target feature (the class variable).</p> <p> TYPE: <code>str</code> </p> <code>estimation_procedure_id</code> <p>ID of the estimation procedure for the task.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>estimation_procedure_type</code> <p>Type of the estimation procedure for the task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Estimation parameters for the task.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Name of the evaluation measure for the task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>URL of the data splits for the task.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>task_id</code> <p>Refers to the unique identifier of task.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    target_name: str,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n    task_id: int | None = None,\n):\n    super().__init__(\n        task_id=task_id,\n        task_type_id=task_type_id,\n        task_type=task_type,\n        data_set_id=data_set_id,\n        estimation_procedure_id=estimation_procedure_id,\n        estimation_procedure_type=estimation_procedure_type,\n        estimation_parameters=estimation_parameters,\n        evaluation_measure=evaluation_measure,\n        data_splits_url=data_splits_url,\n    )\n\n    self.target_name = target_name\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.estimation_parameters","title":"estimation_parameters  <code>property</code> <code>writable</code>","text":"<pre><code>estimation_parameters: dict[str, str] | None\n</code></pre> <p>Return the estimation parameters for the task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.get_X_and_y","title":"get_X_and_y","text":"<pre><code>get_X_and_y() -&gt; tuple[DataFrame, Series | DataFrame | None]\n</code></pre> <p>Get data associated with the current task.</p> RETURNS DESCRIPTION <code>tuple - X and y</code> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_X_and_y(self) -&gt; tuple[pd.DataFrame, pd.Series | pd.DataFrame | None]:\n    \"\"\"Get data associated with the current task.\n\n    Returns\n    -------\n    tuple - X and y\n\n    \"\"\"\n    dataset = self.get_dataset()\n    if self.task_type_id not in (\n        TaskType.SUPERVISED_CLASSIFICATION,\n        TaskType.SUPERVISED_REGRESSION,\n        TaskType.LEARNING_CURVE,\n    ):\n        raise NotImplementedError(self.task_type)\n\n    X, y, _, _ = dataset.get_data(target=self.target_name)\n    return X, y\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLSupervisedTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask","title":"OpenMLTask","text":"<pre><code>OpenMLTask(task_id: int | None, task_type_id: TaskType, task_type: str, data_set_id: int, estimation_procedure_id: int = 1, estimation_procedure_type: str | None = None, estimation_parameters: dict[str, str] | None = None, evaluation_measure: str | None = None, data_splits_url: str | None = None)\n</code></pre> <p>               Bases: <code>OpenMLBase</code></p> <p>OpenML Task object.</p> PARAMETER DESCRIPTION <code>task_id</code> <p>Refers to the unique identifier of OpenML task.</p> <p> TYPE: <code>int | None</code> </p> <code>task_type_id</code> <p>Refers to the type of OpenML task.</p> <p> TYPE: <code>TaskType</code> </p> <code>task_type</code> <p>Refers to the OpenML task.</p> <p> TYPE: <code>str</code> </p> <code>data_set_id</code> <p>Refers to the data.</p> <p> TYPE: <code>int</code> </p> <code>estimation_procedure_id</code> <p>Refers to the type of estimates used.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>estimation_procedure_type</code> <p>Refers to the type of estimation procedure used for the OpenML task.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>estimation_parameters</code> <p>Estimation parameters used for the OpenML task.</p> <p> TYPE: <code>dict[str, str] | None</code> DEFAULT: <code>None</code> </p> <code>evaluation_measure</code> <p>Refers to the evaluation measure.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <code>data_splits_url</code> <p>Refers to the URL of the data splits used for the OpenML task.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> Source code in <code>openml/tasks/task.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    task_id: int | None,\n    task_type_id: TaskType,\n    task_type: str,\n    data_set_id: int,\n    estimation_procedure_id: int = 1,\n    estimation_procedure_type: str | None = None,\n    estimation_parameters: dict[str, str] | None = None,\n    evaluation_measure: str | None = None,\n    data_splits_url: str | None = None,\n):\n    self.task_id = int(task_id) if task_id is not None else None\n    self.task_type_id = task_type_id\n    self.task_type = task_type\n    self.dataset_id = int(data_set_id)\n    self.evaluation_measure = evaluation_measure\n    self.estimation_procedure: _EstimationProcedure = {\n        \"type\": estimation_procedure_type,\n        \"parameters\": estimation_parameters,\n        \"data_splits_url\": data_splits_url,\n    }\n    self.estimation_procedure_id = estimation_procedure_id\n    self.split: OpenMLSplit | None = None\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.id","title":"id  <code>property</code>","text":"<pre><code>id: int | None\n</code></pre> <p>Return the OpenML ID of this task.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.openml_url","title":"openml_url  <code>property</code>","text":"<pre><code>openml_url: str | None\n</code></pre> <p>The URL of the object on the server, if it was uploaded, else None.</p>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.download_split","title":"download_split","text":"<pre><code>download_split() -&gt; OpenMLSplit\n</code></pre> <p>Download the OpenML split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def download_split(self) -&gt; OpenMLSplit:\n    \"\"\"Download the OpenML split for a given task.\"\"\"\n    # TODO(eddiebergman): Can this every be `None`?\n    assert self.task_id is not None\n    cache_dir = _create_cache_directory_for_id(\"tasks\", self.task_id)\n    cached_split_file = cache_dir / \"datasplits.arff\"\n\n    try:\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n    except OSError:\n        # Next, download and cache the associated split file\n        self._download_split(cached_split_file)\n        split = OpenMLSplit._from_arff_file(cached_split_file)\n\n    return split\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(**kwargs: Any) -&gt; OpenMLDataset\n</code></pre> <p>Download dataset associated with task.</p> <p>Accepts the same keyword arguments as the <code>openml.datasets.get_dataset</code>.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_dataset(self, **kwargs: Any) -&gt; datasets.OpenMLDataset:\n    \"\"\"Download dataset associated with task.\n\n    Accepts the same keyword arguments as the `openml.datasets.get_dataset`.\n    \"\"\"\n    return datasets.get_dataset(self.dataset_id, **kwargs)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.get_split_dimensions","title":"get_split_dimensions","text":"<pre><code>get_split_dimensions() -&gt; tuple[int, int, int]\n</code></pre> <p>Get the (repeats, folds, samples) of the split for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_split_dimensions(self) -&gt; tuple[int, int, int]:\n    \"\"\"Get the (repeats, folds, samples) of the split for a given task.\"\"\"\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.repeats, self.split.folds, self.split.samples\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.get_train_test_split_indices","title":"get_train_test_split_indices","text":"<pre><code>get_train_test_split_indices(fold: int = 0, repeat: int = 0, sample: int = 0) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Get the indices of the train and test splits for a given task.</p> Source code in <code>openml/tasks/task.py</code> <pre><code>def get_train_test_split_indices(\n    self,\n    fold: int = 0,\n    repeat: int = 0,\n    sample: int = 0,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Get the indices of the train and test splits for a given task.\"\"\"\n    # Replace with retrieve from cache\n    if self.split is None:\n        self.split = self.download_split()\n\n    return self.split.get(repeat=repeat, fold=fold, sample=sample)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.open_in_browser","title":"open_in_browser","text":"<pre><code>open_in_browser() -&gt; None\n</code></pre> <p>Opens the OpenML web page corresponding to this object in your default browser.</p> Source code in <code>openml/base.py</code> <pre><code>def open_in_browser(self) -&gt; None:\n    \"\"\"Opens the OpenML web page corresponding to this object in your default browser.\"\"\"\n    if self.openml_url is None:\n        raise ValueError(\n            \"Cannot open element on OpenML.org when attribute `openml_url` is `None`\",\n        )\n\n    webbrowser.open(self.openml_url)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.publish","title":"publish","text":"<pre><code>publish() -&gt; OpenMLBase\n</code></pre> <p>Publish the object on the OpenML server.</p> Source code in <code>openml/base.py</code> <pre><code>def publish(self) -&gt; OpenMLBase:\n    \"\"\"Publish the object on the OpenML server.\"\"\"\n    file_elements = self._get_file_elements()\n\n    if \"description\" not in file_elements:\n        file_elements[\"description\"] = self._to_xml()\n\n    call = f\"{_get_rest_api_type_alias(self)}/\"\n    response_text = openml._api_calls._perform_api_call(\n        call,\n        \"post\",\n        file_elements=file_elements,\n    )\n    xml_response = xmltodict.parse(response_text)\n\n    self._parse_publish_response(xml_response)\n    return self\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.push_tag","title":"push_tag","text":"<pre><code>push_tag(tag: str) -&gt; None\n</code></pre> <p>Annotates this entity with a tag on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def push_tag(self, tag: str) -&gt; None:\n    \"\"\"Annotates this entity with a tag on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.remove_tag","title":"remove_tag","text":"<pre><code>remove_tag(tag: str) -&gt; None\n</code></pre> <p>Removes a tag from this entity on the server.</p> PARAMETER DESCRIPTION <code>tag</code> <p>Tag to attach to the flow.</p> <p> TYPE: <code>str</code> </p> Source code in <code>openml/base.py</code> <pre><code>def remove_tag(self, tag: str) -&gt; None:\n    \"\"\"Removes a tag from this entity on the server.\n\n    Parameters\n    ----------\n    tag : str\n        Tag to attach to the flow.\n    \"\"\"\n    _tag_openml_base(self, tag, untag=True)\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.OpenMLTask.url_for_id","title":"url_for_id  <code>classmethod</code>","text":"<pre><code>url_for_id(id_: int) -&gt; str\n</code></pre> <p>Return the OpenML URL for the object of the class entity with the given id.</p> Source code in <code>openml/base.py</code> <pre><code>@classmethod\ndef url_for_id(cls, id_: int) -&gt; str:\n    \"\"\"Return the OpenML URL for the object of the class entity with the given id.\"\"\"\n    # Sample url for a flow: openml.org/f/123\n    return f\"{openml.config.get_server_base_url()}/{cls._entity_letter()}/{id_}\"\n</code></pre>"},{"location":"reference/tasks/task/#openml.tasks.task.TaskType","title":"TaskType","text":"<p>               Bases: <code>Enum</code></p> <p>Possible task types as defined in OpenML.</p>"}]}